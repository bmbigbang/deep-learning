{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram addr2vec\n",
    "\n",
    "In this notebook, I'll convert addresses to vectors through TensorFlow and understand the ability to correct for mistyping.\n",
    "\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "When you're dealing with words in text, you end up with tens of thousands of classes to predict, one for each word. Trying to one-hot encode these words is massively inefficient, you'll have one element set to 1 and the other 50,000 set to 0. The matrix multiplication going into the first hidden layer will have almost all of the resulting values be zero. This a huge waste of computation. \n",
    "\n",
    "![one-hot encodings](assets/one_hot_encoding.png)\n",
    "\n",
    "To solve this problem and greatly increase the efficiency of our networks, we use what are called embeddings. Embeddings are just a fully connected layer like you've seen before. We call this layer the embedding layer and the weights are embedding weights. We skip the multiplication into the embedding layer by instead directly grabbing the hidden layer values from the weight matrix. We can do this because the multiplication of a one-hot encoded vector with a matrix returns the row of the matrix corresponding the index of the \"on\" input unit.\n",
    "\n",
    "![lookup](assets/lookup_matrix.png)\n",
    "\n",
    "Instead of doing the matrix multiplication, we use the weight matrix as a lookup table. We encode the words as integers, for example \"heart\" is encoded as 958, \"mind\" as 18094. Then to get hidden layer values for \"heart\", you just take the 958th row of the embedding matrix. This process is called an **embedding lookup** and the number of hidden units is the **embedding dimension**.\n",
    "\n",
    "<img src='assets/tokenize_lookup.png' width=500>\n",
    " \n",
    "There is nothing magical going on here. The embedding lookup table is just a weight matrix. The embedding layer is just a hidden layer. The lookup is just a shortcut for the matrix multiplication. The lookup table is trained just like any weight matrix as well.\n",
    "\n",
    "Embeddings aren't only used for words of course. You can use them for any model where you have a massive number of classes. A particular type of model called **Word2Vec** uses the embedding layer to find vector representations of words that contain semantic meaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [openaddress US North East dataset](https://s3.amazonaws.com/data.openaddresses.io/openaddr-collected-us_northeast.zip), and extract onto 'openaddr' directory if not found. read the csv files and load address dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names available: Index(['LON', 'LAT', 'NUMBER', 'STREET', 'UNIT', 'CITY', 'DISTRICT', 'REGION',\n",
      "       'POSTCODE', 'ID', 'HASH'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (3,4,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "dataset_folder_path = 'openaddr'\n",
    "dataset_filename = 'openaddr-collected-us_northeast.zip'\n",
    "dataset_name = 'Openaddress Dataset'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(dataset_filename):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/data.openaddresses.io/openaddr-collected-us_northeast.zip',\n",
    "            dataset_filename,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with zipfile.ZipFile(dataset_filename) as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder_path)\n",
    "\n",
    "\n",
    "id_to_address = {}\n",
    "address_to_id = {}\n",
    "i = 0\n",
    "for state in os.listdir('./openaddr/us'):\n",
    "    \n",
    "    for filename in glob.glob('./openaddr/us/{}/*.csv'.format(state)):\n",
    "        if i == 0:\n",
    "            print(\"Column names available: {}\".format(csv.columns))\n",
    "        csv = pd.read_csv(filename)\n",
    "        stack = np.stack((csv['STREET'], csv['UNIT'], \n",
    "                          csv['CITY'], csv['DISTRICT'], csv['REGION'], ), axis=-1)\n",
    "        for j in stack:\n",
    "            addr = \" \".join([str(k).lower()\n",
    "                             for k in j if not isinstance(k, type(np.nan))])\n",
    "            addr +=  ' ' + state.lower()\n",
    "            if addr not in address_to_id:\n",
    "                id_to_address[i] = addr\n",
    "                address_to_id[addr] = i\n",
    "                i += 1\n",
    "            \n",
    "if '' in address_to_id:\n",
    "    i = address_to_id['']\n",
    "    del address_to_id['']\n",
    "    del id_to_address[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here I'm fixing up the text to make training easier. This comes from the `utils` module I wrote. The `preprocess` function coverts any punctuation into tokens, so a period is changed to ` <PERIOD> `. In this data set, there aren't any periods, but it will help in other NLP problems. I'm also removing all words that show up five or fewer times in the dataset. This will greatly reduce issues due to noise in the data and improve the quality of the vector representations. If you want to write your own functions for this stuff, go for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e pond rd somerset me me', 'quaker ln somerset me me', 'templinville ln somerset me me', 'redmond ln somerset me me', 'rockwood ln somerset me me', 'rocky ridge ln somerset me me', 'rome rd somerset me me', 'rose ln somerset me me', 'rosenthal ln somerset me me', 'ross hill rd somerset me me', 'sand hill rd somerset me me', 'smithfield rd somerset me me', 'stevens rd somerset me me', 'sunset meadow ln somerset me me', 'thomas ln somerset me me', 'valley ln somerset me me', 'village rd somerset me me', 'thompson rd york me me', 'timber ridge dr york me me', 'trout brook rd york me me', 'tuckers way york me me', 'walkers ln york me me', 'walkers ln a york me me', 'welch ln york me me', 'west ln york me me', 'windward ln york me me', 'abby ln york me me', 'adams st york me me', 'alden dr york me me', 'alexander dr york me me']\n"
     ]
    }
   ],
   "source": [
    "print([i for i in address_to_id.keys()][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total addresses: 2329546\n",
      "Total unique words: 29696\n"
     ]
    }
   ],
   "source": [
    "print(\"Total addresses: {}\".format(len(address_to_id)))\n",
    "\n",
    "vocab_to_id = {}\n",
    "int_to_vocab = {}\n",
    "idx = 0\n",
    "for i, address in id_to_address.items():\n",
    "    for j in range(len(address) - 2):\n",
    "        if address[j:j+3] not in vocab_to_id:\n",
    "            vocab_to_id[address[j:j+3]] = idx\n",
    "            int_to_vocab[idx] = address[j:j+3]\n",
    "            idx += 1\n",
    "\n",
    "vocab_size = len(vocab_to_id)\n",
    "print(\"Total unique words: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "for i, address in id_to_address.items():\n",
    "    X = np.zeros(vocab_size)\n",
    "    for j in range(len(address) - 2):\n",
    "        X[vocab_to_id[address[j:j+3]]] = 1.0\n",
    "    features.append(X)\n",
    "    labels.append(address)\n",
    "    \n",
    "print(\"Example feature vector and label:\")\n",
    "print(features[500])\n",
    "print(labels[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of the encoding layer (the hidden layer)\n",
    "encoding_dim = 32\n",
    "\n",
    "# Input and target placeholders\n",
    "inp_shape = vocab_size\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32 ,(None, inp_shape), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32 ,(None, inp_shape), name='targets')\n",
    "\n",
    "# Output of hidden layer, single fully connected layer here with ReLU activation\n",
    "encoded = tf.layers.dense(inputs_, encoding_dim, activation=tf.nn.relu)\n",
    "\n",
    "# Output layer logits, fully connected layer with no activation\n",
    "logits = tf.layers.dense(encoded, inp_shape, activation=None)\n",
    "# Sigmoid output from logits\n",
    "decoded = tf.nn.sigmoid(logits, name='outputs')\n",
    "\n",
    "# Sigmoid cross-entropy loss\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "# Mean of the loss\n",
    "cost = tf.reduce_mean(loss)\n",
    "\n",
    "# Adam optimizer\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
