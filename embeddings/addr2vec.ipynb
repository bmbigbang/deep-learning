{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram addr2vec\n",
    "\n",
    "In this notebook, I'll convert addresses to vectors through TensorFlow and understand the ability to correct for mistyping.\n",
    "\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "When you're dealing with words in text, you end up with tens of thousands of classes to predict, one for each word. Trying to one-hot encode these words is massively inefficient, you'll have one element set to 1 and the other 50,000 set to 0. The matrix multiplication going into the first hidden layer will have almost all of the resulting values be zero. This a huge waste of computation. \n",
    "\n",
    "![one-hot encodings](assets/one_hot_encoding.png)\n",
    "\n",
    "To solve this problem and greatly increase the efficiency of our networks, we use what are called embeddings. Embeddings are just a fully connected layer like you've seen before. We call this layer the embedding layer and the weights are embedding weights. We skip the multiplication into the embedding layer by instead directly grabbing the hidden layer values from the weight matrix. We can do this because the multiplication of a one-hot encoded vector with a matrix returns the row of the matrix corresponding the index of the \"on\" input unit.\n",
    "\n",
    "![lookup](assets/lookup_matrix.png)\n",
    "\n",
    "Instead of doing the matrix multiplication, we use the weight matrix as a lookup table. We encode the words as integers, for example \"heart\" is encoded as 958, \"mind\" as 18094. Then to get hidden layer values for \"heart\", you just take the 958th row of the embedding matrix. This process is called an **embedding lookup** and the number of hidden units is the **embedding dimension**.\n",
    "\n",
    "<img src='assets/tokenize_lookup.png' width=500>\n",
    " \n",
    "There is nothing magical going on here. The embedding lookup table is just a weight matrix. The embedding layer is just a hidden layer. The lookup is just a shortcut for the matrix multiplication. The lookup table is trained just like any weight matrix as well.\n",
    "\n",
    "Embeddings aren't only used for words of course. You can use them for any model where you have a massive number of classes. A particular type of model called **Word2Vec** uses the embedding layer to find vector representations of words that contain semantic meaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [openaddress US North East dataset](https://s3.amazonaws.com/data.openaddresses.io/openaddr-collected-us_northeast.zip), and extract onto 'openaddr' directory if not found. read the csv files and load address dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names available: Index(['LON', 'LAT', 'NUMBER', 'STREET', 'UNIT', 'CITY', 'DISTRICT', 'REGION',\n",
      "       'POSTCODE', 'ID', 'HASH'],\n",
      "      dtype='object')\n",
      "Example data: LON                       -78.5994\n",
      "LAT                        42.6937\n",
      "NUMBER                        2393\n",
      "STREET                    LEWIS RD\n",
      "UNIT                           NaN\n",
      "CITY                        Aurora\n",
      "DISTRICT                       NaN\n",
      "REGION                         NaN\n",
      "POSTCODE                14139 9710\n",
      "ID          1424892010000003022000\n",
      "HASH              d6ba36e791e84371\n",
      "Name: 1, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (3,4,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "dataset_folder_path = 'openaddr'\n",
    "dataset_filename = 'openaddr-collected-us_northeast.zip'\n",
    "dataset_name = 'Openaddress Dataset'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(dataset_filename):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/data.openaddresses.io/openaddr-collected-us_northeast.zip',\n",
    "            dataset_filename,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with zipfile.ZipFile(dataset_filename) as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder_path)\n",
    "\n",
    "\n",
    "id_to_address = {}\n",
    "address_to_id = {}\n",
    "i = 0\n",
    "for state in os.listdir('./openaddr/us'):\n",
    "    \n",
    "    for filename in glob.glob('./openaddr/us/{}/*.csv'.format(state)):\n",
    "        csv = pd.read_csv(filename)\n",
    "        if i == 0:\n",
    "            print(\"Column names available: {}\".format(csv.columns))\n",
    "            print(\"Example data: {}\".format(csv.iloc[1]))\n",
    "\n",
    "        stack = np.stack((csv['CITY'],), axis=-1)\n",
    "        for j in stack:\n",
    "            addr = \" \".join([str(k).lower()\n",
    "                             for k in j if not isinstance(k, type(np.nan))])\n",
    "            if addr not in address_to_id and addr != '' and addr != ' ':\n",
    "                id_to_address[i] = addr\n",
    "                address_to_id[addr] = i\n",
    "                i += 1\n",
    "\n",
    "del csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here I'm fixing up the text to make training easier. This comes from the `utils` module I wrote. The `preprocess` function coverts any punctuation into tokens, so a period is changed to ` <PERIOD> `. In this data set, there aren't any periods, but it will help in other NLP problems. I'm also removing all words that show up five or fewer times in the dataset. This will greatly reduce issues due to noise in the data and improve the quality of the vector representations. If you want to write your own functions for this stuff, go for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aurora', 'wales', 'orchard park', 'holland', 'boston', 'colden', 'concord', 'collins', 'north collins', 'sardinia', 'brant', 'evans', 'eden', 'hamburg', 'lackawanna', 'west seneca', 'elma', 'marilla', 'cheektowaga', 'lancaster', 'alden', 'grand island', 'amherst', 'clarence', 'newstead', 'tonawanda', 'buffalo', 'goshen', 'monroe', 'washingtonville']\n"
     ]
    }
   ],
   "source": [
    "print([i for i in address_to_id.keys()][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total addresses: 4155\n",
      "Total unique letters: 45\n"
     ]
    }
   ],
   "source": [
    "print(\"Total addresses: {}\".format(len(address_to_id)))\n",
    "\n",
    "vocab_to_id = {}\n",
    "int_to_vocab = {}\n",
    "idx = 0\n",
    "for address in address_to_id.keys():\n",
    "    for j in range(len(address)):\n",
    "        if address[j] not in vocab_to_id:\n",
    "            vocab_to_id[address[j]] = idx\n",
    "            int_to_vocab[idx] = address[j]\n",
    "            idx += 1\n",
    "\n",
    "vocab_size = len(vocab_to_id)\n",
    "print(\"Total unique letters: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example feature vector and label:\n",
      "[1.000e-03 1.000e-03 5.010e-01 5.010e-01 1.000e-03 5.010e-01 1.001e+00\n",
      " 1.000e-03 5.010e-01 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03\n",
      " 5.010e-01 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03\n",
      " 5.010e-01 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03\n",
      " 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03\n",
      " 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03\n",
      " 1.000e-03 1.000e-03 1.000e-03]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "for address, idx in address_to_id.items():\n",
    "    X = np.zeros(vocab_size)\n",
    "    Y = np.zeros(len(address_to_id))\n",
    "    for j in range(len(address)):\n",
    "        X[vocab_to_id[address[j]]] += 1 \n",
    "    \n",
    "    # normalize input sizes \n",
    "    X = ((X - X.min()) / (X.max() - X.min())) + 0.001\n",
    "    \n",
    "    features.append(X)\n",
    "    \n",
    "    Y[idx] = 1\n",
    "    labels.append(Y)\n",
    "    \n",
    "print(\"Example feature vector and label:\")\n",
    "print(features[500])\n",
    "print(labels[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the encoding layer (the hidden layer)\n",
    "encoding_dim = 200\n",
    "alpha = 0.1\n",
    "\n",
    "# Input and target placeholders\n",
    "inp_shape = vocab_size\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32 ,(None, inp_shape), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32 ,(None, len(address_to_id)), name='targets')\n",
    "\n",
    "# Input layer is 45\n",
    "x1 = tf.expand_dims(inputs_, axis=1)\n",
    "x1 = tf.layers.conv1d(x1, filters=64, kernel_size=1, strides=1, padding='valid')\n",
    "x1 = tf.nn.dropout(x1, 0.5)\n",
    "\n",
    "relu1 = tf.maximum(alpha * x1, x1)\n",
    "# 41x64\n",
    "\n",
    "x2 = tf.layers.conv1d(relu1, filters=128, kernel_size=1, strides=1, padding='valid')\n",
    "x2 = tf.nn.dropout(x2, 0.5)\n",
    "relu2 = tf.maximum(alpha * x2, x2)\n",
    "# 37x128\n",
    "\n",
    "x3 = tf.layers.conv1d(relu2, filters=256, kernel_size=1, strides=1, padding='valid')\n",
    "x3 = tf.nn.dropout(x3, 0.5)\n",
    "relu3 = tf.maximum(alpha * x3, x3)\n",
    "# 33x256\n",
    "\n",
    "first_h_dim = tf.layers.dense(relu3, 200)\n",
    "relu3 = tf.maximum(alpha * first_h_dim, first_h_dim)\n",
    "encoded = tf.layers.dense(relu3, encoding_dim)\n",
    "\n",
    "# Output layer logits\n",
    "logits = tf.layers.dense(encoded, len(address_to_id), activation=None, name='outputs')\n",
    "# Sigmoid output from logits\n",
    "#decoded = tf.nn.sigmoid(logits, name='outputs')\n",
    "\n",
    "# loss = tf.log(tf.reduce_sum(tf.abs(targets_ - logits)) + 1)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "# Mean of the loss\n",
    "cost = tf.reduce_mean(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000... Training loss: 8.3289\n",
      "Epoch: 1/2000... Training loss: 8.3322\n",
      "Epoch: 1/2000... Training loss: 8.3343\n",
      "Epoch: 1/2000... Training loss: 8.3421\n",
      "Epoch: 1/2000... Training loss: 8.3346\n",
      "Epoch: 1/2000... Training loss: 8.3396\n",
      "Epoch: 1/2000... Training loss: 8.3348\n",
      "Epoch: 1/2000... Training loss: 8.3435\n",
      "Epoch: 1/2000... Training loss: 8.3437\n",
      "Epoch: 1/2000... Training loss: 8.3407\n",
      "Epoch: 1/2000... Training loss: 8.3367\n",
      "Epoch: 1/2000... Training loss: 8.3413\n",
      "Epoch: 1/2000... Training loss: 8.3525\n",
      "Epoch: 1/2000... Training loss: 8.3484\n",
      "Epoch: 1/2000... Training loss: 8.3520\n",
      "Epoch: 1/2000... Training loss: 8.3542\n",
      "Epoch: 1/2000... Training loss: 8.3533\n",
      "Epoch: 1/2000... Training loss: 8.3561\n",
      "Epoch: 1/2000... Training loss: 8.3507\n",
      "Epoch: 1/2000... Training loss: 8.3539\n",
      "Epoch: 1/2000... Training loss: 8.3552\n",
      "Epoch: 1/2000... Training loss: 8.3506\n",
      "Epoch: 1/2000... Training loss: 8.3508\n",
      "Epoch: 1/2000... Training loss: 8.3510\n",
      "Epoch: 1/2000... Training loss: 8.3469\n",
      "Epoch: 1/2000... Training loss: 8.3447\n",
      "Epoch: 1/2000... Training loss: 8.3395\n",
      "Epoch: 1/2000... Training loss: 8.3393\n",
      "Epoch: 1/2000... Training loss: 8.3380\n",
      "Epoch: 1/2000... Training loss: 8.3381\n",
      "Epoch: 1/2000... Training loss: 8.3343\n",
      "Epoch: 2/2000... Training loss: 8.3501\n",
      "Epoch: 2/2000... Training loss: 8.3518\n",
      "Epoch: 2/2000... Training loss: 8.3491\n",
      "Epoch: 2/2000... Training loss: 8.3525\n",
      "Epoch: 2/2000... Training loss: 8.3494\n",
      "Epoch: 2/2000... Training loss: 8.3497\n",
      "Epoch: 2/2000... Training loss: 8.3447\n",
      "Epoch: 2/2000... Training loss: 8.3402\n",
      "Epoch: 2/2000... Training loss: 8.3397\n",
      "Epoch: 2/2000... Training loss: 8.3395\n",
      "Epoch: 2/2000... Training loss: 8.3391\n",
      "Epoch: 2/2000... Training loss: 8.3387\n",
      "Epoch: 2/2000... Training loss: 8.3346\n",
      "Epoch: 2/2000... Training loss: 8.3323\n",
      "Epoch: 2/2000... Training loss: 8.3305\n",
      "Epoch: 2/2000... Training loss: 8.3328\n",
      "Epoch: 2/2000... Training loss: 8.3327\n",
      "Epoch: 2/2000... Training loss: 8.3254\n",
      "Epoch: 2/2000... Training loss: 8.3319\n",
      "Epoch: 2/2000... Training loss: 8.3327\n",
      "Epoch: 2/2000... Training loss: 8.3314\n",
      "Epoch: 2/2000... Training loss: 8.3296\n",
      "Epoch: 2/2000... Training loss: 8.3312\n",
      "Epoch: 2/2000... Training loss: 8.3365\n",
      "Epoch: 2/2000... Training loss: 8.3331\n",
      "Epoch: 2/2000... Training loss: 8.3353\n",
      "Epoch: 2/2000... Training loss: 8.3354\n",
      "Epoch: 2/2000... Training loss: 8.3325\n",
      "Epoch: 2/2000... Training loss: 8.3335\n",
      "Epoch: 2/2000... Training loss: 8.3333\n",
      "Epoch: 2/2000... Training loss: 8.3340\n",
      "Epoch: 3/2000... Training loss: 8.3299\n",
      "Epoch: 3/2000... Training loss: 8.3319\n",
      "Epoch: 3/2000... Training loss: 8.3317\n",
      "Epoch: 3/2000... Training loss: 8.3296\n",
      "Epoch: 3/2000... Training loss: 8.3299\n",
      "Epoch: 3/2000... Training loss: 8.3309\n",
      "Epoch: 3/2000... Training loss: 8.3341\n",
      "Epoch: 3/2000... Training loss: 8.3333\n",
      "Epoch: 3/2000... Training loss: 8.3309\n",
      "Epoch: 3/2000... Training loss: 8.3295\n",
      "Epoch: 3/2000... Training loss: 8.3250\n",
      "Epoch: 3/2000... Training loss: 8.3267\n",
      "Epoch: 3/2000... Training loss: 8.3260\n",
      "Epoch: 3/2000... Training loss: 8.3306\n",
      "Epoch: 3/2000... Training loss: 8.3277\n",
      "Epoch: 3/2000... Training loss: 8.3285\n",
      "Epoch: 3/2000... Training loss: 8.3311\n",
      "Epoch: 3/2000... Training loss: 8.3292\n",
      "Epoch: 3/2000... Training loss: 8.3238\n",
      "Epoch: 3/2000... Training loss: 8.3275\n",
      "Epoch: 3/2000... Training loss: 8.3303\n",
      "Epoch: 3/2000... Training loss: 8.3290\n",
      "Epoch: 3/2000... Training loss: 8.3249\n",
      "Epoch: 3/2000... Training loss: 8.3316\n",
      "Epoch: 3/2000... Training loss: 8.3287\n",
      "Epoch: 3/2000... Training loss: 8.3301\n",
      "Epoch: 3/2000... Training loss: 8.3294\n",
      "Epoch: 3/2000... Training loss: 8.3326\n",
      "Epoch: 3/2000... Training loss: 8.3288\n",
      "Epoch: 3/2000... Training loss: 8.3309\n",
      "Epoch: 3/2000... Training loss: 8.3304\n",
      "Epoch: 4/2000... Training loss: 8.3245\n",
      "Epoch: 4/2000... Training loss: 8.3271\n",
      "Epoch: 4/2000... Training loss: 8.3227\n",
      "Epoch: 4/2000... Training loss: 8.3261\n",
      "Epoch: 4/2000... Training loss: 8.3237\n",
      "Epoch: 4/2000... Training loss: 8.3275\n",
      "Epoch: 4/2000... Training loss: 8.3257\n",
      "Epoch: 4/2000... Training loss: 8.3244\n",
      "Epoch: 4/2000... Training loss: 8.3233\n",
      "Epoch: 4/2000... Training loss: 8.3210\n",
      "Epoch: 4/2000... Training loss: 8.3181\n",
      "Epoch: 4/2000... Training loss: 8.3254\n",
      "Epoch: 4/2000... Training loss: 8.3226\n",
      "Epoch: 4/2000... Training loss: 8.3280\n",
      "Epoch: 4/2000... Training loss: 8.3228\n",
      "Epoch: 4/2000... Training loss: 8.3204\n",
      "Epoch: 4/2000... Training loss: 8.3262\n",
      "Epoch: 4/2000... Training loss: 8.3176\n",
      "Epoch: 4/2000... Training loss: 8.3120\n",
      "Epoch: 4/2000... Training loss: 8.3204\n",
      "Epoch: 4/2000... Training loss: 8.3211\n",
      "Epoch: 4/2000... Training loss: 8.3134\n",
      "Epoch: 4/2000... Training loss: 8.3143\n",
      "Epoch: 4/2000... Training loss: 8.3239\n",
      "Epoch: 4/2000... Training loss: 8.3215\n",
      "Epoch: 4/2000... Training loss: 8.3280\n",
      "Epoch: 4/2000... Training loss: 8.3239\n",
      "Epoch: 4/2000... Training loss: 8.3265\n",
      "Epoch: 4/2000... Training loss: 8.3180\n",
      "Epoch: 4/2000... Training loss: 8.3309\n",
      "Epoch: 4/2000... Training loss: 8.3265\n",
      "Epoch: 5/2000... Training loss: 8.3223\n",
      "Epoch: 5/2000... Training loss: 8.3228\n",
      "Epoch: 5/2000... Training loss: 8.3225\n",
      "Epoch: 5/2000... Training loss: 8.3214\n",
      "Epoch: 5/2000... Training loss: 8.3208\n",
      "Epoch: 5/2000... Training loss: 8.3208\n",
      "Epoch: 5/2000... Training loss: 8.3184\n",
      "Epoch: 5/2000... Training loss: 8.3104\n",
      "Epoch: 5/2000... Training loss: 8.3161\n",
      "Epoch: 5/2000... Training loss: 8.3013\n",
      "Epoch: 5/2000... Training loss: 8.2967\n",
      "Epoch: 5/2000... Training loss: 8.3123\n",
      "Epoch: 5/2000... Training loss: 8.3207\n",
      "Epoch: 5/2000... Training loss: 8.3212\n",
      "Epoch: 5/2000... Training loss: 8.3097\n",
      "Epoch: 5/2000... Training loss: 8.3148\n",
      "Epoch: 5/2000... Training loss: 8.3244\n",
      "Epoch: 5/2000... Training loss: 8.3094\n",
      "Epoch: 5/2000... Training loss: 8.2837\n",
      "Epoch: 5/2000... Training loss: 8.3041\n",
      "Epoch: 5/2000... Training loss: 8.3089\n",
      "Epoch: 5/2000... Training loss: 8.2920\n",
      "Epoch: 5/2000... Training loss: 8.2874\n",
      "Epoch: 5/2000... Training loss: 8.3278\n",
      "Epoch: 5/2000... Training loss: 8.3086\n",
      "Epoch: 5/2000... Training loss: 8.3187\n",
      "Epoch: 5/2000... Training loss: 8.3097\n",
      "Epoch: 5/2000... Training loss: 8.3151\n",
      "Epoch: 5/2000... Training loss: 8.3038\n",
      "Epoch: 5/2000... Training loss: 8.3118\n",
      "Epoch: 5/2000... Training loss: 8.3141\n",
      "Epoch: 6/2000... Training loss: 8.3070\n",
      "Epoch: 6/2000... Training loss: 8.3133\n",
      "Epoch: 6/2000... Training loss: 8.3117\n",
      "Epoch: 6/2000... Training loss: 8.3105\n",
      "Epoch: 6/2000... Training loss: 8.3027\n",
      "Epoch: 6/2000... Training loss: 8.3099\n",
      "Epoch: 6/2000... Training loss: 8.3024\n",
      "Epoch: 6/2000... Training loss: 8.3065\n",
      "Epoch: 6/2000... Training loss: 8.2976\n",
      "Epoch: 6/2000... Training loss: 8.2812\n",
      "Epoch: 6/2000... Training loss: 8.2762\n",
      "Epoch: 6/2000... Training loss: 8.3086\n",
      "Epoch: 6/2000... Training loss: 8.3081\n",
      "Epoch: 6/2000... Training loss: 8.3123\n",
      "Epoch: 6/2000... Training loss: 8.3042\n",
      "Epoch: 6/2000... Training loss: 8.3062\n",
      "Epoch: 6/2000... Training loss: 8.3130\n",
      "Epoch: 6/2000... Training loss: 8.2793\n",
      "Epoch: 6/2000... Training loss: 8.2526\n",
      "Epoch: 6/2000... Training loss: 8.2643\n",
      "Epoch: 6/2000... Training loss: 8.2830\n",
      "Epoch: 6/2000... Training loss: 8.2497\n",
      "Epoch: 6/2000... Training loss: 8.2571\n",
      "Epoch: 6/2000... Training loss: 8.3176\n",
      "Epoch: 6/2000... Training loss: 8.2815\n",
      "Epoch: 6/2000... Training loss: 8.3002\n",
      "Epoch: 6/2000... Training loss: 8.2877\n",
      "Epoch: 6/2000... Training loss: 8.2950\n",
      "Epoch: 6/2000... Training loss: 8.2524\n",
      "Epoch: 6/2000... Training loss: 8.2856\n",
      "Epoch: 6/2000... Training loss: 8.2827\n",
      "Epoch: 7/2000... Training loss: 8.2757\n",
      "Epoch: 7/2000... Training loss: 8.2775\n",
      "Epoch: 7/2000... Training loss: 8.2957\n",
      "Epoch: 7/2000... Training loss: 8.2949\n",
      "Epoch: 7/2000... Training loss: 8.2719\n",
      "Epoch: 7/2000... Training loss: 8.2775\n",
      "Epoch: 7/2000... Training loss: 8.2790\n",
      "Epoch: 7/2000... Training loss: 8.2814\n",
      "Epoch: 7/2000... Training loss: 8.2646\n",
      "Epoch: 7/2000... Training loss: 8.2524\n",
      "Epoch: 7/2000... Training loss: 8.2158\n",
      "Epoch: 7/2000... Training loss: 8.2734\n",
      "Epoch: 7/2000... Training loss: 8.2997\n",
      "Epoch: 7/2000... Training loss: 8.2856\n",
      "Epoch: 7/2000... Training loss: 8.2837\n",
      "Epoch: 7/2000... Training loss: 8.2852\n",
      "Epoch: 7/2000... Training loss: 8.3100\n",
      "Epoch: 7/2000... Training loss: 8.2545\n",
      "Epoch: 7/2000... Training loss: 8.2182\n",
      "Epoch: 7/2000... Training loss: 8.2277\n",
      "Epoch: 7/2000... Training loss: 8.2567\n",
      "Epoch: 7/2000... Training loss: 8.1792\n",
      "Epoch: 7/2000... Training loss: 8.1880\n",
      "Epoch: 7/2000... Training loss: 8.2984\n",
      "Epoch: 7/2000... Training loss: 8.2530\n",
      "Epoch: 7/2000... Training loss: 8.2562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/2000... Training loss: 8.2658\n",
      "Epoch: 7/2000... Training loss: 8.2407\n",
      "Epoch: 7/2000... Training loss: 8.2031\n",
      "Epoch: 7/2000... Training loss: 8.2225\n",
      "Epoch: 7/2000... Training loss: 8.2150\n",
      "Epoch: 8/2000... Training loss: 8.2150\n",
      "Epoch: 8/2000... Training loss: 8.2197\n",
      "Epoch: 8/2000... Training loss: 8.2380\n",
      "Epoch: 8/2000... Training loss: 8.2271\n",
      "Epoch: 8/2000... Training loss: 8.2029\n",
      "Epoch: 8/2000... Training loss: 8.2145\n",
      "Epoch: 8/2000... Training loss: 8.2029\n",
      "Epoch: 8/2000... Training loss: 8.2105\n",
      "Epoch: 8/2000... Training loss: 8.2018\n",
      "Epoch: 8/2000... Training loss: 8.1402\n",
      "Epoch: 8/2000... Training loss: 8.0856\n",
      "Epoch: 8/2000... Training loss: 8.1832\n",
      "Epoch: 8/2000... Training loss: 8.2264\n",
      "Epoch: 8/2000... Training loss: 8.2322\n",
      "Epoch: 8/2000... Training loss: 8.2161\n",
      "Epoch: 8/2000... Training loss: 8.2163\n",
      "Epoch: 8/2000... Training loss: 8.2052\n",
      "Epoch: 8/2000... Training loss: 8.1549\n",
      "Epoch: 8/2000... Training loss: 8.0876\n",
      "Epoch: 8/2000... Training loss: 8.0671\n",
      "Epoch: 8/2000... Training loss: 8.1574\n",
      "Epoch: 8/2000... Training loss: 8.0637\n",
      "Epoch: 8/2000... Training loss: 8.0785\n",
      "Epoch: 8/2000... Training loss: 8.1749\n",
      "Epoch: 8/2000... Training loss: 8.1478\n",
      "Epoch: 8/2000... Training loss: 8.1877\n",
      "Epoch: 8/2000... Training loss: 8.1393\n",
      "Epoch: 8/2000... Training loss: 8.1438\n",
      "Epoch: 8/2000... Training loss: 8.0123\n",
      "Epoch: 8/2000... Training loss: 8.0748\n",
      "Epoch: 8/2000... Training loss: 8.0880\n",
      "Epoch: 9/2000... Training loss: 8.0668\n",
      "Epoch: 9/2000... Training loss: 8.0533\n",
      "Epoch: 9/2000... Training loss: 8.0811\n",
      "Epoch: 9/2000... Training loss: 8.0458\n",
      "Epoch: 9/2000... Training loss: 8.0192\n",
      "Epoch: 9/2000... Training loss: 7.9995\n",
      "Epoch: 9/2000... Training loss: 7.9757\n",
      "Epoch: 9/2000... Training loss: 8.0108\n",
      "Epoch: 9/2000... Training loss: 7.9482\n",
      "Epoch: 9/2000... Training loss: 7.8849\n",
      "Epoch: 9/2000... Training loss: 7.7742\n",
      "Epoch: 9/2000... Training loss: 7.9972\n",
      "Epoch: 9/2000... Training loss: 7.9328\n",
      "Epoch: 9/2000... Training loss: 8.0153\n",
      "Epoch: 9/2000... Training loss: 7.9304\n",
      "Epoch: 9/2000... Training loss: 7.9870\n",
      "Epoch: 9/2000... Training loss: 7.8296\n",
      "Epoch: 9/2000... Training loss: 7.8999\n",
      "Epoch: 9/2000... Training loss: 7.8481\n",
      "Epoch: 9/2000... Training loss: 7.8043\n",
      "Epoch: 9/2000... Training loss: 7.8481\n",
      "Epoch: 9/2000... Training loss: 7.9149\n",
      "Epoch: 9/2000... Training loss: 7.7570\n",
      "Epoch: 9/2000... Training loss: 7.9002\n",
      "Epoch: 9/2000... Training loss: 7.8656\n",
      "Epoch: 9/2000... Training loss: 7.8767\n",
      "Epoch: 9/2000... Training loss: 7.9804\n",
      "Epoch: 9/2000... Training loss: 7.8931\n",
      "Epoch: 9/2000... Training loss: 7.8279\n",
      "Epoch: 9/2000... Training loss: 7.8322\n",
      "Epoch: 9/2000... Training loss: 7.7873\n",
      "Epoch: 10/2000... Training loss: 7.8970\n",
      "Epoch: 10/2000... Training loss: 7.8167\n",
      "Epoch: 10/2000... Training loss: 7.8438\n",
      "Epoch: 10/2000... Training loss: 7.8001\n",
      "Epoch: 10/2000... Training loss: 7.7461\n",
      "Epoch: 10/2000... Training loss: 7.6876\n",
      "Epoch: 10/2000... Training loss: 7.7326\n",
      "Epoch: 10/2000... Training loss: 7.7959\n",
      "Epoch: 10/2000... Training loss: 7.8012\n",
      "Epoch: 10/2000... Training loss: 7.6845\n",
      "Epoch: 10/2000... Training loss: 7.4343\n",
      "Epoch: 10/2000... Training loss: 7.7555\n",
      "Epoch: 10/2000... Training loss: 7.6411\n",
      "Epoch: 10/2000... Training loss: 7.7385\n",
      "Epoch: 10/2000... Training loss: 7.6373\n",
      "Epoch: 10/2000... Training loss: 7.7439\n",
      "Epoch: 10/2000... Training loss: 7.6777\n",
      "Epoch: 10/2000... Training loss: 7.5960\n",
      "Epoch: 10/2000... Training loss: 7.4859\n",
      "Epoch: 10/2000... Training loss: 7.4897\n",
      "Epoch: 10/2000... Training loss: 7.5357\n",
      "Epoch: 10/2000... Training loss: 7.6003\n",
      "Epoch: 10/2000... Training loss: 7.5068\n",
      "Epoch: 10/2000... Training loss: 7.6210\n",
      "Epoch: 10/2000... Training loss: 7.4444\n",
      "Epoch: 10/2000... Training loss: 7.6155\n",
      "Epoch: 10/2000... Training loss: 7.6794\n",
      "Epoch: 10/2000... Training loss: 7.5620\n",
      "Epoch: 10/2000... Training loss: 7.5431\n",
      "Epoch: 10/2000... Training loss: 7.5935\n",
      "Epoch: 10/2000... Training loss: 7.4475\n",
      "Epoch: 11/2000... Training loss: 7.4123\n",
      "Epoch: 11/2000... Training loss: 7.3178\n",
      "Epoch: 11/2000... Training loss: 7.4371\n",
      "Epoch: 11/2000... Training loss: 7.3968\n",
      "Epoch: 11/2000... Training loss: 7.4731\n",
      "Epoch: 11/2000... Training loss: 7.4450\n",
      "Epoch: 11/2000... Training loss: 7.5310\n",
      "Epoch: 11/2000... Training loss: 7.4717\n",
      "Epoch: 11/2000... Training loss: 7.5798\n",
      "Epoch: 11/2000... Training loss: 7.5028\n",
      "Epoch: 11/2000... Training loss: 7.2904\n",
      "Epoch: 11/2000... Training loss: 7.4212\n",
      "Epoch: 11/2000... Training loss: 7.4425\n",
      "Epoch: 11/2000... Training loss: 7.5164\n",
      "Epoch: 11/2000... Training loss: 7.3755\n",
      "Epoch: 11/2000... Training loss: 7.5318\n",
      "Epoch: 11/2000... Training loss: 7.4856\n",
      "Epoch: 11/2000... Training loss: 7.1688\n",
      "Epoch: 11/2000... Training loss: 7.0538\n",
      "Epoch: 11/2000... Training loss: 6.9505\n",
      "Epoch: 11/2000... Training loss: 7.3744\n",
      "Epoch: 11/2000... Training loss: 7.3010\n",
      "Epoch: 11/2000... Training loss: 7.3820\n",
      "Epoch: 11/2000... Training loss: 7.5913\n",
      "Epoch: 11/2000... Training loss: 7.3158\n",
      "Epoch: 11/2000... Training loss: 7.4930\n",
      "Epoch: 11/2000... Training loss: 7.4174\n",
      "Epoch: 11/2000... Training loss: 7.3214\n",
      "Epoch: 11/2000... Training loss: 7.3593\n",
      "Epoch: 11/2000... Training loss: 7.4657\n",
      "Epoch: 11/2000... Training loss: 7.3059\n",
      "Epoch: 12/2000... Training loss: 7.4446\n",
      "Epoch: 12/2000... Training loss: 7.3476\n",
      "Epoch: 12/2000... Training loss: 7.3689\n",
      "Epoch: 12/2000... Training loss: 7.3923\n",
      "Epoch: 12/2000... Training loss: 7.4457\n",
      "Epoch: 12/2000... Training loss: 7.4095\n",
      "Epoch: 12/2000... Training loss: 7.3088\n",
      "Epoch: 12/2000... Training loss: 7.2943\n",
      "Epoch: 12/2000... Training loss: 7.2772\n",
      "Epoch: 12/2000... Training loss: 7.3836\n",
      "Epoch: 12/2000... Training loss: 7.3422\n",
      "Epoch: 12/2000... Training loss: 7.1882\n",
      "Epoch: 12/2000... Training loss: 7.2171\n",
      "Epoch: 12/2000... Training loss: 7.2196\n",
      "Epoch: 12/2000... Training loss: 7.2733\n",
      "Epoch: 12/2000... Training loss: 7.3798\n",
      "Epoch: 12/2000... Training loss: 7.6125\n",
      "Epoch: 12/2000... Training loss: 7.2273\n",
      "Epoch: 12/2000... Training loss: 7.3515\n",
      "Epoch: 12/2000... Training loss: 7.4029\n",
      "Epoch: 12/2000... Training loss: 7.5545\n",
      "Epoch: 12/2000... Training loss: 7.4498\n",
      "Epoch: 12/2000... Training loss: 7.5062\n",
      "Epoch: 12/2000... Training loss: 7.8186\n",
      "Epoch: 12/2000... Training loss: 7.6839\n",
      "Epoch: 12/2000... Training loss: 7.8550\n",
      "Epoch: 12/2000... Training loss: 7.8887\n",
      "Epoch: 12/2000... Training loss: 7.9491\n",
      "Epoch: 12/2000... Training loss: 7.8723\n",
      "Epoch: 12/2000... Training loss: 7.7683\n",
      "Epoch: 12/2000... Training loss: 7.6264\n",
      "Epoch: 13/2000... Training loss: 7.6974\n",
      "Epoch: 13/2000... Training loss: 7.5169\n",
      "Epoch: 13/2000... Training loss: 7.5997\n",
      "Epoch: 13/2000... Training loss: 7.4416\n",
      "Epoch: 13/2000... Training loss: 7.4392\n",
      "Epoch: 13/2000... Training loss: 7.3026\n",
      "Epoch: 13/2000... Training loss: 7.2049\n",
      "Epoch: 13/2000... Training loss: 7.5066\n",
      "Epoch: 13/2000... Training loss: 7.3434\n",
      "Epoch: 13/2000... Training loss: 7.5511\n",
      "Epoch: 13/2000... Training loss: 7.9765\n",
      "Epoch: 13/2000... Training loss: 7.5225\n",
      "Epoch: 13/2000... Training loss: 7.4118\n",
      "Epoch: 13/2000... Training loss: 7.5592\n",
      "Epoch: 13/2000... Training loss: 7.5735\n",
      "Epoch: 13/2000... Training loss: 7.6461\n",
      "Epoch: 13/2000... Training loss: 7.6204\n",
      "Epoch: 13/2000... Training loss: 7.8030\n",
      "Epoch: 13/2000... Training loss: 8.0498\n",
      "Epoch: 13/2000... Training loss: 8.0721\n",
      "Epoch: 13/2000... Training loss: 7.8080\n",
      "Epoch: 13/2000... Training loss: 7.9266\n",
      "Epoch: 13/2000... Training loss: 7.7513\n",
      "Epoch: 13/2000... Training loss: 7.6187\n",
      "Epoch: 13/2000... Training loss: 7.3303\n",
      "Epoch: 13/2000... Training loss: 7.4793\n",
      "Epoch: 13/2000... Training loss: 7.3936\n",
      "Epoch: 13/2000... Training loss: 7.3681\n",
      "Epoch: 13/2000... Training loss: 7.2113\n",
      "Epoch: 13/2000... Training loss: 7.0861\n",
      "Epoch: 13/2000... Training loss: 7.0599\n",
      "Epoch: 14/2000... Training loss: 7.1677\n",
      "Epoch: 14/2000... Training loss: 6.9847\n",
      "Epoch: 14/2000... Training loss: 7.0460\n",
      "Epoch: 14/2000... Training loss: 6.9953\n",
      "Epoch: 14/2000... Training loss: 7.0991\n",
      "Epoch: 14/2000... Training loss: 7.1305\n",
      "Epoch: 14/2000... Training loss: 7.0307\n",
      "Epoch: 14/2000... Training loss: 7.2217\n",
      "Epoch: 14/2000... Training loss: 7.0777\n",
      "Epoch: 14/2000... Training loss: 7.2294\n",
      "Epoch: 14/2000... Training loss: 7.3239\n",
      "Epoch: 14/2000... Training loss: 7.0890\n",
      "Epoch: 14/2000... Training loss: 7.0018\n",
      "Epoch: 14/2000... Training loss: 7.1197\n",
      "Epoch: 14/2000... Training loss: 6.9278\n",
      "Epoch: 14/2000... Training loss: 7.0267\n",
      "Epoch: 14/2000... Training loss: 6.8808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/2000... Training loss: 6.9821\n",
      "Epoch: 14/2000... Training loss: 7.2592\n",
      "Epoch: 14/2000... Training loss: 7.1720\n",
      "Epoch: 14/2000... Training loss: 7.0675\n",
      "Epoch: 14/2000... Training loss: 7.3492\n",
      "Epoch: 14/2000... Training loss: 7.0549\n",
      "Epoch: 14/2000... Training loss: 6.9941\n",
      "Epoch: 14/2000... Training loss: 6.7749\n",
      "Epoch: 14/2000... Training loss: 6.9314\n",
      "Epoch: 14/2000... Training loss: 7.0359\n",
      "Epoch: 14/2000... Training loss: 7.0308\n",
      "Epoch: 14/2000... Training loss: 6.9854\n",
      "Epoch: 14/2000... Training loss: 6.9090\n",
      "Epoch: 14/2000... Training loss: 6.8549\n",
      "Epoch: 15/2000... Training loss: 6.8899\n",
      "Epoch: 15/2000... Training loss: 6.6352\n",
      "Epoch: 15/2000... Training loss: 6.7345\n",
      "Epoch: 15/2000... Training loss: 6.7667\n",
      "Epoch: 15/2000... Training loss: 6.7725\n",
      "Epoch: 15/2000... Training loss: 6.8042\n",
      "Epoch: 15/2000... Training loss: 6.7322\n",
      "Epoch: 15/2000... Training loss: 6.9045\n",
      "Epoch: 15/2000... Training loss: 6.6731\n",
      "Epoch: 15/2000... Training loss: 6.8823\n",
      "Epoch: 15/2000... Training loss: 7.1366\n",
      "Epoch: 15/2000... Training loss: 6.9496\n",
      "Epoch: 15/2000... Training loss: 6.7333\n",
      "Epoch: 15/2000... Training loss: 6.9653\n",
      "Epoch: 15/2000... Training loss: 6.7674\n",
      "Epoch: 15/2000... Training loss: 6.9135\n",
      "Epoch: 15/2000... Training loss: 6.8159\n",
      "Epoch: 15/2000... Training loss: 6.9589\n",
      "Epoch: 15/2000... Training loss: 7.1043\n",
      "Epoch: 15/2000... Training loss: 7.0235\n",
      "Epoch: 15/2000... Training loss: 6.6731\n",
      "Epoch: 15/2000... Training loss: 6.8342\n",
      "Epoch: 15/2000... Training loss: 6.6608\n",
      "Epoch: 15/2000... Training loss: 6.6388\n",
      "Epoch: 15/2000... Training loss: 6.4370\n",
      "Epoch: 15/2000... Training loss: 6.6115\n",
      "Epoch: 15/2000... Training loss: 6.7000\n",
      "Epoch: 15/2000... Training loss: 6.6761\n",
      "Epoch: 15/2000... Training loss: 6.6739\n",
      "Epoch: 15/2000... Training loss: 6.8693\n",
      "Epoch: 15/2000... Training loss: 6.6298\n",
      "Epoch: 16/2000... Training loss: 6.6266\n",
      "Epoch: 16/2000... Training loss: 6.5179\n",
      "Epoch: 16/2000... Training loss: 6.7370\n",
      "Epoch: 16/2000... Training loss: 6.5676\n",
      "Epoch: 16/2000... Training loss: 6.5623\n",
      "Epoch: 16/2000... Training loss: 6.4541\n",
      "Epoch: 16/2000... Training loss: 6.6022\n",
      "Epoch: 16/2000... Training loss: 6.4633\n",
      "Epoch: 16/2000... Training loss: 6.5935\n",
      "Epoch: 16/2000... Training loss: 6.4823\n",
      "Epoch: 16/2000... Training loss: 6.5632\n",
      "Epoch: 16/2000... Training loss: 6.6255\n",
      "Epoch: 16/2000... Training loss: 6.4096\n",
      "Epoch: 16/2000... Training loss: 6.6022\n",
      "Epoch: 16/2000... Training loss: 6.5715\n",
      "Epoch: 16/2000... Training loss: 6.6418\n",
      "Epoch: 16/2000... Training loss: 6.5776\n",
      "Epoch: 16/2000... Training loss: 6.6189\n",
      "Epoch: 16/2000... Training loss: 6.9356\n",
      "Epoch: 16/2000... Training loss: 6.7447\n",
      "Epoch: 16/2000... Training loss: 6.5274\n",
      "Epoch: 16/2000... Training loss: 6.5303\n",
      "Epoch: 16/2000... Training loss: 6.4607\n",
      "Epoch: 16/2000... Training loss: 6.3575\n",
      "Epoch: 16/2000... Training loss: 6.4116\n",
      "Epoch: 16/2000... Training loss: 6.3145\n",
      "Epoch: 16/2000... Training loss: 6.2424\n",
      "Epoch: 16/2000... Training loss: 6.3603\n",
      "Epoch: 16/2000... Training loss: 6.3453\n",
      "Epoch: 16/2000... Training loss: 6.4423\n",
      "Epoch: 16/2000... Training loss: 6.3027\n",
      "Epoch: 17/2000... Training loss: 6.2231\n",
      "Epoch: 17/2000... Training loss: 6.1972\n",
      "Epoch: 17/2000... Training loss: 6.5312\n",
      "Epoch: 17/2000... Training loss: 6.3505\n",
      "Epoch: 17/2000... Training loss: 6.5092\n",
      "Epoch: 17/2000... Training loss: 6.4646\n",
      "Epoch: 17/2000... Training loss: 6.4856\n",
      "Epoch: 17/2000... Training loss: 6.5191\n",
      "Epoch: 17/2000... Training loss: 6.4302\n",
      "Epoch: 17/2000... Training loss: 6.5825\n",
      "Epoch: 17/2000... Training loss: 6.3924\n",
      "Epoch: 17/2000... Training loss: 6.4414\n",
      "Epoch: 17/2000... Training loss: 6.3024\n",
      "Epoch: 17/2000... Training loss: 6.3844\n",
      "Epoch: 17/2000... Training loss: 6.3444\n",
      "Epoch: 17/2000... Training loss: 6.4600\n",
      "Epoch: 17/2000... Training loss: 6.3139\n",
      "Epoch: 17/2000... Training loss: 6.3192\n",
      "Epoch: 17/2000... Training loss: 6.4126\n",
      "Epoch: 17/2000... Training loss: 6.4665\n",
      "Epoch: 17/2000... Training loss: 6.4508\n",
      "Epoch: 17/2000... Training loss: 6.4564\n",
      "Epoch: 17/2000... Training loss: 6.4626\n",
      "Epoch: 17/2000... Training loss: 6.4453\n",
      "Epoch: 17/2000... Training loss: 6.3572\n",
      "Epoch: 17/2000... Training loss: 6.4600\n",
      "Epoch: 17/2000... Training loss: 6.2764\n",
      "Epoch: 17/2000... Training loss: 6.3890\n",
      "Epoch: 17/2000... Training loss: 6.2694\n",
      "Epoch: 17/2000... Training loss: 6.3472\n",
      "Epoch: 17/2000... Training loss: 6.1785\n",
      "Epoch: 18/2000... Training loss: 6.3386\n",
      "Epoch: 18/2000... Training loss: 5.9488\n",
      "Epoch: 18/2000... Training loss: 6.2308\n",
      "Epoch: 18/2000... Training loss: 5.9870\n",
      "Epoch: 18/2000... Training loss: 6.2744\n",
      "Epoch: 18/2000... Training loss: 6.1293\n",
      "Epoch: 18/2000... Training loss: 6.2507\n",
      "Epoch: 18/2000... Training loss: 6.3688\n",
      "Epoch: 18/2000... Training loss: 6.3918\n",
      "Epoch: 18/2000... Training loss: 6.5574\n",
      "Epoch: 18/2000... Training loss: 6.4053\n",
      "Epoch: 18/2000... Training loss: 6.7003\n",
      "Epoch: 18/2000... Training loss: 6.4285\n",
      "Epoch: 18/2000... Training loss: 6.4246\n",
      "Epoch: 18/2000... Training loss: 6.5178\n",
      "Epoch: 18/2000... Training loss: 6.2913\n",
      "Epoch: 18/2000... Training loss: 6.4783\n",
      "Epoch: 18/2000... Training loss: 6.1384\n",
      "Epoch: 18/2000... Training loss: 6.0863\n",
      "Epoch: 18/2000... Training loss: 6.1017\n",
      "Epoch: 18/2000... Training loss: 6.1684\n",
      "Epoch: 18/2000... Training loss: 6.0341\n",
      "Epoch: 18/2000... Training loss: 6.1478\n",
      "Epoch: 18/2000... Training loss: 6.2557\n",
      "Epoch: 18/2000... Training loss: 6.1641\n",
      "Epoch: 18/2000... Training loss: 6.0589\n",
      "Epoch: 18/2000... Training loss: 6.2487\n",
      "Epoch: 18/2000... Training loss: 6.3554\n",
      "Epoch: 18/2000... Training loss: 6.5346\n",
      "Epoch: 18/2000... Training loss: 6.4579\n",
      "Epoch: 18/2000... Training loss: 6.2956\n",
      "Epoch: 19/2000... Training loss: 6.1917\n",
      "Epoch: 19/2000... Training loss: 6.0295\n",
      "Epoch: 19/2000... Training loss: 6.2273\n",
      "Epoch: 19/2000... Training loss: 6.0574\n",
      "Epoch: 19/2000... Training loss: 6.1620\n",
      "Epoch: 19/2000... Training loss: 5.9335\n",
      "Epoch: 19/2000... Training loss: 6.0879\n",
      "Epoch: 19/2000... Training loss: 5.9914\n",
      "Epoch: 19/2000... Training loss: 6.0007\n",
      "Epoch: 19/2000... Training loss: 6.3001\n",
      "Epoch: 19/2000... Training loss: 6.1263\n",
      "Epoch: 19/2000... Training loss: 6.2084\n",
      "Epoch: 19/2000... Training loss: 6.0923\n",
      "Epoch: 19/2000... Training loss: 6.4315\n",
      "Epoch: 19/2000... Training loss: 6.2994\n",
      "Epoch: 19/2000... Training loss: 6.3872\n",
      "Epoch: 19/2000... Training loss: 6.3797\n",
      "Epoch: 19/2000... Training loss: 6.2737\n",
      "Epoch: 19/2000... Training loss: 6.2604\n",
      "Epoch: 19/2000... Training loss: 6.3863\n",
      "Epoch: 19/2000... Training loss: 6.2328\n",
      "Epoch: 19/2000... Training loss: 6.2571\n",
      "Epoch: 19/2000... Training loss: 6.1492\n",
      "Epoch: 19/2000... Training loss: 6.1212\n",
      "Epoch: 19/2000... Training loss: 6.0455\n",
      "Epoch: 19/2000... Training loss: 6.0717\n",
      "Epoch: 19/2000... Training loss: 6.0829\n",
      "Epoch: 19/2000... Training loss: 6.0599\n",
      "Epoch: 19/2000... Training loss: 5.9390\n",
      "Epoch: 19/2000... Training loss: 5.8356\n",
      "Epoch: 19/2000... Training loss: 6.0338\n",
      "Epoch: 20/2000... Training loss: 6.2518\n",
      "Epoch: 20/2000... Training loss: 5.8724\n",
      "Epoch: 20/2000... Training loss: 6.1340\n",
      "Epoch: 20/2000... Training loss: 6.0500\n",
      "Epoch: 20/2000... Training loss: 6.1127\n",
      "Epoch: 20/2000... Training loss: 6.1234\n",
      "Epoch: 20/2000... Training loss: 6.2066\n",
      "Epoch: 20/2000... Training loss: 6.1577\n",
      "Epoch: 20/2000... Training loss: 6.0785\n",
      "Epoch: 20/2000... Training loss: 6.1790\n",
      "Epoch: 20/2000... Training loss: 6.3830\n",
      "Epoch: 20/2000... Training loss: 6.2563\n",
      "Epoch: 20/2000... Training loss: 5.8287\n",
      "Epoch: 20/2000... Training loss: 6.0900\n",
      "Epoch: 20/2000... Training loss: 5.8842\n",
      "Epoch: 20/2000... Training loss: 5.9349\n",
      "Epoch: 20/2000... Training loss: 5.8517\n",
      "Epoch: 20/2000... Training loss: 6.0640\n",
      "Epoch: 20/2000... Training loss: 5.9202\n",
      "Epoch: 20/2000... Training loss: 6.0336\n",
      "Epoch: 20/2000... Training loss: 6.0903\n",
      "Epoch: 20/2000... Training loss: 6.0442\n",
      "Epoch: 20/2000... Training loss: 6.1622\n",
      "Epoch: 20/2000... Training loss: 6.1624\n",
      "Epoch: 20/2000... Training loss: 6.0759\n",
      "Epoch: 20/2000... Training loss: 6.2489\n",
      "Epoch: 20/2000... Training loss: 6.1483\n",
      "Epoch: 20/2000... Training loss: 6.1961\n",
      "Epoch: 20/2000... Training loss: 5.9276\n",
      "Epoch: 20/2000... Training loss: 5.9395\n",
      "Epoch: 20/2000... Training loss: 6.0088\n",
      "Epoch: 21/2000... Training loss: 5.9263\n",
      "Epoch: 21/2000... Training loss: 5.7461\n",
      "Epoch: 21/2000... Training loss: 5.9337\n",
      "Epoch: 21/2000... Training loss: 5.9530\n",
      "Epoch: 21/2000... Training loss: 5.9234\n",
      "Epoch: 21/2000... Training loss: 5.7864\n",
      "Epoch: 21/2000... Training loss: 5.9151\n",
      "Epoch: 21/2000... Training loss: 6.0173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/2000... Training loss: 5.9568\n",
      "Epoch: 21/2000... Training loss: 6.3130\n",
      "Epoch: 21/2000... Training loss: 6.2530\n",
      "Epoch: 21/2000... Training loss: 6.1378\n",
      "Epoch: 21/2000... Training loss: 5.9577\n",
      "Epoch: 21/2000... Training loss: 6.0485\n",
      "Epoch: 21/2000... Training loss: 5.9834\n",
      "Epoch: 21/2000... Training loss: 5.9615\n",
      "Epoch: 21/2000... Training loss: 5.9164\n",
      "Epoch: 21/2000... Training loss: 5.8780\n",
      "Epoch: 21/2000... Training loss: 6.0509\n",
      "Epoch: 21/2000... Training loss: 5.8714\n",
      "Epoch: 21/2000... Training loss: 5.7197\n",
      "Epoch: 21/2000... Training loss: 6.0167\n",
      "Epoch: 21/2000... Training loss: 6.0350\n",
      "Epoch: 21/2000... Training loss: 5.8057\n",
      "Epoch: 21/2000... Training loss: 5.8424\n",
      "Epoch: 21/2000... Training loss: 6.0925\n",
      "Epoch: 21/2000... Training loss: 5.9040\n",
      "Epoch: 21/2000... Training loss: 6.1310\n",
      "Epoch: 21/2000... Training loss: 5.9517\n",
      "Epoch: 21/2000... Training loss: 5.9562\n",
      "Epoch: 21/2000... Training loss: 5.9311\n",
      "Epoch: 22/2000... Training loss: 6.0913\n",
      "Epoch: 22/2000... Training loss: 5.9322\n",
      "Epoch: 22/2000... Training loss: 6.0893\n",
      "Epoch: 22/2000... Training loss: 5.8483\n",
      "Epoch: 22/2000... Training loss: 5.8629\n",
      "Epoch: 22/2000... Training loss: 5.7360\n",
      "Epoch: 22/2000... Training loss: 5.8649\n",
      "Epoch: 22/2000... Training loss: 5.7526\n",
      "Epoch: 22/2000... Training loss: 5.6853\n",
      "Epoch: 22/2000... Training loss: 5.8341\n",
      "Epoch: 22/2000... Training loss: 5.9186\n",
      "Epoch: 22/2000... Training loss: 5.8516\n",
      "Epoch: 22/2000... Training loss: 5.8697\n",
      "Epoch: 22/2000... Training loss: 5.7631\n",
      "Epoch: 22/2000... Training loss: 5.9206\n",
      "Epoch: 22/2000... Training loss: 5.8825\n",
      "Epoch: 22/2000... Training loss: 5.8557\n",
      "Epoch: 22/2000... Training loss: 6.0372\n",
      "Epoch: 22/2000... Training loss: 5.9368\n",
      "Epoch: 22/2000... Training loss: 5.9742\n",
      "Epoch: 22/2000... Training loss: 5.7614\n",
      "Epoch: 22/2000... Training loss: 5.9444\n",
      "Epoch: 22/2000... Training loss: 5.9674\n",
      "Epoch: 22/2000... Training loss: 5.8432\n",
      "Epoch: 22/2000... Training loss: 5.7221\n",
      "Epoch: 22/2000... Training loss: 5.9100\n",
      "Epoch: 22/2000... Training loss: 5.7739\n",
      "Epoch: 22/2000... Training loss: 5.9176\n",
      "Epoch: 22/2000... Training loss: 5.9162\n",
      "Epoch: 22/2000... Training loss: 5.7059\n",
      "Epoch: 22/2000... Training loss: 5.8084\n",
      "Epoch: 23/2000... Training loss: 5.8986\n",
      "Epoch: 23/2000... Training loss: 5.5587\n",
      "Epoch: 23/2000... Training loss: 6.0320\n",
      "Epoch: 23/2000... Training loss: 5.9643\n",
      "Epoch: 23/2000... Training loss: 5.8201\n",
      "Epoch: 23/2000... Training loss: 5.8151\n",
      "Epoch: 23/2000... Training loss: 5.9209\n",
      "Epoch: 23/2000... Training loss: 5.9126\n",
      "Epoch: 23/2000... Training loss: 5.7119\n",
      "Epoch: 23/2000... Training loss: 5.9161\n",
      "Epoch: 23/2000... Training loss: 5.7861\n",
      "Epoch: 23/2000... Training loss: 5.8183\n",
      "Epoch: 23/2000... Training loss: 5.7485\n",
      "Epoch: 23/2000... Training loss: 5.7023\n",
      "Epoch: 23/2000... Training loss: 5.7276\n",
      "Epoch: 23/2000... Training loss: 5.7691\n",
      "Epoch: 23/2000... Training loss: 5.8597\n",
      "Epoch: 23/2000... Training loss: 5.7558\n",
      "Epoch: 23/2000... Training loss: 5.8422\n",
      "Epoch: 23/2000... Training loss: 5.9627\n",
      "Epoch: 23/2000... Training loss: 5.7008\n",
      "Epoch: 23/2000... Training loss: 5.9760\n",
      "Epoch: 23/2000... Training loss: 5.8512\n",
      "Epoch: 23/2000... Training loss: 5.7684\n",
      "Epoch: 23/2000... Training loss: 5.7014\n",
      "Epoch: 23/2000... Training loss: 5.7679\n",
      "Epoch: 23/2000... Training loss: 5.8428\n",
      "Epoch: 23/2000... Training loss: 5.8232\n",
      "Epoch: 23/2000... Training loss: 5.8487\n",
      "Epoch: 23/2000... Training loss: 5.8449\n",
      "Epoch: 23/2000... Training loss: 5.8544\n",
      "Epoch: 24/2000... Training loss: 5.7360\n",
      "Epoch: 24/2000... Training loss: 5.4825\n",
      "Epoch: 24/2000... Training loss: 5.8714\n",
      "Epoch: 24/2000... Training loss: 5.5635\n",
      "Epoch: 24/2000... Training loss: 5.7502\n",
      "Epoch: 24/2000... Training loss: 5.5466\n",
      "Epoch: 24/2000... Training loss: 5.5552\n",
      "Epoch: 24/2000... Training loss: 5.6696\n",
      "Epoch: 24/2000... Training loss: 5.8802\n",
      "Epoch: 24/2000... Training loss: 5.8539\n",
      "Epoch: 24/2000... Training loss: 5.6651\n",
      "Epoch: 24/2000... Training loss: 5.5890\n",
      "Epoch: 24/2000... Training loss: 5.6882\n",
      "Epoch: 24/2000... Training loss: 5.8527\n",
      "Epoch: 24/2000... Training loss: 5.7800\n",
      "Epoch: 24/2000... Training loss: 5.5988\n",
      "Epoch: 24/2000... Training loss: 5.7973\n",
      "Epoch: 24/2000... Training loss: 5.5725\n",
      "Epoch: 24/2000... Training loss: 5.6695\n",
      "Epoch: 24/2000... Training loss: 5.8446\n",
      "Epoch: 24/2000... Training loss: 5.6575\n",
      "Epoch: 24/2000... Training loss: 5.7267\n",
      "Epoch: 24/2000... Training loss: 5.5702\n",
      "Epoch: 24/2000... Training loss: 5.6597\n",
      "Epoch: 24/2000... Training loss: 5.6074\n",
      "Epoch: 24/2000... Training loss: 5.7481\n",
      "Epoch: 24/2000... Training loss: 5.6338\n",
      "Epoch: 24/2000... Training loss: 5.7810\n",
      "Epoch: 24/2000... Training loss: 5.7408\n",
      "Epoch: 24/2000... Training loss: 5.6709\n",
      "Epoch: 24/2000... Training loss: 5.6291\n",
      "Epoch: 25/2000... Training loss: 5.6267\n",
      "Epoch: 25/2000... Training loss: 5.6411\n",
      "Epoch: 25/2000... Training loss: 5.4802\n",
      "Epoch: 25/2000... Training loss: 5.6955\n",
      "Epoch: 25/2000... Training loss: 5.6056\n",
      "Epoch: 25/2000... Training loss: 5.5703\n",
      "Epoch: 25/2000... Training loss: 5.6752\n",
      "Epoch: 25/2000... Training loss: 5.6307\n",
      "Epoch: 25/2000... Training loss: 5.7161\n",
      "Epoch: 25/2000... Training loss: 5.8051\n",
      "Epoch: 25/2000... Training loss: 5.6804\n",
      "Epoch: 25/2000... Training loss: 5.6132\n",
      "Epoch: 25/2000... Training loss: 5.6417\n",
      "Epoch: 25/2000... Training loss: 5.7792\n",
      "Epoch: 25/2000... Training loss: 5.5425\n",
      "Epoch: 25/2000... Training loss: 5.5313\n",
      "Epoch: 25/2000... Training loss: 5.8621\n",
      "Epoch: 25/2000... Training loss: 5.7562\n",
      "Epoch: 25/2000... Training loss: 5.7435\n",
      "Epoch: 25/2000... Training loss: 5.6507\n",
      "Epoch: 25/2000... Training loss: 5.6602\n",
      "Epoch: 25/2000... Training loss: 5.6528\n",
      "Epoch: 25/2000... Training loss: 5.6819\n",
      "Epoch: 25/2000... Training loss: 5.6227\n",
      "Epoch: 25/2000... Training loss: 5.5395\n",
      "Epoch: 25/2000... Training loss: 5.7150\n",
      "Epoch: 25/2000... Training loss: 5.5844\n",
      "Epoch: 25/2000... Training loss: 5.6440\n",
      "Epoch: 25/2000... Training loss: 5.6115\n",
      "Epoch: 25/2000... Training loss: 5.7208\n",
      "Epoch: 25/2000... Training loss: 5.7470\n",
      "Epoch: 26/2000... Training loss: 5.4131\n",
      "Epoch: 26/2000... Training loss: 5.4481\n",
      "Epoch: 26/2000... Training loss: 5.6437\n",
      "Epoch: 26/2000... Training loss: 5.4677\n",
      "Epoch: 26/2000... Training loss: 5.6088\n",
      "Epoch: 26/2000... Training loss: 5.5933\n",
      "Epoch: 26/2000... Training loss: 5.6278\n",
      "Epoch: 26/2000... Training loss: 5.5453\n",
      "Epoch: 26/2000... Training loss: 5.6835\n",
      "Epoch: 26/2000... Training loss: 5.7121\n",
      "Epoch: 26/2000... Training loss: 5.5945\n",
      "Epoch: 26/2000... Training loss: 5.5711\n",
      "Epoch: 26/2000... Training loss: 5.4495\n",
      "Epoch: 26/2000... Training loss: 5.6309\n",
      "Epoch: 26/2000... Training loss: 5.5238\n",
      "Epoch: 26/2000... Training loss: 5.5267\n",
      "Epoch: 26/2000... Training loss: 5.7687\n",
      "Epoch: 26/2000... Training loss: 5.6093\n",
      "Epoch: 26/2000... Training loss: 5.6254\n",
      "Epoch: 26/2000... Training loss: 5.4576\n",
      "Epoch: 26/2000... Training loss: 5.6139\n",
      "Epoch: 26/2000... Training loss: 5.5013\n",
      "Epoch: 26/2000... Training loss: 5.5593\n",
      "Epoch: 26/2000... Training loss: 5.6614\n",
      "Epoch: 26/2000... Training loss: 5.6187\n",
      "Epoch: 26/2000... Training loss: 5.6265\n",
      "Epoch: 26/2000... Training loss: 5.5116\n",
      "Epoch: 26/2000... Training loss: 5.6218\n",
      "Epoch: 26/2000... Training loss: 5.4424\n",
      "Epoch: 26/2000... Training loss: 5.5423\n",
      "Epoch: 26/2000... Training loss: 5.6848\n",
      "Epoch: 27/2000... Training loss: 5.4738\n",
      "Epoch: 27/2000... Training loss: 5.3804\n",
      "Epoch: 27/2000... Training loss: 5.3763\n",
      "Epoch: 27/2000... Training loss: 5.3339\n",
      "Epoch: 27/2000... Training loss: 5.4288\n",
      "Epoch: 27/2000... Training loss: 5.3189\n",
      "Epoch: 27/2000... Training loss: 5.5445\n",
      "Epoch: 27/2000... Training loss: 5.6806\n",
      "Epoch: 27/2000... Training loss: 5.6386\n",
      "Epoch: 27/2000... Training loss: 5.6648\n",
      "Epoch: 27/2000... Training loss: 5.7847\n",
      "Epoch: 27/2000... Training loss: 5.6285\n",
      "Epoch: 27/2000... Training loss: 5.5913\n",
      "Epoch: 27/2000... Training loss: 5.3747\n",
      "Epoch: 27/2000... Training loss: 5.5796\n",
      "Epoch: 27/2000... Training loss: 5.4463\n",
      "Epoch: 27/2000... Training loss: 5.4872\n",
      "Epoch: 27/2000... Training loss: 5.3741\n",
      "Epoch: 27/2000... Training loss: 5.4366\n",
      "Epoch: 27/2000... Training loss: 5.5371\n",
      "Epoch: 27/2000... Training loss: 5.6444\n",
      "Epoch: 27/2000... Training loss: 5.1934\n",
      "Epoch: 27/2000... Training loss: 5.5646\n",
      "Epoch: 27/2000... Training loss: 5.6529\n",
      "Epoch: 27/2000... Training loss: 5.5340\n",
      "Epoch: 27/2000... Training loss: 5.7547\n",
      "Epoch: 27/2000... Training loss: 5.5753\n",
      "Epoch: 27/2000... Training loss: 5.6982\n",
      "Epoch: 27/2000... Training loss: 5.4389\n",
      "Epoch: 27/2000... Training loss: 5.5117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/2000... Training loss: 5.5542\n",
      "Epoch: 28/2000... Training loss: 5.2918\n",
      "Epoch: 28/2000... Training loss: 5.2977\n",
      "Epoch: 28/2000... Training loss: 5.3845\n",
      "Epoch: 28/2000... Training loss: 5.3129\n",
      "Epoch: 28/2000... Training loss: 5.3487\n",
      "Epoch: 28/2000... Training loss: 5.2764\n",
      "Epoch: 28/2000... Training loss: 5.3687\n",
      "Epoch: 28/2000... Training loss: 5.4652\n",
      "Epoch: 28/2000... Training loss: 5.5003\n",
      "Epoch: 28/2000... Training loss: 5.6866\n",
      "Epoch: 28/2000... Training loss: 5.6434\n",
      "Epoch: 28/2000... Training loss: 5.6801\n",
      "Epoch: 28/2000... Training loss: 5.4957\n",
      "Epoch: 28/2000... Training loss: 5.3885\n",
      "Epoch: 28/2000... Training loss: 5.7060\n",
      "Epoch: 28/2000... Training loss: 5.5060\n",
      "Epoch: 28/2000... Training loss: 5.3955\n",
      "Epoch: 28/2000... Training loss: 5.4282\n",
      "Epoch: 28/2000... Training loss: 5.5651\n",
      "Epoch: 28/2000... Training loss: 5.4739\n",
      "Epoch: 28/2000... Training loss: 5.3082\n",
      "Epoch: 28/2000... Training loss: 5.4160\n",
      "Epoch: 28/2000... Training loss: 5.7078\n",
      "Epoch: 28/2000... Training loss: 5.5102\n",
      "Epoch: 28/2000... Training loss: 5.3847\n",
      "Epoch: 28/2000... Training loss: 5.4462\n",
      "Epoch: 28/2000... Training loss: 5.5261\n",
      "Epoch: 28/2000... Training loss: 5.4572\n",
      "Epoch: 28/2000... Training loss: 5.4148\n",
      "Epoch: 28/2000... Training loss: 5.3440\n",
      "Epoch: 28/2000... Training loss: 5.4558\n",
      "Epoch: 29/2000... Training loss: 5.4971\n",
      "Epoch: 29/2000... Training loss: 5.2340\n",
      "Epoch: 29/2000... Training loss: 5.3828\n",
      "Epoch: 29/2000... Training loss: 5.1986\n",
      "Epoch: 29/2000... Training loss: 5.4335\n",
      "Epoch: 29/2000... Training loss: 5.1798\n",
      "Epoch: 29/2000... Training loss: 5.3984\n",
      "Epoch: 29/2000... Training loss: 5.2673\n",
      "Epoch: 29/2000... Training loss: 5.3869\n",
      "Epoch: 29/2000... Training loss: 5.4639\n",
      "Epoch: 29/2000... Training loss: 5.4751\n",
      "Epoch: 29/2000... Training loss: 5.6417\n",
      "Epoch: 29/2000... Training loss: 5.4201\n",
      "Epoch: 29/2000... Training loss: 5.3644\n",
      "Epoch: 29/2000... Training loss: 5.3352\n",
      "Epoch: 29/2000... Training loss: 5.2331\n",
      "Epoch: 29/2000... Training loss: 5.3905\n",
      "Epoch: 29/2000... Training loss: 5.4456\n",
      "Epoch: 29/2000... Training loss: 5.5878\n",
      "Epoch: 29/2000... Training loss: 5.4240\n",
      "Epoch: 29/2000... Training loss: 5.4406\n",
      "Epoch: 29/2000... Training loss: 5.5174\n",
      "Epoch: 29/2000... Training loss: 5.4348\n",
      "Epoch: 29/2000... Training loss: 5.4456\n",
      "Epoch: 29/2000... Training loss: 5.3783\n",
      "Epoch: 29/2000... Training loss: 5.3823\n",
      "Epoch: 29/2000... Training loss: 5.3499\n",
      "Epoch: 29/2000... Training loss: 5.4361\n",
      "Epoch: 29/2000... Training loss: 5.5298\n",
      "Epoch: 29/2000... Training loss: 5.3533\n",
      "Epoch: 29/2000... Training loss: 5.4391\n",
      "Epoch: 30/2000... Training loss: 5.3581\n",
      "Epoch: 30/2000... Training loss: 5.3682\n",
      "Epoch: 30/2000... Training loss: 5.3736\n",
      "Epoch: 30/2000... Training loss: 5.1962\n",
      "Epoch: 30/2000... Training loss: 5.3457\n",
      "Epoch: 30/2000... Training loss: 5.4158\n",
      "Epoch: 30/2000... Training loss: 5.3670\n",
      "Epoch: 30/2000... Training loss: 5.4172\n",
      "Epoch: 30/2000... Training loss: 5.3274\n",
      "Epoch: 30/2000... Training loss: 5.4985\n",
      "Epoch: 30/2000... Training loss: 5.3536\n",
      "Epoch: 30/2000... Training loss: 5.3820\n",
      "Epoch: 30/2000... Training loss: 5.5738\n",
      "Epoch: 30/2000... Training loss: 5.3089\n",
      "Epoch: 30/2000... Training loss: 5.4286\n",
      "Epoch: 30/2000... Training loss: 5.4857\n",
      "Epoch: 30/2000... Training loss: 5.4064\n",
      "Epoch: 30/2000... Training loss: 5.3908\n",
      "Epoch: 30/2000... Training loss: 5.2568\n",
      "Epoch: 30/2000... Training loss: 5.3867\n",
      "Epoch: 30/2000... Training loss: 5.3822\n",
      "Epoch: 30/2000... Training loss: 5.4596\n",
      "Epoch: 30/2000... Training loss: 5.5170\n",
      "Epoch: 30/2000... Training loss: 5.5424\n",
      "Epoch: 30/2000... Training loss: 5.3674\n",
      "Epoch: 30/2000... Training loss: 5.2848\n",
      "Epoch: 30/2000... Training loss: 5.3463\n",
      "Epoch: 30/2000... Training loss: 5.2780\n",
      "Epoch: 30/2000... Training loss: 5.3810\n",
      "Epoch: 30/2000... Training loss: 5.2314\n",
      "Epoch: 30/2000... Training loss: 5.2838\n",
      "Epoch: 31/2000... Training loss: 5.2843\n",
      "Epoch: 31/2000... Training loss: 5.0779\n",
      "Epoch: 31/2000... Training loss: 5.2408\n",
      "Epoch: 31/2000... Training loss: 5.1710\n",
      "Epoch: 31/2000... Training loss: 5.2685\n",
      "Epoch: 31/2000... Training loss: 5.2298\n",
      "Epoch: 31/2000... Training loss: 5.4493\n",
      "Epoch: 31/2000... Training loss: 5.5499\n",
      "Epoch: 31/2000... Training loss: 5.3478\n",
      "Epoch: 31/2000... Training loss: 5.5769\n",
      "Epoch: 31/2000... Training loss: 5.4533\n",
      "Epoch: 31/2000... Training loss: 5.4232\n",
      "Epoch: 31/2000... Training loss: 5.2884\n",
      "Epoch: 31/2000... Training loss: 5.2871\n",
      "Epoch: 31/2000... Training loss: 5.3438\n",
      "Epoch: 31/2000... Training loss: 5.1908\n",
      "Epoch: 31/2000... Training loss: 5.4890\n",
      "Epoch: 31/2000... Training loss: 5.2743\n",
      "Epoch: 31/2000... Training loss: 5.2893\n",
      "Epoch: 31/2000... Training loss: 5.1566\n",
      "Epoch: 31/2000... Training loss: 5.2625\n",
      "Epoch: 31/2000... Training loss: 5.2787\n",
      "Epoch: 31/2000... Training loss: 5.4235\n",
      "Epoch: 31/2000... Training loss: 5.4681\n",
      "Epoch: 31/2000... Training loss: 5.4846\n",
      "Epoch: 31/2000... Training loss: 5.3046\n",
      "Epoch: 31/2000... Training loss: 5.5239\n",
      "Epoch: 31/2000... Training loss: 5.3870\n",
      "Epoch: 31/2000... Training loss: 5.4102\n",
      "Epoch: 31/2000... Training loss: 5.2034\n",
      "Epoch: 31/2000... Training loss: 5.3391\n",
      "Epoch: 32/2000... Training loss: 5.4496\n",
      "Epoch: 32/2000... Training loss: 4.8956\n",
      "Epoch: 32/2000... Training loss: 5.1866\n",
      "Epoch: 32/2000... Training loss: 5.1606\n",
      "Epoch: 32/2000... Training loss: 5.2124\n",
      "Epoch: 32/2000... Training loss: 5.2025\n",
      "Epoch: 32/2000... Training loss: 5.2521\n",
      "Epoch: 32/2000... Training loss: 5.1346\n",
      "Epoch: 32/2000... Training loss: 5.3174\n",
      "Epoch: 32/2000... Training loss: 5.5425\n",
      "Epoch: 32/2000... Training loss: 5.2204\n",
      "Epoch: 32/2000... Training loss: 5.2251\n",
      "Epoch: 32/2000... Training loss: 5.3676\n",
      "Epoch: 32/2000... Training loss: 5.2499\n",
      "Epoch: 32/2000... Training loss: 5.4137\n",
      "Epoch: 32/2000... Training loss: 5.2623\n",
      "Epoch: 32/2000... Training loss: 5.4502\n",
      "Epoch: 32/2000... Training loss: 5.1478\n",
      "Epoch: 32/2000... Training loss: 5.4627\n",
      "Epoch: 32/2000... Training loss: 5.2634\n",
      "Epoch: 32/2000... Training loss: 5.3543\n",
      "Epoch: 32/2000... Training loss: 5.3969\n",
      "Epoch: 32/2000... Training loss: 5.2489\n",
      "Epoch: 32/2000... Training loss: 5.3284\n",
      "Epoch: 32/2000... Training loss: 5.3060\n",
      "Epoch: 32/2000... Training loss: 5.3174\n",
      "Epoch: 32/2000... Training loss: 5.2516\n",
      "Epoch: 32/2000... Training loss: 5.3761\n",
      "Epoch: 32/2000... Training loss: 5.1988\n",
      "Epoch: 32/2000... Training loss: 5.2668\n",
      "Epoch: 32/2000... Training loss: 5.3937\n",
      "Epoch: 33/2000... Training loss: 5.2322\n",
      "Epoch: 33/2000... Training loss: 5.1033\n",
      "Epoch: 33/2000... Training loss: 5.0130\n",
      "Epoch: 33/2000... Training loss: 5.1174\n",
      "Epoch: 33/2000... Training loss: 5.2663\n",
      "Epoch: 33/2000... Training loss: 5.0145\n",
      "Epoch: 33/2000... Training loss: 5.3400\n",
      "Epoch: 33/2000... Training loss: 5.1932\n",
      "Epoch: 33/2000... Training loss: 5.4084\n",
      "Epoch: 33/2000... Training loss: 5.4017\n",
      "Epoch: 33/2000... Training loss: 5.1591\n",
      "Epoch: 33/2000... Training loss: 5.2308\n",
      "Epoch: 33/2000... Training loss: 5.0942\n",
      "Epoch: 33/2000... Training loss: 5.3840\n",
      "Epoch: 33/2000... Training loss: 5.3051\n",
      "Epoch: 33/2000... Training loss: 5.1022\n",
      "Epoch: 33/2000... Training loss: 5.2697\n",
      "Epoch: 33/2000... Training loss: 5.2579\n",
      "Epoch: 33/2000... Training loss: 5.2989\n",
      "Epoch: 33/2000... Training loss: 5.1988\n",
      "Epoch: 33/2000... Training loss: 5.1390\n",
      "Epoch: 33/2000... Training loss: 5.1678\n",
      "Epoch: 33/2000... Training loss: 5.1807\n",
      "Epoch: 33/2000... Training loss: 5.2205\n",
      "Epoch: 33/2000... Training loss: 5.0185\n",
      "Epoch: 33/2000... Training loss: 5.2493\n",
      "Epoch: 33/2000... Training loss: 5.1464\n",
      "Epoch: 33/2000... Training loss: 5.0836\n",
      "Epoch: 33/2000... Training loss: 5.1206\n",
      "Epoch: 33/2000... Training loss: 5.3767\n",
      "Epoch: 33/2000... Training loss: 5.1588\n",
      "Epoch: 34/2000... Training loss: 5.1106\n",
      "Epoch: 34/2000... Training loss: 5.1218\n",
      "Epoch: 34/2000... Training loss: 5.1661\n",
      "Epoch: 34/2000... Training loss: 5.1788\n",
      "Epoch: 34/2000... Training loss: 5.1060\n",
      "Epoch: 34/2000... Training loss: 5.1830\n",
      "Epoch: 34/2000... Training loss: 5.3004\n",
      "Epoch: 34/2000... Training loss: 5.2169\n",
      "Epoch: 34/2000... Training loss: 5.2318\n",
      "Epoch: 34/2000... Training loss: 5.1192\n",
      "Epoch: 34/2000... Training loss: 5.1152\n",
      "Epoch: 34/2000... Training loss: 5.4083\n",
      "Epoch: 34/2000... Training loss: 5.1401\n",
      "Epoch: 34/2000... Training loss: 5.2356\n",
      "Epoch: 34/2000... Training loss: 5.1794\n",
      "Epoch: 34/2000... Training loss: 5.2343\n",
      "Epoch: 34/2000... Training loss: 5.2657\n",
      "Epoch: 34/2000... Training loss: 5.1281\n",
      "Epoch: 34/2000... Training loss: 5.0514\n",
      "Epoch: 34/2000... Training loss: 5.2468\n",
      "Epoch: 34/2000... Training loss: 5.3118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/2000... Training loss: 5.2661\n",
      "Epoch: 34/2000... Training loss: 5.3357\n",
      "Epoch: 34/2000... Training loss: 5.1783\n",
      "Epoch: 34/2000... Training loss: 5.3759\n",
      "Epoch: 34/2000... Training loss: 5.2445\n",
      "Epoch: 34/2000... Training loss: 5.2349\n",
      "Epoch: 34/2000... Training loss: 5.1325\n",
      "Epoch: 34/2000... Training loss: 5.1937\n",
      "Epoch: 34/2000... Training loss: 5.1113\n",
      "Epoch: 34/2000... Training loss: 5.2180\n",
      "Epoch: 35/2000... Training loss: 5.1569\n",
      "Epoch: 35/2000... Training loss: 4.9810\n",
      "Epoch: 35/2000... Training loss: 4.9487\n",
      "Epoch: 35/2000... Training loss: 5.0290\n",
      "Epoch: 35/2000... Training loss: 5.0028\n",
      "Epoch: 35/2000... Training loss: 4.9962\n",
      "Epoch: 35/2000... Training loss: 5.1349\n",
      "Epoch: 35/2000... Training loss: 5.0762\n",
      "Epoch: 35/2000... Training loss: 5.0017\n",
      "Epoch: 35/2000... Training loss: 5.3331\n",
      "Epoch: 35/2000... Training loss: 5.0523\n",
      "Epoch: 35/2000... Training loss: 5.2258\n",
      "Epoch: 35/2000... Training loss: 5.2077\n",
      "Epoch: 35/2000... Training loss: 5.1686\n",
      "Epoch: 35/2000... Training loss: 5.1234\n",
      "Epoch: 35/2000... Training loss: 5.1271\n",
      "Epoch: 35/2000... Training loss: 5.2425\n",
      "Epoch: 35/2000... Training loss: 5.3195\n",
      "Epoch: 35/2000... Training loss: 5.3151\n",
      "Epoch: 35/2000... Training loss: 5.1840\n",
      "Epoch: 35/2000... Training loss: 5.0571\n",
      "Epoch: 35/2000... Training loss: 5.1147\n",
      "Epoch: 35/2000... Training loss: 5.3566\n",
      "Epoch: 35/2000... Training loss: 5.2830\n",
      "Epoch: 35/2000... Training loss: 5.1053\n",
      "Epoch: 35/2000... Training loss: 5.1193\n",
      "Epoch: 35/2000... Training loss: 5.2023\n",
      "Epoch: 35/2000... Training loss: 5.2993\n",
      "Epoch: 35/2000... Training loss: 5.0954\n",
      "Epoch: 35/2000... Training loss: 5.0330\n",
      "Epoch: 35/2000... Training loss: 5.0337\n",
      "Epoch: 36/2000... Training loss: 5.1777\n",
      "Epoch: 36/2000... Training loss: 4.9543\n",
      "Epoch: 36/2000... Training loss: 4.8604\n",
      "Epoch: 36/2000... Training loss: 4.9190\n",
      "Epoch: 36/2000... Training loss: 5.0751\n",
      "Epoch: 36/2000... Training loss: 4.9992\n",
      "Epoch: 36/2000... Training loss: 5.2680\n",
      "Epoch: 36/2000... Training loss: 5.0774\n",
      "Epoch: 36/2000... Training loss: 4.9948\n",
      "Epoch: 36/2000... Training loss: 5.3135\n",
      "Epoch: 36/2000... Training loss: 5.1679\n",
      "Epoch: 36/2000... Training loss: 5.1748\n",
      "Epoch: 36/2000... Training loss: 5.0802\n",
      "Epoch: 36/2000... Training loss: 5.2242\n",
      "Epoch: 36/2000... Training loss: 5.2070\n",
      "Epoch: 36/2000... Training loss: 5.0482\n",
      "Epoch: 36/2000... Training loss: 5.2604\n",
      "Epoch: 36/2000... Training loss: 5.1696\n",
      "Epoch: 36/2000... Training loss: 5.1859\n",
      "Epoch: 36/2000... Training loss: 5.1088\n",
      "Epoch: 36/2000... Training loss: 5.1355\n",
      "Epoch: 36/2000... Training loss: 5.1311\n",
      "Epoch: 36/2000... Training loss: 5.2685\n",
      "Epoch: 36/2000... Training loss: 5.1474\n",
      "Epoch: 36/2000... Training loss: 5.0182\n",
      "Epoch: 36/2000... Training loss: 5.2584\n",
      "Epoch: 36/2000... Training loss: 5.2631\n",
      "Epoch: 36/2000... Training loss: 5.0798\n",
      "Epoch: 36/2000... Training loss: 5.2580\n",
      "Epoch: 36/2000... Training loss: 5.1224\n",
      "Epoch: 36/2000... Training loss: 4.9618\n",
      "Epoch: 37/2000... Training loss: 4.9884\n",
      "Epoch: 37/2000... Training loss: 4.9446\n",
      "Epoch: 37/2000... Training loss: 5.0559\n",
      "Epoch: 37/2000... Training loss: 4.9575\n",
      "Epoch: 37/2000... Training loss: 4.7994\n",
      "Epoch: 37/2000... Training loss: 5.0196\n",
      "Epoch: 37/2000... Training loss: 4.9606\n",
      "Epoch: 37/2000... Training loss: 5.0772\n",
      "Epoch: 37/2000... Training loss: 5.0153\n",
      "Epoch: 37/2000... Training loss: 5.1096\n",
      "Epoch: 37/2000... Training loss: 5.4782\n",
      "Epoch: 37/2000... Training loss: 5.0474\n",
      "Epoch: 37/2000... Training loss: 4.9567\n",
      "Epoch: 37/2000... Training loss: 5.0817\n",
      "Epoch: 37/2000... Training loss: 5.0993\n",
      "Epoch: 37/2000... Training loss: 4.9559\n",
      "Epoch: 37/2000... Training loss: 5.2719\n",
      "Epoch: 37/2000... Training loss: 5.2773\n",
      "Epoch: 37/2000... Training loss: 4.9677\n",
      "Epoch: 37/2000... Training loss: 4.9358\n",
      "Epoch: 37/2000... Training loss: 5.1010\n",
      "Epoch: 37/2000... Training loss: 4.9614\n",
      "Epoch: 37/2000... Training loss: 5.3079\n",
      "Epoch: 37/2000... Training loss: 5.1146\n",
      "Epoch: 37/2000... Training loss: 4.9172\n",
      "Epoch: 37/2000... Training loss: 5.0498\n",
      "Epoch: 37/2000... Training loss: 5.0254\n",
      "Epoch: 37/2000... Training loss: 5.1357\n",
      "Epoch: 37/2000... Training loss: 5.0302\n",
      "Epoch: 37/2000... Training loss: 5.1391\n",
      "Epoch: 37/2000... Training loss: 5.1279\n",
      "Epoch: 38/2000... Training loss: 4.9751\n",
      "Epoch: 38/2000... Training loss: 4.8455\n",
      "Epoch: 38/2000... Training loss: 4.8456\n",
      "Epoch: 38/2000... Training loss: 4.8256\n",
      "Epoch: 38/2000... Training loss: 4.8030\n",
      "Epoch: 38/2000... Training loss: 4.9499\n",
      "Epoch: 38/2000... Training loss: 4.9891\n",
      "Epoch: 38/2000... Training loss: 5.1350\n",
      "Epoch: 38/2000... Training loss: 4.9591\n",
      "Epoch: 38/2000... Training loss: 5.1451\n",
      "Epoch: 38/2000... Training loss: 5.0685\n",
      "Epoch: 38/2000... Training loss: 5.0718\n",
      "Epoch: 38/2000... Training loss: 5.2053\n",
      "Epoch: 38/2000... Training loss: 4.9901\n",
      "Epoch: 38/2000... Training loss: 4.9976\n",
      "Epoch: 38/2000... Training loss: 5.0232\n",
      "Epoch: 38/2000... Training loss: 4.9586\n",
      "Epoch: 38/2000... Training loss: 4.9911\n",
      "Epoch: 38/2000... Training loss: 5.1991\n",
      "Epoch: 38/2000... Training loss: 4.8777\n",
      "Epoch: 38/2000... Training loss: 4.9854\n",
      "Epoch: 38/2000... Training loss: 4.8956\n",
      "Epoch: 38/2000... Training loss: 5.1772\n",
      "Epoch: 38/2000... Training loss: 5.1976\n",
      "Epoch: 38/2000... Training loss: 4.9097\n",
      "Epoch: 38/2000... Training loss: 4.9806\n",
      "Epoch: 38/2000... Training loss: 5.2337\n",
      "Epoch: 38/2000... Training loss: 5.0420\n",
      "Epoch: 38/2000... Training loss: 4.9913\n",
      "Epoch: 38/2000... Training loss: 5.0650\n",
      "Epoch: 38/2000... Training loss: 5.0685\n",
      "Epoch: 39/2000... Training loss: 4.9555\n",
      "Epoch: 39/2000... Training loss: 4.9392\n",
      "Epoch: 39/2000... Training loss: 4.8393\n",
      "Epoch: 39/2000... Training loss: 4.9926\n",
      "Epoch: 39/2000... Training loss: 4.9182\n",
      "Epoch: 39/2000... Training loss: 4.9887\n",
      "Epoch: 39/2000... Training loss: 4.8877\n",
      "Epoch: 39/2000... Training loss: 4.9826\n",
      "Epoch: 39/2000... Training loss: 4.8214\n",
      "Epoch: 39/2000... Training loss: 5.0607\n",
      "Epoch: 39/2000... Training loss: 4.9875\n",
      "Epoch: 39/2000... Training loss: 5.1235\n",
      "Epoch: 39/2000... Training loss: 4.8152\n",
      "Epoch: 39/2000... Training loss: 4.9956\n",
      "Epoch: 39/2000... Training loss: 5.0380\n",
      "Epoch: 39/2000... Training loss: 4.9590\n",
      "Epoch: 39/2000... Training loss: 5.0791\n",
      "Epoch: 39/2000... Training loss: 4.9897\n",
      "Epoch: 39/2000... Training loss: 5.0892\n",
      "Epoch: 39/2000... Training loss: 4.9219\n",
      "Epoch: 39/2000... Training loss: 4.9675\n",
      "Epoch: 39/2000... Training loss: 4.8950\n",
      "Epoch: 39/2000... Training loss: 4.9239\n",
      "Epoch: 39/2000... Training loss: 4.9701\n",
      "Epoch: 39/2000... Training loss: 5.0444\n",
      "Epoch: 39/2000... Training loss: 4.8145\n",
      "Epoch: 39/2000... Training loss: 5.0219\n",
      "Epoch: 39/2000... Training loss: 4.9984\n",
      "Epoch: 39/2000... Training loss: 5.0248\n",
      "Epoch: 39/2000... Training loss: 4.7521\n",
      "Epoch: 39/2000... Training loss: 5.1165\n",
      "Epoch: 40/2000... Training loss: 5.0038\n",
      "Epoch: 40/2000... Training loss: 4.7471\n",
      "Epoch: 40/2000... Training loss: 4.9992\n",
      "Epoch: 40/2000... Training loss: 4.9648\n",
      "Epoch: 40/2000... Training loss: 4.9485\n",
      "Epoch: 40/2000... Training loss: 4.9420\n",
      "Epoch: 40/2000... Training loss: 5.1293\n",
      "Epoch: 40/2000... Training loss: 4.9953\n",
      "Epoch: 40/2000... Training loss: 4.9790\n",
      "Epoch: 40/2000... Training loss: 4.9806\n",
      "Epoch: 40/2000... Training loss: 5.0685\n",
      "Epoch: 40/2000... Training loss: 4.9643\n",
      "Epoch: 40/2000... Training loss: 4.7294\n",
      "Epoch: 40/2000... Training loss: 4.9753\n",
      "Epoch: 40/2000... Training loss: 5.0749\n",
      "Epoch: 40/2000... Training loss: 4.9488\n",
      "Epoch: 40/2000... Training loss: 4.9824\n",
      "Epoch: 40/2000... Training loss: 5.0605\n",
      "Epoch: 40/2000... Training loss: 4.9246\n",
      "Epoch: 40/2000... Training loss: 4.9194\n",
      "Epoch: 40/2000... Training loss: 5.0197\n",
      "Epoch: 40/2000... Training loss: 5.0084\n",
      "Epoch: 40/2000... Training loss: 5.1151\n",
      "Epoch: 40/2000... Training loss: 5.0198\n",
      "Epoch: 40/2000... Training loss: 4.9525\n",
      "Epoch: 40/2000... Training loss: 4.9905\n",
      "Epoch: 40/2000... Training loss: 4.8827\n",
      "Epoch: 40/2000... Training loss: 5.1409\n",
      "Epoch: 40/2000... Training loss: 5.0389\n",
      "Epoch: 40/2000... Training loss: 4.7868\n",
      "Epoch: 40/2000... Training loss: 4.9515\n",
      "Epoch: 41/2000... Training loss: 4.9400\n",
      "Epoch: 41/2000... Training loss: 4.8049\n",
      "Epoch: 41/2000... Training loss: 4.8659\n",
      "Epoch: 41/2000... Training loss: 4.8782\n",
      "Epoch: 41/2000... Training loss: 4.8778\n",
      "Epoch: 41/2000... Training loss: 4.8448\n",
      "Epoch: 41/2000... Training loss: 4.7790\n",
      "Epoch: 41/2000... Training loss: 5.0371\n",
      "Epoch: 41/2000... Training loss: 4.8076\n",
      "Epoch: 41/2000... Training loss: 4.9956\n",
      "Epoch: 41/2000... Training loss: 4.9244\n",
      "Epoch: 41/2000... Training loss: 4.8712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/2000... Training loss: 4.8674\n",
      "Epoch: 41/2000... Training loss: 4.9960\n",
      "Epoch: 41/2000... Training loss: 5.0131\n",
      "Epoch: 41/2000... Training loss: 5.0312\n",
      "Epoch: 41/2000... Training loss: 5.0962\n",
      "Epoch: 41/2000... Training loss: 5.0052\n",
      "Epoch: 41/2000... Training loss: 5.1209\n",
      "Epoch: 41/2000... Training loss: 4.7695\n",
      "Epoch: 41/2000... Training loss: 5.0014\n",
      "Epoch: 41/2000... Training loss: 4.7479\n",
      "Epoch: 41/2000... Training loss: 4.8040\n",
      "Epoch: 41/2000... Training loss: 5.0171\n",
      "Epoch: 41/2000... Training loss: 5.1064\n",
      "Epoch: 41/2000... Training loss: 4.9855\n",
      "Epoch: 41/2000... Training loss: 4.9610\n",
      "Epoch: 41/2000... Training loss: 4.7589\n",
      "Epoch: 41/2000... Training loss: 5.0272\n",
      "Epoch: 41/2000... Training loss: 4.6869\n",
      "Epoch: 41/2000... Training loss: 4.9836\n",
      "Epoch: 42/2000... Training loss: 4.9444\n",
      "Epoch: 42/2000... Training loss: 4.7260\n",
      "Epoch: 42/2000... Training loss: 4.8343\n",
      "Epoch: 42/2000... Training loss: 4.9857\n",
      "Epoch: 42/2000... Training loss: 4.9670\n",
      "Epoch: 42/2000... Training loss: 5.0305\n",
      "Epoch: 42/2000... Training loss: 4.8929\n",
      "Epoch: 42/2000... Training loss: 4.9411\n",
      "Epoch: 42/2000... Training loss: 4.7885\n",
      "Epoch: 42/2000... Training loss: 5.0343\n",
      "Epoch: 42/2000... Training loss: 5.0545\n",
      "Epoch: 42/2000... Training loss: 4.8210\n",
      "Epoch: 42/2000... Training loss: 4.8813\n",
      "Epoch: 42/2000... Training loss: 4.9418\n",
      "Epoch: 42/2000... Training loss: 4.9668\n",
      "Epoch: 42/2000... Training loss: 4.6926\n",
      "Epoch: 42/2000... Training loss: 4.9736\n",
      "Epoch: 42/2000... Training loss: 4.9527\n",
      "Epoch: 42/2000... Training loss: 4.9124\n",
      "Epoch: 42/2000... Training loss: 4.7977\n",
      "Epoch: 42/2000... Training loss: 4.8343\n",
      "Epoch: 42/2000... Training loss: 4.8916\n",
      "Epoch: 42/2000... Training loss: 4.8479\n",
      "Epoch: 42/2000... Training loss: 4.9707\n",
      "Epoch: 42/2000... Training loss: 4.8114\n",
      "Epoch: 42/2000... Training loss: 5.0307\n",
      "Epoch: 42/2000... Training loss: 5.0753\n",
      "Epoch: 42/2000... Training loss: 5.0839\n",
      "Epoch: 42/2000... Training loss: 4.7832\n",
      "Epoch: 42/2000... Training loss: 4.7351\n",
      "Epoch: 42/2000... Training loss: 4.9421\n",
      "Epoch: 43/2000... Training loss: 4.8563\n",
      "Epoch: 43/2000... Training loss: 4.6394\n",
      "Epoch: 43/2000... Training loss: 4.8629\n",
      "Epoch: 43/2000... Training loss: 4.6021\n",
      "Epoch: 43/2000... Training loss: 4.9267\n",
      "Epoch: 43/2000... Training loss: 4.8137\n",
      "Epoch: 43/2000... Training loss: 4.8636\n",
      "Epoch: 43/2000... Training loss: 4.8467\n",
      "Epoch: 43/2000... Training loss: 4.8667\n",
      "Epoch: 43/2000... Training loss: 4.8121\n",
      "Epoch: 43/2000... Training loss: 4.9666\n",
      "Epoch: 43/2000... Training loss: 4.8083\n",
      "Epoch: 43/2000... Training loss: 4.8295\n",
      "Epoch: 43/2000... Training loss: 4.9341\n",
      "Epoch: 43/2000... Training loss: 4.7996\n",
      "Epoch: 43/2000... Training loss: 4.8759\n",
      "Epoch: 43/2000... Training loss: 5.0223\n",
      "Epoch: 43/2000... Training loss: 4.9406\n",
      "Epoch: 43/2000... Training loss: 4.9357\n",
      "Epoch: 43/2000... Training loss: 4.8486\n",
      "Epoch: 43/2000... Training loss: 4.7743\n",
      "Epoch: 43/2000... Training loss: 4.7644\n",
      "Epoch: 43/2000... Training loss: 4.8588\n",
      "Epoch: 43/2000... Training loss: 5.0642\n",
      "Epoch: 43/2000... Training loss: 4.9494\n",
      "Epoch: 43/2000... Training loss: 4.9242\n",
      "Epoch: 43/2000... Training loss: 4.7287\n",
      "Epoch: 43/2000... Training loss: 5.0150\n",
      "Epoch: 43/2000... Training loss: 4.9616\n",
      "Epoch: 43/2000... Training loss: 4.8052\n",
      "Epoch: 43/2000... Training loss: 4.7128\n",
      "Epoch: 44/2000... Training loss: 4.6650\n",
      "Epoch: 44/2000... Training loss: 4.8124\n",
      "Epoch: 44/2000... Training loss: 4.7342\n",
      "Epoch: 44/2000... Training loss: 4.8077\n",
      "Epoch: 44/2000... Training loss: 4.8580\n",
      "Epoch: 44/2000... Training loss: 4.7628\n",
      "Epoch: 44/2000... Training loss: 4.7124\n",
      "Epoch: 44/2000... Training loss: 4.9282\n",
      "Epoch: 44/2000... Training loss: 4.7729\n",
      "Epoch: 44/2000... Training loss: 4.8604\n",
      "Epoch: 44/2000... Training loss: 4.9281\n",
      "Epoch: 44/2000... Training loss: 4.7425\n",
      "Epoch: 44/2000... Training loss: 4.7458\n",
      "Epoch: 44/2000... Training loss: 4.7651\n",
      "Epoch: 44/2000... Training loss: 4.7982\n",
      "Epoch: 44/2000... Training loss: 4.8143\n",
      "Epoch: 44/2000... Training loss: 4.9115\n",
      "Epoch: 44/2000... Training loss: 4.8553\n",
      "Epoch: 44/2000... Training loss: 4.8367\n",
      "Epoch: 44/2000... Training loss: 4.8076\n",
      "Epoch: 44/2000... Training loss: 4.7595\n",
      "Epoch: 44/2000... Training loss: 4.8513\n",
      "Epoch: 44/2000... Training loss: 4.9070\n",
      "Epoch: 44/2000... Training loss: 4.9148\n",
      "Epoch: 44/2000... Training loss: 4.7649\n",
      "Epoch: 44/2000... Training loss: 4.8407\n",
      "Epoch: 44/2000... Training loss: 4.8524\n",
      "Epoch: 44/2000... Training loss: 4.9423\n",
      "Epoch: 44/2000... Training loss: 4.8865\n",
      "Epoch: 44/2000... Training loss: 4.6496\n",
      "Epoch: 44/2000... Training loss: 4.7627\n",
      "Epoch: 45/2000... Training loss: 4.7363\n",
      "Epoch: 45/2000... Training loss: 4.6090\n",
      "Epoch: 45/2000... Training loss: 4.7380\n",
      "Epoch: 45/2000... Training loss: 4.8855\n",
      "Epoch: 45/2000... Training loss: 4.8324\n",
      "Epoch: 45/2000... Training loss: 4.6463\n",
      "Epoch: 45/2000... Training loss: 4.6492\n",
      "Epoch: 45/2000... Training loss: 4.8764\n",
      "Epoch: 45/2000... Training loss: 4.7218\n",
      "Epoch: 45/2000... Training loss: 4.5991\n",
      "Epoch: 45/2000... Training loss: 4.6748\n",
      "Epoch: 45/2000... Training loss: 4.8351\n",
      "Epoch: 45/2000... Training loss: 4.7603\n",
      "Epoch: 45/2000... Training loss: 4.9011\n",
      "Epoch: 45/2000... Training loss: 4.8694\n",
      "Epoch: 45/2000... Training loss: 4.6851\n",
      "Epoch: 45/2000... Training loss: 4.8816\n",
      "Epoch: 45/2000... Training loss: 4.7858\n",
      "Epoch: 45/2000... Training loss: 4.9523\n",
      "Epoch: 45/2000... Training loss: 4.9231\n",
      "Epoch: 45/2000... Training loss: 4.8417\n",
      "Epoch: 45/2000... Training loss: 4.7176\n",
      "Epoch: 45/2000... Training loss: 4.8856\n",
      "Epoch: 45/2000... Training loss: 4.8740\n",
      "Epoch: 45/2000... Training loss: 4.7865\n",
      "Epoch: 45/2000... Training loss: 4.9432\n",
      "Epoch: 45/2000... Training loss: 4.6800\n",
      "Epoch: 45/2000... Training loss: 4.8328\n",
      "Epoch: 45/2000... Training loss: 4.9941\n",
      "Epoch: 45/2000... Training loss: 4.8726\n",
      "Epoch: 45/2000... Training loss: 4.6347\n",
      "Epoch: 46/2000... Training loss: 4.7294\n",
      "Epoch: 46/2000... Training loss: 4.6994\n",
      "Epoch: 46/2000... Training loss: 4.6910\n",
      "Epoch: 46/2000... Training loss: 4.8513\n",
      "Epoch: 46/2000... Training loss: 4.7035\n",
      "Epoch: 46/2000... Training loss: 4.6617\n",
      "Epoch: 46/2000... Training loss: 4.9112\n",
      "Epoch: 46/2000... Training loss: 4.7417\n",
      "Epoch: 46/2000... Training loss: 4.6858\n",
      "Epoch: 46/2000... Training loss: 4.8701\n",
      "Epoch: 46/2000... Training loss: 4.7152\n",
      "Epoch: 46/2000... Training loss: 4.6512\n",
      "Epoch: 46/2000... Training loss: 4.7231\n",
      "Epoch: 46/2000... Training loss: 4.6471\n",
      "Epoch: 46/2000... Training loss: 4.7144\n",
      "Epoch: 46/2000... Training loss: 4.7281\n",
      "Epoch: 46/2000... Training loss: 4.9129\n",
      "Epoch: 46/2000... Training loss: 4.7068\n",
      "Epoch: 46/2000... Training loss: 4.7646\n",
      "Epoch: 46/2000... Training loss: 4.6958\n",
      "Epoch: 46/2000... Training loss: 4.6380\n",
      "Epoch: 46/2000... Training loss: 4.6717\n",
      "Epoch: 46/2000... Training loss: 4.7589\n",
      "Epoch: 46/2000... Training loss: 4.8854\n",
      "Epoch: 46/2000... Training loss: 4.5428\n",
      "Epoch: 46/2000... Training loss: 4.4898\n",
      "Epoch: 46/2000... Training loss: 4.8956\n",
      "Epoch: 46/2000... Training loss: 4.7250\n",
      "Epoch: 46/2000... Training loss: 5.0298\n",
      "Epoch: 46/2000... Training loss: 4.5118\n",
      "Epoch: 46/2000... Training loss: 4.9731\n",
      "Epoch: 47/2000... Training loss: 4.6668\n",
      "Epoch: 47/2000... Training loss: 4.4242\n",
      "Epoch: 47/2000... Training loss: 4.6634\n",
      "Epoch: 47/2000... Training loss: 4.5009\n",
      "Epoch: 47/2000... Training loss: 4.6612\n",
      "Epoch: 47/2000... Training loss: 4.6104\n",
      "Epoch: 47/2000... Training loss: 4.6103\n",
      "Epoch: 47/2000... Training loss: 4.8847\n",
      "Epoch: 47/2000... Training loss: 4.5209\n",
      "Epoch: 47/2000... Training loss: 4.9063\n",
      "Epoch: 47/2000... Training loss: 4.8743\n",
      "Epoch: 47/2000... Training loss: 4.5939\n",
      "Epoch: 47/2000... Training loss: 4.6683\n",
      "Epoch: 47/2000... Training loss: 4.8147\n",
      "Epoch: 47/2000... Training loss: 4.8649\n",
      "Epoch: 47/2000... Training loss: 4.7530\n",
      "Epoch: 47/2000... Training loss: 4.8768\n",
      "Epoch: 47/2000... Training loss: 4.8017\n",
      "Epoch: 47/2000... Training loss: 4.7673\n",
      "Epoch: 47/2000... Training loss: 4.6311\n",
      "Epoch: 47/2000... Training loss: 4.6623\n",
      "Epoch: 47/2000... Training loss: 4.7769\n",
      "Epoch: 47/2000... Training loss: 4.7298\n",
      "Epoch: 47/2000... Training loss: 4.7263\n",
      "Epoch: 47/2000... Training loss: 4.7082\n",
      "Epoch: 47/2000... Training loss: 4.7237\n",
      "Epoch: 47/2000... Training loss: 4.7393\n",
      "Epoch: 47/2000... Training loss: 4.5949\n",
      "Epoch: 47/2000... Training loss: 4.7949\n",
      "Epoch: 47/2000... Training loss: 4.4460\n",
      "Epoch: 47/2000... Training loss: 4.6767\n",
      "Epoch: 48/2000... Training loss: 4.6609\n",
      "Epoch: 48/2000... Training loss: 4.7318\n",
      "Epoch: 48/2000... Training loss: 4.5524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48/2000... Training loss: 4.5208\n",
      "Epoch: 48/2000... Training loss: 4.5771\n",
      "Epoch: 48/2000... Training loss: 4.6630\n",
      "Epoch: 48/2000... Training loss: 4.5836\n",
      "Epoch: 48/2000... Training loss: 4.7156\n",
      "Epoch: 48/2000... Training loss: 4.6548\n",
      "Epoch: 48/2000... Training loss: 4.6075\n",
      "Epoch: 48/2000... Training loss: 4.6210\n",
      "Epoch: 48/2000... Training loss: 4.5835\n",
      "Epoch: 48/2000... Training loss: 4.6107\n",
      "Epoch: 48/2000... Training loss: 4.6583\n",
      "Epoch: 48/2000... Training loss: 4.7146\n",
      "Epoch: 48/2000... Training loss: 4.6671\n",
      "Epoch: 48/2000... Training loss: 4.7585\n",
      "Epoch: 48/2000... Training loss: 4.6897\n",
      "Epoch: 48/2000... Training loss: 4.6350\n",
      "Epoch: 48/2000... Training loss: 4.4194\n",
      "Epoch: 48/2000... Training loss: 4.8798\n",
      "Epoch: 48/2000... Training loss: 4.5876\n",
      "Epoch: 48/2000... Training loss: 4.8420\n",
      "Epoch: 48/2000... Training loss: 4.7515\n",
      "Epoch: 48/2000... Training loss: 4.7944\n",
      "Epoch: 48/2000... Training loss: 4.5868\n",
      "Epoch: 48/2000... Training loss: 4.8066\n",
      "Epoch: 48/2000... Training loss: 4.6148\n",
      "Epoch: 48/2000... Training loss: 4.6025\n",
      "Epoch: 48/2000... Training loss: 4.5817\n",
      "Epoch: 48/2000... Training loss: 4.5782\n",
      "Epoch: 49/2000... Training loss: 4.7191\n",
      "Epoch: 49/2000... Training loss: 4.3891\n",
      "Epoch: 49/2000... Training loss: 4.7787\n",
      "Epoch: 49/2000... Training loss: 4.7470\n",
      "Epoch: 49/2000... Training loss: 4.6195\n",
      "Epoch: 49/2000... Training loss: 4.7266\n",
      "Epoch: 49/2000... Training loss: 4.6166\n",
      "Epoch: 49/2000... Training loss: 4.6781\n",
      "Epoch: 49/2000... Training loss: 4.7504\n",
      "Epoch: 49/2000... Training loss: 4.7807\n",
      "Epoch: 49/2000... Training loss: 4.6038\n",
      "Epoch: 49/2000... Training loss: 4.6245\n",
      "Epoch: 49/2000... Training loss: 4.6296\n",
      "Epoch: 49/2000... Training loss: 4.6865\n",
      "Epoch: 49/2000... Training loss: 4.8680\n",
      "Epoch: 49/2000... Training loss: 4.8223\n",
      "Epoch: 49/2000... Training loss: 4.8358\n",
      "Epoch: 49/2000... Training loss: 4.4368\n",
      "Epoch: 49/2000... Training loss: 4.5120\n",
      "Epoch: 49/2000... Training loss: 4.3613\n",
      "Epoch: 49/2000... Training loss: 4.6644\n",
      "Epoch: 49/2000... Training loss: 4.5697\n",
      "Epoch: 49/2000... Training loss: 4.8535\n",
      "Epoch: 49/2000... Training loss: 4.8008\n",
      "Epoch: 49/2000... Training loss: 4.7021\n",
      "Epoch: 49/2000... Training loss: 4.8732\n",
      "Epoch: 49/2000... Training loss: 4.6944\n",
      "Epoch: 49/2000... Training loss: 4.5251\n",
      "Epoch: 49/2000... Training loss: 4.5983\n",
      "Epoch: 49/2000... Training loss: 4.4995\n",
      "Epoch: 49/2000... Training loss: 4.7204\n",
      "Epoch: 50/2000... Training loss: 4.4792\n",
      "Epoch: 50/2000... Training loss: 4.5303\n",
      "Epoch: 50/2000... Training loss: 4.5448\n",
      "Epoch: 50/2000... Training loss: 4.4198\n",
      "Epoch: 50/2000... Training loss: 4.6720\n",
      "Epoch: 50/2000... Training loss: 4.4278\n",
      "Epoch: 50/2000... Training loss: 4.5803\n",
      "Epoch: 50/2000... Training loss: 4.5651\n",
      "Epoch: 50/2000... Training loss: 4.6922\n",
      "Epoch: 50/2000... Training loss: 4.6934\n",
      "Epoch: 50/2000... Training loss: 4.8015\n",
      "Epoch: 50/2000... Training loss: 4.7902\n",
      "Epoch: 50/2000... Training loss: 4.4953\n",
      "Epoch: 50/2000... Training loss: 4.5893\n",
      "Epoch: 50/2000... Training loss: 4.6300\n",
      "Epoch: 50/2000... Training loss: 4.6617\n",
      "Epoch: 50/2000... Training loss: 4.8262\n",
      "Epoch: 50/2000... Training loss: 4.7763\n",
      "Epoch: 50/2000... Training loss: 4.6545\n",
      "Epoch: 50/2000... Training loss: 4.5543\n",
      "Epoch: 50/2000... Training loss: 4.5937\n",
      "Epoch: 50/2000... Training loss: 4.7409\n",
      "Epoch: 50/2000... Training loss: 4.5158\n",
      "Epoch: 50/2000... Training loss: 4.6890\n",
      "Epoch: 50/2000... Training loss: 4.4880\n",
      "Epoch: 50/2000... Training loss: 4.5104\n",
      "Epoch: 50/2000... Training loss: 4.7109\n",
      "Epoch: 50/2000... Training loss: 4.7199\n",
      "Epoch: 50/2000... Training loss: 4.6827\n",
      "Epoch: 50/2000... Training loss: 4.4876\n",
      "Epoch: 50/2000... Training loss: 4.5982\n",
      "Epoch: 51/2000... Training loss: 4.5437\n",
      "Epoch: 51/2000... Training loss: 4.5821\n",
      "Epoch: 51/2000... Training loss: 4.6119\n",
      "Epoch: 51/2000... Training loss: 4.2832\n",
      "Epoch: 51/2000... Training loss: 4.6030\n",
      "Epoch: 51/2000... Training loss: 4.3584\n",
      "Epoch: 51/2000... Training loss: 4.5085\n",
      "Epoch: 51/2000... Training loss: 4.4984\n",
      "Epoch: 51/2000... Training loss: 4.6098\n",
      "Epoch: 51/2000... Training loss: 4.8006\n",
      "Epoch: 51/2000... Training loss: 4.6992\n",
      "Epoch: 51/2000... Training loss: 4.4820\n",
      "Epoch: 51/2000... Training loss: 4.8212\n",
      "Epoch: 51/2000... Training loss: 4.4399\n",
      "Epoch: 51/2000... Training loss: 4.4740\n",
      "Epoch: 51/2000... Training loss: 4.6737\n",
      "Epoch: 51/2000... Training loss: 4.6817\n",
      "Epoch: 51/2000... Training loss: 4.6485\n",
      "Epoch: 51/2000... Training loss: 4.6062\n",
      "Epoch: 51/2000... Training loss: 4.4726\n",
      "Epoch: 51/2000... Training loss: 4.6220\n",
      "Epoch: 51/2000... Training loss: 4.6624\n",
      "Epoch: 51/2000... Training loss: 4.5184\n",
      "Epoch: 51/2000... Training loss: 4.5794\n",
      "Epoch: 51/2000... Training loss: 4.4822\n",
      "Epoch: 51/2000... Training loss: 4.5326\n",
      "Epoch: 51/2000... Training loss: 4.6206\n",
      "Epoch: 51/2000... Training loss: 4.4816\n",
      "Epoch: 51/2000... Training loss: 4.5059\n",
      "Epoch: 51/2000... Training loss: 4.6030\n",
      "Epoch: 51/2000... Training loss: 4.5291\n",
      "Epoch: 52/2000... Training loss: 4.6368\n",
      "Epoch: 52/2000... Training loss: 4.5016\n",
      "Epoch: 52/2000... Training loss: 4.5352\n",
      "Epoch: 52/2000... Training loss: 4.5147\n",
      "Epoch: 52/2000... Training loss: 4.3569\n",
      "Epoch: 52/2000... Training loss: 4.3336\n",
      "Epoch: 52/2000... Training loss: 4.4668\n",
      "Epoch: 52/2000... Training loss: 4.4390\n",
      "Epoch: 52/2000... Training loss: 4.6209\n",
      "Epoch: 52/2000... Training loss: 4.8229\n",
      "Epoch: 52/2000... Training loss: 4.5383\n",
      "Epoch: 52/2000... Training loss: 4.2803\n",
      "Epoch: 52/2000... Training loss: 4.5948\n",
      "Epoch: 52/2000... Training loss: 4.5398\n",
      "Epoch: 52/2000... Training loss: 4.7514\n",
      "Epoch: 52/2000... Training loss: 4.7492\n",
      "Epoch: 52/2000... Training loss: 4.6117\n",
      "Epoch: 52/2000... Training loss: 4.5226\n",
      "Epoch: 52/2000... Training loss: 4.5705\n",
      "Epoch: 52/2000... Training loss: 4.3795\n",
      "Epoch: 52/2000... Training loss: 4.5313\n",
      "Epoch: 52/2000... Training loss: 4.6908\n",
      "Epoch: 52/2000... Training loss: 4.5370\n",
      "Epoch: 52/2000... Training loss: 4.7021\n",
      "Epoch: 52/2000... Training loss: 4.5609\n",
      "Epoch: 52/2000... Training loss: 4.8364\n",
      "Epoch: 52/2000... Training loss: 4.5375\n",
      "Epoch: 52/2000... Training loss: 4.5152\n",
      "Epoch: 52/2000... Training loss: 4.4436\n",
      "Epoch: 52/2000... Training loss: 4.6175\n",
      "Epoch: 52/2000... Training loss: 4.5481\n",
      "Epoch: 53/2000... Training loss: 4.6192\n",
      "Epoch: 53/2000... Training loss: 4.3924\n",
      "Epoch: 53/2000... Training loss: 4.4552\n",
      "Epoch: 53/2000... Training loss: 4.4867\n",
      "Epoch: 53/2000... Training loss: 4.4373\n",
      "Epoch: 53/2000... Training loss: 4.4043\n",
      "Epoch: 53/2000... Training loss: 4.4430\n",
      "Epoch: 53/2000... Training loss: 4.5788\n",
      "Epoch: 53/2000... Training loss: 4.4600\n",
      "Epoch: 53/2000... Training loss: 4.6145\n",
      "Epoch: 53/2000... Training loss: 4.5875\n",
      "Epoch: 53/2000... Training loss: 4.4886\n",
      "Epoch: 53/2000... Training loss: 4.5637\n",
      "Epoch: 53/2000... Training loss: 4.5600\n",
      "Epoch: 53/2000... Training loss: 4.6060\n",
      "Epoch: 53/2000... Training loss: 4.4946\n",
      "Epoch: 53/2000... Training loss: 4.7031\n",
      "Epoch: 53/2000... Training loss: 4.3767\n",
      "Epoch: 53/2000... Training loss: 4.5837\n",
      "Epoch: 53/2000... Training loss: 4.5832\n",
      "Epoch: 53/2000... Training loss: 4.6421\n",
      "Epoch: 53/2000... Training loss: 4.4464\n",
      "Epoch: 53/2000... Training loss: 4.5008\n",
      "Epoch: 53/2000... Training loss: 4.5181\n",
      "Epoch: 53/2000... Training loss: 4.4056\n",
      "Epoch: 53/2000... Training loss: 4.6122\n",
      "Epoch: 53/2000... Training loss: 4.3222\n",
      "Epoch: 53/2000... Training loss: 4.4975\n",
      "Epoch: 53/2000... Training loss: 4.3982\n",
      "Epoch: 53/2000... Training loss: 4.4241\n",
      "Epoch: 53/2000... Training loss: 4.4558\n",
      "Epoch: 54/2000... Training loss: 4.5569\n",
      "Epoch: 54/2000... Training loss: 4.3134\n",
      "Epoch: 54/2000... Training loss: 4.4130\n",
      "Epoch: 54/2000... Training loss: 4.4909\n",
      "Epoch: 54/2000... Training loss: 4.5410\n",
      "Epoch: 54/2000... Training loss: 4.4681\n",
      "Epoch: 54/2000... Training loss: 4.5037\n",
      "Epoch: 54/2000... Training loss: 4.5145\n",
      "Epoch: 54/2000... Training loss: 4.4296\n",
      "Epoch: 54/2000... Training loss: 4.6667\n",
      "Epoch: 54/2000... Training loss: 4.5317\n",
      "Epoch: 54/2000... Training loss: 4.2598\n",
      "Epoch: 54/2000... Training loss: 4.5879\n",
      "Epoch: 54/2000... Training loss: 4.4409\n",
      "Epoch: 54/2000... Training loss: 4.4322\n",
      "Epoch: 54/2000... Training loss: 4.4560\n",
      "Epoch: 54/2000... Training loss: 4.5531\n",
      "Epoch: 54/2000... Training loss: 4.3628\n",
      "Epoch: 54/2000... Training loss: 4.4563\n",
      "Epoch: 54/2000... Training loss: 4.5205\n",
      "Epoch: 54/2000... Training loss: 4.4203\n",
      "Epoch: 54/2000... Training loss: 4.4802\n",
      "Epoch: 54/2000... Training loss: 4.6254\n",
      "Epoch: 54/2000... Training loss: 4.7216\n",
      "Epoch: 54/2000... Training loss: 4.3682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54/2000... Training loss: 4.7216\n",
      "Epoch: 54/2000... Training loss: 4.6429\n",
      "Epoch: 54/2000... Training loss: 4.6674\n",
      "Epoch: 54/2000... Training loss: 4.5146\n",
      "Epoch: 54/2000... Training loss: 4.4171\n",
      "Epoch: 54/2000... Training loss: 4.4351\n",
      "Epoch: 55/2000... Training loss: 4.4671\n",
      "Epoch: 55/2000... Training loss: 4.3300\n",
      "Epoch: 55/2000... Training loss: 4.4213\n",
      "Epoch: 55/2000... Training loss: 4.3572\n",
      "Epoch: 55/2000... Training loss: 4.3874\n",
      "Epoch: 55/2000... Training loss: 4.4158\n",
      "Epoch: 55/2000... Training loss: 4.4405\n",
      "Epoch: 55/2000... Training loss: 4.5488\n",
      "Epoch: 55/2000... Training loss: 4.4515\n",
      "Epoch: 55/2000... Training loss: 4.6169\n",
      "Epoch: 55/2000... Training loss: 4.5610\n",
      "Epoch: 55/2000... Training loss: 4.4512\n",
      "Epoch: 55/2000... Training loss: 4.4690\n",
      "Epoch: 55/2000... Training loss: 4.2821\n",
      "Epoch: 55/2000... Training loss: 4.5494\n",
      "Epoch: 55/2000... Training loss: 4.6373\n",
      "Epoch: 55/2000... Training loss: 4.4121\n",
      "Epoch: 55/2000... Training loss: 4.6399\n",
      "Epoch: 55/2000... Training loss: 4.5450\n",
      "Epoch: 55/2000... Training loss: 4.4393\n",
      "Epoch: 55/2000... Training loss: 4.5481\n",
      "Epoch: 55/2000... Training loss: 4.4842\n",
      "Epoch: 55/2000... Training loss: 4.3184\n",
      "Epoch: 55/2000... Training loss: 4.4433\n",
      "Epoch: 55/2000... Training loss: 4.4728\n",
      "Epoch: 55/2000... Training loss: 4.4754\n",
      "Epoch: 55/2000... Training loss: 4.5391\n",
      "Epoch: 55/2000... Training loss: 4.6171\n",
      "Epoch: 55/2000... Training loss: 4.5373\n",
      "Epoch: 55/2000... Training loss: 4.4955\n",
      "Epoch: 55/2000... Training loss: 4.6432\n",
      "Epoch: 56/2000... Training loss: 4.4728\n",
      "Epoch: 56/2000... Training loss: 4.2523\n",
      "Epoch: 56/2000... Training loss: 4.4418\n",
      "Epoch: 56/2000... Training loss: 4.3513\n",
      "Epoch: 56/2000... Training loss: 4.4516\n",
      "Epoch: 56/2000... Training loss: 4.0813\n",
      "Epoch: 56/2000... Training loss: 4.3974\n",
      "Epoch: 56/2000... Training loss: 4.2577\n",
      "Epoch: 56/2000... Training loss: 4.2628\n",
      "Epoch: 56/2000... Training loss: 4.4236\n",
      "Epoch: 56/2000... Training loss: 4.6749\n",
      "Epoch: 56/2000... Training loss: 4.5063\n",
      "Epoch: 56/2000... Training loss: 4.3532\n",
      "Epoch: 56/2000... Training loss: 4.4859\n",
      "Epoch: 56/2000... Training loss: 4.2188\n",
      "Epoch: 56/2000... Training loss: 4.4235\n",
      "Epoch: 56/2000... Training loss: 4.4332\n",
      "Epoch: 56/2000... Training loss: 4.2719\n",
      "Epoch: 56/2000... Training loss: 4.4056\n",
      "Epoch: 56/2000... Training loss: 4.3769\n",
      "Epoch: 56/2000... Training loss: 4.3316\n",
      "Epoch: 56/2000... Training loss: 4.3646\n",
      "Epoch: 56/2000... Training loss: 4.5272\n",
      "Epoch: 56/2000... Training loss: 4.3915\n",
      "Epoch: 56/2000... Training loss: 4.4484\n",
      "Epoch: 56/2000... Training loss: 4.5872\n",
      "Epoch: 56/2000... Training loss: 4.3306\n",
      "Epoch: 56/2000... Training loss: 4.3630\n",
      "Epoch: 56/2000... Training loss: 4.2295\n",
      "Epoch: 56/2000... Training loss: 4.4094\n",
      "Epoch: 56/2000... Training loss: 4.4278\n",
      "Epoch: 57/2000... Training loss: 4.5588\n",
      "Epoch: 57/2000... Training loss: 4.2057\n",
      "Epoch: 57/2000... Training loss: 4.2997\n",
      "Epoch: 57/2000... Training loss: 4.4667\n",
      "Epoch: 57/2000... Training loss: 4.2855\n",
      "Epoch: 57/2000... Training loss: 4.4837\n",
      "Epoch: 57/2000... Training loss: 4.3612\n",
      "Epoch: 57/2000... Training loss: 4.3049\n",
      "Epoch: 57/2000... Training loss: 4.2005\n",
      "Epoch: 57/2000... Training loss: 4.3596\n",
      "Epoch: 57/2000... Training loss: 4.3644\n",
      "Epoch: 57/2000... Training loss: 4.3244\n",
      "Epoch: 57/2000... Training loss: 4.1879\n",
      "Epoch: 57/2000... Training loss: 4.4785\n",
      "Epoch: 57/2000... Training loss: 4.4035\n",
      "Epoch: 57/2000... Training loss: 4.5452\n",
      "Epoch: 57/2000... Training loss: 4.2829\n",
      "Epoch: 57/2000... Training loss: 4.3493\n",
      "Epoch: 57/2000... Training loss: 4.4101\n",
      "Epoch: 57/2000... Training loss: 4.2451\n",
      "Epoch: 57/2000... Training loss: 4.3568\n",
      "Epoch: 57/2000... Training loss: 4.4643\n",
      "Epoch: 57/2000... Training loss: 4.2859\n",
      "Epoch: 57/2000... Training loss: 4.3588\n",
      "Epoch: 57/2000... Training loss: 4.3830\n",
      "Epoch: 57/2000... Training loss: 4.3397\n",
      "Epoch: 57/2000... Training loss: 4.5067\n",
      "Epoch: 57/2000... Training loss: 4.2471\n",
      "Epoch: 57/2000... Training loss: 4.3029\n",
      "Epoch: 57/2000... Training loss: 4.3248\n",
      "Epoch: 57/2000... Training loss: 4.5432\n",
      "Epoch: 58/2000... Training loss: 4.3021\n",
      "Epoch: 58/2000... Training loss: 4.3085\n",
      "Epoch: 58/2000... Training loss: 4.1226\n",
      "Epoch: 58/2000... Training loss: 4.4409\n",
      "Epoch: 58/2000... Training loss: 4.5836\n",
      "Epoch: 58/2000... Training loss: 4.2601\n",
      "Epoch: 58/2000... Training loss: 4.2199\n",
      "Epoch: 58/2000... Training loss: 4.2406\n",
      "Epoch: 58/2000... Training loss: 4.4143\n",
      "Epoch: 58/2000... Training loss: 4.5143\n",
      "Epoch: 58/2000... Training loss: 4.3350\n",
      "Epoch: 58/2000... Training loss: 4.2645\n",
      "Epoch: 58/2000... Training loss: 4.2047\n",
      "Epoch: 58/2000... Training loss: 4.3477\n",
      "Epoch: 58/2000... Training loss: 4.3892\n",
      "Epoch: 58/2000... Training loss: 4.2565\n",
      "Epoch: 58/2000... Training loss: 4.2981\n",
      "Epoch: 58/2000... Training loss: 4.4371\n",
      "Epoch: 58/2000... Training loss: 4.4786\n",
      "Epoch: 58/2000... Training loss: 4.4252\n",
      "Epoch: 58/2000... Training loss: 4.2390\n",
      "Epoch: 58/2000... Training loss: 4.5128\n",
      "Epoch: 58/2000... Training loss: 4.3823\n",
      "Epoch: 58/2000... Training loss: 4.4089\n",
      "Epoch: 58/2000... Training loss: 4.4018\n",
      "Epoch: 58/2000... Training loss: 4.2941\n",
      "Epoch: 58/2000... Training loss: 4.3963\n",
      "Epoch: 58/2000... Training loss: 4.5893\n",
      "Epoch: 58/2000... Training loss: 4.4386\n",
      "Epoch: 58/2000... Training loss: 4.2738\n",
      "Epoch: 58/2000... Training loss: 4.4134\n",
      "Epoch: 59/2000... Training loss: 4.2838\n",
      "Epoch: 59/2000... Training loss: 4.1852\n",
      "Epoch: 59/2000... Training loss: 4.3154\n",
      "Epoch: 59/2000... Training loss: 4.1236\n",
      "Epoch: 59/2000... Training loss: 4.3703\n",
      "Epoch: 59/2000... Training loss: 4.2209\n",
      "Epoch: 59/2000... Training loss: 4.0239\n",
      "Epoch: 59/2000... Training loss: 4.2886\n",
      "Epoch: 59/2000... Training loss: 4.4148\n",
      "Epoch: 59/2000... Training loss: 4.3619\n",
      "Epoch: 59/2000... Training loss: 4.6738\n",
      "Epoch: 59/2000... Training loss: 4.2793\n",
      "Epoch: 59/2000... Training loss: 4.3660\n",
      "Epoch: 59/2000... Training loss: 4.1674\n",
      "Epoch: 59/2000... Training loss: 4.2983\n",
      "Epoch: 59/2000... Training loss: 4.4126\n",
      "Epoch: 59/2000... Training loss: 4.5235\n",
      "Epoch: 59/2000... Training loss: 4.2839\n",
      "Epoch: 59/2000... Training loss: 4.4039\n",
      "Epoch: 59/2000... Training loss: 4.2743\n",
      "Epoch: 59/2000... Training loss: 4.2441\n",
      "Epoch: 59/2000... Training loss: 4.4698\n",
      "Epoch: 59/2000... Training loss: 4.2880\n",
      "Epoch: 59/2000... Training loss: 4.3762\n",
      "Epoch: 59/2000... Training loss: 4.3830\n",
      "Epoch: 59/2000... Training loss: 4.4312\n",
      "Epoch: 59/2000... Training loss: 4.4139\n",
      "Epoch: 59/2000... Training loss: 4.3046\n",
      "Epoch: 59/2000... Training loss: 4.1221\n",
      "Epoch: 59/2000... Training loss: 4.3737\n",
      "Epoch: 59/2000... Training loss: 4.1418\n",
      "Epoch: 60/2000... Training loss: 4.3295\n",
      "Epoch: 60/2000... Training loss: 4.2100\n",
      "Epoch: 60/2000... Training loss: 4.1907\n",
      "Epoch: 60/2000... Training loss: 4.2637\n",
      "Epoch: 60/2000... Training loss: 4.2900\n",
      "Epoch: 60/2000... Training loss: 4.1529\n",
      "Epoch: 60/2000... Training loss: 4.1445\n",
      "Epoch: 60/2000... Training loss: 4.2752\n",
      "Epoch: 60/2000... Training loss: 4.4284\n",
      "Epoch: 60/2000... Training loss: 4.3527\n",
      "Epoch: 60/2000... Training loss: 4.4849\n",
      "Epoch: 60/2000... Training loss: 4.2431\n",
      "Epoch: 60/2000... Training loss: 4.1715\n",
      "Epoch: 60/2000... Training loss: 4.3244\n",
      "Epoch: 60/2000... Training loss: 4.3514\n",
      "Epoch: 60/2000... Training loss: 4.2596\n",
      "Epoch: 60/2000... Training loss: 4.3601\n",
      "Epoch: 60/2000... Training loss: 4.3481\n",
      "Epoch: 60/2000... Training loss: 4.3620\n",
      "Epoch: 60/2000... Training loss: 4.0342\n",
      "Epoch: 60/2000... Training loss: 4.2610\n",
      "Epoch: 60/2000... Training loss: 4.4072\n",
      "Epoch: 60/2000... Training loss: 4.4421\n",
      "Epoch: 60/2000... Training loss: 4.3724\n",
      "Epoch: 60/2000... Training loss: 4.3703\n",
      "Epoch: 60/2000... Training loss: 4.4412\n",
      "Epoch: 60/2000... Training loss: 4.3447\n",
      "Epoch: 60/2000... Training loss: 4.3030\n",
      "Epoch: 60/2000... Training loss: 4.3948\n",
      "Epoch: 60/2000... Training loss: 4.4210\n",
      "Epoch: 60/2000... Training loss: 4.2985\n",
      "Epoch: 61/2000... Training loss: 4.2297\n",
      "Epoch: 61/2000... Training loss: 4.2749\n",
      "Epoch: 61/2000... Training loss: 4.2950\n",
      "Epoch: 61/2000... Training loss: 4.2522\n",
      "Epoch: 61/2000... Training loss: 4.4577\n",
      "Epoch: 61/2000... Training loss: 4.1232\n",
      "Epoch: 61/2000... Training loss: 4.3079\n",
      "Epoch: 61/2000... Training loss: 4.2886\n",
      "Epoch: 61/2000... Training loss: 4.2845\n",
      "Epoch: 61/2000... Training loss: 4.2222\n",
      "Epoch: 61/2000... Training loss: 4.4543\n",
      "Epoch: 61/2000... Training loss: 4.1222\n",
      "Epoch: 61/2000... Training loss: 4.1448\n",
      "Epoch: 61/2000... Training loss: 4.1853\n",
      "Epoch: 61/2000... Training loss: 4.4126\n",
      "Epoch: 61/2000... Training loss: 4.1332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/2000... Training loss: 4.6986\n",
      "Epoch: 61/2000... Training loss: 4.1213\n",
      "Epoch: 61/2000... Training loss: 4.1562\n",
      "Epoch: 61/2000... Training loss: 4.1964\n",
      "Epoch: 61/2000... Training loss: 4.2968\n",
      "Epoch: 61/2000... Training loss: 4.1811\n",
      "Epoch: 61/2000... Training loss: 4.3624\n",
      "Epoch: 61/2000... Training loss: 4.4366\n",
      "Epoch: 61/2000... Training loss: 4.3525\n",
      "Epoch: 61/2000... Training loss: 4.3310\n",
      "Epoch: 61/2000... Training loss: 4.4193\n",
      "Epoch: 61/2000... Training loss: 4.3878\n",
      "Epoch: 61/2000... Training loss: 4.1894\n",
      "Epoch: 61/2000... Training loss: 4.2790\n",
      "Epoch: 61/2000... Training loss: 3.9180\n",
      "Epoch: 62/2000... Training loss: 4.0741\n",
      "Epoch: 62/2000... Training loss: 4.1148\n",
      "Epoch: 62/2000... Training loss: 4.2477\n",
      "Epoch: 62/2000... Training loss: 4.1989\n",
      "Epoch: 62/2000... Training loss: 4.2873\n",
      "Epoch: 62/2000... Training loss: 4.0930\n",
      "Epoch: 62/2000... Training loss: 4.2645\n",
      "Epoch: 62/2000... Training loss: 4.1785\n",
      "Epoch: 62/2000... Training loss: 4.1399\n",
      "Epoch: 62/2000... Training loss: 4.2556\n",
      "Epoch: 62/2000... Training loss: 4.3204\n",
      "Epoch: 62/2000... Training loss: 4.1450\n",
      "Epoch: 62/2000... Training loss: 4.1687\n",
      "Epoch: 62/2000... Training loss: 4.3771\n",
      "Epoch: 62/2000... Training loss: 4.3001\n",
      "Epoch: 62/2000... Training loss: 4.1844\n",
      "Epoch: 62/2000... Training loss: 4.3448\n",
      "Epoch: 62/2000... Training loss: 4.3386\n",
      "Epoch: 62/2000... Training loss: 4.0294\n",
      "Epoch: 62/2000... Training loss: 4.1972\n",
      "Epoch: 62/2000... Training loss: 4.3062\n",
      "Epoch: 62/2000... Training loss: 4.3143\n",
      "Epoch: 62/2000... Training loss: 4.3488\n",
      "Epoch: 62/2000... Training loss: 4.3532\n",
      "Epoch: 62/2000... Training loss: 4.3102\n",
      "Epoch: 62/2000... Training loss: 4.2958\n",
      "Epoch: 62/2000... Training loss: 4.3582\n",
      "Epoch: 62/2000... Training loss: 4.3019\n",
      "Epoch: 62/2000... Training loss: 4.2657\n",
      "Epoch: 62/2000... Training loss: 4.2210\n",
      "Epoch: 62/2000... Training loss: 4.4311\n",
      "Epoch: 63/2000... Training loss: 4.1281\n",
      "Epoch: 63/2000... Training loss: 3.8552\n",
      "Epoch: 63/2000... Training loss: 3.9818\n",
      "Epoch: 63/2000... Training loss: 4.1939\n",
      "Epoch: 63/2000... Training loss: 4.2444\n",
      "Epoch: 63/2000... Training loss: 4.1553\n",
      "Epoch: 63/2000... Training loss: 4.2153\n",
      "Epoch: 63/2000... Training loss: 4.2116\n",
      "Epoch: 63/2000... Training loss: 4.2145\n",
      "Epoch: 63/2000... Training loss: 4.2444\n",
      "Epoch: 63/2000... Training loss: 4.2562\n",
      "Epoch: 63/2000... Training loss: 4.0872\n",
      "Epoch: 63/2000... Training loss: 4.2973\n",
      "Epoch: 63/2000... Training loss: 4.1598\n",
      "Epoch: 63/2000... Training loss: 4.4207\n",
      "Epoch: 63/2000... Training loss: 4.4387\n",
      "Epoch: 63/2000... Training loss: 4.0816\n",
      "Epoch: 63/2000... Training loss: 4.2819\n",
      "Epoch: 63/2000... Training loss: 4.4612\n",
      "Epoch: 63/2000... Training loss: 4.0854\n",
      "Epoch: 63/2000... Training loss: 4.4777\n",
      "Epoch: 63/2000... Training loss: 4.3199\n",
      "Epoch: 63/2000... Training loss: 4.1271\n",
      "Epoch: 63/2000... Training loss: 4.1112\n",
      "Epoch: 63/2000... Training loss: 4.2918\n",
      "Epoch: 63/2000... Training loss: 4.3726\n",
      "Epoch: 63/2000... Training loss: 4.2921\n",
      "Epoch: 63/2000... Training loss: 4.1349\n",
      "Epoch: 63/2000... Training loss: 4.2563\n",
      "Epoch: 63/2000... Training loss: 4.2269\n",
      "Epoch: 63/2000... Training loss: 4.2022\n",
      "Epoch: 64/2000... Training loss: 4.4947\n",
      "Epoch: 64/2000... Training loss: 3.9677\n",
      "Epoch: 64/2000... Training loss: 3.9444\n",
      "Epoch: 64/2000... Training loss: 4.1277\n",
      "Epoch: 64/2000... Training loss: 4.2046\n",
      "Epoch: 64/2000... Training loss: 4.0668\n",
      "Epoch: 64/2000... Training loss: 4.1330\n",
      "Epoch: 64/2000... Training loss: 4.2410\n",
      "Epoch: 64/2000... Training loss: 3.8673\n",
      "Epoch: 64/2000... Training loss: 4.4455\n",
      "Epoch: 64/2000... Training loss: 4.3209\n",
      "Epoch: 64/2000... Training loss: 3.9881\n",
      "Epoch: 64/2000... Training loss: 4.1366\n",
      "Epoch: 64/2000... Training loss: 4.2270\n",
      "Epoch: 64/2000... Training loss: 4.2257\n",
      "Epoch: 64/2000... Training loss: 4.4541\n",
      "Epoch: 64/2000... Training loss: 4.1098\n",
      "Epoch: 64/2000... Training loss: 4.1422\n",
      "Epoch: 64/2000... Training loss: 4.2485\n",
      "Epoch: 64/2000... Training loss: 4.0992\n",
      "Epoch: 64/2000... Training loss: 4.1823\n",
      "Epoch: 64/2000... Training loss: 4.1378\n",
      "Epoch: 64/2000... Training loss: 4.3463\n",
      "Epoch: 64/2000... Training loss: 4.4798\n",
      "Epoch: 64/2000... Training loss: 4.3402\n",
      "Epoch: 64/2000... Training loss: 4.3082\n",
      "Epoch: 64/2000... Training loss: 4.1130\n",
      "Epoch: 64/2000... Training loss: 4.2395\n",
      "Epoch: 64/2000... Training loss: 4.2182\n",
      "Epoch: 64/2000... Training loss: 4.1781\n",
      "Epoch: 64/2000... Training loss: 3.9916\n",
      "Epoch: 65/2000... Training loss: 4.2936\n",
      "Epoch: 65/2000... Training loss: 4.0765\n",
      "Epoch: 65/2000... Training loss: 3.9650\n",
      "Epoch: 65/2000... Training loss: 3.8639\n",
      "Epoch: 65/2000... Training loss: 4.2383\n",
      "Epoch: 65/2000... Training loss: 4.2420\n",
      "Epoch: 65/2000... Training loss: 4.0257\n",
      "Epoch: 65/2000... Training loss: 4.2977\n",
      "Epoch: 65/2000... Training loss: 4.1681\n",
      "Epoch: 65/2000... Training loss: 4.1522\n",
      "Epoch: 65/2000... Training loss: 4.2101\n",
      "Epoch: 65/2000... Training loss: 4.1963\n",
      "Epoch: 65/2000... Training loss: 4.2453\n",
      "Epoch: 65/2000... Training loss: 4.1099\n",
      "Epoch: 65/2000... Training loss: 4.2344\n",
      "Epoch: 65/2000... Training loss: 4.3046\n",
      "Epoch: 65/2000... Training loss: 4.2181\n",
      "Epoch: 65/2000... Training loss: 4.0053\n",
      "Epoch: 65/2000... Training loss: 4.2073\n",
      "Epoch: 65/2000... Training loss: 4.2507\n",
      "Epoch: 65/2000... Training loss: 4.0833\n",
      "Epoch: 65/2000... Training loss: 4.0892\n",
      "Epoch: 65/2000... Training loss: 4.1851\n",
      "Epoch: 65/2000... Training loss: 4.3475\n",
      "Epoch: 65/2000... Training loss: 4.0176\n",
      "Epoch: 65/2000... Training loss: 4.1230\n",
      "Epoch: 65/2000... Training loss: 4.2774\n",
      "Epoch: 65/2000... Training loss: 4.2934\n",
      "Epoch: 65/2000... Training loss: 4.0667\n",
      "Epoch: 65/2000... Training loss: 4.1308\n",
      "Epoch: 65/2000... Training loss: 4.0534\n",
      "Epoch: 66/2000... Training loss: 4.1521\n",
      "Epoch: 66/2000... Training loss: 3.9113\n",
      "Epoch: 66/2000... Training loss: 3.9565\n",
      "Epoch: 66/2000... Training loss: 3.8732\n",
      "Epoch: 66/2000... Training loss: 4.0848\n",
      "Epoch: 66/2000... Training loss: 4.0967\n",
      "Epoch: 66/2000... Training loss: 4.0520\n",
      "Epoch: 66/2000... Training loss: 4.2260\n",
      "Epoch: 66/2000... Training loss: 3.9934\n",
      "Epoch: 66/2000... Training loss: 4.1325\n",
      "Epoch: 66/2000... Training loss: 4.3382\n",
      "Epoch: 66/2000... Training loss: 4.0215\n",
      "Epoch: 66/2000... Training loss: 4.2230\n",
      "Epoch: 66/2000... Training loss: 4.2376\n",
      "Epoch: 66/2000... Training loss: 4.2163\n",
      "Epoch: 66/2000... Training loss: 4.1930\n",
      "Epoch: 66/2000... Training loss: 4.0886\n",
      "Epoch: 66/2000... Training loss: 4.2426\n",
      "Epoch: 66/2000... Training loss: 4.2194\n",
      "Epoch: 66/2000... Training loss: 4.1135\n",
      "Epoch: 66/2000... Training loss: 4.1285\n",
      "Epoch: 66/2000... Training loss: 4.3289\n",
      "Epoch: 66/2000... Training loss: 4.2623\n",
      "Epoch: 66/2000... Training loss: 4.0705\n",
      "Epoch: 66/2000... Training loss: 4.2215\n",
      "Epoch: 66/2000... Training loss: 4.0325\n",
      "Epoch: 66/2000... Training loss: 4.2956\n",
      "Epoch: 66/2000... Training loss: 4.4119\n",
      "Epoch: 66/2000... Training loss: 4.1375\n",
      "Epoch: 66/2000... Training loss: 4.2195\n",
      "Epoch: 66/2000... Training loss: 4.1874\n",
      "Epoch: 67/2000... Training loss: 4.3112\n",
      "Epoch: 67/2000... Training loss: 4.1845\n",
      "Epoch: 67/2000... Training loss: 4.0931\n",
      "Epoch: 67/2000... Training loss: 3.9858\n",
      "Epoch: 67/2000... Training loss: 4.1344\n",
      "Epoch: 67/2000... Training loss: 3.9937\n",
      "Epoch: 67/2000... Training loss: 4.1140\n",
      "Epoch: 67/2000... Training loss: 4.1421\n",
      "Epoch: 67/2000... Training loss: 4.1373\n",
      "Epoch: 67/2000... Training loss: 4.2722\n",
      "Epoch: 67/2000... Training loss: 4.2505\n",
      "Epoch: 67/2000... Training loss: 4.1559\n",
      "Epoch: 67/2000... Training loss: 4.1029\n",
      "Epoch: 67/2000... Training loss: 4.2399\n",
      "Epoch: 67/2000... Training loss: 3.9581\n",
      "Epoch: 67/2000... Training loss: 4.2098\n",
      "Epoch: 67/2000... Training loss: 3.9815\n",
      "Epoch: 67/2000... Training loss: 4.0236\n",
      "Epoch: 67/2000... Training loss: 4.2285\n",
      "Epoch: 67/2000... Training loss: 3.9128\n",
      "Epoch: 67/2000... Training loss: 4.0935\n",
      "Epoch: 67/2000... Training loss: 4.2723\n",
      "Epoch: 67/2000... Training loss: 4.1373\n",
      "Epoch: 67/2000... Training loss: 4.2579\n",
      "Epoch: 67/2000... Training loss: 4.0137\n",
      "Epoch: 67/2000... Training loss: 4.2501\n",
      "Epoch: 67/2000... Training loss: 4.3460\n",
      "Epoch: 67/2000... Training loss: 4.0349\n",
      "Epoch: 67/2000... Training loss: 4.2350\n",
      "Epoch: 67/2000... Training loss: 4.1863\n",
      "Epoch: 67/2000... Training loss: 4.2552\n",
      "Epoch: 68/2000... Training loss: 4.3106\n",
      "Epoch: 68/2000... Training loss: 4.0429\n",
      "Epoch: 68/2000... Training loss: 3.9600\n",
      "Epoch: 68/2000... Training loss: 4.0921\n",
      "Epoch: 68/2000... Training loss: 3.9838\n",
      "Epoch: 68/2000... Training loss: 3.9503\n",
      "Epoch: 68/2000... Training loss: 4.0634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68/2000... Training loss: 4.1058\n",
      "Epoch: 68/2000... Training loss: 3.9866\n",
      "Epoch: 68/2000... Training loss: 4.2237\n",
      "Epoch: 68/2000... Training loss: 4.1637\n",
      "Epoch: 68/2000... Training loss: 4.0977\n",
      "Epoch: 68/2000... Training loss: 4.0890\n",
      "Epoch: 68/2000... Training loss: 4.1246\n",
      "Epoch: 68/2000... Training loss: 4.2760\n",
      "Epoch: 68/2000... Training loss: 4.1354\n",
      "Epoch: 68/2000... Training loss: 4.1258\n",
      "Epoch: 68/2000... Training loss: 3.9338\n",
      "Epoch: 68/2000... Training loss: 4.1486\n",
      "Epoch: 68/2000... Training loss: 4.0512\n",
      "Epoch: 68/2000... Training loss: 4.1246\n",
      "Epoch: 68/2000... Training loss: 4.0793\n",
      "Epoch: 68/2000... Training loss: 4.1257\n",
      "Epoch: 68/2000... Training loss: 4.1689\n",
      "Epoch: 68/2000... Training loss: 3.9188\n",
      "Epoch: 68/2000... Training loss: 4.0884\n",
      "Epoch: 68/2000... Training loss: 4.1287\n",
      "Epoch: 68/2000... Training loss: 3.9283\n",
      "Epoch: 68/2000... Training loss: 4.0070\n",
      "Epoch: 68/2000... Training loss: 4.2776\n",
      "Epoch: 68/2000... Training loss: 4.0991\n",
      "Epoch: 69/2000... Training loss: 4.3208\n",
      "Epoch: 69/2000... Training loss: 3.8442\n",
      "Epoch: 69/2000... Training loss: 4.0136\n",
      "Epoch: 69/2000... Training loss: 3.9830\n",
      "Epoch: 69/2000... Training loss: 4.2124\n",
      "Epoch: 69/2000... Training loss: 4.1044\n",
      "Epoch: 69/2000... Training loss: 4.1056\n",
      "Epoch: 69/2000... Training loss: 4.2217\n",
      "Epoch: 69/2000... Training loss: 4.1301\n",
      "Epoch: 69/2000... Training loss: 4.2071\n",
      "Epoch: 69/2000... Training loss: 4.1922\n",
      "Epoch: 69/2000... Training loss: 4.0813\n",
      "Epoch: 69/2000... Training loss: 3.9756\n",
      "Epoch: 69/2000... Training loss: 4.2154\n",
      "Epoch: 69/2000... Training loss: 4.1318\n",
      "Epoch: 69/2000... Training loss: 4.2053\n",
      "Epoch: 69/2000... Training loss: 4.0593\n",
      "Epoch: 69/2000... Training loss: 4.1106\n",
      "Epoch: 69/2000... Training loss: 3.9224\n",
      "Epoch: 69/2000... Training loss: 3.9290\n",
      "Epoch: 69/2000... Training loss: 4.0999\n",
      "Epoch: 69/2000... Training loss: 4.0955\n",
      "Epoch: 69/2000... Training loss: 4.1203\n",
      "Epoch: 69/2000... Training loss: 4.1629\n",
      "Epoch: 69/2000... Training loss: 4.1803\n",
      "Epoch: 69/2000... Training loss: 4.1179\n",
      "Epoch: 69/2000... Training loss: 4.1539\n",
      "Epoch: 69/2000... Training loss: 3.9021\n",
      "Epoch: 69/2000... Training loss: 4.0158\n",
      "Epoch: 69/2000... Training loss: 4.0522\n",
      "Epoch: 69/2000... Training loss: 4.0644\n",
      "Epoch: 70/2000... Training loss: 4.1665\n",
      "Epoch: 70/2000... Training loss: 3.9186\n",
      "Epoch: 70/2000... Training loss: 4.0813\n",
      "Epoch: 70/2000... Training loss: 4.0656\n",
      "Epoch: 70/2000... Training loss: 4.0501\n",
      "Epoch: 70/2000... Training loss: 4.0533\n",
      "Epoch: 70/2000... Training loss: 4.1597\n",
      "Epoch: 70/2000... Training loss: 4.1279\n",
      "Epoch: 70/2000... Training loss: 4.0129\n",
      "Epoch: 70/2000... Training loss: 4.2174\n",
      "Epoch: 70/2000... Training loss: 4.1645\n",
      "Epoch: 70/2000... Training loss: 3.8102\n",
      "Epoch: 70/2000... Training loss: 4.0208\n",
      "Epoch: 70/2000... Training loss: 3.9858\n",
      "Epoch: 70/2000... Training loss: 4.1991\n",
      "Epoch: 70/2000... Training loss: 4.0644\n",
      "Epoch: 70/2000... Training loss: 4.1994\n",
      "Epoch: 70/2000... Training loss: 4.0613\n",
      "Epoch: 70/2000... Training loss: 3.9923\n",
      "Epoch: 70/2000... Training loss: 4.0183\n",
      "Epoch: 70/2000... Training loss: 4.0502\n",
      "Epoch: 70/2000... Training loss: 3.8866\n",
      "Epoch: 70/2000... Training loss: 4.2744\n",
      "Epoch: 70/2000... Training loss: 4.1553\n",
      "Epoch: 70/2000... Training loss: 3.9178\n",
      "Epoch: 70/2000... Training loss: 4.0731\n",
      "Epoch: 70/2000... Training loss: 4.1068\n",
      "Epoch: 70/2000... Training loss: 4.0945\n",
      "Epoch: 70/2000... Training loss: 4.0877\n",
      "Epoch: 70/2000... Training loss: 4.0035\n",
      "Epoch: 70/2000... Training loss: 3.7936\n",
      "Epoch: 71/2000... Training loss: 4.2001\n",
      "Epoch: 71/2000... Training loss: 3.8611\n",
      "Epoch: 71/2000... Training loss: 3.8826\n",
      "Epoch: 71/2000... Training loss: 3.8689\n",
      "Epoch: 71/2000... Training loss: 4.0025\n",
      "Epoch: 71/2000... Training loss: 3.8158\n",
      "Epoch: 71/2000... Training loss: 4.0075\n",
      "Epoch: 71/2000... Training loss: 4.0434\n",
      "Epoch: 71/2000... Training loss: 4.0001\n",
      "Epoch: 71/2000... Training loss: 4.0747\n",
      "Epoch: 71/2000... Training loss: 3.9967\n",
      "Epoch: 71/2000... Training loss: 4.0940\n",
      "Epoch: 71/2000... Training loss: 4.1127\n",
      "Epoch: 71/2000... Training loss: 4.1261\n",
      "Epoch: 71/2000... Training loss: 4.1466\n",
      "Epoch: 71/2000... Training loss: 4.0828\n",
      "Epoch: 71/2000... Training loss: 4.0322\n",
      "Epoch: 71/2000... Training loss: 3.8843\n",
      "Epoch: 71/2000... Training loss: 4.0444\n",
      "Epoch: 71/2000... Training loss: 3.9128\n",
      "Epoch: 71/2000... Training loss: 4.0611\n",
      "Epoch: 71/2000... Training loss: 3.9614\n",
      "Epoch: 71/2000... Training loss: 4.0932\n",
      "Epoch: 71/2000... Training loss: 4.0507\n",
      "Epoch: 71/2000... Training loss: 3.9952\n",
      "Epoch: 71/2000... Training loss: 4.0849\n",
      "Epoch: 71/2000... Training loss: 4.3177\n",
      "Epoch: 71/2000... Training loss: 3.8634\n",
      "Epoch: 71/2000... Training loss: 4.2487\n",
      "Epoch: 71/2000... Training loss: 4.1008\n",
      "Epoch: 71/2000... Training loss: 3.9340\n",
      "Epoch: 72/2000... Training loss: 4.0934\n",
      "Epoch: 72/2000... Training loss: 3.7900\n",
      "Epoch: 72/2000... Training loss: 3.8980\n",
      "Epoch: 72/2000... Training loss: 3.9009\n",
      "Epoch: 72/2000... Training loss: 3.7999\n",
      "Epoch: 72/2000... Training loss: 3.7964\n",
      "Epoch: 72/2000... Training loss: 3.9300\n",
      "Epoch: 72/2000... Training loss: 4.2825\n",
      "Epoch: 72/2000... Training loss: 4.1272\n",
      "Epoch: 72/2000... Training loss: 4.0476\n",
      "Epoch: 72/2000... Training loss: 4.0037\n",
      "Epoch: 72/2000... Training loss: 3.9364\n",
      "Epoch: 72/2000... Training loss: 3.9130\n",
      "Epoch: 72/2000... Training loss: 4.0437\n",
      "Epoch: 72/2000... Training loss: 4.1367\n",
      "Epoch: 72/2000... Training loss: 4.0762\n",
      "Epoch: 72/2000... Training loss: 4.1864\n",
      "Epoch: 72/2000... Training loss: 4.0522\n",
      "Epoch: 72/2000... Training loss: 4.1807\n",
      "Epoch: 72/2000... Training loss: 3.9994\n",
      "Epoch: 72/2000... Training loss: 4.1263\n",
      "Epoch: 72/2000... Training loss: 4.0924\n",
      "Epoch: 72/2000... Training loss: 3.9024\n",
      "Epoch: 72/2000... Training loss: 4.1476\n",
      "Epoch: 72/2000... Training loss: 4.1146\n",
      "Epoch: 72/2000... Training loss: 3.8618\n",
      "Epoch: 72/2000... Training loss: 4.2688\n",
      "Epoch: 72/2000... Training loss: 3.8706\n",
      "Epoch: 72/2000... Training loss: 4.1012\n",
      "Epoch: 72/2000... Training loss: 4.1560\n",
      "Epoch: 72/2000... Training loss: 3.8902\n",
      "Epoch: 73/2000... Training loss: 3.8204\n",
      "Epoch: 73/2000... Training loss: 4.0480\n",
      "Epoch: 73/2000... Training loss: 3.8678\n",
      "Epoch: 73/2000... Training loss: 3.8598\n",
      "Epoch: 73/2000... Training loss: 4.1723\n",
      "Epoch: 73/2000... Training loss: 3.9631\n",
      "Epoch: 73/2000... Training loss: 3.9174\n",
      "Epoch: 73/2000... Training loss: 3.9632\n",
      "Epoch: 73/2000... Training loss: 4.0172\n",
      "Epoch: 73/2000... Training loss: 4.0932\n",
      "Epoch: 73/2000... Training loss: 4.0459\n",
      "Epoch: 73/2000... Training loss: 4.0361\n",
      "Epoch: 73/2000... Training loss: 3.8811\n",
      "Epoch: 73/2000... Training loss: 3.9204\n",
      "Epoch: 73/2000... Training loss: 4.2066\n",
      "Epoch: 73/2000... Training loss: 3.9674\n",
      "Epoch: 73/2000... Training loss: 4.0957\n",
      "Epoch: 73/2000... Training loss: 3.8863\n",
      "Epoch: 73/2000... Training loss: 3.9147\n",
      "Epoch: 73/2000... Training loss: 3.7476\n",
      "Epoch: 73/2000... Training loss: 4.0248\n",
      "Epoch: 73/2000... Training loss: 3.9711\n",
      "Epoch: 73/2000... Training loss: 4.0806\n",
      "Epoch: 73/2000... Training loss: 4.1418\n",
      "Epoch: 73/2000... Training loss: 3.9934\n",
      "Epoch: 73/2000... Training loss: 3.9923\n",
      "Epoch: 73/2000... Training loss: 4.1979\n",
      "Epoch: 73/2000... Training loss: 4.1117\n",
      "Epoch: 73/2000... Training loss: 4.0386\n",
      "Epoch: 73/2000... Training loss: 4.1178\n",
      "Epoch: 73/2000... Training loss: 3.8295\n",
      "Epoch: 74/2000... Training loss: 3.8479\n",
      "Epoch: 74/2000... Training loss: 4.1153\n",
      "Epoch: 74/2000... Training loss: 3.8876\n",
      "Epoch: 74/2000... Training loss: 4.1645\n",
      "Epoch: 74/2000... Training loss: 3.9504\n",
      "Epoch: 74/2000... Training loss: 3.8606\n",
      "Epoch: 74/2000... Training loss: 3.7899\n",
      "Epoch: 74/2000... Training loss: 3.9087\n",
      "Epoch: 74/2000... Training loss: 3.9577\n",
      "Epoch: 74/2000... Training loss: 4.0617\n",
      "Epoch: 74/2000... Training loss: 3.9556\n",
      "Epoch: 74/2000... Training loss: 3.6658\n",
      "Epoch: 74/2000... Training loss: 4.0783\n",
      "Epoch: 74/2000... Training loss: 3.9130\n",
      "Epoch: 74/2000... Training loss: 4.2067\n",
      "Epoch: 74/2000... Training loss: 3.9587\n",
      "Epoch: 74/2000... Training loss: 3.8968\n",
      "Epoch: 74/2000... Training loss: 4.0169\n",
      "Epoch: 74/2000... Training loss: 3.9706\n",
      "Epoch: 74/2000... Training loss: 3.9094\n",
      "Epoch: 74/2000... Training loss: 3.9593\n",
      "Epoch: 74/2000... Training loss: 4.2548\n",
      "Epoch: 74/2000... Training loss: 3.8703\n",
      "Epoch: 74/2000... Training loss: 4.1137\n",
      "Epoch: 74/2000... Training loss: 3.8792\n",
      "Epoch: 74/2000... Training loss: 3.8903\n",
      "Epoch: 74/2000... Training loss: 4.0551\n",
      "Epoch: 74/2000... Training loss: 3.8671\n",
      "Epoch: 74/2000... Training loss: 4.0160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74/2000... Training loss: 4.1321\n",
      "Epoch: 74/2000... Training loss: 4.0494\n",
      "Epoch: 75/2000... Training loss: 3.9648\n",
      "Epoch: 75/2000... Training loss: 3.9294\n",
      "Epoch: 75/2000... Training loss: 3.7085\n",
      "Epoch: 75/2000... Training loss: 3.8938\n",
      "Epoch: 75/2000... Training loss: 3.7773\n",
      "Epoch: 75/2000... Training loss: 4.1177\n",
      "Epoch: 75/2000... Training loss: 3.7969\n",
      "Epoch: 75/2000... Training loss: 4.2887\n",
      "Epoch: 75/2000... Training loss: 3.9544\n",
      "Epoch: 75/2000... Training loss: 3.9194\n",
      "Epoch: 75/2000... Training loss: 3.8673\n",
      "Epoch: 75/2000... Training loss: 3.8117\n",
      "Epoch: 75/2000... Training loss: 3.8271\n",
      "Epoch: 75/2000... Training loss: 3.9793\n",
      "Epoch: 75/2000... Training loss: 3.9857\n",
      "Epoch: 75/2000... Training loss: 3.7616\n",
      "Epoch: 75/2000... Training loss: 3.8817\n",
      "Epoch: 75/2000... Training loss: 3.7661\n",
      "Epoch: 75/2000... Training loss: 4.0325\n",
      "Epoch: 75/2000... Training loss: 3.8742\n",
      "Epoch: 75/2000... Training loss: 3.8727\n",
      "Epoch: 75/2000... Training loss: 4.1000\n",
      "Epoch: 75/2000... Training loss: 3.9442\n",
      "Epoch: 75/2000... Training loss: 4.0328\n",
      "Epoch: 75/2000... Training loss: 3.7692\n",
      "Epoch: 75/2000... Training loss: 4.1432\n",
      "Epoch: 75/2000... Training loss: 4.1624\n",
      "Epoch: 75/2000... Training loss: 4.0357\n",
      "Epoch: 75/2000... Training loss: 3.7860\n",
      "Epoch: 75/2000... Training loss: 3.6430\n",
      "Epoch: 75/2000... Training loss: 3.8053\n",
      "Epoch: 76/2000... Training loss: 3.6805\n",
      "Epoch: 76/2000... Training loss: 3.8499\n",
      "Epoch: 76/2000... Training loss: 3.9812\n",
      "Epoch: 76/2000... Training loss: 3.9539\n",
      "Epoch: 76/2000... Training loss: 3.8515\n",
      "Epoch: 76/2000... Training loss: 3.7282\n",
      "Epoch: 76/2000... Training loss: 3.7884\n",
      "Epoch: 76/2000... Training loss: 3.9184\n",
      "Epoch: 76/2000... Training loss: 4.0087\n",
      "Epoch: 76/2000... Training loss: 3.9013\n",
      "Epoch: 76/2000... Training loss: 3.8849\n",
      "Epoch: 76/2000... Training loss: 3.8453\n",
      "Epoch: 76/2000... Training loss: 3.7692\n",
      "Epoch: 76/2000... Training loss: 3.8964\n",
      "Epoch: 76/2000... Training loss: 4.0977\n",
      "Epoch: 76/2000... Training loss: 3.9980\n",
      "Epoch: 76/2000... Training loss: 3.8475\n",
      "Epoch: 76/2000... Training loss: 3.8143\n",
      "Epoch: 76/2000... Training loss: 3.9157\n",
      "Epoch: 76/2000... Training loss: 4.1359\n",
      "Epoch: 76/2000... Training loss: 3.9726\n",
      "Epoch: 76/2000... Training loss: 3.9249\n",
      "Epoch: 76/2000... Training loss: 3.8214\n",
      "Epoch: 76/2000... Training loss: 4.0834\n",
      "Epoch: 76/2000... Training loss: 3.9542\n",
      "Epoch: 76/2000... Training loss: 4.0506\n",
      "Epoch: 76/2000... Training loss: 3.9385\n",
      "Epoch: 76/2000... Training loss: 3.8492\n",
      "Epoch: 76/2000... Training loss: 3.6841\n",
      "Epoch: 76/2000... Training loss: 3.8237\n",
      "Epoch: 76/2000... Training loss: 3.9129\n",
      "Epoch: 77/2000... Training loss: 3.9198\n",
      "Epoch: 77/2000... Training loss: 3.8539\n",
      "Epoch: 77/2000... Training loss: 3.7665\n",
      "Epoch: 77/2000... Training loss: 3.8536\n",
      "Epoch: 77/2000... Training loss: 3.8122\n",
      "Epoch: 77/2000... Training loss: 3.9380\n",
      "Epoch: 77/2000... Training loss: 3.7176\n",
      "Epoch: 77/2000... Training loss: 3.8998\n",
      "Epoch: 77/2000... Training loss: 3.9980\n",
      "Epoch: 77/2000... Training loss: 3.9530\n",
      "Epoch: 77/2000... Training loss: 4.1913\n",
      "Epoch: 77/2000... Training loss: 3.8287\n",
      "Epoch: 77/2000... Training loss: 3.9681\n",
      "Epoch: 77/2000... Training loss: 3.9243\n",
      "Epoch: 77/2000... Training loss: 3.7580\n",
      "Epoch: 77/2000... Training loss: 3.8740\n",
      "Epoch: 77/2000... Training loss: 3.9832\n",
      "Epoch: 77/2000... Training loss: 3.9557\n",
      "Epoch: 77/2000... Training loss: 3.8188\n",
      "Epoch: 77/2000... Training loss: 4.0094\n",
      "Epoch: 77/2000... Training loss: 4.0201\n",
      "Epoch: 77/2000... Training loss: 3.8221\n",
      "Epoch: 77/2000... Training loss: 3.8178\n",
      "Epoch: 77/2000... Training loss: 3.8903\n",
      "Epoch: 77/2000... Training loss: 3.7273\n",
      "Epoch: 77/2000... Training loss: 3.9992\n",
      "Epoch: 77/2000... Training loss: 3.9085\n",
      "Epoch: 77/2000... Training loss: 4.1885\n",
      "Epoch: 77/2000... Training loss: 3.7876\n",
      "Epoch: 77/2000... Training loss: 3.8309\n",
      "Epoch: 77/2000... Training loss: 4.0077\n",
      "Epoch: 78/2000... Training loss: 3.7933\n",
      "Epoch: 78/2000... Training loss: 3.9325\n",
      "Epoch: 78/2000... Training loss: 4.1400\n",
      "Epoch: 78/2000... Training loss: 3.8325\n",
      "Epoch: 78/2000... Training loss: 3.8769\n",
      "Epoch: 78/2000... Training loss: 3.9592\n",
      "Epoch: 78/2000... Training loss: 3.8584\n",
      "Epoch: 78/2000... Training loss: 3.9617\n",
      "Epoch: 78/2000... Training loss: 3.9592\n",
      "Epoch: 78/2000... Training loss: 3.8999\n",
      "Epoch: 78/2000... Training loss: 4.1251\n",
      "Epoch: 78/2000... Training loss: 3.8801\n",
      "Epoch: 78/2000... Training loss: 3.6967\n",
      "Epoch: 78/2000... Training loss: 4.0131\n",
      "Epoch: 78/2000... Training loss: 3.8247\n",
      "Epoch: 78/2000... Training loss: 3.8352\n",
      "Epoch: 78/2000... Training loss: 3.9159\n",
      "Epoch: 78/2000... Training loss: 3.7764\n",
      "Epoch: 78/2000... Training loss: 4.1070\n",
      "Epoch: 78/2000... Training loss: 3.7597\n",
      "Epoch: 78/2000... Training loss: 3.6514\n",
      "Epoch: 78/2000... Training loss: 3.9767\n",
      "Epoch: 78/2000... Training loss: 3.8671\n",
      "Epoch: 78/2000... Training loss: 4.1467\n",
      "Epoch: 78/2000... Training loss: 3.9100\n",
      "Epoch: 78/2000... Training loss: 4.0736\n",
      "Epoch: 78/2000... Training loss: 4.0738\n",
      "Epoch: 78/2000... Training loss: 3.9556\n",
      "Epoch: 78/2000... Training loss: 3.8089\n",
      "Epoch: 78/2000... Training loss: 3.6578\n",
      "Epoch: 78/2000... Training loss: 3.8440\n",
      "Epoch: 79/2000... Training loss: 3.9356\n",
      "Epoch: 79/2000... Training loss: 3.7116\n",
      "Epoch: 79/2000... Training loss: 3.9064\n",
      "Epoch: 79/2000... Training loss: 3.7895\n",
      "Epoch: 79/2000... Training loss: 3.7597\n",
      "Epoch: 79/2000... Training loss: 3.7767\n",
      "Epoch: 79/2000... Training loss: 3.8703\n",
      "Epoch: 79/2000... Training loss: 3.9792\n",
      "Epoch: 79/2000... Training loss: 3.8185\n",
      "Epoch: 79/2000... Training loss: 4.0374\n",
      "Epoch: 79/2000... Training loss: 4.0747\n",
      "Epoch: 79/2000... Training loss: 3.7682\n",
      "Epoch: 79/2000... Training loss: 3.7483\n",
      "Epoch: 79/2000... Training loss: 3.9595\n",
      "Epoch: 79/2000... Training loss: 3.8906\n",
      "Epoch: 79/2000... Training loss: 3.9929\n",
      "Epoch: 79/2000... Training loss: 3.9572\n",
      "Epoch: 79/2000... Training loss: 3.8510\n",
      "Epoch: 79/2000... Training loss: 3.8178\n",
      "Epoch: 79/2000... Training loss: 3.9096\n",
      "Epoch: 79/2000... Training loss: 3.6195\n",
      "Epoch: 79/2000... Training loss: 3.8255\n",
      "Epoch: 79/2000... Training loss: 3.8688\n",
      "Epoch: 79/2000... Training loss: 4.0635\n",
      "Epoch: 79/2000... Training loss: 3.7747\n",
      "Epoch: 79/2000... Training loss: 3.8882\n",
      "Epoch: 79/2000... Training loss: 4.0197\n",
      "Epoch: 79/2000... Training loss: 3.7677\n",
      "Epoch: 79/2000... Training loss: 3.9313\n",
      "Epoch: 79/2000... Training loss: 3.8855\n",
      "Epoch: 79/2000... Training loss: 3.7323\n",
      "Epoch: 80/2000... Training loss: 3.7894\n",
      "Epoch: 80/2000... Training loss: 3.7758\n",
      "Epoch: 80/2000... Training loss: 3.9375\n",
      "Epoch: 80/2000... Training loss: 3.9451\n",
      "Epoch: 80/2000... Training loss: 3.8434\n",
      "Epoch: 80/2000... Training loss: 3.7478\n",
      "Epoch: 80/2000... Training loss: 3.7589\n",
      "Epoch: 80/2000... Training loss: 3.8547\n",
      "Epoch: 80/2000... Training loss: 3.8248\n",
      "Epoch: 80/2000... Training loss: 3.9269\n",
      "Epoch: 80/2000... Training loss: 3.8036\n",
      "Epoch: 80/2000... Training loss: 3.8492\n",
      "Epoch: 80/2000... Training loss: 3.8623\n",
      "Epoch: 80/2000... Training loss: 3.9008\n",
      "Epoch: 80/2000... Training loss: 3.8350\n",
      "Epoch: 80/2000... Training loss: 4.0219\n",
      "Epoch: 80/2000... Training loss: 3.8176\n",
      "Epoch: 80/2000... Training loss: 3.9083\n",
      "Epoch: 80/2000... Training loss: 3.8269\n",
      "Epoch: 80/2000... Training loss: 3.9137\n",
      "Epoch: 80/2000... Training loss: 3.8507\n",
      "Epoch: 80/2000... Training loss: 3.9750\n",
      "Epoch: 80/2000... Training loss: 3.9708\n",
      "Epoch: 80/2000... Training loss: 3.8159\n",
      "Epoch: 80/2000... Training loss: 3.9265\n",
      "Epoch: 80/2000... Training loss: 3.9962\n",
      "Epoch: 80/2000... Training loss: 3.7560\n",
      "Epoch: 80/2000... Training loss: 3.8804\n",
      "Epoch: 80/2000... Training loss: 4.1507\n",
      "Epoch: 80/2000... Training loss: 3.8004\n",
      "Epoch: 80/2000... Training loss: 3.8806\n",
      "Epoch: 81/2000... Training loss: 3.6521\n",
      "Epoch: 81/2000... Training loss: 3.8480\n",
      "Epoch: 81/2000... Training loss: 3.9664\n",
      "Epoch: 81/2000... Training loss: 3.8166\n",
      "Epoch: 81/2000... Training loss: 3.8950\n",
      "Epoch: 81/2000... Training loss: 3.5999\n",
      "Epoch: 81/2000... Training loss: 3.8726\n",
      "Epoch: 81/2000... Training loss: 3.6537\n",
      "Epoch: 81/2000... Training loss: 3.9140\n",
      "Epoch: 81/2000... Training loss: 3.7590\n",
      "Epoch: 81/2000... Training loss: 3.7470\n",
      "Epoch: 81/2000... Training loss: 3.6916\n",
      "Epoch: 81/2000... Training loss: 3.9188\n",
      "Epoch: 81/2000... Training loss: 3.5372\n",
      "Epoch: 81/2000... Training loss: 3.7923\n",
      "Epoch: 81/2000... Training loss: 3.8289\n",
      "Epoch: 81/2000... Training loss: 3.9065\n",
      "Epoch: 81/2000... Training loss: 3.7743\n",
      "Epoch: 81/2000... Training loss: 3.7701\n",
      "Epoch: 81/2000... Training loss: 3.9126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/2000... Training loss: 3.9578\n",
      "Epoch: 81/2000... Training loss: 3.7267\n",
      "Epoch: 81/2000... Training loss: 3.8240\n",
      "Epoch: 81/2000... Training loss: 3.8436\n",
      "Epoch: 81/2000... Training loss: 3.6533\n",
      "Epoch: 81/2000... Training loss: 4.2365\n",
      "Epoch: 81/2000... Training loss: 3.9420\n",
      "Epoch: 81/2000... Training loss: 3.9309\n",
      "Epoch: 81/2000... Training loss: 3.6801\n",
      "Epoch: 81/2000... Training loss: 3.8025\n",
      "Epoch: 81/2000... Training loss: 3.7850\n",
      "Epoch: 82/2000... Training loss: 3.8410\n",
      "Epoch: 82/2000... Training loss: 3.7319\n",
      "Epoch: 82/2000... Training loss: 3.7991\n",
      "Epoch: 82/2000... Training loss: 3.6767\n",
      "Epoch: 82/2000... Training loss: 3.7338\n",
      "Epoch: 82/2000... Training loss: 3.5930\n",
      "Epoch: 82/2000... Training loss: 3.8387\n",
      "Epoch: 82/2000... Training loss: 3.8419\n",
      "Epoch: 82/2000... Training loss: 3.9495\n",
      "Epoch: 82/2000... Training loss: 3.7681\n",
      "Epoch: 82/2000... Training loss: 3.7449\n",
      "Epoch: 82/2000... Training loss: 3.7425\n",
      "Epoch: 82/2000... Training loss: 3.8673\n",
      "Epoch: 82/2000... Training loss: 3.6997\n",
      "Epoch: 82/2000... Training loss: 3.8015\n",
      "Epoch: 82/2000... Training loss: 3.6207\n",
      "Epoch: 82/2000... Training loss: 3.7062\n",
      "Epoch: 82/2000... Training loss: 3.8404\n",
      "Epoch: 82/2000... Training loss: 3.6822\n",
      "Epoch: 82/2000... Training loss: 3.8695\n",
      "Epoch: 82/2000... Training loss: 3.6904\n",
      "Epoch: 82/2000... Training loss: 3.7980\n",
      "Epoch: 82/2000... Training loss: 3.8684\n",
      "Epoch: 82/2000... Training loss: 3.7642\n",
      "Epoch: 82/2000... Training loss: 3.7736\n",
      "Epoch: 82/2000... Training loss: 3.8504\n",
      "Epoch: 82/2000... Training loss: 3.8355\n",
      "Epoch: 82/2000... Training loss: 3.8392\n",
      "Epoch: 82/2000... Training loss: 3.7538\n",
      "Epoch: 82/2000... Training loss: 3.8827\n",
      "Epoch: 82/2000... Training loss: 3.8408\n",
      "Epoch: 83/2000... Training loss: 3.8023\n",
      "Epoch: 83/2000... Training loss: 3.8047\n",
      "Epoch: 83/2000... Training loss: 3.7959\n",
      "Epoch: 83/2000... Training loss: 3.7337\n",
      "Epoch: 83/2000... Training loss: 3.9086\n",
      "Epoch: 83/2000... Training loss: 3.9401\n",
      "Epoch: 83/2000... Training loss: 3.7331\n",
      "Epoch: 83/2000... Training loss: 3.6757\n",
      "Epoch: 83/2000... Training loss: 3.8609\n",
      "Epoch: 83/2000... Training loss: 3.7599\n",
      "Epoch: 83/2000... Training loss: 4.0085\n",
      "Epoch: 83/2000... Training loss: 3.6642\n",
      "Epoch: 83/2000... Training loss: 3.7596\n",
      "Epoch: 83/2000... Training loss: 3.8645\n",
      "Epoch: 83/2000... Training loss: 3.7795\n",
      "Epoch: 83/2000... Training loss: 3.9164\n",
      "Epoch: 83/2000... Training loss: 3.7706\n",
      "Epoch: 83/2000... Training loss: 3.7602\n",
      "Epoch: 83/2000... Training loss: 3.6357\n",
      "Epoch: 83/2000... Training loss: 3.7131\n",
      "Epoch: 83/2000... Training loss: 3.7896\n",
      "Epoch: 83/2000... Training loss: 3.9477\n",
      "Epoch: 83/2000... Training loss: 3.8519\n",
      "Epoch: 83/2000... Training loss: 3.9431\n",
      "Epoch: 83/2000... Training loss: 4.0166\n",
      "Epoch: 83/2000... Training loss: 3.7803\n",
      "Epoch: 83/2000... Training loss: 3.9366\n",
      "Epoch: 83/2000... Training loss: 3.9715\n",
      "Epoch: 83/2000... Training loss: 3.8716\n",
      "Epoch: 83/2000... Training loss: 4.1753\n",
      "Epoch: 83/2000... Training loss: 3.7241\n",
      "Epoch: 84/2000... Training loss: 3.8024\n",
      "Epoch: 84/2000... Training loss: 3.6892\n",
      "Epoch: 84/2000... Training loss: 3.6323\n",
      "Epoch: 84/2000... Training loss: 3.7200\n",
      "Epoch: 84/2000... Training loss: 3.9028\n",
      "Epoch: 84/2000... Training loss: 3.9328\n",
      "Epoch: 84/2000... Training loss: 3.7252\n",
      "Epoch: 84/2000... Training loss: 3.7149\n",
      "Epoch: 84/2000... Training loss: 3.5988\n",
      "Epoch: 84/2000... Training loss: 3.8322\n",
      "Epoch: 84/2000... Training loss: 3.8633\n",
      "Epoch: 84/2000... Training loss: 3.6987\n",
      "Epoch: 84/2000... Training loss: 3.9519\n",
      "Epoch: 84/2000... Training loss: 3.6029\n",
      "Epoch: 84/2000... Training loss: 3.6654\n",
      "Epoch: 84/2000... Training loss: 3.6580\n",
      "Epoch: 84/2000... Training loss: 4.0971\n",
      "Epoch: 84/2000... Training loss: 3.8773\n",
      "Epoch: 84/2000... Training loss: 3.7497\n",
      "Epoch: 84/2000... Training loss: 3.7214\n",
      "Epoch: 84/2000... Training loss: 3.9896\n",
      "Epoch: 84/2000... Training loss: 3.7686\n",
      "Epoch: 84/2000... Training loss: 3.5776\n",
      "Epoch: 84/2000... Training loss: 3.9876\n",
      "Epoch: 84/2000... Training loss: 3.9219\n",
      "Epoch: 84/2000... Training loss: 3.7276\n",
      "Epoch: 84/2000... Training loss: 3.9328\n",
      "Epoch: 84/2000... Training loss: 3.9850\n",
      "Epoch: 84/2000... Training loss: 3.8352\n",
      "Epoch: 84/2000... Training loss: 3.6475\n",
      "Epoch: 84/2000... Training loss: 3.6153\n",
      "Epoch: 85/2000... Training loss: 3.7117\n",
      "Epoch: 85/2000... Training loss: 3.6501\n",
      "Epoch: 85/2000... Training loss: 3.7373\n",
      "Epoch: 85/2000... Training loss: 3.7289\n",
      "Epoch: 85/2000... Training loss: 3.9560\n",
      "Epoch: 85/2000... Training loss: 3.5196\n",
      "Epoch: 85/2000... Training loss: 3.6520\n",
      "Epoch: 85/2000... Training loss: 3.7243\n",
      "Epoch: 85/2000... Training loss: 3.7670\n",
      "Epoch: 85/2000... Training loss: 3.6134\n",
      "Epoch: 85/2000... Training loss: 4.0201\n",
      "Epoch: 85/2000... Training loss: 3.5088\n",
      "Epoch: 85/2000... Training loss: 3.6996\n",
      "Epoch: 85/2000... Training loss: 3.6614\n",
      "Epoch: 85/2000... Training loss: 3.7221\n",
      "Epoch: 85/2000... Training loss: 4.0462\n",
      "Epoch: 85/2000... Training loss: 3.6716\n",
      "Epoch: 85/2000... Training loss: 3.9062\n",
      "Epoch: 85/2000... Training loss: 3.7891\n",
      "Epoch: 85/2000... Training loss: 3.7074\n",
      "Epoch: 85/2000... Training loss: 3.9033\n",
      "Epoch: 85/2000... Training loss: 3.8662\n",
      "Epoch: 85/2000... Training loss: 3.8939\n",
      "Epoch: 85/2000... Training loss: 3.9286\n",
      "Epoch: 85/2000... Training loss: 3.6536\n",
      "Epoch: 85/2000... Training loss: 3.9598\n",
      "Epoch: 85/2000... Training loss: 3.7599\n",
      "Epoch: 85/2000... Training loss: 3.8078\n",
      "Epoch: 85/2000... Training loss: 3.8200\n",
      "Epoch: 85/2000... Training loss: 3.8466\n",
      "Epoch: 85/2000... Training loss: 3.8448\n",
      "Epoch: 86/2000... Training loss: 3.6557\n",
      "Epoch: 86/2000... Training loss: 3.4640\n",
      "Epoch: 86/2000... Training loss: 3.8373\n",
      "Epoch: 86/2000... Training loss: 3.8028\n",
      "Epoch: 86/2000... Training loss: 3.9283\n",
      "Epoch: 86/2000... Training loss: 3.7425\n",
      "Epoch: 86/2000... Training loss: 3.7864\n",
      "Epoch: 86/2000... Training loss: 3.6553\n",
      "Epoch: 86/2000... Training loss: 3.6237\n",
      "Epoch: 86/2000... Training loss: 3.6020\n",
      "Epoch: 86/2000... Training loss: 3.8708\n",
      "Epoch: 86/2000... Training loss: 3.6677\n",
      "Epoch: 86/2000... Training loss: 3.7413\n",
      "Epoch: 86/2000... Training loss: 3.6970\n",
      "Epoch: 86/2000... Training loss: 3.7944\n",
      "Epoch: 86/2000... Training loss: 3.9543\n",
      "Epoch: 86/2000... Training loss: 3.6751\n",
      "Epoch: 86/2000... Training loss: 3.4844\n",
      "Epoch: 86/2000... Training loss: 3.7412\n",
      "Epoch: 86/2000... Training loss: 3.5968\n",
      "Epoch: 86/2000... Training loss: 3.7408\n",
      "Epoch: 86/2000... Training loss: 3.7262\n",
      "Epoch: 86/2000... Training loss: 3.7786\n",
      "Epoch: 86/2000... Training loss: 3.9307\n",
      "Epoch: 86/2000... Training loss: 3.7154\n",
      "Epoch: 86/2000... Training loss: 3.6949\n",
      "Epoch: 86/2000... Training loss: 3.7109\n",
      "Epoch: 86/2000... Training loss: 3.8700\n",
      "Epoch: 86/2000... Training loss: 3.7710\n",
      "Epoch: 86/2000... Training loss: 3.6873\n",
      "Epoch: 86/2000... Training loss: 3.6462\n",
      "Epoch: 87/2000... Training loss: 3.6187\n",
      "Epoch: 87/2000... Training loss: 3.6922\n",
      "Epoch: 87/2000... Training loss: 3.6239\n",
      "Epoch: 87/2000... Training loss: 3.5603\n",
      "Epoch: 87/2000... Training loss: 3.7062\n",
      "Epoch: 87/2000... Training loss: 3.6697\n",
      "Epoch: 87/2000... Training loss: 3.7384\n",
      "Epoch: 87/2000... Training loss: 3.6473\n",
      "Epoch: 87/2000... Training loss: 3.5118\n",
      "Epoch: 87/2000... Training loss: 3.6412\n",
      "Epoch: 87/2000... Training loss: 3.7563\n",
      "Epoch: 87/2000... Training loss: 3.6891\n",
      "Epoch: 87/2000... Training loss: 3.6987\n",
      "Epoch: 87/2000... Training loss: 3.6871\n",
      "Epoch: 87/2000... Training loss: 3.5074\n",
      "Epoch: 87/2000... Training loss: 3.8118\n",
      "Epoch: 87/2000... Training loss: 3.8164\n",
      "Epoch: 87/2000... Training loss: 3.7399\n",
      "Epoch: 87/2000... Training loss: 3.8904\n",
      "Epoch: 87/2000... Training loss: 3.7066\n",
      "Epoch: 87/2000... Training loss: 3.6732\n",
      "Epoch: 87/2000... Training loss: 3.4985\n",
      "Epoch: 87/2000... Training loss: 3.6837\n",
      "Epoch: 87/2000... Training loss: 3.7496\n",
      "Epoch: 87/2000... Training loss: 3.8899\n",
      "Epoch: 87/2000... Training loss: 3.5694\n",
      "Epoch: 87/2000... Training loss: 3.6748\n",
      "Epoch: 87/2000... Training loss: 3.6558\n",
      "Epoch: 87/2000... Training loss: 3.5840\n",
      "Epoch: 87/2000... Training loss: 3.6722\n",
      "Epoch: 87/2000... Training loss: 3.6500\n",
      "Epoch: 88/2000... Training loss: 3.8684\n",
      "Epoch: 88/2000... Training loss: 3.5257\n",
      "Epoch: 88/2000... Training loss: 3.4907\n",
      "Epoch: 88/2000... Training loss: 3.7509\n",
      "Epoch: 88/2000... Training loss: 3.7329\n",
      "Epoch: 88/2000... Training loss: 3.8570\n",
      "Epoch: 88/2000... Training loss: 3.8429\n",
      "Epoch: 88/2000... Training loss: 3.7096\n",
      "Epoch: 88/2000... Training loss: 3.6075\n",
      "Epoch: 88/2000... Training loss: 3.4872\n",
      "Epoch: 88/2000... Training loss: 3.8071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88/2000... Training loss: 3.6425\n",
      "Epoch: 88/2000... Training loss: 3.7198\n",
      "Epoch: 88/2000... Training loss: 3.6713\n",
      "Epoch: 88/2000... Training loss: 3.5720\n",
      "Epoch: 88/2000... Training loss: 3.9015\n",
      "Epoch: 88/2000... Training loss: 3.5982\n",
      "Epoch: 88/2000... Training loss: 3.7191\n",
      "Epoch: 88/2000... Training loss: 3.5215\n",
      "Epoch: 88/2000... Training loss: 3.5472\n",
      "Epoch: 88/2000... Training loss: 3.6060\n",
      "Epoch: 88/2000... Training loss: 3.5684\n",
      "Epoch: 88/2000... Training loss: 3.4194\n",
      "Epoch: 88/2000... Training loss: 3.7109\n",
      "Epoch: 88/2000... Training loss: 3.6755\n",
      "Epoch: 88/2000... Training loss: 3.8285\n",
      "Epoch: 88/2000... Training loss: 3.4885\n",
      "Epoch: 88/2000... Training loss: 3.6940\n",
      "Epoch: 88/2000... Training loss: 3.5454\n",
      "Epoch: 88/2000... Training loss: 3.7404\n",
      "Epoch: 88/2000... Training loss: 3.6754\n",
      "Epoch: 89/2000... Training loss: 3.6900\n",
      "Epoch: 89/2000... Training loss: 3.6122\n",
      "Epoch: 89/2000... Training loss: 3.7378\n",
      "Epoch: 89/2000... Training loss: 3.5784\n",
      "Epoch: 89/2000... Training loss: 3.6114\n",
      "Epoch: 89/2000... Training loss: 3.6007\n",
      "Epoch: 89/2000... Training loss: 3.7373\n",
      "Epoch: 89/2000... Training loss: 3.7787\n",
      "Epoch: 89/2000... Training loss: 3.6322\n",
      "Epoch: 89/2000... Training loss: 3.4495\n",
      "Epoch: 89/2000... Training loss: 3.5139\n",
      "Epoch: 89/2000... Training loss: 3.6117\n",
      "Epoch: 89/2000... Training loss: 3.6149\n",
      "Epoch: 89/2000... Training loss: 3.6325\n",
      "Epoch: 89/2000... Training loss: 3.8153\n",
      "Epoch: 89/2000... Training loss: 3.6392\n",
      "Epoch: 89/2000... Training loss: 3.5156\n",
      "Epoch: 89/2000... Training loss: 3.8593\n",
      "Epoch: 89/2000... Training loss: 3.7783\n",
      "Epoch: 89/2000... Training loss: 3.6394\n",
      "Epoch: 89/2000... Training loss: 3.6679\n",
      "Epoch: 89/2000... Training loss: 3.6772\n",
      "Epoch: 89/2000... Training loss: 3.6914\n",
      "Epoch: 89/2000... Training loss: 3.9065\n",
      "Epoch: 89/2000... Training loss: 3.8523\n",
      "Epoch: 89/2000... Training loss: 3.9023\n",
      "Epoch: 89/2000... Training loss: 3.7262\n",
      "Epoch: 89/2000... Training loss: 4.0123\n",
      "Epoch: 89/2000... Training loss: 3.7036\n",
      "Epoch: 89/2000... Training loss: 3.6021\n",
      "Epoch: 89/2000... Training loss: 3.7887\n",
      "Epoch: 90/2000... Training loss: 3.6612\n",
      "Epoch: 90/2000... Training loss: 3.6975\n",
      "Epoch: 90/2000... Training loss: 3.5260\n",
      "Epoch: 90/2000... Training loss: 3.7201\n",
      "Epoch: 90/2000... Training loss: 3.7520\n",
      "Epoch: 90/2000... Training loss: 3.6549\n",
      "Epoch: 90/2000... Training loss: 3.4443\n",
      "Epoch: 90/2000... Training loss: 3.5713\n",
      "Epoch: 90/2000... Training loss: 3.6712\n",
      "Epoch: 90/2000... Training loss: 3.7840\n",
      "Epoch: 90/2000... Training loss: 3.5292\n",
      "Epoch: 90/2000... Training loss: 3.7358\n",
      "Epoch: 90/2000... Training loss: 3.6229\n",
      "Epoch: 90/2000... Training loss: 3.5845\n",
      "Epoch: 90/2000... Training loss: 3.6583\n",
      "Epoch: 90/2000... Training loss: 3.7016\n",
      "Epoch: 90/2000... Training loss: 3.4535\n",
      "Epoch: 90/2000... Training loss: 3.8632\n",
      "Epoch: 90/2000... Training loss: 3.8098\n",
      "Epoch: 90/2000... Training loss: 3.5051\n",
      "Epoch: 90/2000... Training loss: 3.8055\n",
      "Epoch: 90/2000... Training loss: 3.4896\n",
      "Epoch: 90/2000... Training loss: 3.8639\n",
      "Epoch: 90/2000... Training loss: 3.8362\n",
      "Epoch: 90/2000... Training loss: 3.5541\n",
      "Epoch: 90/2000... Training loss: 3.8175\n",
      "Epoch: 90/2000... Training loss: 3.6276\n",
      "Epoch: 90/2000... Training loss: 3.6242\n",
      "Epoch: 90/2000... Training loss: 3.5751\n",
      "Epoch: 90/2000... Training loss: 3.4513\n",
      "Epoch: 90/2000... Training loss: 3.7318\n",
      "Epoch: 91/2000... Training loss: 3.5691\n",
      "Epoch: 91/2000... Training loss: 3.4907\n",
      "Epoch: 91/2000... Training loss: 3.5723\n",
      "Epoch: 91/2000... Training loss: 3.5602\n",
      "Epoch: 91/2000... Training loss: 3.7918\n",
      "Epoch: 91/2000... Training loss: 3.6670\n",
      "Epoch: 91/2000... Training loss: 3.7605\n",
      "Epoch: 91/2000... Training loss: 3.6729\n",
      "Epoch: 91/2000... Training loss: 3.6044\n",
      "Epoch: 91/2000... Training loss: 3.5317\n",
      "Epoch: 91/2000... Training loss: 3.6495\n",
      "Epoch: 91/2000... Training loss: 3.6459\n",
      "Epoch: 91/2000... Training loss: 3.7556\n",
      "Epoch: 91/2000... Training loss: 3.5611\n",
      "Epoch: 91/2000... Training loss: 3.6989\n",
      "Epoch: 91/2000... Training loss: 3.7043\n",
      "Epoch: 91/2000... Training loss: 3.8268\n",
      "Epoch: 91/2000... Training loss: 3.5871\n",
      "Epoch: 91/2000... Training loss: 3.5831\n",
      "Epoch: 91/2000... Training loss: 3.6361\n",
      "Epoch: 91/2000... Training loss: 3.7715\n",
      "Epoch: 91/2000... Training loss: 3.6116\n",
      "Epoch: 91/2000... Training loss: 3.5307\n",
      "Epoch: 91/2000... Training loss: 3.6445\n",
      "Epoch: 91/2000... Training loss: 3.6243\n",
      "Epoch: 91/2000... Training loss: 3.6983\n",
      "Epoch: 91/2000... Training loss: 3.5516\n",
      "Epoch: 91/2000... Training loss: 3.6314\n",
      "Epoch: 91/2000... Training loss: 3.5215\n",
      "Epoch: 91/2000... Training loss: 3.6744\n",
      "Epoch: 91/2000... Training loss: 3.6127\n",
      "Epoch: 92/2000... Training loss: 3.6318\n",
      "Epoch: 92/2000... Training loss: 3.6679\n",
      "Epoch: 92/2000... Training loss: 3.4047\n",
      "Epoch: 92/2000... Training loss: 3.5855\n",
      "Epoch: 92/2000... Training loss: 3.4936\n",
      "Epoch: 92/2000... Training loss: 3.6114\n",
      "Epoch: 92/2000... Training loss: 3.5017\n",
      "Epoch: 92/2000... Training loss: 3.6324\n",
      "Epoch: 92/2000... Training loss: 3.5461\n",
      "Epoch: 92/2000... Training loss: 3.6384\n",
      "Epoch: 92/2000... Training loss: 3.5813\n",
      "Epoch: 92/2000... Training loss: 3.6731\n",
      "Epoch: 92/2000... Training loss: 3.6218\n",
      "Epoch: 92/2000... Training loss: 3.5024\n",
      "Epoch: 92/2000... Training loss: 3.8870\n",
      "Epoch: 92/2000... Training loss: 3.8107\n",
      "Epoch: 92/2000... Training loss: 3.7155\n",
      "Epoch: 92/2000... Training loss: 3.6960\n",
      "Epoch: 92/2000... Training loss: 3.5657\n",
      "Epoch: 92/2000... Training loss: 3.8718\n",
      "Epoch: 92/2000... Training loss: 3.6828\n",
      "Epoch: 92/2000... Training loss: 3.5734\n",
      "Epoch: 92/2000... Training loss: 3.5907\n",
      "Epoch: 92/2000... Training loss: 3.6576\n",
      "Epoch: 92/2000... Training loss: 3.5722\n",
      "Epoch: 92/2000... Training loss: 3.5388\n",
      "Epoch: 92/2000... Training loss: 3.6860\n",
      "Epoch: 92/2000... Training loss: 3.6810\n",
      "Epoch: 92/2000... Training loss: 3.7580\n",
      "Epoch: 92/2000... Training loss: 3.6680\n",
      "Epoch: 92/2000... Training loss: 3.7183\n",
      "Epoch: 93/2000... Training loss: 3.5516\n",
      "Epoch: 93/2000... Training loss: 3.4857\n",
      "Epoch: 93/2000... Training loss: 3.6678\n",
      "Epoch: 93/2000... Training loss: 3.6340\n",
      "Epoch: 93/2000... Training loss: 3.6889\n",
      "Epoch: 93/2000... Training loss: 3.5989\n",
      "Epoch: 93/2000... Training loss: 3.5522\n",
      "Epoch: 93/2000... Training loss: 3.6241\n",
      "Epoch: 93/2000... Training loss: 3.4465\n",
      "Epoch: 93/2000... Training loss: 3.4310\n",
      "Epoch: 93/2000... Training loss: 3.8735\n",
      "Epoch: 93/2000... Training loss: 3.6388\n",
      "Epoch: 93/2000... Training loss: 3.5525\n",
      "Epoch: 93/2000... Training loss: 3.4482\n",
      "Epoch: 93/2000... Training loss: 3.8059\n",
      "Epoch: 93/2000... Training loss: 3.7172\n",
      "Epoch: 93/2000... Training loss: 3.7460\n",
      "Epoch: 93/2000... Training loss: 3.6450\n",
      "Epoch: 93/2000... Training loss: 3.8206\n",
      "Epoch: 93/2000... Training loss: 3.5360\n",
      "Epoch: 93/2000... Training loss: 3.3960\n",
      "Epoch: 93/2000... Training loss: 3.4978\n",
      "Epoch: 93/2000... Training loss: 3.6487\n",
      "Epoch: 93/2000... Training loss: 3.7329\n",
      "Epoch: 93/2000... Training loss: 3.6226\n",
      "Epoch: 93/2000... Training loss: 3.6941\n",
      "Epoch: 93/2000... Training loss: 3.5041\n",
      "Epoch: 93/2000... Training loss: 3.8525\n",
      "Epoch: 93/2000... Training loss: 3.7370\n",
      "Epoch: 93/2000... Training loss: 3.6968\n",
      "Epoch: 93/2000... Training loss: 3.6823\n",
      "Epoch: 94/2000... Training loss: 3.4752\n",
      "Epoch: 94/2000... Training loss: 3.7247\n",
      "Epoch: 94/2000... Training loss: 3.5084\n",
      "Epoch: 94/2000... Training loss: 3.4262\n",
      "Epoch: 94/2000... Training loss: 3.4441\n",
      "Epoch: 94/2000... Training loss: 3.4295\n",
      "Epoch: 94/2000... Training loss: 3.5552\n",
      "Epoch: 94/2000... Training loss: 3.5259\n",
      "Epoch: 94/2000... Training loss: 3.4357\n",
      "Epoch: 94/2000... Training loss: 3.5458\n",
      "Epoch: 94/2000... Training loss: 3.5647\n",
      "Epoch: 94/2000... Training loss: 3.6200\n",
      "Epoch: 94/2000... Training loss: 3.5994\n",
      "Epoch: 94/2000... Training loss: 3.5347\n",
      "Epoch: 94/2000... Training loss: 3.4918\n",
      "Epoch: 94/2000... Training loss: 3.7824\n",
      "Epoch: 94/2000... Training loss: 3.5112\n",
      "Epoch: 94/2000... Training loss: 3.6055\n",
      "Epoch: 94/2000... Training loss: 3.5495\n",
      "Epoch: 94/2000... Training loss: 3.6260\n",
      "Epoch: 94/2000... Training loss: 3.5530\n",
      "Epoch: 94/2000... Training loss: 3.4810\n",
      "Epoch: 94/2000... Training loss: 3.4479\n",
      "Epoch: 94/2000... Training loss: 3.5796\n",
      "Epoch: 94/2000... Training loss: 3.4347\n",
      "Epoch: 94/2000... Training loss: 3.6382\n",
      "Epoch: 94/2000... Training loss: 3.4088\n",
      "Epoch: 94/2000... Training loss: 3.5416\n",
      "Epoch: 94/2000... Training loss: 3.5304\n",
      "Epoch: 94/2000... Training loss: 3.5668\n",
      "Epoch: 94/2000... Training loss: 3.6230\n",
      "Epoch: 95/2000... Training loss: 3.3622\n",
      "Epoch: 95/2000... Training loss: 3.4036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95/2000... Training loss: 3.3840\n",
      "Epoch: 95/2000... Training loss: 3.6501\n",
      "Epoch: 95/2000... Training loss: 3.3824\n",
      "Epoch: 95/2000... Training loss: 3.6597\n",
      "Epoch: 95/2000... Training loss: 3.5112\n",
      "Epoch: 95/2000... Training loss: 3.5977\n",
      "Epoch: 95/2000... Training loss: 3.3966\n",
      "Epoch: 95/2000... Training loss: 3.7443\n",
      "Epoch: 95/2000... Training loss: 3.7064\n",
      "Epoch: 95/2000... Training loss: 3.4944\n",
      "Epoch: 95/2000... Training loss: 3.5699\n",
      "Epoch: 95/2000... Training loss: 3.6475\n",
      "Epoch: 95/2000... Training loss: 3.4857\n",
      "Epoch: 95/2000... Training loss: 3.5361\n",
      "Epoch: 95/2000... Training loss: 3.4236\n",
      "Epoch: 95/2000... Training loss: 3.7362\n",
      "Epoch: 95/2000... Training loss: 3.5540\n",
      "Epoch: 95/2000... Training loss: 3.5595\n",
      "Epoch: 95/2000... Training loss: 3.3798\n",
      "Epoch: 95/2000... Training loss: 3.4056\n",
      "Epoch: 95/2000... Training loss: 3.5809\n",
      "Epoch: 95/2000... Training loss: 3.7620\n",
      "Epoch: 95/2000... Training loss: 3.5423\n",
      "Epoch: 95/2000... Training loss: 3.6657\n",
      "Epoch: 95/2000... Training loss: 3.8225\n",
      "Epoch: 95/2000... Training loss: 3.6191\n",
      "Epoch: 95/2000... Training loss: 3.5818\n",
      "Epoch: 95/2000... Training loss: 3.4964\n",
      "Epoch: 95/2000... Training loss: 3.4304\n",
      "Epoch: 96/2000... Training loss: 3.4784\n",
      "Epoch: 96/2000... Training loss: 3.4654\n",
      "Epoch: 96/2000... Training loss: 3.5817\n",
      "Epoch: 96/2000... Training loss: 3.5609\n",
      "Epoch: 96/2000... Training loss: 3.7236\n",
      "Epoch: 96/2000... Training loss: 3.6937\n",
      "Epoch: 96/2000... Training loss: 3.5894\n",
      "Epoch: 96/2000... Training loss: 3.4369\n",
      "Epoch: 96/2000... Training loss: 3.5772\n",
      "Epoch: 96/2000... Training loss: 3.5031\n",
      "Epoch: 96/2000... Training loss: 3.8308\n",
      "Epoch: 96/2000... Training loss: 3.4631\n",
      "Epoch: 96/2000... Training loss: 3.5426\n",
      "Epoch: 96/2000... Training loss: 3.6059\n",
      "Epoch: 96/2000... Training loss: 3.6334\n",
      "Epoch: 96/2000... Training loss: 3.7071\n",
      "Epoch: 96/2000... Training loss: 3.5696\n",
      "Epoch: 96/2000... Training loss: 3.5401\n",
      "Epoch: 96/2000... Training loss: 3.5114\n",
      "Epoch: 96/2000... Training loss: 3.4204\n",
      "Epoch: 96/2000... Training loss: 3.5908\n",
      "Epoch: 96/2000... Training loss: 3.5513\n",
      "Epoch: 96/2000... Training loss: 3.5471\n",
      "Epoch: 96/2000... Training loss: 3.5591\n",
      "Epoch: 96/2000... Training loss: 3.6198\n",
      "Epoch: 96/2000... Training loss: 3.3835\n",
      "Epoch: 96/2000... Training loss: 3.3751\n",
      "Epoch: 96/2000... Training loss: 3.6967\n",
      "Epoch: 96/2000... Training loss: 3.5077\n",
      "Epoch: 96/2000... Training loss: 3.7098\n",
      "Epoch: 96/2000... Training loss: 3.5370\n",
      "Epoch: 97/2000... Training loss: 3.3673\n",
      "Epoch: 97/2000... Training loss: 3.3976\n",
      "Epoch: 97/2000... Training loss: 3.2337\n",
      "Epoch: 97/2000... Training loss: 3.5732\n",
      "Epoch: 97/2000... Training loss: 3.4106\n",
      "Epoch: 97/2000... Training loss: 3.6182\n",
      "Epoch: 97/2000... Training loss: 3.5501\n",
      "Epoch: 97/2000... Training loss: 3.5310\n",
      "Epoch: 97/2000... Training loss: 3.3925\n",
      "Epoch: 97/2000... Training loss: 3.8547\n",
      "Epoch: 97/2000... Training loss: 3.6201\n",
      "Epoch: 97/2000... Training loss: 3.7071\n",
      "Epoch: 97/2000... Training loss: 3.6839\n",
      "Epoch: 97/2000... Training loss: 3.5585\n",
      "Epoch: 97/2000... Training loss: 3.5233\n",
      "Epoch: 97/2000... Training loss: 3.5581\n",
      "Epoch: 97/2000... Training loss: 3.6296\n",
      "Epoch: 97/2000... Training loss: 3.6555\n",
      "Epoch: 97/2000... Training loss: 3.5767\n",
      "Epoch: 97/2000... Training loss: 3.4326\n",
      "Epoch: 97/2000... Training loss: 3.5217\n",
      "Epoch: 97/2000... Training loss: 3.5283\n",
      "Epoch: 97/2000... Training loss: 3.2717\n",
      "Epoch: 97/2000... Training loss: 3.5299\n",
      "Epoch: 97/2000... Training loss: 3.5734\n",
      "Epoch: 97/2000... Training loss: 3.6517\n",
      "Epoch: 97/2000... Training loss: 3.8730\n",
      "Epoch: 97/2000... Training loss: 3.2032\n",
      "Epoch: 97/2000... Training loss: 3.6771\n",
      "Epoch: 97/2000... Training loss: 3.5753\n",
      "Epoch: 97/2000... Training loss: 3.7019\n",
      "Epoch: 98/2000... Training loss: 3.3848\n",
      "Epoch: 98/2000... Training loss: 3.3846\n",
      "Epoch: 98/2000... Training loss: 3.4094\n",
      "Epoch: 98/2000... Training loss: 3.4237\n",
      "Epoch: 98/2000... Training loss: 3.3878\n",
      "Epoch: 98/2000... Training loss: 3.4433\n",
      "Epoch: 98/2000... Training loss: 3.5938\n",
      "Epoch: 98/2000... Training loss: 3.5158\n",
      "Epoch: 98/2000... Training loss: 3.4224\n",
      "Epoch: 98/2000... Training loss: 3.4557\n",
      "Epoch: 98/2000... Training loss: 3.4854\n",
      "Epoch: 98/2000... Training loss: 3.8629\n",
      "Epoch: 98/2000... Training loss: 3.4353\n",
      "Epoch: 98/2000... Training loss: 3.6889\n",
      "Epoch: 98/2000... Training loss: 3.4421\n",
      "Epoch: 98/2000... Training loss: 3.5763\n",
      "Epoch: 98/2000... Training loss: 3.5982\n",
      "Epoch: 98/2000... Training loss: 3.6279\n",
      "Epoch: 98/2000... Training loss: 3.5871\n",
      "Epoch: 98/2000... Training loss: 3.4022\n",
      "Epoch: 98/2000... Training loss: 3.5977\n",
      "Epoch: 98/2000... Training loss: 3.5382\n",
      "Epoch: 98/2000... Training loss: 3.4584\n",
      "Epoch: 98/2000... Training loss: 3.6977\n",
      "Epoch: 98/2000... Training loss: 3.6892\n",
      "Epoch: 98/2000... Training loss: 3.4944\n",
      "Epoch: 98/2000... Training loss: 3.5931\n",
      "Epoch: 98/2000... Training loss: 3.5830\n",
      "Epoch: 98/2000... Training loss: 3.3823\n",
      "Epoch: 98/2000... Training loss: 3.5983\n",
      "Epoch: 98/2000... Training loss: 3.3907\n",
      "Epoch: 99/2000... Training loss: 3.5387\n",
      "Epoch: 99/2000... Training loss: 3.2865\n",
      "Epoch: 99/2000... Training loss: 3.3869\n",
      "Epoch: 99/2000... Training loss: 3.4891\n",
      "Epoch: 99/2000... Training loss: 3.4897\n",
      "Epoch: 99/2000... Training loss: 3.6394\n",
      "Epoch: 99/2000... Training loss: 3.7725\n",
      "Epoch: 99/2000... Training loss: 3.4838\n",
      "Epoch: 99/2000... Training loss: 3.3224\n",
      "Epoch: 99/2000... Training loss: 3.4061\n",
      "Epoch: 99/2000... Training loss: 3.7180\n",
      "Epoch: 99/2000... Training loss: 3.3918\n",
      "Epoch: 99/2000... Training loss: 3.4711\n",
      "Epoch: 99/2000... Training loss: 3.6349\n",
      "Epoch: 99/2000... Training loss: 3.4813\n",
      "Epoch: 99/2000... Training loss: 3.3333\n",
      "Epoch: 99/2000... Training loss: 3.7513\n",
      "Epoch: 99/2000... Training loss: 3.4327\n",
      "Epoch: 99/2000... Training loss: 3.4164\n",
      "Epoch: 99/2000... Training loss: 3.5671\n",
      "Epoch: 99/2000... Training loss: 3.3720\n",
      "Epoch: 99/2000... Training loss: 3.4000\n",
      "Epoch: 99/2000... Training loss: 3.5623\n",
      "Epoch: 99/2000... Training loss: 3.6406\n",
      "Epoch: 99/2000... Training loss: 3.4515\n",
      "Epoch: 99/2000... Training loss: 3.4894\n",
      "Epoch: 99/2000... Training loss: 3.7854\n",
      "Epoch: 99/2000... Training loss: 3.5285\n",
      "Epoch: 99/2000... Training loss: 3.5978\n",
      "Epoch: 99/2000... Training loss: 3.4649\n",
      "Epoch: 99/2000... Training loss: 3.3606\n",
      "Epoch: 100/2000... Training loss: 3.4504\n",
      "Epoch: 100/2000... Training loss: 3.5723\n",
      "Epoch: 100/2000... Training loss: 3.6448\n",
      "Epoch: 100/2000... Training loss: 3.2535\n",
      "Epoch: 100/2000... Training loss: 3.5550\n",
      "Epoch: 100/2000... Training loss: 3.5804\n",
      "Epoch: 100/2000... Training loss: 3.5639\n",
      "Epoch: 100/2000... Training loss: 3.3272\n",
      "Epoch: 100/2000... Training loss: 3.5641\n",
      "Epoch: 100/2000... Training loss: 3.3567\n",
      "Epoch: 100/2000... Training loss: 3.9077\n",
      "Epoch: 100/2000... Training loss: 3.0259\n",
      "Epoch: 100/2000... Training loss: 3.6374\n",
      "Epoch: 100/2000... Training loss: 3.4037\n",
      "Epoch: 100/2000... Training loss: 3.4566\n",
      "Epoch: 100/2000... Training loss: 3.4983\n",
      "Epoch: 100/2000... Training loss: 3.4686\n",
      "Epoch: 100/2000... Training loss: 3.4785\n",
      "Epoch: 100/2000... Training loss: 3.4868\n",
      "Epoch: 100/2000... Training loss: 3.2459\n",
      "Epoch: 100/2000... Training loss: 3.3637\n",
      "Epoch: 100/2000... Training loss: 3.5888\n",
      "Epoch: 100/2000... Training loss: 3.5462\n",
      "Epoch: 100/2000... Training loss: 3.6921\n",
      "Epoch: 100/2000... Training loss: 3.5941\n",
      "Epoch: 100/2000... Training loss: 3.4082\n",
      "Epoch: 100/2000... Training loss: 3.6936\n",
      "Epoch: 100/2000... Training loss: 3.5763\n",
      "Epoch: 100/2000... Training loss: 3.4072\n",
      "Epoch: 100/2000... Training loss: 3.5179\n",
      "Epoch: 100/2000... Training loss: 3.2944\n",
      "Epoch: 101/2000... Training loss: 3.4543\n",
      "Epoch: 101/2000... Training loss: 3.6951\n",
      "Epoch: 101/2000... Training loss: 3.3725\n",
      "Epoch: 101/2000... Training loss: 3.3930\n",
      "Epoch: 101/2000... Training loss: 3.4767\n",
      "Epoch: 101/2000... Training loss: 3.3983\n",
      "Epoch: 101/2000... Training loss: 3.2234\n",
      "Epoch: 101/2000... Training loss: 3.3247\n",
      "Epoch: 101/2000... Training loss: 3.5904\n",
      "Epoch: 101/2000... Training loss: 3.3781\n",
      "Epoch: 101/2000... Training loss: 3.6426\n",
      "Epoch: 101/2000... Training loss: 3.4846\n",
      "Epoch: 101/2000... Training loss: 3.5956\n",
      "Epoch: 101/2000... Training loss: 3.4085\n",
      "Epoch: 101/2000... Training loss: 3.5684\n",
      "Epoch: 101/2000... Training loss: 3.6927\n",
      "Epoch: 101/2000... Training loss: 3.5806\n",
      "Epoch: 101/2000... Training loss: 3.5836\n",
      "Epoch: 101/2000... Training loss: 3.5858\n",
      "Epoch: 101/2000... Training loss: 3.3339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101/2000... Training loss: 3.3785\n",
      "Epoch: 101/2000... Training loss: 3.4748\n",
      "Epoch: 101/2000... Training loss: 3.5016\n",
      "Epoch: 101/2000... Training loss: 3.4110\n",
      "Epoch: 101/2000... Training loss: 3.6201\n",
      "Epoch: 101/2000... Training loss: 3.5519\n",
      "Epoch: 101/2000... Training loss: 3.3255\n",
      "Epoch: 101/2000... Training loss: 3.6764\n",
      "Epoch: 101/2000... Training loss: 3.3226\n",
      "Epoch: 101/2000... Training loss: 3.2840\n",
      "Epoch: 101/2000... Training loss: 3.4813\n",
      "Epoch: 102/2000... Training loss: 3.3905\n",
      "Epoch: 102/2000... Training loss: 3.5923\n",
      "Epoch: 102/2000... Training loss: 3.4234\n",
      "Epoch: 102/2000... Training loss: 3.3283\n",
      "Epoch: 102/2000... Training loss: 3.6905\n",
      "Epoch: 102/2000... Training loss: 3.2521\n",
      "Epoch: 102/2000... Training loss: 3.5058\n",
      "Epoch: 102/2000... Training loss: 3.3899\n",
      "Epoch: 102/2000... Training loss: 3.6333\n",
      "Epoch: 102/2000... Training loss: 3.3670\n",
      "Epoch: 102/2000... Training loss: 3.4490\n",
      "Epoch: 102/2000... Training loss: 3.5225\n",
      "Epoch: 102/2000... Training loss: 3.3501\n",
      "Epoch: 102/2000... Training loss: 3.3872\n",
      "Epoch: 102/2000... Training loss: 3.6202\n",
      "Epoch: 102/2000... Training loss: 3.7122\n",
      "Epoch: 102/2000... Training loss: 3.5075\n",
      "Epoch: 102/2000... Training loss: 3.5227\n",
      "Epoch: 102/2000... Training loss: 3.4600\n",
      "Epoch: 102/2000... Training loss: 3.6189\n",
      "Epoch: 102/2000... Training loss: 3.5055\n",
      "Epoch: 102/2000... Training loss: 3.6890\n",
      "Epoch: 102/2000... Training loss: 3.3726\n",
      "Epoch: 102/2000... Training loss: 3.6827\n",
      "Epoch: 102/2000... Training loss: 3.5976\n",
      "Epoch: 102/2000... Training loss: 3.4689\n",
      "Epoch: 102/2000... Training loss: 3.3810\n",
      "Epoch: 102/2000... Training loss: 3.4307\n",
      "Epoch: 102/2000... Training loss: 3.6698\n",
      "Epoch: 102/2000... Training loss: 3.4256\n",
      "Epoch: 102/2000... Training loss: 3.4883\n",
      "Epoch: 103/2000... Training loss: 3.4459\n",
      "Epoch: 103/2000... Training loss: 3.2745\n",
      "Epoch: 103/2000... Training loss: 3.4293\n",
      "Epoch: 103/2000... Training loss: 3.5308\n",
      "Epoch: 103/2000... Training loss: 3.4059\n",
      "Epoch: 103/2000... Training loss: 3.8886\n",
      "Epoch: 103/2000... Training loss: 3.2923\n",
      "Epoch: 103/2000... Training loss: 3.5412\n",
      "Epoch: 103/2000... Training loss: 3.3738\n",
      "Epoch: 103/2000... Training loss: 3.4137\n",
      "Epoch: 103/2000... Training loss: 3.6046\n",
      "Epoch: 103/2000... Training loss: 3.4454\n",
      "Epoch: 103/2000... Training loss: 3.3904\n",
      "Epoch: 103/2000... Training loss: 3.4917\n",
      "Epoch: 103/2000... Training loss: 3.5180\n",
      "Epoch: 103/2000... Training loss: 3.5800\n",
      "Epoch: 103/2000... Training loss: 3.3614\n",
      "Epoch: 103/2000... Training loss: 3.6797\n",
      "Epoch: 103/2000... Training loss: 3.6069\n",
      "Epoch: 103/2000... Training loss: 3.4553\n",
      "Epoch: 103/2000... Training loss: 3.6718\n",
      "Epoch: 103/2000... Training loss: 3.5157\n",
      "Epoch: 103/2000... Training loss: 3.4188\n",
      "Epoch: 103/2000... Training loss: 3.4873\n",
      "Epoch: 103/2000... Training loss: 3.3684\n",
      "Epoch: 103/2000... Training loss: 3.5027\n",
      "Epoch: 103/2000... Training loss: 3.5021\n",
      "Epoch: 103/2000... Training loss: 3.5075\n",
      "Epoch: 103/2000... Training loss: 3.5348\n",
      "Epoch: 103/2000... Training loss: 3.3466\n",
      "Epoch: 103/2000... Training loss: 3.5749\n",
      "Epoch: 104/2000... Training loss: 3.3507\n",
      "Epoch: 104/2000... Training loss: 3.3926\n",
      "Epoch: 104/2000... Training loss: 3.5087\n",
      "Epoch: 104/2000... Training loss: 3.4659\n",
      "Epoch: 104/2000... Training loss: 3.6622\n",
      "Epoch: 104/2000... Training loss: 3.4092\n",
      "Epoch: 104/2000... Training loss: 3.2565\n",
      "Epoch: 104/2000... Training loss: 3.4394\n",
      "Epoch: 104/2000... Training loss: 3.3785\n",
      "Epoch: 104/2000... Training loss: 3.1355\n",
      "Epoch: 104/2000... Training loss: 3.3998\n",
      "Epoch: 104/2000... Training loss: 3.6286\n",
      "Epoch: 104/2000... Training loss: 3.3489\n",
      "Epoch: 104/2000... Training loss: 3.3466\n",
      "Epoch: 104/2000... Training loss: 3.2669\n",
      "Epoch: 104/2000... Training loss: 3.2723\n",
      "Epoch: 104/2000... Training loss: 3.4808\n",
      "Epoch: 104/2000... Training loss: 3.5209\n",
      "Epoch: 104/2000... Training loss: 3.5074\n",
      "Epoch: 104/2000... Training loss: 3.4275\n",
      "Epoch: 104/2000... Training loss: 3.3974\n",
      "Epoch: 104/2000... Training loss: 3.6097\n",
      "Epoch: 104/2000... Training loss: 3.2088\n",
      "Epoch: 104/2000... Training loss: 3.4604\n",
      "Epoch: 104/2000... Training loss: 3.5557\n",
      "Epoch: 104/2000... Training loss: 3.4694\n",
      "Epoch: 104/2000... Training loss: 3.5158\n",
      "Epoch: 104/2000... Training loss: 3.5328\n",
      "Epoch: 104/2000... Training loss: 3.4409\n",
      "Epoch: 104/2000... Training loss: 3.3787\n",
      "Epoch: 104/2000... Training loss: 3.6095\n",
      "Epoch: 105/2000... Training loss: 3.1899\n",
      "Epoch: 105/2000... Training loss: 3.5348\n",
      "Epoch: 105/2000... Training loss: 3.1253\n",
      "Epoch: 105/2000... Training loss: 3.1856\n",
      "Epoch: 105/2000... Training loss: 3.3824\n",
      "Epoch: 105/2000... Training loss: 3.3455\n",
      "Epoch: 105/2000... Training loss: 3.2978\n",
      "Epoch: 105/2000... Training loss: 3.3269\n",
      "Epoch: 105/2000... Training loss: 3.4877\n",
      "Epoch: 105/2000... Training loss: 3.2848\n",
      "Epoch: 105/2000... Training loss: 3.2916\n",
      "Epoch: 105/2000... Training loss: 3.6053\n",
      "Epoch: 105/2000... Training loss: 3.1975\n",
      "Epoch: 105/2000... Training loss: 3.7035\n",
      "Epoch: 105/2000... Training loss: 3.3525\n",
      "Epoch: 105/2000... Training loss: 3.4844\n",
      "Epoch: 105/2000... Training loss: 3.4656\n",
      "Epoch: 105/2000... Training loss: 3.6025\n",
      "Epoch: 105/2000... Training loss: 3.5793\n",
      "Epoch: 105/2000... Training loss: 3.2471\n",
      "Epoch: 105/2000... Training loss: 3.3748\n",
      "Epoch: 105/2000... Training loss: 3.4143\n",
      "Epoch: 105/2000... Training loss: 3.4327\n",
      "Epoch: 105/2000... Training loss: 3.3936\n",
      "Epoch: 105/2000... Training loss: 3.2707\n",
      "Epoch: 105/2000... Training loss: 3.3594\n",
      "Epoch: 105/2000... Training loss: 3.4515\n",
      "Epoch: 105/2000... Training loss: 3.5140\n",
      "Epoch: 105/2000... Training loss: 3.4992\n",
      "Epoch: 105/2000... Training loss: 3.2420\n",
      "Epoch: 105/2000... Training loss: 3.3021\n",
      "Epoch: 106/2000... Training loss: 3.4439\n",
      "Epoch: 106/2000... Training loss: 3.4299\n",
      "Epoch: 106/2000... Training loss: 3.4119\n",
      "Epoch: 106/2000... Training loss: 3.4612\n",
      "Epoch: 106/2000... Training loss: 3.4169\n",
      "Epoch: 106/2000... Training loss: 3.2980\n",
      "Epoch: 106/2000... Training loss: 3.0862\n",
      "Epoch: 106/2000... Training loss: 3.5367\n",
      "Epoch: 106/2000... Training loss: 3.4614\n",
      "Epoch: 106/2000... Training loss: 3.3260\n",
      "Epoch: 106/2000... Training loss: 3.4638\n",
      "Epoch: 106/2000... Training loss: 3.3807\n",
      "Epoch: 106/2000... Training loss: 3.6986\n",
      "Epoch: 106/2000... Training loss: 3.6237\n",
      "Epoch: 106/2000... Training loss: 3.4091\n",
      "Epoch: 106/2000... Training loss: 3.6803\n",
      "Epoch: 106/2000... Training loss: 3.5152\n",
      "Epoch: 106/2000... Training loss: 3.5004\n",
      "Epoch: 106/2000... Training loss: 3.6052\n",
      "Epoch: 106/2000... Training loss: 3.3670\n",
      "Epoch: 106/2000... Training loss: 3.3881\n",
      "Epoch: 106/2000... Training loss: 3.4591\n",
      "Epoch: 106/2000... Training loss: 3.4197\n",
      "Epoch: 106/2000... Training loss: 3.7294\n",
      "Epoch: 106/2000... Training loss: 3.3817\n",
      "Epoch: 106/2000... Training loss: 3.4631\n",
      "Epoch: 106/2000... Training loss: 3.3848\n",
      "Epoch: 106/2000... Training loss: 3.3511\n",
      "Epoch: 106/2000... Training loss: 3.4685\n",
      "Epoch: 106/2000... Training loss: 3.4938\n",
      "Epoch: 106/2000... Training loss: 3.4765\n",
      "Epoch: 107/2000... Training loss: 3.1830\n",
      "Epoch: 107/2000... Training loss: 3.4183\n",
      "Epoch: 107/2000... Training loss: 3.1257\n",
      "Epoch: 107/2000... Training loss: 3.2151\n",
      "Epoch: 107/2000... Training loss: 3.4974\n",
      "Epoch: 107/2000... Training loss: 3.3980\n",
      "Epoch: 107/2000... Training loss: 3.3648\n",
      "Epoch: 107/2000... Training loss: 3.6442\n",
      "Epoch: 107/2000... Training loss: 3.3730\n",
      "Epoch: 107/2000... Training loss: 3.5268\n",
      "Epoch: 107/2000... Training loss: 3.6629\n",
      "Epoch: 107/2000... Training loss: 3.2314\n",
      "Epoch: 107/2000... Training loss: 3.3071\n",
      "Epoch: 107/2000... Training loss: 3.3031\n",
      "Epoch: 107/2000... Training loss: 3.4569\n",
      "Epoch: 107/2000... Training loss: 3.4468\n",
      "Epoch: 107/2000... Training loss: 3.5022\n",
      "Epoch: 107/2000... Training loss: 3.5010\n",
      "Epoch: 107/2000... Training loss: 3.4068\n",
      "Epoch: 107/2000... Training loss: 2.9916\n",
      "Epoch: 107/2000... Training loss: 3.4690\n",
      "Epoch: 107/2000... Training loss: 3.3771\n",
      "Epoch: 107/2000... Training loss: 3.4207\n",
      "Epoch: 107/2000... Training loss: 3.5608\n",
      "Epoch: 107/2000... Training loss: 3.3639\n",
      "Epoch: 107/2000... Training loss: 3.4428\n",
      "Epoch: 107/2000... Training loss: 3.2988\n",
      "Epoch: 107/2000... Training loss: 3.4044\n",
      "Epoch: 107/2000... Training loss: 3.2534\n",
      "Epoch: 107/2000... Training loss: 3.4054\n",
      "Epoch: 107/2000... Training loss: 3.4855\n",
      "Epoch: 108/2000... Training loss: 3.3385\n",
      "Epoch: 108/2000... Training loss: 3.4163\n",
      "Epoch: 108/2000... Training loss: 3.3923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108/2000... Training loss: 3.4267\n",
      "Epoch: 108/2000... Training loss: 3.5248\n",
      "Epoch: 108/2000... Training loss: 3.3085\n",
      "Epoch: 108/2000... Training loss: 3.2494\n",
      "Epoch: 108/2000... Training loss: 3.3605\n",
      "Epoch: 108/2000... Training loss: 3.2146\n",
      "Epoch: 108/2000... Training loss: 3.1218\n",
      "Epoch: 108/2000... Training loss: 3.4470\n",
      "Epoch: 108/2000... Training loss: 3.2654\n",
      "Epoch: 108/2000... Training loss: 3.5368\n",
      "Epoch: 108/2000... Training loss: 3.5918\n",
      "Epoch: 108/2000... Training loss: 3.2942\n",
      "Epoch: 108/2000... Training loss: 3.4929\n",
      "Epoch: 108/2000... Training loss: 3.3367\n",
      "Epoch: 108/2000... Training loss: 3.3915\n",
      "Epoch: 108/2000... Training loss: 3.4388\n",
      "Epoch: 108/2000... Training loss: 3.1881\n",
      "Epoch: 108/2000... Training loss: 3.1945\n",
      "Epoch: 108/2000... Training loss: 3.3978\n",
      "Epoch: 108/2000... Training loss: 3.4007\n",
      "Epoch: 108/2000... Training loss: 3.5319\n",
      "Epoch: 108/2000... Training loss: 3.4246\n",
      "Epoch: 108/2000... Training loss: 3.3529\n",
      "Epoch: 108/2000... Training loss: 3.3615\n",
      "Epoch: 108/2000... Training loss: 3.3999\n",
      "Epoch: 108/2000... Training loss: 3.4389\n",
      "Epoch: 108/2000... Training loss: 3.3528\n",
      "Epoch: 108/2000... Training loss: 3.3343\n",
      "Epoch: 109/2000... Training loss: 3.2371\n",
      "Epoch: 109/2000... Training loss: 3.5635\n",
      "Epoch: 109/2000... Training loss: 3.2926\n",
      "Epoch: 109/2000... Training loss: 3.5843\n",
      "Epoch: 109/2000... Training loss: 3.4873\n",
      "Epoch: 109/2000... Training loss: 3.2821\n",
      "Epoch: 109/2000... Training loss: 3.3577\n",
      "Epoch: 109/2000... Training loss: 3.4723\n",
      "Epoch: 109/2000... Training loss: 3.1416\n",
      "Epoch: 109/2000... Training loss: 3.3193\n",
      "Epoch: 109/2000... Training loss: 3.3560\n",
      "Epoch: 109/2000... Training loss: 3.0539\n",
      "Epoch: 109/2000... Training loss: 3.3949\n",
      "Epoch: 109/2000... Training loss: 3.4152\n",
      "Epoch: 109/2000... Training loss: 3.2627\n",
      "Epoch: 109/2000... Training loss: 3.4126\n",
      "Epoch: 109/2000... Training loss: 3.3814\n",
      "Epoch: 109/2000... Training loss: 3.3284\n",
      "Epoch: 109/2000... Training loss: 3.3548\n",
      "Epoch: 109/2000... Training loss: 3.3362\n",
      "Epoch: 109/2000... Training loss: 3.4061\n",
      "Epoch: 109/2000... Training loss: 3.4748\n",
      "Epoch: 109/2000... Training loss: 3.1168\n",
      "Epoch: 109/2000... Training loss: 3.3541\n",
      "Epoch: 109/2000... Training loss: 3.3974\n",
      "Epoch: 109/2000... Training loss: 3.5327\n",
      "Epoch: 109/2000... Training loss: 3.5520\n",
      "Epoch: 109/2000... Training loss: 3.3076\n",
      "Epoch: 109/2000... Training loss: 3.2830\n",
      "Epoch: 109/2000... Training loss: 3.3990\n",
      "Epoch: 109/2000... Training loss: 3.5284\n",
      "Epoch: 110/2000... Training loss: 3.3699\n",
      "Epoch: 110/2000... Training loss: 3.2458\n",
      "Epoch: 110/2000... Training loss: 3.2576\n",
      "Epoch: 110/2000... Training loss: 3.0858\n",
      "Epoch: 110/2000... Training loss: 3.3357\n",
      "Epoch: 110/2000... Training loss: 3.3397\n",
      "Epoch: 110/2000... Training loss: 3.3580\n",
      "Epoch: 110/2000... Training loss: 3.4484\n",
      "Epoch: 110/2000... Training loss: 3.2482\n",
      "Epoch: 110/2000... Training loss: 3.2852\n",
      "Epoch: 110/2000... Training loss: 3.5043\n",
      "Epoch: 110/2000... Training loss: 3.4133\n",
      "Epoch: 110/2000... Training loss: 3.2732\n",
      "Epoch: 110/2000... Training loss: 3.3310\n",
      "Epoch: 110/2000... Training loss: 3.2191\n",
      "Epoch: 110/2000... Training loss: 3.3349\n",
      "Epoch: 110/2000... Training loss: 3.3456\n",
      "Epoch: 110/2000... Training loss: 3.3766\n",
      "Epoch: 110/2000... Training loss: 3.2241\n",
      "Epoch: 110/2000... Training loss: 3.2905\n",
      "Epoch: 110/2000... Training loss: 3.4745\n",
      "Epoch: 110/2000... Training loss: 3.3226\n",
      "Epoch: 110/2000... Training loss: 3.4298\n",
      "Epoch: 110/2000... Training loss: 3.3511\n",
      "Epoch: 110/2000... Training loss: 3.2776\n",
      "Epoch: 110/2000... Training loss: 3.2427\n",
      "Epoch: 110/2000... Training loss: 3.4105\n",
      "Epoch: 110/2000... Training loss: 3.5752\n",
      "Epoch: 110/2000... Training loss: 3.2784\n",
      "Epoch: 110/2000... Training loss: 3.3726\n",
      "Epoch: 110/2000... Training loss: 3.5059\n",
      "Epoch: 111/2000... Training loss: 3.1437\n",
      "Epoch: 111/2000... Training loss: 3.2370\n",
      "Epoch: 111/2000... Training loss: 3.4891\n",
      "Epoch: 111/2000... Training loss: 3.3115\n",
      "Epoch: 111/2000... Training loss: 3.2485\n",
      "Epoch: 111/2000... Training loss: 3.5312\n",
      "Epoch: 111/2000... Training loss: 3.2450\n",
      "Epoch: 111/2000... Training loss: 3.1901\n",
      "Epoch: 111/2000... Training loss: 3.1983\n",
      "Epoch: 111/2000... Training loss: 3.4602\n",
      "Epoch: 111/2000... Training loss: 3.2995\n",
      "Epoch: 111/2000... Training loss: 3.4652\n",
      "Epoch: 111/2000... Training loss: 3.4634\n",
      "Epoch: 111/2000... Training loss: 3.1182\n",
      "Epoch: 111/2000... Training loss: 3.4799\n",
      "Epoch: 111/2000... Training loss: 3.2834\n",
      "Epoch: 111/2000... Training loss: 3.3576\n",
      "Epoch: 111/2000... Training loss: 3.4241\n",
      "Epoch: 111/2000... Training loss: 3.4639\n",
      "Epoch: 111/2000... Training loss: 3.4731\n",
      "Epoch: 111/2000... Training loss: 3.5196\n",
      "Epoch: 111/2000... Training loss: 3.3118\n",
      "Epoch: 111/2000... Training loss: 3.3446\n",
      "Epoch: 111/2000... Training loss: 3.3737\n",
      "Epoch: 111/2000... Training loss: 3.4407\n",
      "Epoch: 111/2000... Training loss: 3.5772\n",
      "Epoch: 111/2000... Training loss: 3.2846\n",
      "Epoch: 111/2000... Training loss: 3.3317\n",
      "Epoch: 111/2000... Training loss: 3.2382\n",
      "Epoch: 111/2000... Training loss: 3.3616\n",
      "Epoch: 111/2000... Training loss: 3.4167\n",
      "Epoch: 112/2000... Training loss: 3.2733\n",
      "Epoch: 112/2000... Training loss: 3.3363\n",
      "Epoch: 112/2000... Training loss: 3.4724\n",
      "Epoch: 112/2000... Training loss: 3.2540\n",
      "Epoch: 112/2000... Training loss: 3.3601\n",
      "Epoch: 112/2000... Training loss: 3.1615\n",
      "Epoch: 112/2000... Training loss: 3.1980\n",
      "Epoch: 112/2000... Training loss: 3.4103\n",
      "Epoch: 112/2000... Training loss: 3.3298\n",
      "Epoch: 112/2000... Training loss: 3.3491\n",
      "Epoch: 112/2000... Training loss: 3.1637\n",
      "Epoch: 112/2000... Training loss: 3.1285\n",
      "Epoch: 112/2000... Training loss: 3.2795\n",
      "Epoch: 112/2000... Training loss: 3.3773\n",
      "Epoch: 112/2000... Training loss: 3.1458\n",
      "Epoch: 112/2000... Training loss: 3.2690\n",
      "Epoch: 112/2000... Training loss: 3.2988\n",
      "Epoch: 112/2000... Training loss: 3.3139\n",
      "Epoch: 112/2000... Training loss: 3.2490\n",
      "Epoch: 112/2000... Training loss: 3.2778\n",
      "Epoch: 112/2000... Training loss: 3.4843\n",
      "Epoch: 112/2000... Training loss: 3.2994\n",
      "Epoch: 112/2000... Training loss: 3.3205\n",
      "Epoch: 112/2000... Training loss: 3.4448\n",
      "Epoch: 112/2000... Training loss: 3.4667\n",
      "Epoch: 112/2000... Training loss: 3.1486\n",
      "Epoch: 112/2000... Training loss: 3.5927\n",
      "Epoch: 112/2000... Training loss: 3.2480\n",
      "Epoch: 112/2000... Training loss: 3.2114\n",
      "Epoch: 112/2000... Training loss: 3.4229\n",
      "Epoch: 112/2000... Training loss: 3.3455\n",
      "Epoch: 113/2000... Training loss: 3.0249\n",
      "Epoch: 113/2000... Training loss: 3.3695\n",
      "Epoch: 113/2000... Training loss: 3.3916\n",
      "Epoch: 113/2000... Training loss: 3.2772\n",
      "Epoch: 113/2000... Training loss: 3.2678\n",
      "Epoch: 113/2000... Training loss: 3.1260\n",
      "Epoch: 113/2000... Training loss: 3.3519\n",
      "Epoch: 113/2000... Training loss: 3.4904\n",
      "Epoch: 113/2000... Training loss: 3.3084\n",
      "Epoch: 113/2000... Training loss: 3.2393\n",
      "Epoch: 113/2000... Training loss: 3.2640\n",
      "Epoch: 113/2000... Training loss: 3.3573\n",
      "Epoch: 113/2000... Training loss: 3.3765\n",
      "Epoch: 113/2000... Training loss: 3.4769\n",
      "Epoch: 113/2000... Training loss: 3.4611\n",
      "Epoch: 113/2000... Training loss: 3.0341\n",
      "Epoch: 113/2000... Training loss: 3.3931\n",
      "Epoch: 113/2000... Training loss: 3.1741\n",
      "Epoch: 113/2000... Training loss: 3.2828\n",
      "Epoch: 113/2000... Training loss: 3.2544\n",
      "Epoch: 113/2000... Training loss: 3.4631\n",
      "Epoch: 113/2000... Training loss: 3.3070\n",
      "Epoch: 113/2000... Training loss: 3.2054\n",
      "Epoch: 113/2000... Training loss: 3.1007\n",
      "Epoch: 113/2000... Training loss: 3.2613\n",
      "Epoch: 113/2000... Training loss: 3.4260\n",
      "Epoch: 113/2000... Training loss: 3.3814\n",
      "Epoch: 113/2000... Training loss: 3.5257\n",
      "Epoch: 113/2000... Training loss: 3.2042\n",
      "Epoch: 113/2000... Training loss: 3.3001\n",
      "Epoch: 113/2000... Training loss: 3.3099\n",
      "Epoch: 114/2000... Training loss: 3.1231\n",
      "Epoch: 114/2000... Training loss: 3.1168\n",
      "Epoch: 114/2000... Training loss: 3.0486\n",
      "Epoch: 114/2000... Training loss: 3.0436\n",
      "Epoch: 114/2000... Training loss: 3.1972\n",
      "Epoch: 114/2000... Training loss: 3.2239\n",
      "Epoch: 114/2000... Training loss: 3.3787\n",
      "Epoch: 114/2000... Training loss: 3.3635\n",
      "Epoch: 114/2000... Training loss: 3.3909\n",
      "Epoch: 114/2000... Training loss: 3.3001\n",
      "Epoch: 114/2000... Training loss: 3.1542\n",
      "Epoch: 114/2000... Training loss: 3.3319\n",
      "Epoch: 114/2000... Training loss: 3.3218\n",
      "Epoch: 114/2000... Training loss: 3.3879\n",
      "Epoch: 114/2000... Training loss: 3.1384\n",
      "Epoch: 114/2000... Training loss: 3.1254\n",
      "Epoch: 114/2000... Training loss: 3.4303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114/2000... Training loss: 3.3702\n",
      "Epoch: 114/2000... Training loss: 3.2615\n",
      "Epoch: 114/2000... Training loss: 3.2246\n",
      "Epoch: 114/2000... Training loss: 3.4824\n",
      "Epoch: 114/2000... Training loss: 3.1549\n",
      "Epoch: 114/2000... Training loss: 3.1168\n",
      "Epoch: 114/2000... Training loss: 3.3664\n",
      "Epoch: 114/2000... Training loss: 3.2188\n",
      "Epoch: 114/2000... Training loss: 3.3331\n",
      "Epoch: 114/2000... Training loss: 3.5336\n",
      "Epoch: 114/2000... Training loss: 3.2972\n",
      "Epoch: 114/2000... Training loss: 3.1893\n",
      "Epoch: 114/2000... Training loss: 3.1234\n",
      "Epoch: 114/2000... Training loss: 3.1025\n",
      "Epoch: 115/2000... Training loss: 3.0263\n",
      "Epoch: 115/2000... Training loss: 3.2628\n",
      "Epoch: 115/2000... Training loss: 3.2723\n",
      "Epoch: 115/2000... Training loss: 3.2502\n",
      "Epoch: 115/2000... Training loss: 3.4942\n",
      "Epoch: 115/2000... Training loss: 3.3164\n",
      "Epoch: 115/2000... Training loss: 3.5545\n",
      "Epoch: 115/2000... Training loss: 3.3713\n",
      "Epoch: 115/2000... Training loss: 3.1263\n",
      "Epoch: 115/2000... Training loss: 3.1631\n",
      "Epoch: 115/2000... Training loss: 3.3895\n",
      "Epoch: 115/2000... Training loss: 3.1302\n",
      "Epoch: 115/2000... Training loss: 3.0925\n",
      "Epoch: 115/2000... Training loss: 3.3464\n",
      "Epoch: 115/2000... Training loss: 3.3242\n",
      "Epoch: 115/2000... Training loss: 3.2137\n",
      "Epoch: 115/2000... Training loss: 3.5126\n",
      "Epoch: 115/2000... Training loss: 3.2882\n",
      "Epoch: 115/2000... Training loss: 3.4276\n",
      "Epoch: 115/2000... Training loss: 3.0752\n",
      "Epoch: 115/2000... Training loss: 3.3715\n",
      "Epoch: 115/2000... Training loss: 3.3458\n",
      "Epoch: 115/2000... Training loss: 2.9979\n",
      "Epoch: 115/2000... Training loss: 3.3961\n",
      "Epoch: 115/2000... Training loss: 3.5643\n",
      "Epoch: 115/2000... Training loss: 3.2049\n",
      "Epoch: 115/2000... Training loss: 3.4146\n",
      "Epoch: 115/2000... Training loss: 3.2062\n",
      "Epoch: 115/2000... Training loss: 3.4587\n",
      "Epoch: 115/2000... Training loss: 3.5070\n",
      "Epoch: 115/2000... Training loss: 3.2270\n",
      "Epoch: 116/2000... Training loss: 3.1990\n",
      "Epoch: 116/2000... Training loss: 3.2989\n",
      "Epoch: 116/2000... Training loss: 3.2378\n",
      "Epoch: 116/2000... Training loss: 3.3505\n",
      "Epoch: 116/2000... Training loss: 3.4075\n",
      "Epoch: 116/2000... Training loss: 3.1913\n",
      "Epoch: 116/2000... Training loss: 3.3760\n",
      "Epoch: 116/2000... Training loss: 3.2091\n",
      "Epoch: 116/2000... Training loss: 3.2426\n",
      "Epoch: 116/2000... Training loss: 3.2491\n",
      "Epoch: 116/2000... Training loss: 3.3126\n",
      "Epoch: 116/2000... Training loss: 3.4108\n",
      "Epoch: 116/2000... Training loss: 3.1809\n",
      "Epoch: 116/2000... Training loss: 3.3704\n",
      "Epoch: 116/2000... Training loss: 3.4294\n",
      "Epoch: 116/2000... Training loss: 3.3120\n",
      "Epoch: 116/2000... Training loss: 3.3743\n",
      "Epoch: 116/2000... Training loss: 3.1248\n",
      "Epoch: 116/2000... Training loss: 3.2277\n",
      "Epoch: 116/2000... Training loss: 3.1890\n",
      "Epoch: 116/2000... Training loss: 3.2247\n",
      "Epoch: 116/2000... Training loss: 3.3434\n",
      "Epoch: 116/2000... Training loss: 3.1559\n",
      "Epoch: 116/2000... Training loss: 3.3386\n",
      "Epoch: 116/2000... Training loss: 3.2980\n",
      "Epoch: 116/2000... Training loss: 3.4256\n",
      "Epoch: 116/2000... Training loss: 3.4944\n",
      "Epoch: 116/2000... Training loss: 3.2828\n",
      "Epoch: 116/2000... Training loss: 3.2672\n",
      "Epoch: 116/2000... Training loss: 3.1450\n",
      "Epoch: 116/2000... Training loss: 3.1628\n",
      "Epoch: 117/2000... Training loss: 3.2877\n",
      "Epoch: 117/2000... Training loss: 3.2968\n",
      "Epoch: 117/2000... Training loss: 3.3047\n",
      "Epoch: 117/2000... Training loss: 3.2407\n",
      "Epoch: 117/2000... Training loss: 3.2015\n",
      "Epoch: 117/2000... Training loss: 3.1399\n",
      "Epoch: 117/2000... Training loss: 3.2161\n",
      "Epoch: 117/2000... Training loss: 3.1789\n",
      "Epoch: 117/2000... Training loss: 3.2459\n",
      "Epoch: 117/2000... Training loss: 3.0168\n",
      "Epoch: 117/2000... Training loss: 3.2557\n",
      "Epoch: 117/2000... Training loss: 3.2488\n",
      "Epoch: 117/2000... Training loss: 3.1812\n",
      "Epoch: 117/2000... Training loss: 3.2459\n",
      "Epoch: 117/2000... Training loss: 3.4313\n",
      "Epoch: 117/2000... Training loss: 3.1702\n",
      "Epoch: 117/2000... Training loss: 3.0770\n",
      "Epoch: 117/2000... Training loss: 3.4286\n",
      "Epoch: 117/2000... Training loss: 3.2513\n",
      "Epoch: 117/2000... Training loss: 3.4464\n",
      "Epoch: 117/2000... Training loss: 3.1481\n",
      "Epoch: 117/2000... Training loss: 3.3301\n",
      "Epoch: 117/2000... Training loss: 3.1656\n",
      "Epoch: 117/2000... Training loss: 3.4410\n",
      "Epoch: 117/2000... Training loss: 3.2594\n",
      "Epoch: 117/2000... Training loss: 3.1372\n",
      "Epoch: 117/2000... Training loss: 3.1630\n",
      "Epoch: 117/2000... Training loss: 3.4266\n",
      "Epoch: 117/2000... Training loss: 3.0582\n",
      "Epoch: 117/2000... Training loss: 3.2001\n",
      "Epoch: 117/2000... Training loss: 3.2655\n",
      "Epoch: 118/2000... Training loss: 3.1941\n",
      "Epoch: 118/2000... Training loss: 3.1120\n",
      "Epoch: 118/2000... Training loss: 3.0755\n",
      "Epoch: 118/2000... Training loss: 3.1443\n",
      "Epoch: 118/2000... Training loss: 3.1825\n",
      "Epoch: 118/2000... Training loss: 3.2314\n",
      "Epoch: 118/2000... Training loss: 3.1974\n",
      "Epoch: 118/2000... Training loss: 3.0072\n",
      "Epoch: 118/2000... Training loss: 2.9970\n",
      "Epoch: 118/2000... Training loss: 3.1387\n",
      "Epoch: 118/2000... Training loss: 3.2514\n",
      "Epoch: 118/2000... Training loss: 3.2201\n",
      "Epoch: 118/2000... Training loss: 3.2008\n",
      "Epoch: 118/2000... Training loss: 3.3508\n",
      "Epoch: 118/2000... Training loss: 3.2415\n",
      "Epoch: 118/2000... Training loss: 3.1959\n",
      "Epoch: 118/2000... Training loss: 3.5352\n",
      "Epoch: 118/2000... Training loss: 3.3408\n",
      "Epoch: 118/2000... Training loss: 3.0888\n",
      "Epoch: 118/2000... Training loss: 3.3799\n",
      "Epoch: 118/2000... Training loss: 3.1425\n",
      "Epoch: 118/2000... Training loss: 3.2986\n",
      "Epoch: 118/2000... Training loss: 3.2060\n",
      "Epoch: 118/2000... Training loss: 3.1892\n",
      "Epoch: 118/2000... Training loss: 3.3103\n",
      "Epoch: 118/2000... Training loss: 3.4371\n",
      "Epoch: 118/2000... Training loss: 3.2953\n",
      "Epoch: 118/2000... Training loss: 3.1117\n",
      "Epoch: 118/2000... Training loss: 3.2134\n",
      "Epoch: 118/2000... Training loss: 3.2004\n",
      "Epoch: 118/2000... Training loss: 3.1016\n",
      "Epoch: 119/2000... Training loss: 3.1972\n",
      "Epoch: 119/2000... Training loss: 3.1477\n",
      "Epoch: 119/2000... Training loss: 3.1451\n",
      "Epoch: 119/2000... Training loss: 3.2348\n",
      "Epoch: 119/2000... Training loss: 3.1000\n",
      "Epoch: 119/2000... Training loss: 3.2078\n",
      "Epoch: 119/2000... Training loss: 3.0739\n",
      "Epoch: 119/2000... Training loss: 3.2310\n",
      "Epoch: 119/2000... Training loss: 3.2116\n",
      "Epoch: 119/2000... Training loss: 3.2629\n",
      "Epoch: 119/2000... Training loss: 3.2969\n",
      "Epoch: 119/2000... Training loss: 3.0902\n",
      "Epoch: 119/2000... Training loss: 3.1994\n",
      "Epoch: 119/2000... Training loss: 3.1144\n",
      "Epoch: 119/2000... Training loss: 3.3013\n",
      "Epoch: 119/2000... Training loss: 3.2590\n",
      "Epoch: 119/2000... Training loss: 3.1560\n",
      "Epoch: 119/2000... Training loss: 3.2902\n",
      "Epoch: 119/2000... Training loss: 3.0702\n",
      "Epoch: 119/2000... Training loss: 3.2038\n",
      "Epoch: 119/2000... Training loss: 3.2091\n",
      "Epoch: 119/2000... Training loss: 3.0697\n",
      "Epoch: 119/2000... Training loss: 3.0700\n",
      "Epoch: 119/2000... Training loss: 3.2158\n",
      "Epoch: 119/2000... Training loss: 3.3000\n",
      "Epoch: 119/2000... Training loss: 3.3503\n",
      "Epoch: 119/2000... Training loss: 3.1342\n",
      "Epoch: 119/2000... Training loss: 3.1429\n",
      "Epoch: 119/2000... Training loss: 3.2443\n",
      "Epoch: 119/2000... Training loss: 3.0068\n",
      "Epoch: 119/2000... Training loss: 3.2237\n",
      "Epoch: 120/2000... Training loss: 2.9851\n",
      "Epoch: 120/2000... Training loss: 3.1832\n",
      "Epoch: 120/2000... Training loss: 3.1447\n",
      "Epoch: 120/2000... Training loss: 2.9176\n",
      "Epoch: 120/2000... Training loss: 3.1452\n",
      "Epoch: 120/2000... Training loss: 3.1419\n",
      "Epoch: 120/2000... Training loss: 3.2687\n",
      "Epoch: 120/2000... Training loss: 3.1471\n",
      "Epoch: 120/2000... Training loss: 3.4864\n",
      "Epoch: 120/2000... Training loss: 2.9458\n",
      "Epoch: 120/2000... Training loss: 3.1326\n",
      "Epoch: 120/2000... Training loss: 3.2334\n",
      "Epoch: 120/2000... Training loss: 3.2591\n",
      "Epoch: 120/2000... Training loss: 3.2565\n",
      "Epoch: 120/2000... Training loss: 3.3420\n",
      "Epoch: 120/2000... Training loss: 3.3071\n",
      "Epoch: 120/2000... Training loss: 2.9676\n",
      "Epoch: 120/2000... Training loss: 3.0719\n",
      "Epoch: 120/2000... Training loss: 3.3229\n",
      "Epoch: 120/2000... Training loss: 3.2408\n",
      "Epoch: 120/2000... Training loss: 3.0945\n",
      "Epoch: 120/2000... Training loss: 3.1444\n",
      "Epoch: 120/2000... Training loss: 3.2200\n",
      "Epoch: 120/2000... Training loss: 3.3722\n",
      "Epoch: 120/2000... Training loss: 3.3258\n",
      "Epoch: 120/2000... Training loss: 3.3214\n",
      "Epoch: 120/2000... Training loss: 3.3893\n",
      "Epoch: 120/2000... Training loss: 3.1944\n",
      "Epoch: 120/2000... Training loss: 3.3691\n",
      "Epoch: 120/2000... Training loss: 3.1658\n",
      "Epoch: 120/2000... Training loss: 3.0800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121/2000... Training loss: 3.0510\n",
      "Epoch: 121/2000... Training loss: 3.2643\n",
      "Epoch: 121/2000... Training loss: 3.0557\n",
      "Epoch: 121/2000... Training loss: 3.1538\n",
      "Epoch: 121/2000... Training loss: 3.3344\n",
      "Epoch: 121/2000... Training loss: 3.3286\n",
      "Epoch: 121/2000... Training loss: 3.1435\n",
      "Epoch: 121/2000... Training loss: 3.1736\n",
      "Epoch: 121/2000... Training loss: 3.3707\n",
      "Epoch: 121/2000... Training loss: 3.4405\n",
      "Epoch: 121/2000... Training loss: 3.2408\n",
      "Epoch: 121/2000... Training loss: 3.4232\n",
      "Epoch: 121/2000... Training loss: 3.1645\n",
      "Epoch: 121/2000... Training loss: 3.1350\n",
      "Epoch: 121/2000... Training loss: 3.2399\n",
      "Epoch: 121/2000... Training loss: 3.3029\n",
      "Epoch: 121/2000... Training loss: 3.2269\n",
      "Epoch: 121/2000... Training loss: 3.3446\n",
      "Epoch: 121/2000... Training loss: 3.1042\n",
      "Epoch: 121/2000... Training loss: 3.1521\n",
      "Epoch: 121/2000... Training loss: 3.1847\n",
      "Epoch: 121/2000... Training loss: 2.9735\n",
      "Epoch: 121/2000... Training loss: 3.1555\n",
      "Epoch: 121/2000... Training loss: 3.5210\n",
      "Epoch: 121/2000... Training loss: 3.1055\n",
      "Epoch: 121/2000... Training loss: 3.3061\n",
      "Epoch: 121/2000... Training loss: 3.1025\n",
      "Epoch: 121/2000... Training loss: 3.0798\n",
      "Epoch: 121/2000... Training loss: 3.3571\n",
      "Epoch: 121/2000... Training loss: 3.1923\n",
      "Epoch: 121/2000... Training loss: 2.9694\n",
      "Epoch: 122/2000... Training loss: 3.4823\n",
      "Epoch: 122/2000... Training loss: 3.3444\n",
      "Epoch: 122/2000... Training loss: 3.1835\n",
      "Epoch: 122/2000... Training loss: 3.1109\n",
      "Epoch: 122/2000... Training loss: 3.1847\n",
      "Epoch: 122/2000... Training loss: 3.1489\n",
      "Epoch: 122/2000... Training loss: 3.3622\n",
      "Epoch: 122/2000... Training loss: 3.2154\n",
      "Epoch: 122/2000... Training loss: 3.2802\n",
      "Epoch: 122/2000... Training loss: 3.4073\n",
      "Epoch: 122/2000... Training loss: 3.0546\n",
      "Epoch: 122/2000... Training loss: 3.2715\n",
      "Epoch: 122/2000... Training loss: 3.3729\n",
      "Epoch: 122/2000... Training loss: 3.2535\n",
      "Epoch: 122/2000... Training loss: 3.1313\n",
      "Epoch: 122/2000... Training loss: 3.1056\n",
      "Epoch: 122/2000... Training loss: 3.1743\n",
      "Epoch: 122/2000... Training loss: 3.1497\n",
      "Epoch: 122/2000... Training loss: 3.1790\n",
      "Epoch: 122/2000... Training loss: 3.0115\n",
      "Epoch: 122/2000... Training loss: 3.4608\n",
      "Epoch: 122/2000... Training loss: 3.2855\n",
      "Epoch: 122/2000... Training loss: 3.0498\n",
      "Epoch: 122/2000... Training loss: 3.3010\n",
      "Epoch: 122/2000... Training loss: 3.0004\n",
      "Epoch: 122/2000... Training loss: 3.1264\n",
      "Epoch: 122/2000... Training loss: 3.0564\n",
      "Epoch: 122/2000... Training loss: 3.2536\n",
      "Epoch: 122/2000... Training loss: 3.3807\n",
      "Epoch: 122/2000... Training loss: 3.2461\n",
      "Epoch: 122/2000... Training loss: 3.3446\n",
      "Epoch: 123/2000... Training loss: 3.1697\n",
      "Epoch: 123/2000... Training loss: 3.1757\n",
      "Epoch: 123/2000... Training loss: 2.9111\n",
      "Epoch: 123/2000... Training loss: 3.1099\n",
      "Epoch: 123/2000... Training loss: 3.0273\n",
      "Epoch: 123/2000... Training loss: 3.3196\n",
      "Epoch: 123/2000... Training loss: 2.9665\n",
      "Epoch: 123/2000... Training loss: 2.9934\n",
      "Epoch: 123/2000... Training loss: 3.1640\n",
      "Epoch: 123/2000... Training loss: 3.0250\n",
      "Epoch: 123/2000... Training loss: 3.2220\n",
      "Epoch: 123/2000... Training loss: 3.1387\n",
      "Epoch: 123/2000... Training loss: 2.9746\n",
      "Epoch: 123/2000... Training loss: 3.2308\n",
      "Epoch: 123/2000... Training loss: 3.2267\n",
      "Epoch: 123/2000... Training loss: 2.8701\n",
      "Epoch: 123/2000... Training loss: 3.3052\n",
      "Epoch: 123/2000... Training loss: 3.2405\n",
      "Epoch: 123/2000... Training loss: 3.2299\n",
      "Epoch: 123/2000... Training loss: 3.2156\n",
      "Epoch: 123/2000... Training loss: 3.3183\n",
      "Epoch: 123/2000... Training loss: 3.2273\n",
      "Epoch: 123/2000... Training loss: 2.9973\n",
      "Epoch: 123/2000... Training loss: 3.0471\n",
      "Epoch: 123/2000... Training loss: 3.1731\n",
      "Epoch: 123/2000... Training loss: 3.3395\n",
      "Epoch: 123/2000... Training loss: 3.0861\n",
      "Epoch: 123/2000... Training loss: 3.1997\n",
      "Epoch: 123/2000... Training loss: 3.1375\n",
      "Epoch: 123/2000... Training loss: 3.1514\n",
      "Epoch: 123/2000... Training loss: 3.0043\n",
      "Epoch: 124/2000... Training loss: 3.1539\n",
      "Epoch: 124/2000... Training loss: 2.9913\n",
      "Epoch: 124/2000... Training loss: 3.1838\n",
      "Epoch: 124/2000... Training loss: 3.2868\n",
      "Epoch: 124/2000... Training loss: 3.2016\n",
      "Epoch: 124/2000... Training loss: 3.1746\n",
      "Epoch: 124/2000... Training loss: 2.9811\n",
      "Epoch: 124/2000... Training loss: 3.0253\n",
      "Epoch: 124/2000... Training loss: 3.4836\n",
      "Epoch: 124/2000... Training loss: 3.2527\n",
      "Epoch: 124/2000... Training loss: 2.8730\n",
      "Epoch: 124/2000... Training loss: 3.0015\n",
      "Epoch: 124/2000... Training loss: 2.9206\n",
      "Epoch: 124/2000... Training loss: 2.9476\n",
      "Epoch: 124/2000... Training loss: 3.2922\n",
      "Epoch: 124/2000... Training loss: 3.0648\n",
      "Epoch: 124/2000... Training loss: 3.0770\n",
      "Epoch: 124/2000... Training loss: 3.3052\n",
      "Epoch: 124/2000... Training loss: 3.2783\n",
      "Epoch: 124/2000... Training loss: 3.0390\n",
      "Epoch: 124/2000... Training loss: 2.8592\n",
      "Epoch: 124/2000... Training loss: 3.0492\n",
      "Epoch: 124/2000... Training loss: 3.1892\n",
      "Epoch: 124/2000... Training loss: 3.3655\n",
      "Epoch: 124/2000... Training loss: 3.1262\n",
      "Epoch: 124/2000... Training loss: 3.1983\n",
      "Epoch: 124/2000... Training loss: 2.9566\n",
      "Epoch: 124/2000... Training loss: 3.1368\n",
      "Epoch: 124/2000... Training loss: 3.1011\n",
      "Epoch: 124/2000... Training loss: 3.1084\n",
      "Epoch: 124/2000... Training loss: 3.1526\n",
      "Epoch: 125/2000... Training loss: 3.0987\n",
      "Epoch: 125/2000... Training loss: 3.1156\n",
      "Epoch: 125/2000... Training loss: 3.0356\n",
      "Epoch: 125/2000... Training loss: 3.1882\n",
      "Epoch: 125/2000... Training loss: 3.1315\n",
      "Epoch: 125/2000... Training loss: 3.0434\n",
      "Epoch: 125/2000... Training loss: 3.2501\n",
      "Epoch: 125/2000... Training loss: 3.1796\n",
      "Epoch: 125/2000... Training loss: 3.1522\n",
      "Epoch: 125/2000... Training loss: 3.2667\n",
      "Epoch: 125/2000... Training loss: 3.2796\n",
      "Epoch: 125/2000... Training loss: 2.9222\n",
      "Epoch: 125/2000... Training loss: 2.9753\n",
      "Epoch: 125/2000... Training loss: 3.1021\n",
      "Epoch: 125/2000... Training loss: 2.9843\n",
      "Epoch: 125/2000... Training loss: 3.1057\n",
      "Epoch: 125/2000... Training loss: 3.2577\n",
      "Epoch: 125/2000... Training loss: 3.0799\n",
      "Epoch: 125/2000... Training loss: 3.1061\n",
      "Epoch: 125/2000... Training loss: 3.0655\n",
      "Epoch: 125/2000... Training loss: 3.1819\n",
      "Epoch: 125/2000... Training loss: 3.1872\n",
      "Epoch: 125/2000... Training loss: 2.9639\n",
      "Epoch: 125/2000... Training loss: 3.1647\n",
      "Epoch: 125/2000... Training loss: 2.8653\n",
      "Epoch: 125/2000... Training loss: 3.3466\n",
      "Epoch: 125/2000... Training loss: 3.0481\n",
      "Epoch: 125/2000... Training loss: 3.0707\n",
      "Epoch: 125/2000... Training loss: 2.9074\n",
      "Epoch: 125/2000... Training loss: 2.9625\n",
      "Epoch: 125/2000... Training loss: 2.8075\n",
      "Epoch: 126/2000... Training loss: 3.0829\n",
      "Epoch: 126/2000... Training loss: 3.0676\n",
      "Epoch: 126/2000... Training loss: 2.9316\n",
      "Epoch: 126/2000... Training loss: 2.9983\n",
      "Epoch: 126/2000... Training loss: 3.2174\n",
      "Epoch: 126/2000... Training loss: 3.0723\n",
      "Epoch: 126/2000... Training loss: 3.1282\n",
      "Epoch: 126/2000... Training loss: 3.1794\n",
      "Epoch: 126/2000... Training loss: 3.2076\n",
      "Epoch: 126/2000... Training loss: 3.0758\n",
      "Epoch: 126/2000... Training loss: 3.1402\n",
      "Epoch: 126/2000... Training loss: 3.3285\n",
      "Epoch: 126/2000... Training loss: 3.2390\n",
      "Epoch: 126/2000... Training loss: 2.9177\n",
      "Epoch: 126/2000... Training loss: 2.9614\n",
      "Epoch: 126/2000... Training loss: 3.1757\n",
      "Epoch: 126/2000... Training loss: 3.2146\n",
      "Epoch: 126/2000... Training loss: 3.0904\n",
      "Epoch: 126/2000... Training loss: 3.1783\n",
      "Epoch: 126/2000... Training loss: 3.2406\n",
      "Epoch: 126/2000... Training loss: 2.9935\n",
      "Epoch: 126/2000... Training loss: 3.2713\n",
      "Epoch: 126/2000... Training loss: 3.1248\n",
      "Epoch: 126/2000... Training loss: 3.2528\n",
      "Epoch: 126/2000... Training loss: 3.2262\n",
      "Epoch: 126/2000... Training loss: 3.1935\n",
      "Epoch: 126/2000... Training loss: 2.9500\n",
      "Epoch: 126/2000... Training loss: 3.1410\n",
      "Epoch: 126/2000... Training loss: 2.8431\n",
      "Epoch: 126/2000... Training loss: 3.2844\n",
      "Epoch: 126/2000... Training loss: 3.2492\n",
      "Epoch: 127/2000... Training loss: 3.1088\n",
      "Epoch: 127/2000... Training loss: 3.0427\n",
      "Epoch: 127/2000... Training loss: 3.0344\n",
      "Epoch: 127/2000... Training loss: 3.0339\n",
      "Epoch: 127/2000... Training loss: 3.1946\n",
      "Epoch: 127/2000... Training loss: 3.0470\n",
      "Epoch: 127/2000... Training loss: 3.1707\n",
      "Epoch: 127/2000... Training loss: 3.2337\n",
      "Epoch: 127/2000... Training loss: 3.1264\n",
      "Epoch: 127/2000... Training loss: 3.1918\n",
      "Epoch: 127/2000... Training loss: 3.0695\n",
      "Epoch: 127/2000... Training loss: 3.0469\n",
      "Epoch: 127/2000... Training loss: 2.9636\n",
      "Epoch: 127/2000... Training loss: 2.8976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 127/2000... Training loss: 3.0088\n",
      "Epoch: 127/2000... Training loss: 3.0910\n",
      "Epoch: 127/2000... Training loss: 3.0528\n",
      "Epoch: 127/2000... Training loss: 3.1149\n",
      "Epoch: 127/2000... Training loss: 3.1844\n",
      "Epoch: 127/2000... Training loss: 3.1324\n",
      "Epoch: 127/2000... Training loss: 3.0795\n",
      "Epoch: 127/2000... Training loss: 3.1909\n",
      "Epoch: 127/2000... Training loss: 3.3383\n",
      "Epoch: 127/2000... Training loss: 3.0862\n",
      "Epoch: 127/2000... Training loss: 3.3234\n",
      "Epoch: 127/2000... Training loss: 3.3282\n",
      "Epoch: 127/2000... Training loss: 3.2125\n",
      "Epoch: 127/2000... Training loss: 2.9465\n",
      "Epoch: 127/2000... Training loss: 3.0424\n",
      "Epoch: 127/2000... Training loss: 3.0284\n",
      "Epoch: 127/2000... Training loss: 2.9918\n",
      "Epoch: 128/2000... Training loss: 3.2047\n",
      "Epoch: 128/2000... Training loss: 3.0621\n",
      "Epoch: 128/2000... Training loss: 3.0157\n",
      "Epoch: 128/2000... Training loss: 3.1694\n",
      "Epoch: 128/2000... Training loss: 3.0126\n",
      "Epoch: 128/2000... Training loss: 2.9648\n",
      "Epoch: 128/2000... Training loss: 2.8445\n",
      "Epoch: 128/2000... Training loss: 3.0117\n",
      "Epoch: 128/2000... Training loss: 2.9712\n",
      "Epoch: 128/2000... Training loss: 3.0231\n",
      "Epoch: 128/2000... Training loss: 3.3069\n",
      "Epoch: 128/2000... Training loss: 3.2315\n",
      "Epoch: 128/2000... Training loss: 2.9887\n",
      "Epoch: 128/2000... Training loss: 2.9102\n",
      "Epoch: 128/2000... Training loss: 3.1356\n",
      "Epoch: 128/2000... Training loss: 3.0206\n",
      "Epoch: 128/2000... Training loss: 3.1044\n",
      "Epoch: 128/2000... Training loss: 3.3216\n",
      "Epoch: 128/2000... Training loss: 3.2122\n",
      "Epoch: 128/2000... Training loss: 3.2539\n",
      "Epoch: 128/2000... Training loss: 2.9721\n",
      "Epoch: 128/2000... Training loss: 3.1782\n",
      "Epoch: 128/2000... Training loss: 3.1392\n",
      "Epoch: 128/2000... Training loss: 3.3168\n",
      "Epoch: 128/2000... Training loss: 2.8387\n",
      "Epoch: 128/2000... Training loss: 3.1279\n",
      "Epoch: 128/2000... Training loss: 3.1797\n",
      "Epoch: 128/2000... Training loss: 3.0759\n",
      "Epoch: 128/2000... Training loss: 3.2179\n",
      "Epoch: 128/2000... Training loss: 2.8194\n",
      "Epoch: 128/2000... Training loss: 2.9499\n",
      "Epoch: 129/2000... Training loss: 3.0170\n",
      "Epoch: 129/2000... Training loss: 3.2135\n",
      "Epoch: 129/2000... Training loss: 3.2890\n",
      "Epoch: 129/2000... Training loss: 3.1575\n",
      "Epoch: 129/2000... Training loss: 3.1980\n",
      "Epoch: 129/2000... Training loss: 3.1103\n",
      "Epoch: 129/2000... Training loss: 2.9608\n",
      "Epoch: 129/2000... Training loss: 2.9317\n",
      "Epoch: 129/2000... Training loss: 3.1232\n",
      "Epoch: 129/2000... Training loss: 3.2249\n",
      "Epoch: 129/2000... Training loss: 3.0210\n",
      "Epoch: 129/2000... Training loss: 2.9356\n",
      "Epoch: 129/2000... Training loss: 3.2096\n",
      "Epoch: 129/2000... Training loss: 3.1113\n",
      "Epoch: 129/2000... Training loss: 3.1416\n",
      "Epoch: 129/2000... Training loss: 3.3247\n",
      "Epoch: 129/2000... Training loss: 3.3283\n",
      "Epoch: 129/2000... Training loss: 3.2050\n",
      "Epoch: 129/2000... Training loss: 2.8500\n",
      "Epoch: 129/2000... Training loss: 2.9464\n",
      "Epoch: 129/2000... Training loss: 3.0495\n",
      "Epoch: 129/2000... Training loss: 3.0693\n",
      "Epoch: 129/2000... Training loss: 2.9329\n",
      "Epoch: 129/2000... Training loss: 3.1176\n",
      "Epoch: 129/2000... Training loss: 3.1937\n",
      "Epoch: 129/2000... Training loss: 3.2418\n",
      "Epoch: 129/2000... Training loss: 3.2168\n",
      "Epoch: 129/2000... Training loss: 2.9691\n",
      "Epoch: 129/2000... Training loss: 2.9389\n",
      "Epoch: 129/2000... Training loss: 2.9661\n",
      "Epoch: 129/2000... Training loss: 3.0237\n",
      "Epoch: 130/2000... Training loss: 3.1678\n",
      "Epoch: 130/2000... Training loss: 3.0031\n",
      "Epoch: 130/2000... Training loss: 3.3700\n",
      "Epoch: 130/2000... Training loss: 2.9525\n",
      "Epoch: 130/2000... Training loss: 3.1259\n",
      "Epoch: 130/2000... Training loss: 3.0347\n",
      "Epoch: 130/2000... Training loss: 2.9441\n",
      "Epoch: 130/2000... Training loss: 2.9583\n",
      "Epoch: 130/2000... Training loss: 3.3996\n",
      "Epoch: 130/2000... Training loss: 3.2116\n",
      "Epoch: 130/2000... Training loss: 3.2064\n",
      "Epoch: 130/2000... Training loss: 3.1914\n",
      "Epoch: 130/2000... Training loss: 3.1063\n",
      "Epoch: 130/2000... Training loss: 3.1388\n",
      "Epoch: 130/2000... Training loss: 3.1741\n",
      "Epoch: 130/2000... Training loss: 3.0655\n",
      "Epoch: 130/2000... Training loss: 3.2442\n",
      "Epoch: 130/2000... Training loss: 3.0902\n",
      "Epoch: 130/2000... Training loss: 2.9941\n",
      "Epoch: 130/2000... Training loss: 3.1505\n",
      "Epoch: 130/2000... Training loss: 2.9011\n",
      "Epoch: 130/2000... Training loss: 2.9007\n",
      "Epoch: 130/2000... Training loss: 2.8860\n",
      "Epoch: 130/2000... Training loss: 3.2340\n",
      "Epoch: 130/2000... Training loss: 3.2181\n",
      "Epoch: 130/2000... Training loss: 3.3063\n",
      "Epoch: 130/2000... Training loss: 3.1505\n",
      "Epoch: 130/2000... Training loss: 3.1595\n",
      "Epoch: 130/2000... Training loss: 2.7180\n",
      "Epoch: 130/2000... Training loss: 3.0916\n",
      "Epoch: 130/2000... Training loss: 3.1366\n",
      "Epoch: 131/2000... Training loss: 3.2327\n",
      "Epoch: 131/2000... Training loss: 3.1514\n",
      "Epoch: 131/2000... Training loss: 3.2954\n",
      "Epoch: 131/2000... Training loss: 3.0689\n",
      "Epoch: 131/2000... Training loss: 2.7178\n",
      "Epoch: 131/2000... Training loss: 3.2263\n",
      "Epoch: 131/2000... Training loss: 3.1304\n",
      "Epoch: 131/2000... Training loss: 2.9370\n",
      "Epoch: 131/2000... Training loss: 3.1607\n",
      "Epoch: 131/2000... Training loss: 3.0573\n",
      "Epoch: 131/2000... Training loss: 2.9528\n",
      "Epoch: 131/2000... Training loss: 2.9377\n",
      "Epoch: 131/2000... Training loss: 2.9654\n",
      "Epoch: 131/2000... Training loss: 3.0579\n",
      "Epoch: 131/2000... Training loss: 3.2186\n",
      "Epoch: 131/2000... Training loss: 3.1558\n",
      "Epoch: 131/2000... Training loss: 2.9450\n",
      "Epoch: 131/2000... Training loss: 3.0192\n",
      "Epoch: 131/2000... Training loss: 3.1096\n",
      "Epoch: 131/2000... Training loss: 3.1911\n",
      "Epoch: 131/2000... Training loss: 3.2014\n",
      "Epoch: 131/2000... Training loss: 2.7937\n",
      "Epoch: 131/2000... Training loss: 3.1548\n",
      "Epoch: 131/2000... Training loss: 3.1248\n",
      "Epoch: 131/2000... Training loss: 3.2648\n",
      "Epoch: 131/2000... Training loss: 3.0547\n",
      "Epoch: 131/2000... Training loss: 3.0921\n",
      "Epoch: 131/2000... Training loss: 3.1168\n",
      "Epoch: 131/2000... Training loss: 2.8624\n",
      "Epoch: 131/2000... Training loss: 3.1294\n",
      "Epoch: 131/2000... Training loss: 3.2007\n",
      "Epoch: 132/2000... Training loss: 2.9495\n",
      "Epoch: 132/2000... Training loss: 2.9605\n",
      "Epoch: 132/2000... Training loss: 3.0466\n",
      "Epoch: 132/2000... Training loss: 2.7651\n",
      "Epoch: 132/2000... Training loss: 3.1123\n",
      "Epoch: 132/2000... Training loss: 3.0239\n",
      "Epoch: 132/2000... Training loss: 2.9521\n",
      "Epoch: 132/2000... Training loss: 3.1003\n",
      "Epoch: 132/2000... Training loss: 3.1588\n",
      "Epoch: 132/2000... Training loss: 3.2926\n",
      "Epoch: 132/2000... Training loss: 2.9834\n",
      "Epoch: 132/2000... Training loss: 2.9758\n",
      "Epoch: 132/2000... Training loss: 3.0701\n",
      "Epoch: 132/2000... Training loss: 3.1096\n",
      "Epoch: 132/2000... Training loss: 3.0493\n",
      "Epoch: 132/2000... Training loss: 3.2381\n",
      "Epoch: 132/2000... Training loss: 3.1369\n",
      "Epoch: 132/2000... Training loss: 3.0445\n",
      "Epoch: 132/2000... Training loss: 2.8850\n",
      "Epoch: 132/2000... Training loss: 3.0103\n",
      "Epoch: 132/2000... Training loss: 3.1849\n",
      "Epoch: 132/2000... Training loss: 3.0231\n",
      "Epoch: 132/2000... Training loss: 3.0237\n",
      "Epoch: 132/2000... Training loss: 3.1025\n",
      "Epoch: 132/2000... Training loss: 3.0509\n",
      "Epoch: 132/2000... Training loss: 2.9800\n",
      "Epoch: 132/2000... Training loss: 2.8698\n",
      "Epoch: 132/2000... Training loss: 2.8196\n",
      "Epoch: 132/2000... Training loss: 2.9212\n",
      "Epoch: 132/2000... Training loss: 3.1328\n",
      "Epoch: 132/2000... Training loss: 2.9933\n",
      "Epoch: 133/2000... Training loss: 3.2040\n",
      "Epoch: 133/2000... Training loss: 3.0086\n",
      "Epoch: 133/2000... Training loss: 2.8847\n",
      "Epoch: 133/2000... Training loss: 2.8873\n",
      "Epoch: 133/2000... Training loss: 3.2329\n",
      "Epoch: 133/2000... Training loss: 3.0518\n",
      "Epoch: 133/2000... Training loss: 3.1770\n",
      "Epoch: 133/2000... Training loss: 2.7949\n",
      "Epoch: 133/2000... Training loss: 2.8174\n",
      "Epoch: 133/2000... Training loss: 3.2698\n",
      "Epoch: 133/2000... Training loss: 2.9382\n",
      "Epoch: 133/2000... Training loss: 2.9721\n",
      "Epoch: 133/2000... Training loss: 2.9928\n",
      "Epoch: 133/2000... Training loss: 2.9323\n",
      "Epoch: 133/2000... Training loss: 2.9443\n",
      "Epoch: 133/2000... Training loss: 3.0425\n",
      "Epoch: 133/2000... Training loss: 3.0442\n",
      "Epoch: 133/2000... Training loss: 3.0479\n",
      "Epoch: 133/2000... Training loss: 2.9111\n",
      "Epoch: 133/2000... Training loss: 2.8930\n",
      "Epoch: 133/2000... Training loss: 3.0974\n",
      "Epoch: 133/2000... Training loss: 2.7693\n",
      "Epoch: 133/2000... Training loss: 3.1804\n",
      "Epoch: 133/2000... Training loss: 2.8182\n",
      "Epoch: 133/2000... Training loss: 3.1138\n",
      "Epoch: 133/2000... Training loss: 3.1055\n",
      "Epoch: 133/2000... Training loss: 3.1614\n",
      "Epoch: 133/2000... Training loss: 3.0567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 133/2000... Training loss: 2.9191\n",
      "Epoch: 133/2000... Training loss: 3.3275\n",
      "Epoch: 133/2000... Training loss: 3.0812\n",
      "Epoch: 134/2000... Training loss: 2.9697\n",
      "Epoch: 134/2000... Training loss: 2.9371\n",
      "Epoch: 134/2000... Training loss: 2.9656\n",
      "Epoch: 134/2000... Training loss: 2.9181\n",
      "Epoch: 134/2000... Training loss: 2.9676\n",
      "Epoch: 134/2000... Training loss: 3.0909\n",
      "Epoch: 134/2000... Training loss: 2.8791\n",
      "Epoch: 134/2000... Training loss: 2.9301\n",
      "Epoch: 134/2000... Training loss: 3.1630\n",
      "Epoch: 134/2000... Training loss: 2.9483\n",
      "Epoch: 134/2000... Training loss: 2.9152\n",
      "Epoch: 134/2000... Training loss: 3.3413\n",
      "Epoch: 134/2000... Training loss: 3.0314\n",
      "Epoch: 134/2000... Training loss: 3.0233\n",
      "Epoch: 134/2000... Training loss: 3.1033\n",
      "Epoch: 134/2000... Training loss: 3.0041\n",
      "Epoch: 134/2000... Training loss: 3.0882\n",
      "Epoch: 134/2000... Training loss: 3.1370\n",
      "Epoch: 134/2000... Training loss: 2.9536\n",
      "Epoch: 134/2000... Training loss: 3.1354\n",
      "Epoch: 134/2000... Training loss: 3.0936\n",
      "Epoch: 134/2000... Training loss: 3.1063\n",
      "Epoch: 134/2000... Training loss: 2.9334\n",
      "Epoch: 134/2000... Training loss: 2.9519\n",
      "Epoch: 134/2000... Training loss: 3.2629\n",
      "Epoch: 134/2000... Training loss: 2.9131\n",
      "Epoch: 134/2000... Training loss: 3.0938\n",
      "Epoch: 134/2000... Training loss: 3.0323\n",
      "Epoch: 134/2000... Training loss: 2.8115\n",
      "Epoch: 134/2000... Training loss: 3.1525\n",
      "Epoch: 134/2000... Training loss: 2.8652\n",
      "Epoch: 135/2000... Training loss: 2.9980\n",
      "Epoch: 135/2000... Training loss: 3.0712\n",
      "Epoch: 135/2000... Training loss: 3.0998\n",
      "Epoch: 135/2000... Training loss: 2.9589\n",
      "Epoch: 135/2000... Training loss: 3.1033\n",
      "Epoch: 135/2000... Training loss: 3.0359\n",
      "Epoch: 135/2000... Training loss: 2.8628\n",
      "Epoch: 135/2000... Training loss: 2.9438\n",
      "Epoch: 135/2000... Training loss: 3.2715\n",
      "Epoch: 135/2000... Training loss: 2.8231\n",
      "Epoch: 135/2000... Training loss: 2.8680\n",
      "Epoch: 135/2000... Training loss: 3.0209\n",
      "Epoch: 135/2000... Training loss: 2.8730\n",
      "Epoch: 135/2000... Training loss: 2.9523\n",
      "Epoch: 135/2000... Training loss: 3.1683\n",
      "Epoch: 135/2000... Training loss: 3.1830\n",
      "Epoch: 135/2000... Training loss: 2.9665\n",
      "Epoch: 135/2000... Training loss: 3.1751\n",
      "Epoch: 135/2000... Training loss: 3.0555\n",
      "Epoch: 135/2000... Training loss: 3.0143\n",
      "Epoch: 135/2000... Training loss: 3.1721\n",
      "Epoch: 135/2000... Training loss: 2.8996\n",
      "Epoch: 135/2000... Training loss: 2.8057\n",
      "Epoch: 135/2000... Training loss: 2.9570\n",
      "Epoch: 135/2000... Training loss: 2.9764\n",
      "Epoch: 135/2000... Training loss: 2.9715\n",
      "Epoch: 135/2000... Training loss: 2.9451\n",
      "Epoch: 135/2000... Training loss: 2.8882\n",
      "Epoch: 135/2000... Training loss: 2.8452\n",
      "Epoch: 135/2000... Training loss: 3.0385\n",
      "Epoch: 135/2000... Training loss: 2.8763\n",
      "Epoch: 136/2000... Training loss: 2.9107\n",
      "Epoch: 136/2000... Training loss: 2.9210\n",
      "Epoch: 136/2000... Training loss: 2.8648\n",
      "Epoch: 136/2000... Training loss: 3.0279\n",
      "Epoch: 136/2000... Training loss: 2.8207\n",
      "Epoch: 136/2000... Training loss: 2.8130\n",
      "Epoch: 136/2000... Training loss: 2.8223\n",
      "Epoch: 136/2000... Training loss: 2.9586\n",
      "Epoch: 136/2000... Training loss: 3.1368\n",
      "Epoch: 136/2000... Training loss: 2.7996\n",
      "Epoch: 136/2000... Training loss: 2.9680\n",
      "Epoch: 136/2000... Training loss: 2.9868\n",
      "Epoch: 136/2000... Training loss: 2.8456\n",
      "Epoch: 136/2000... Training loss: 3.0888\n",
      "Epoch: 136/2000... Training loss: 3.1312\n",
      "Epoch: 136/2000... Training loss: 2.6631\n",
      "Epoch: 136/2000... Training loss: 3.0881\n",
      "Epoch: 136/2000... Training loss: 3.3314\n",
      "Epoch: 136/2000... Training loss: 2.9127\n",
      "Epoch: 136/2000... Training loss: 3.0604\n",
      "Epoch: 136/2000... Training loss: 3.3507\n",
      "Epoch: 136/2000... Training loss: 2.9388\n",
      "Epoch: 136/2000... Training loss: 3.0254\n",
      "Epoch: 136/2000... Training loss: 2.8785\n",
      "Epoch: 136/2000... Training loss: 2.9703\n",
      "Epoch: 136/2000... Training loss: 3.0161\n",
      "Epoch: 136/2000... Training loss: 3.2263\n",
      "Epoch: 136/2000... Training loss: 3.1984\n",
      "Epoch: 136/2000... Training loss: 3.2289\n",
      "Epoch: 136/2000... Training loss: 2.9230\n",
      "Epoch: 136/2000... Training loss: 3.0461\n",
      "Epoch: 137/2000... Training loss: 3.1784\n",
      "Epoch: 137/2000... Training loss: 2.8237\n",
      "Epoch: 137/2000... Training loss: 2.8364\n",
      "Epoch: 137/2000... Training loss: 2.9289\n",
      "Epoch: 137/2000... Training loss: 2.7311\n",
      "Epoch: 137/2000... Training loss: 2.9552\n",
      "Epoch: 137/2000... Training loss: 3.0898\n",
      "Epoch: 137/2000... Training loss: 2.8094\n",
      "Epoch: 137/2000... Training loss: 2.9790\n",
      "Epoch: 137/2000... Training loss: 3.1232\n",
      "Epoch: 137/2000... Training loss: 2.9189\n",
      "Epoch: 137/2000... Training loss: 2.8951\n",
      "Epoch: 137/2000... Training loss: 3.0456\n",
      "Epoch: 137/2000... Training loss: 3.0230\n",
      "Epoch: 137/2000... Training loss: 2.9315\n",
      "Epoch: 137/2000... Training loss: 3.0449\n",
      "Epoch: 137/2000... Training loss: 2.9986\n",
      "Epoch: 137/2000... Training loss: 3.2165\n",
      "Epoch: 137/2000... Training loss: 2.6335\n",
      "Epoch: 137/2000... Training loss: 3.0358\n",
      "Epoch: 137/2000... Training loss: 2.9128\n",
      "Epoch: 137/2000... Training loss: 3.1506\n",
      "Epoch: 137/2000... Training loss: 3.1108\n",
      "Epoch: 137/2000... Training loss: 2.8092\n",
      "Epoch: 137/2000... Training loss: 2.7898\n",
      "Epoch: 137/2000... Training loss: 2.8945\n",
      "Epoch: 137/2000... Training loss: 3.0233\n",
      "Epoch: 137/2000... Training loss: 2.7467\n",
      "Epoch: 137/2000... Training loss: 2.8721\n",
      "Epoch: 137/2000... Training loss: 2.9967\n",
      "Epoch: 137/2000... Training loss: 2.7952\n",
      "Epoch: 138/2000... Training loss: 3.0775\n",
      "Epoch: 138/2000... Training loss: 3.0126\n",
      "Epoch: 138/2000... Training loss: 2.7718\n",
      "Epoch: 138/2000... Training loss: 3.2442\n",
      "Epoch: 138/2000... Training loss: 3.0067\n",
      "Epoch: 138/2000... Training loss: 2.9980\n",
      "Epoch: 138/2000... Training loss: 2.9165\n",
      "Epoch: 138/2000... Training loss: 2.9529\n",
      "Epoch: 138/2000... Training loss: 3.1478\n",
      "Epoch: 138/2000... Training loss: 2.9611\n",
      "Epoch: 138/2000... Training loss: 2.7959\n",
      "Epoch: 138/2000... Training loss: 3.0357\n",
      "Epoch: 138/2000... Training loss: 2.9948\n",
      "Epoch: 138/2000... Training loss: 2.7684\n",
      "Epoch: 138/2000... Training loss: 3.0293\n",
      "Epoch: 138/2000... Training loss: 2.8585\n",
      "Epoch: 138/2000... Training loss: 3.1191\n",
      "Epoch: 138/2000... Training loss: 3.1465\n",
      "Epoch: 138/2000... Training loss: 2.8582\n",
      "Epoch: 138/2000... Training loss: 3.0971\n",
      "Epoch: 138/2000... Training loss: 2.9528\n",
      "Epoch: 138/2000... Training loss: 2.9077\n",
      "Epoch: 138/2000... Training loss: 3.2052\n",
      "Epoch: 138/2000... Training loss: 2.9579\n",
      "Epoch: 138/2000... Training loss: 3.0423\n",
      "Epoch: 138/2000... Training loss: 2.9310\n",
      "Epoch: 138/2000... Training loss: 3.0655\n",
      "Epoch: 138/2000... Training loss: 2.8598\n",
      "Epoch: 138/2000... Training loss: 2.8582\n",
      "Epoch: 138/2000... Training loss: 2.8695\n",
      "Epoch: 138/2000... Training loss: 3.1180\n",
      "Epoch: 139/2000... Training loss: 2.8822\n",
      "Epoch: 139/2000... Training loss: 3.0002\n",
      "Epoch: 139/2000... Training loss: 2.9352\n",
      "Epoch: 139/2000... Training loss: 3.3201\n",
      "Epoch: 139/2000... Training loss: 2.8631\n",
      "Epoch: 139/2000... Training loss: 3.1143\n",
      "Epoch: 139/2000... Training loss: 3.1943\n",
      "Epoch: 139/2000... Training loss: 2.8654\n",
      "Epoch: 139/2000... Training loss: 2.9949\n",
      "Epoch: 139/2000... Training loss: 2.7796\n",
      "Epoch: 139/2000... Training loss: 3.1442\n",
      "Epoch: 139/2000... Training loss: 2.8140\n",
      "Epoch: 139/2000... Training loss: 3.0249\n",
      "Epoch: 139/2000... Training loss: 3.0880\n",
      "Epoch: 139/2000... Training loss: 3.0028\n",
      "Epoch: 139/2000... Training loss: 2.9276\n",
      "Epoch: 139/2000... Training loss: 2.8312\n",
      "Epoch: 139/2000... Training loss: 3.0833\n",
      "Epoch: 139/2000... Training loss: 3.0575\n",
      "Epoch: 139/2000... Training loss: 2.8700\n",
      "Epoch: 139/2000... Training loss: 2.9867\n",
      "Epoch: 139/2000... Training loss: 3.0714\n",
      "Epoch: 139/2000... Training loss: 2.9141\n",
      "Epoch: 139/2000... Training loss: 2.8996\n",
      "Epoch: 139/2000... Training loss: 2.8979\n",
      "Epoch: 139/2000... Training loss: 2.9496\n",
      "Epoch: 139/2000... Training loss: 3.0755\n",
      "Epoch: 139/2000... Training loss: 2.8826\n",
      "Epoch: 139/2000... Training loss: 3.2288\n",
      "Epoch: 139/2000... Training loss: 3.0470\n",
      "Epoch: 139/2000... Training loss: 2.9995\n",
      "Epoch: 140/2000... Training loss: 3.0825\n",
      "Epoch: 140/2000... Training loss: 2.7712\n",
      "Epoch: 140/2000... Training loss: 2.9548\n",
      "Epoch: 140/2000... Training loss: 3.0031\n",
      "Epoch: 140/2000... Training loss: 2.9264\n",
      "Epoch: 140/2000... Training loss: 3.0670\n",
      "Epoch: 140/2000... Training loss: 2.8394\n",
      "Epoch: 140/2000... Training loss: 3.1081\n",
      "Epoch: 140/2000... Training loss: 3.0529\n",
      "Epoch: 140/2000... Training loss: 2.8722\n",
      "Epoch: 140/2000... Training loss: 2.9929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140/2000... Training loss: 2.8783\n",
      "Epoch: 140/2000... Training loss: 3.0995\n",
      "Epoch: 140/2000... Training loss: 2.9905\n",
      "Epoch: 140/2000... Training loss: 3.0354\n",
      "Epoch: 140/2000... Training loss: 2.8745\n",
      "Epoch: 140/2000... Training loss: 3.2522\n",
      "Epoch: 140/2000... Training loss: 2.6935\n",
      "Epoch: 140/2000... Training loss: 2.7549\n",
      "Epoch: 140/2000... Training loss: 2.8955\n",
      "Epoch: 140/2000... Training loss: 2.8934\n",
      "Epoch: 140/2000... Training loss: 2.8024\n",
      "Epoch: 140/2000... Training loss: 2.9785\n",
      "Epoch: 140/2000... Training loss: 2.9022\n",
      "Epoch: 140/2000... Training loss: 2.8284\n",
      "Epoch: 140/2000... Training loss: 2.8972\n",
      "Epoch: 140/2000... Training loss: 2.7405\n",
      "Epoch: 140/2000... Training loss: 2.9809\n",
      "Epoch: 140/2000... Training loss: 2.9504\n",
      "Epoch: 140/2000... Training loss: 2.7474\n",
      "Epoch: 140/2000... Training loss: 2.7478\n",
      "Epoch: 141/2000... Training loss: 3.1038\n",
      "Epoch: 141/2000... Training loss: 2.8213\n",
      "Epoch: 141/2000... Training loss: 2.8262\n",
      "Epoch: 141/2000... Training loss: 2.9962\n",
      "Epoch: 141/2000... Training loss: 2.9743\n",
      "Epoch: 141/2000... Training loss: 2.9678\n",
      "Epoch: 141/2000... Training loss: 3.0158\n",
      "Epoch: 141/2000... Training loss: 3.0746\n",
      "Epoch: 141/2000... Training loss: 2.8110\n",
      "Epoch: 141/2000... Training loss: 2.8768\n",
      "Epoch: 141/2000... Training loss: 2.8811\n",
      "Epoch: 141/2000... Training loss: 2.9456\n",
      "Epoch: 141/2000... Training loss: 2.9614\n",
      "Epoch: 141/2000... Training loss: 3.1664\n",
      "Epoch: 141/2000... Training loss: 2.8888\n",
      "Epoch: 141/2000... Training loss: 2.8037\n",
      "Epoch: 141/2000... Training loss: 2.8189\n",
      "Epoch: 141/2000... Training loss: 2.9221\n",
      "Epoch: 141/2000... Training loss: 2.8213\n",
      "Epoch: 141/2000... Training loss: 2.9272\n",
      "Epoch: 141/2000... Training loss: 2.8267\n",
      "Epoch: 141/2000... Training loss: 2.9527\n",
      "Epoch: 141/2000... Training loss: 2.9404\n",
      "Epoch: 141/2000... Training loss: 2.9410\n",
      "Epoch: 141/2000... Training loss: 3.0648\n",
      "Epoch: 141/2000... Training loss: 2.8132\n",
      "Epoch: 141/2000... Training loss: 2.7845\n",
      "Epoch: 141/2000... Training loss: 2.6982\n",
      "Epoch: 141/2000... Training loss: 2.9146\n",
      "Epoch: 141/2000... Training loss: 2.9546\n",
      "Epoch: 141/2000... Training loss: 3.1657\n",
      "Epoch: 142/2000... Training loss: 3.2320\n",
      "Epoch: 142/2000... Training loss: 2.7067\n",
      "Epoch: 142/2000... Training loss: 2.9277\n",
      "Epoch: 142/2000... Training loss: 2.9756\n",
      "Epoch: 142/2000... Training loss: 2.9926\n",
      "Epoch: 142/2000... Training loss: 2.8778\n",
      "Epoch: 142/2000... Training loss: 2.8161\n",
      "Epoch: 142/2000... Training loss: 2.9654\n",
      "Epoch: 142/2000... Training loss: 2.9862\n",
      "Epoch: 142/2000... Training loss: 2.9004\n",
      "Epoch: 142/2000... Training loss: 3.0274\n",
      "Epoch: 142/2000... Training loss: 2.8012\n",
      "Epoch: 142/2000... Training loss: 3.0404\n",
      "Epoch: 142/2000... Training loss: 3.0036\n",
      "Epoch: 142/2000... Training loss: 2.9747\n",
      "Epoch: 142/2000... Training loss: 3.1792\n",
      "Epoch: 142/2000... Training loss: 3.0378\n",
      "Epoch: 142/2000... Training loss: 2.9266\n",
      "Epoch: 142/2000... Training loss: 2.7931\n",
      "Epoch: 142/2000... Training loss: 2.8307\n",
      "Epoch: 142/2000... Training loss: 2.9036\n",
      "Epoch: 142/2000... Training loss: 3.0457\n",
      "Epoch: 142/2000... Training loss: 3.1349\n",
      "Epoch: 142/2000... Training loss: 2.8713\n",
      "Epoch: 142/2000... Training loss: 2.9081\n",
      "Epoch: 142/2000... Training loss: 3.0959\n",
      "Epoch: 142/2000... Training loss: 2.8504\n",
      "Epoch: 142/2000... Training loss: 3.2771\n",
      "Epoch: 142/2000... Training loss: 2.9455\n",
      "Epoch: 142/2000... Training loss: 3.1409\n",
      "Epoch: 142/2000... Training loss: 2.8593\n",
      "Epoch: 143/2000... Training loss: 2.8714\n",
      "Epoch: 143/2000... Training loss: 2.7948\n",
      "Epoch: 143/2000... Training loss: 2.9599\n",
      "Epoch: 143/2000... Training loss: 2.8603\n",
      "Epoch: 143/2000... Training loss: 3.1294\n",
      "Epoch: 143/2000... Training loss: 3.0812\n",
      "Epoch: 143/2000... Training loss: 3.0624\n",
      "Epoch: 143/2000... Training loss: 2.8590\n",
      "Epoch: 143/2000... Training loss: 2.9487\n",
      "Epoch: 143/2000... Training loss: 2.9396\n",
      "Epoch: 143/2000... Training loss: 2.9169\n",
      "Epoch: 143/2000... Training loss: 2.9532\n",
      "Epoch: 143/2000... Training loss: 2.8080\n",
      "Epoch: 143/2000... Training loss: 2.9514\n",
      "Epoch: 143/2000... Training loss: 2.6233\n",
      "Epoch: 143/2000... Training loss: 2.9245\n",
      "Epoch: 143/2000... Training loss: 3.0112\n",
      "Epoch: 143/2000... Training loss: 2.9978\n",
      "Epoch: 143/2000... Training loss: 3.0508\n",
      "Epoch: 143/2000... Training loss: 2.9205\n",
      "Epoch: 143/2000... Training loss: 3.1492\n",
      "Epoch: 143/2000... Training loss: 2.9107\n",
      "Epoch: 143/2000... Training loss: 2.8944\n",
      "Epoch: 143/2000... Training loss: 2.8495\n",
      "Epoch: 143/2000... Training loss: 3.2158\n",
      "Epoch: 143/2000... Training loss: 2.8820\n",
      "Epoch: 143/2000... Training loss: 2.8160\n",
      "Epoch: 143/2000... Training loss: 2.8316\n",
      "Epoch: 143/2000... Training loss: 2.8062\n",
      "Epoch: 143/2000... Training loss: 2.8952\n",
      "Epoch: 143/2000... Training loss: 2.8136\n",
      "Epoch: 144/2000... Training loss: 2.8516\n",
      "Epoch: 144/2000... Training loss: 2.7424\n",
      "Epoch: 144/2000... Training loss: 3.0220\n",
      "Epoch: 144/2000... Training loss: 3.0472\n",
      "Epoch: 144/2000... Training loss: 2.9557\n",
      "Epoch: 144/2000... Training loss: 2.9924\n",
      "Epoch: 144/2000... Training loss: 2.7481\n",
      "Epoch: 144/2000... Training loss: 2.7219\n",
      "Epoch: 144/2000... Training loss: 3.1564\n",
      "Epoch: 144/2000... Training loss: 2.9945\n",
      "Epoch: 144/2000... Training loss: 2.8572\n",
      "Epoch: 144/2000... Training loss: 2.9705\n",
      "Epoch: 144/2000... Training loss: 2.9004\n",
      "Epoch: 144/2000... Training loss: 2.9492\n",
      "Epoch: 144/2000... Training loss: 2.8685\n",
      "Epoch: 144/2000... Training loss: 2.8213\n",
      "Epoch: 144/2000... Training loss: 2.9293\n",
      "Epoch: 144/2000... Training loss: 2.8718\n",
      "Epoch: 144/2000... Training loss: 2.7669\n",
      "Epoch: 144/2000... Training loss: 2.8921\n",
      "Epoch: 144/2000... Training loss: 2.8855\n",
      "Epoch: 144/2000... Training loss: 2.7577\n",
      "Epoch: 144/2000... Training loss: 2.7205\n",
      "Epoch: 144/2000... Training loss: 2.7480\n",
      "Epoch: 144/2000... Training loss: 2.8375\n",
      "Epoch: 144/2000... Training loss: 2.8249\n",
      "Epoch: 144/2000... Training loss: 2.8142\n",
      "Epoch: 144/2000... Training loss: 2.8897\n",
      "Epoch: 144/2000... Training loss: 2.6600\n",
      "Epoch: 144/2000... Training loss: 2.9729\n",
      "Epoch: 144/2000... Training loss: 2.8514\n",
      "Epoch: 145/2000... Training loss: 2.9198\n",
      "Epoch: 145/2000... Training loss: 3.0417\n",
      "Epoch: 145/2000... Training loss: 2.6432\n",
      "Epoch: 145/2000... Training loss: 3.0566\n",
      "Epoch: 145/2000... Training loss: 2.9787\n",
      "Epoch: 145/2000... Training loss: 3.0250\n",
      "Epoch: 145/2000... Training loss: 2.8739\n",
      "Epoch: 145/2000... Training loss: 2.6406\n",
      "Epoch: 145/2000... Training loss: 2.9571\n",
      "Epoch: 145/2000... Training loss: 3.0423\n",
      "Epoch: 145/2000... Training loss: 2.9470\n",
      "Epoch: 145/2000... Training loss: 3.0710\n",
      "Epoch: 145/2000... Training loss: 2.9501\n",
      "Epoch: 145/2000... Training loss: 2.8011\n",
      "Epoch: 145/2000... Training loss: 2.7974\n",
      "Epoch: 145/2000... Training loss: 3.0687\n",
      "Epoch: 145/2000... Training loss: 2.8363\n",
      "Epoch: 145/2000... Training loss: 2.7524\n",
      "Epoch: 145/2000... Training loss: 2.6864\n",
      "Epoch: 145/2000... Training loss: 2.9167\n",
      "Epoch: 145/2000... Training loss: 2.8571\n",
      "Epoch: 145/2000... Training loss: 2.6765\n",
      "Epoch: 145/2000... Training loss: 2.7492\n",
      "Epoch: 145/2000... Training loss: 2.8401\n",
      "Epoch: 145/2000... Training loss: 3.1177\n",
      "Epoch: 145/2000... Training loss: 2.8225\n",
      "Epoch: 145/2000... Training loss: 2.8212\n",
      "Epoch: 145/2000... Training loss: 2.9517\n",
      "Epoch: 145/2000... Training loss: 2.8698\n",
      "Epoch: 145/2000... Training loss: 2.8771\n",
      "Epoch: 145/2000... Training loss: 2.8622\n",
      "Epoch: 146/2000... Training loss: 2.8997\n",
      "Epoch: 146/2000... Training loss: 2.9558\n",
      "Epoch: 146/2000... Training loss: 2.8849\n",
      "Epoch: 146/2000... Training loss: 2.8475\n",
      "Epoch: 146/2000... Training loss: 2.8829\n",
      "Epoch: 146/2000... Training loss: 3.0995\n",
      "Epoch: 146/2000... Training loss: 2.8182\n",
      "Epoch: 146/2000... Training loss: 2.8931\n",
      "Epoch: 146/2000... Training loss: 2.8474\n",
      "Epoch: 146/2000... Training loss: 3.2308\n",
      "Epoch: 146/2000... Training loss: 2.8793\n",
      "Epoch: 146/2000... Training loss: 2.6536\n",
      "Epoch: 146/2000... Training loss: 2.8571\n",
      "Epoch: 146/2000... Training loss: 3.0059\n",
      "Epoch: 146/2000... Training loss: 2.9551\n",
      "Epoch: 146/2000... Training loss: 3.2284\n",
      "Epoch: 146/2000... Training loss: 2.8414\n",
      "Epoch: 146/2000... Training loss: 2.7909\n",
      "Epoch: 146/2000... Training loss: 2.8210\n",
      "Epoch: 146/2000... Training loss: 2.7733\n",
      "Epoch: 146/2000... Training loss: 2.8408\n",
      "Epoch: 146/2000... Training loss: 2.9102\n",
      "Epoch: 146/2000... Training loss: 3.0767\n",
      "Epoch: 146/2000... Training loss: 2.9554\n",
      "Epoch: 146/2000... Training loss: 2.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146/2000... Training loss: 3.0567\n",
      "Epoch: 146/2000... Training loss: 2.7229\n",
      "Epoch: 146/2000... Training loss: 2.5815\n",
      "Epoch: 146/2000... Training loss: 3.0792\n",
      "Epoch: 146/2000... Training loss: 2.8425\n",
      "Epoch: 146/2000... Training loss: 2.8027\n",
      "Epoch: 147/2000... Training loss: 3.0240\n",
      "Epoch: 147/2000... Training loss: 2.9360\n",
      "Epoch: 147/2000... Training loss: 2.7110\n",
      "Epoch: 147/2000... Training loss: 2.7110\n",
      "Epoch: 147/2000... Training loss: 3.0858\n",
      "Epoch: 147/2000... Training loss: 2.9522\n",
      "Epoch: 147/2000... Training loss: 2.9599\n",
      "Epoch: 147/2000... Training loss: 3.1060\n",
      "Epoch: 147/2000... Training loss: 2.8254\n",
      "Epoch: 147/2000... Training loss: 3.2223\n",
      "Epoch: 147/2000... Training loss: 2.9748\n",
      "Epoch: 147/2000... Training loss: 3.0279\n",
      "Epoch: 147/2000... Training loss: 2.7879\n",
      "Epoch: 147/2000... Training loss: 2.8754\n",
      "Epoch: 147/2000... Training loss: 3.0194\n",
      "Epoch: 147/2000... Training loss: 2.8042\n",
      "Epoch: 147/2000... Training loss: 2.9008\n",
      "Epoch: 147/2000... Training loss: 2.9348\n",
      "Epoch: 147/2000... Training loss: 2.9097\n",
      "Epoch: 147/2000... Training loss: 3.0028\n",
      "Epoch: 147/2000... Training loss: 2.6238\n",
      "Epoch: 147/2000... Training loss: 3.0412\n",
      "Epoch: 147/2000... Training loss: 2.7306\n",
      "Epoch: 147/2000... Training loss: 2.6619\n",
      "Epoch: 147/2000... Training loss: 3.1348\n",
      "Epoch: 147/2000... Training loss: 2.9139\n",
      "Epoch: 147/2000... Training loss: 3.0443\n",
      "Epoch: 147/2000... Training loss: 2.9181\n",
      "Epoch: 147/2000... Training loss: 2.8550\n",
      "Epoch: 147/2000... Training loss: 2.9179\n",
      "Epoch: 147/2000... Training loss: 3.0193\n",
      "Epoch: 148/2000... Training loss: 2.7138\n",
      "Epoch: 148/2000... Training loss: 2.9622\n",
      "Epoch: 148/2000... Training loss: 2.7304\n",
      "Epoch: 148/2000... Training loss: 2.9121\n",
      "Epoch: 148/2000... Training loss: 2.9007\n",
      "Epoch: 148/2000... Training loss: 2.8702\n",
      "Epoch: 148/2000... Training loss: 2.6911\n",
      "Epoch: 148/2000... Training loss: 3.0686\n",
      "Epoch: 148/2000... Training loss: 3.0255\n",
      "Epoch: 148/2000... Training loss: 2.8123\n",
      "Epoch: 148/2000... Training loss: 3.0208\n",
      "Epoch: 148/2000... Training loss: 2.9115\n",
      "Epoch: 148/2000... Training loss: 2.8037\n",
      "Epoch: 148/2000... Training loss: 2.8034\n",
      "Epoch: 148/2000... Training loss: 2.7420\n",
      "Epoch: 148/2000... Training loss: 2.7821\n",
      "Epoch: 148/2000... Training loss: 2.8795\n",
      "Epoch: 148/2000... Training loss: 2.8681\n",
      "Epoch: 148/2000... Training loss: 2.7639\n",
      "Epoch: 148/2000... Training loss: 3.0299\n",
      "Epoch: 148/2000... Training loss: 3.0352\n",
      "Epoch: 148/2000... Training loss: 2.6720\n",
      "Epoch: 148/2000... Training loss: 2.9268\n",
      "Epoch: 148/2000... Training loss: 2.7967\n",
      "Epoch: 148/2000... Training loss: 2.6736\n",
      "Epoch: 148/2000... Training loss: 2.9573\n",
      "Epoch: 148/2000... Training loss: 2.9067\n",
      "Epoch: 148/2000... Training loss: 2.9868\n",
      "Epoch: 148/2000... Training loss: 2.7446\n",
      "Epoch: 148/2000... Training loss: 3.0245\n",
      "Epoch: 148/2000... Training loss: 2.8075\n",
      "Epoch: 149/2000... Training loss: 2.8401\n",
      "Epoch: 149/2000... Training loss: 2.9114\n",
      "Epoch: 149/2000... Training loss: 2.7711\n",
      "Epoch: 149/2000... Training loss: 2.6185\n",
      "Epoch: 149/2000... Training loss: 2.9826\n",
      "Epoch: 149/2000... Training loss: 2.8401\n",
      "Epoch: 149/2000... Training loss: 2.8478\n",
      "Epoch: 149/2000... Training loss: 2.8667\n",
      "Epoch: 149/2000... Training loss: 3.1253\n",
      "Epoch: 149/2000... Training loss: 3.0844\n",
      "Epoch: 149/2000... Training loss: 2.9028\n",
      "Epoch: 149/2000... Training loss: 3.0141\n",
      "Epoch: 149/2000... Training loss: 2.7529\n",
      "Epoch: 149/2000... Training loss: 2.9251\n",
      "Epoch: 149/2000... Training loss: 3.0786\n",
      "Epoch: 149/2000... Training loss: 3.0493\n",
      "Epoch: 149/2000... Training loss: 2.8257\n",
      "Epoch: 149/2000... Training loss: 2.8134\n",
      "Epoch: 149/2000... Training loss: 2.8462\n",
      "Epoch: 149/2000... Training loss: 2.8096\n",
      "Epoch: 149/2000... Training loss: 2.9089\n",
      "Epoch: 149/2000... Training loss: 2.7561\n",
      "Epoch: 149/2000... Training loss: 3.1197\n",
      "Epoch: 149/2000... Training loss: 2.8973\n",
      "Epoch: 149/2000... Training loss: 2.9998\n",
      "Epoch: 149/2000... Training loss: 2.5575\n",
      "Epoch: 149/2000... Training loss: 2.9010\n",
      "Epoch: 149/2000... Training loss: 3.0447\n",
      "Epoch: 149/2000... Training loss: 2.7231\n",
      "Epoch: 149/2000... Training loss: 2.7217\n",
      "Epoch: 149/2000... Training loss: 2.9860\n",
      "Epoch: 150/2000... Training loss: 2.9148\n",
      "Epoch: 150/2000... Training loss: 2.8059\n",
      "Epoch: 150/2000... Training loss: 2.5962\n",
      "Epoch: 150/2000... Training loss: 2.9557\n",
      "Epoch: 150/2000... Training loss: 2.9859\n",
      "Epoch: 150/2000... Training loss: 2.9365\n",
      "Epoch: 150/2000... Training loss: 2.9156\n",
      "Epoch: 150/2000... Training loss: 2.9880\n",
      "Epoch: 150/2000... Training loss: 2.9211\n",
      "Epoch: 150/2000... Training loss: 3.0196\n",
      "Epoch: 150/2000... Training loss: 2.9372\n",
      "Epoch: 150/2000... Training loss: 2.7988\n",
      "Epoch: 150/2000... Training loss: 2.7041\n",
      "Epoch: 150/2000... Training loss: 2.9593\n",
      "Epoch: 150/2000... Training loss: 2.8572\n",
      "Epoch: 150/2000... Training loss: 2.8619\n",
      "Epoch: 150/2000... Training loss: 2.9831\n",
      "Epoch: 150/2000... Training loss: 2.6798\n",
      "Epoch: 150/2000... Training loss: 2.9187\n",
      "Epoch: 150/2000... Training loss: 3.0145\n",
      "Epoch: 150/2000... Training loss: 2.9022\n",
      "Epoch: 150/2000... Training loss: 2.7101\n",
      "Epoch: 150/2000... Training loss: 2.7230\n",
      "Epoch: 150/2000... Training loss: 3.0416\n",
      "Epoch: 150/2000... Training loss: 2.8217\n",
      "Epoch: 150/2000... Training loss: 2.9748\n",
      "Epoch: 150/2000... Training loss: 2.9964\n",
      "Epoch: 150/2000... Training loss: 2.9691\n",
      "Epoch: 150/2000... Training loss: 2.7283\n",
      "Epoch: 150/2000... Training loss: 2.8127\n",
      "Epoch: 150/2000... Training loss: 3.0109\n",
      "Epoch: 151/2000... Training loss: 2.7552\n",
      "Epoch: 151/2000... Training loss: 3.1249\n",
      "Epoch: 151/2000... Training loss: 2.6753\n",
      "Epoch: 151/2000... Training loss: 2.6427\n",
      "Epoch: 151/2000... Training loss: 3.0915\n",
      "Epoch: 151/2000... Training loss: 2.7215\n",
      "Epoch: 151/2000... Training loss: 2.6182\n",
      "Epoch: 151/2000... Training loss: 2.8970\n",
      "Epoch: 151/2000... Training loss: 2.9476\n",
      "Epoch: 151/2000... Training loss: 2.8186\n",
      "Epoch: 151/2000... Training loss: 2.9366\n",
      "Epoch: 151/2000... Training loss: 2.8962\n",
      "Epoch: 151/2000... Training loss: 2.7184\n",
      "Epoch: 151/2000... Training loss: 3.1461\n",
      "Epoch: 151/2000... Training loss: 2.7525\n",
      "Epoch: 151/2000... Training loss: 2.8597\n",
      "Epoch: 151/2000... Training loss: 2.8909\n",
      "Epoch: 151/2000... Training loss: 3.0176\n",
      "Epoch: 151/2000... Training loss: 3.0141\n",
      "Epoch: 151/2000... Training loss: 2.8120\n",
      "Epoch: 151/2000... Training loss: 2.8783\n",
      "Epoch: 151/2000... Training loss: 2.7656\n",
      "Epoch: 151/2000... Training loss: 2.8542\n",
      "Epoch: 151/2000... Training loss: 2.9932\n",
      "Epoch: 151/2000... Training loss: 2.7554\n",
      "Epoch: 151/2000... Training loss: 2.8855\n",
      "Epoch: 151/2000... Training loss: 2.9829\n",
      "Epoch: 151/2000... Training loss: 2.7754\n",
      "Epoch: 151/2000... Training loss: 2.7642\n",
      "Epoch: 151/2000... Training loss: 2.7241\n",
      "Epoch: 151/2000... Training loss: 2.6792\n",
      "Epoch: 152/2000... Training loss: 2.9497\n",
      "Epoch: 152/2000... Training loss: 2.6323\n",
      "Epoch: 152/2000... Training loss: 2.7269\n",
      "Epoch: 152/2000... Training loss: 2.9912\n",
      "Epoch: 152/2000... Training loss: 2.7459\n",
      "Epoch: 152/2000... Training loss: 2.9178\n",
      "Epoch: 152/2000... Training loss: 3.0546\n",
      "Epoch: 152/2000... Training loss: 2.7693\n",
      "Epoch: 152/2000... Training loss: 2.8721\n",
      "Epoch: 152/2000... Training loss: 2.9092\n",
      "Epoch: 152/2000... Training loss: 2.8096\n",
      "Epoch: 152/2000... Training loss: 2.6750\n",
      "Epoch: 152/2000... Training loss: 2.7674\n",
      "Epoch: 152/2000... Training loss: 3.0902\n",
      "Epoch: 152/2000... Training loss: 2.8423\n",
      "Epoch: 152/2000... Training loss: 2.8939\n",
      "Epoch: 152/2000... Training loss: 2.8447\n",
      "Epoch: 152/2000... Training loss: 2.8227\n",
      "Epoch: 152/2000... Training loss: 2.9614\n",
      "Epoch: 152/2000... Training loss: 3.1014\n",
      "Epoch: 152/2000... Training loss: 2.7982\n",
      "Epoch: 152/2000... Training loss: 2.8479\n",
      "Epoch: 152/2000... Training loss: 2.8718\n",
      "Epoch: 152/2000... Training loss: 2.7526\n",
      "Epoch: 152/2000... Training loss: 2.8077\n",
      "Epoch: 152/2000... Training loss: 2.7892\n",
      "Epoch: 152/2000... Training loss: 2.9433\n",
      "Epoch: 152/2000... Training loss: 2.8214\n",
      "Epoch: 152/2000... Training loss: 2.7396\n",
      "Epoch: 152/2000... Training loss: 2.9628\n",
      "Epoch: 152/2000... Training loss: 2.9279\n",
      "Epoch: 153/2000... Training loss: 2.4478\n",
      "Epoch: 153/2000... Training loss: 2.8357\n",
      "Epoch: 153/2000... Training loss: 3.0029\n",
      "Epoch: 153/2000... Training loss: 2.8926\n",
      "Epoch: 153/2000... Training loss: 2.7956\n",
      "Epoch: 153/2000... Training loss: 2.7246\n",
      "Epoch: 153/2000... Training loss: 2.9809\n",
      "Epoch: 153/2000... Training loss: 2.8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 153/2000... Training loss: 3.0432\n",
      "Epoch: 153/2000... Training loss: 2.5748\n",
      "Epoch: 153/2000... Training loss: 2.8424\n",
      "Epoch: 153/2000... Training loss: 3.0448\n",
      "Epoch: 153/2000... Training loss: 2.8319\n",
      "Epoch: 153/2000... Training loss: 2.9817\n",
      "Epoch: 153/2000... Training loss: 2.7141\n",
      "Epoch: 153/2000... Training loss: 2.8042\n",
      "Epoch: 153/2000... Training loss: 3.0779\n",
      "Epoch: 153/2000... Training loss: 2.7218\n",
      "Epoch: 153/2000... Training loss: 2.6792\n",
      "Epoch: 153/2000... Training loss: 2.8747\n",
      "Epoch: 153/2000... Training loss: 2.7928\n",
      "Epoch: 153/2000... Training loss: 2.8211\n",
      "Epoch: 153/2000... Training loss: 2.9019\n",
      "Epoch: 153/2000... Training loss: 2.7632\n",
      "Epoch: 153/2000... Training loss: 2.8580\n",
      "Epoch: 153/2000... Training loss: 2.7436\n",
      "Epoch: 153/2000... Training loss: 2.9460\n",
      "Epoch: 153/2000... Training loss: 2.7242\n",
      "Epoch: 153/2000... Training loss: 2.7157\n",
      "Epoch: 153/2000... Training loss: 2.8560\n",
      "Epoch: 153/2000... Training loss: 2.7363\n",
      "Epoch: 154/2000... Training loss: 2.7811\n",
      "Epoch: 154/2000... Training loss: 2.9525\n",
      "Epoch: 154/2000... Training loss: 2.7221\n",
      "Epoch: 154/2000... Training loss: 3.0213\n",
      "Epoch: 154/2000... Training loss: 3.0252\n",
      "Epoch: 154/2000... Training loss: 2.5093\n",
      "Epoch: 154/2000... Training loss: 2.7216\n",
      "Epoch: 154/2000... Training loss: 2.6530\n",
      "Epoch: 154/2000... Training loss: 2.8802\n",
      "Epoch: 154/2000... Training loss: 2.8418\n",
      "Epoch: 154/2000... Training loss: 2.8181\n",
      "Epoch: 154/2000... Training loss: 2.8058\n",
      "Epoch: 154/2000... Training loss: 2.7484\n",
      "Epoch: 154/2000... Training loss: 2.9554\n",
      "Epoch: 154/2000... Training loss: 2.9706\n",
      "Epoch: 154/2000... Training loss: 2.9475\n",
      "Epoch: 154/2000... Training loss: 3.1119\n",
      "Epoch: 154/2000... Training loss: 2.8286\n",
      "Epoch: 154/2000... Training loss: 2.6732\n",
      "Epoch: 154/2000... Training loss: 2.7694\n",
      "Epoch: 154/2000... Training loss: 3.0434\n",
      "Epoch: 154/2000... Training loss: 2.8400\n",
      "Epoch: 154/2000... Training loss: 2.7966\n",
      "Epoch: 154/2000... Training loss: 2.8063\n",
      "Epoch: 154/2000... Training loss: 2.9695\n",
      "Epoch: 154/2000... Training loss: 2.8025\n",
      "Epoch: 154/2000... Training loss: 2.9360\n",
      "Epoch: 154/2000... Training loss: 2.9226\n",
      "Epoch: 154/2000... Training loss: 2.8437\n",
      "Epoch: 154/2000... Training loss: 2.8071\n",
      "Epoch: 154/2000... Training loss: 2.7626\n",
      "Epoch: 155/2000... Training loss: 2.7083\n",
      "Epoch: 155/2000... Training loss: 2.6093\n",
      "Epoch: 155/2000... Training loss: 2.8193\n",
      "Epoch: 155/2000... Training loss: 2.4733\n",
      "Epoch: 155/2000... Training loss: 3.1752\n",
      "Epoch: 155/2000... Training loss: 2.7922\n",
      "Epoch: 155/2000... Training loss: 2.6979\n",
      "Epoch: 155/2000... Training loss: 2.7127\n",
      "Epoch: 155/2000... Training loss: 2.9685\n",
      "Epoch: 155/2000... Training loss: 2.9353\n",
      "Epoch: 155/2000... Training loss: 2.6740\n",
      "Epoch: 155/2000... Training loss: 2.7916\n",
      "Epoch: 155/2000... Training loss: 2.7122\n",
      "Epoch: 155/2000... Training loss: 2.8772\n",
      "Epoch: 155/2000... Training loss: 2.8330\n",
      "Epoch: 155/2000... Training loss: 2.8975\n",
      "Epoch: 155/2000... Training loss: 3.1041\n",
      "Epoch: 155/2000... Training loss: 2.8993\n",
      "Epoch: 155/2000... Training loss: 2.8697\n",
      "Epoch: 155/2000... Training loss: 2.8799\n",
      "Epoch: 155/2000... Training loss: 2.7152\n",
      "Epoch: 155/2000... Training loss: 2.8844\n",
      "Epoch: 155/2000... Training loss: 2.9815\n",
      "Epoch: 155/2000... Training loss: 2.8192\n",
      "Epoch: 155/2000... Training loss: 3.2339\n",
      "Epoch: 155/2000... Training loss: 3.1522\n",
      "Epoch: 155/2000... Training loss: 2.9577\n",
      "Epoch: 155/2000... Training loss: 2.8589\n",
      "Epoch: 155/2000... Training loss: 2.7963\n",
      "Epoch: 155/2000... Training loss: 2.9068\n",
      "Epoch: 155/2000... Training loss: 2.8407\n",
      "Epoch: 156/2000... Training loss: 2.8644\n",
      "Epoch: 156/2000... Training loss: 2.7273\n",
      "Epoch: 156/2000... Training loss: 2.7744\n",
      "Epoch: 156/2000... Training loss: 2.8268\n",
      "Epoch: 156/2000... Training loss: 2.7202\n",
      "Epoch: 156/2000... Training loss: 2.8117\n",
      "Epoch: 156/2000... Training loss: 2.8655\n",
      "Epoch: 156/2000... Training loss: 3.1269\n",
      "Epoch: 156/2000... Training loss: 2.8165\n",
      "Epoch: 156/2000... Training loss: 2.6783\n",
      "Epoch: 156/2000... Training loss: 2.8462\n",
      "Epoch: 156/2000... Training loss: 2.8921\n",
      "Epoch: 156/2000... Training loss: 2.7884\n",
      "Epoch: 156/2000... Training loss: 3.0090\n",
      "Epoch: 156/2000... Training loss: 2.7873\n",
      "Epoch: 156/2000... Training loss: 2.5807\n",
      "Epoch: 156/2000... Training loss: 2.8221\n",
      "Epoch: 156/2000... Training loss: 2.8726\n",
      "Epoch: 156/2000... Training loss: 2.8111\n",
      "Epoch: 156/2000... Training loss: 2.7451\n",
      "Epoch: 156/2000... Training loss: 2.7309\n",
      "Epoch: 156/2000... Training loss: 2.8671\n",
      "Epoch: 156/2000... Training loss: 3.0351\n",
      "Epoch: 156/2000... Training loss: 2.8068\n",
      "Epoch: 156/2000... Training loss: 2.6438\n",
      "Epoch: 156/2000... Training loss: 2.7859\n",
      "Epoch: 156/2000... Training loss: 2.9633\n",
      "Epoch: 156/2000... Training loss: 2.9790\n",
      "Epoch: 156/2000... Training loss: 2.6034\n",
      "Epoch: 156/2000... Training loss: 2.7437\n",
      "Epoch: 156/2000... Training loss: 2.8469\n",
      "Epoch: 157/2000... Training loss: 2.5668\n",
      "Epoch: 157/2000... Training loss: 2.7952\n",
      "Epoch: 157/2000... Training loss: 2.6734\n",
      "Epoch: 157/2000... Training loss: 2.9496\n",
      "Epoch: 157/2000... Training loss: 2.7029\n",
      "Epoch: 157/2000... Training loss: 2.8621\n",
      "Epoch: 157/2000... Training loss: 2.7754\n",
      "Epoch: 157/2000... Training loss: 2.6530\n",
      "Epoch: 157/2000... Training loss: 2.7599\n",
      "Epoch: 157/2000... Training loss: 3.0266\n",
      "Epoch: 157/2000... Training loss: 2.6735\n",
      "Epoch: 157/2000... Training loss: 2.8156\n",
      "Epoch: 157/2000... Training loss: 2.7572\n",
      "Epoch: 157/2000... Training loss: 3.0088\n",
      "Epoch: 157/2000... Training loss: 2.5879\n",
      "Epoch: 157/2000... Training loss: 2.9608\n",
      "Epoch: 157/2000... Training loss: 2.8613\n",
      "Epoch: 157/2000... Training loss: 2.7968\n",
      "Epoch: 157/2000... Training loss: 2.7872\n",
      "Epoch: 157/2000... Training loss: 2.8407\n",
      "Epoch: 157/2000... Training loss: 2.5543\n",
      "Epoch: 157/2000... Training loss: 2.5530\n",
      "Epoch: 157/2000... Training loss: 2.8330\n",
      "Epoch: 157/2000... Training loss: 3.1665\n",
      "Epoch: 157/2000... Training loss: 2.9637\n",
      "Epoch: 157/2000... Training loss: 2.7336\n",
      "Epoch: 157/2000... Training loss: 2.9762\n",
      "Epoch: 157/2000... Training loss: 2.8200\n",
      "Epoch: 157/2000... Training loss: 2.8165\n",
      "Epoch: 157/2000... Training loss: 2.8290\n",
      "Epoch: 157/2000... Training loss: 2.8190\n",
      "Epoch: 158/2000... Training loss: 2.7104\n",
      "Epoch: 158/2000... Training loss: 2.8609\n",
      "Epoch: 158/2000... Training loss: 2.8006\n",
      "Epoch: 158/2000... Training loss: 2.6159\n",
      "Epoch: 158/2000... Training loss: 3.0882\n",
      "Epoch: 158/2000... Training loss: 2.6735\n",
      "Epoch: 158/2000... Training loss: 2.8586\n",
      "Epoch: 158/2000... Training loss: 2.6315\n",
      "Epoch: 158/2000... Training loss: 2.7334\n",
      "Epoch: 158/2000... Training loss: 2.6435\n",
      "Epoch: 158/2000... Training loss: 2.9522\n",
      "Epoch: 158/2000... Training loss: 2.7471\n",
      "Epoch: 158/2000... Training loss: 2.6606\n",
      "Epoch: 158/2000... Training loss: 2.9626\n",
      "Epoch: 158/2000... Training loss: 2.6068\n",
      "Epoch: 158/2000... Training loss: 2.6926\n",
      "Epoch: 158/2000... Training loss: 2.6849\n",
      "Epoch: 158/2000... Training loss: 2.9120\n",
      "Epoch: 158/2000... Training loss: 2.8526\n",
      "Epoch: 158/2000... Training loss: 3.0693\n",
      "Epoch: 158/2000... Training loss: 2.9575\n",
      "Epoch: 158/2000... Training loss: 2.6301\n",
      "Epoch: 158/2000... Training loss: 2.7871\n",
      "Epoch: 158/2000... Training loss: 2.7220\n",
      "Epoch: 158/2000... Training loss: 2.7528\n",
      "Epoch: 158/2000... Training loss: 2.7992\n",
      "Epoch: 158/2000... Training loss: 2.9740\n",
      "Epoch: 158/2000... Training loss: 2.8865\n",
      "Epoch: 158/2000... Training loss: 2.8457\n",
      "Epoch: 158/2000... Training loss: 2.7203\n",
      "Epoch: 158/2000... Training loss: 2.7035\n",
      "Epoch: 159/2000... Training loss: 2.7279\n",
      "Epoch: 159/2000... Training loss: 2.7134\n",
      "Epoch: 159/2000... Training loss: 3.0843\n",
      "Epoch: 159/2000... Training loss: 2.6203\n",
      "Epoch: 159/2000... Training loss: 2.5684\n",
      "Epoch: 159/2000... Training loss: 2.5354\n",
      "Epoch: 159/2000... Training loss: 2.7987\n",
      "Epoch: 159/2000... Training loss: 2.9902\n",
      "Epoch: 159/2000... Training loss: 2.7861\n",
      "Epoch: 159/2000... Training loss: 2.8916\n",
      "Epoch: 159/2000... Training loss: 2.9087\n",
      "Epoch: 159/2000... Training loss: 2.5584\n",
      "Epoch: 159/2000... Training loss: 2.7802\n",
      "Epoch: 159/2000... Training loss: 2.8389\n",
      "Epoch: 159/2000... Training loss: 2.8327\n",
      "Epoch: 159/2000... Training loss: 2.7880\n",
      "Epoch: 159/2000... Training loss: 3.0476\n",
      "Epoch: 159/2000... Training loss: 2.7061\n",
      "Epoch: 159/2000... Training loss: 2.4913\n",
      "Epoch: 159/2000... Training loss: 2.5995\n",
      "Epoch: 159/2000... Training loss: 2.6527\n",
      "Epoch: 159/2000... Training loss: 2.5317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159/2000... Training loss: 2.7592\n",
      "Epoch: 159/2000... Training loss: 2.8261\n",
      "Epoch: 159/2000... Training loss: 2.9403\n",
      "Epoch: 159/2000... Training loss: 2.8894\n",
      "Epoch: 159/2000... Training loss: 2.7093\n",
      "Epoch: 159/2000... Training loss: 2.7951\n",
      "Epoch: 159/2000... Training loss: 2.8625\n",
      "Epoch: 159/2000... Training loss: 2.7395\n",
      "Epoch: 159/2000... Training loss: 3.1203\n",
      "Epoch: 160/2000... Training loss: 2.7287\n",
      "Epoch: 160/2000... Training loss: 2.7219\n",
      "Epoch: 160/2000... Training loss: 3.0021\n",
      "Epoch: 160/2000... Training loss: 2.8131\n",
      "Epoch: 160/2000... Training loss: 2.8583\n",
      "Epoch: 160/2000... Training loss: 2.6760\n",
      "Epoch: 160/2000... Training loss: 2.8283\n",
      "Epoch: 160/2000... Training loss: 2.8626\n",
      "Epoch: 160/2000... Training loss: 2.8299\n",
      "Epoch: 160/2000... Training loss: 2.9195\n",
      "Epoch: 160/2000... Training loss: 2.8471\n",
      "Epoch: 160/2000... Training loss: 2.8795\n",
      "Epoch: 160/2000... Training loss: 2.6310\n",
      "Epoch: 160/2000... Training loss: 2.7204\n",
      "Epoch: 160/2000... Training loss: 2.5110\n",
      "Epoch: 160/2000... Training loss: 2.7742\n",
      "Epoch: 160/2000... Training loss: 2.6205\n",
      "Epoch: 160/2000... Training loss: 2.8951\n",
      "Epoch: 160/2000... Training loss: 2.7128\n",
      "Epoch: 160/2000... Training loss: 2.8006\n",
      "Epoch: 160/2000... Training loss: 2.7595\n",
      "Epoch: 160/2000... Training loss: 2.6144\n",
      "Epoch: 160/2000... Training loss: 2.8250\n",
      "Epoch: 160/2000... Training loss: 2.8824\n",
      "Epoch: 160/2000... Training loss: 2.7636\n",
      "Epoch: 160/2000... Training loss: 2.8900\n",
      "Epoch: 160/2000... Training loss: 2.7189\n",
      "Epoch: 160/2000... Training loss: 2.9010\n",
      "Epoch: 160/2000... Training loss: 2.7885\n",
      "Epoch: 160/2000... Training loss: 2.9217\n",
      "Epoch: 160/2000... Training loss: 2.7724\n",
      "Epoch: 161/2000... Training loss: 2.9086\n",
      "Epoch: 161/2000... Training loss: 3.0627\n",
      "Epoch: 161/2000... Training loss: 2.8969\n",
      "Epoch: 161/2000... Training loss: 2.9954\n",
      "Epoch: 161/2000... Training loss: 2.8311\n",
      "Epoch: 161/2000... Training loss: 2.9011\n",
      "Epoch: 161/2000... Training loss: 2.7918\n",
      "Epoch: 161/2000... Training loss: 3.0272\n",
      "Epoch: 161/2000... Training loss: 2.6704\n",
      "Epoch: 161/2000... Training loss: 2.7408\n",
      "Epoch: 161/2000... Training loss: 2.6054\n",
      "Epoch: 161/2000... Training loss: 2.6693\n",
      "Epoch: 161/2000... Training loss: 2.5956\n",
      "Epoch: 161/2000... Training loss: 2.7571\n",
      "Epoch: 161/2000... Training loss: 2.8143\n",
      "Epoch: 161/2000... Training loss: 2.7508\n",
      "Epoch: 161/2000... Training loss: 2.8421\n",
      "Epoch: 161/2000... Training loss: 2.9413\n",
      "Epoch: 161/2000... Training loss: 3.0846\n",
      "Epoch: 161/2000... Training loss: 2.7485\n",
      "Epoch: 161/2000... Training loss: 2.9079\n",
      "Epoch: 161/2000... Training loss: 2.6413\n",
      "Epoch: 161/2000... Training loss: 2.8234\n",
      "Epoch: 161/2000... Training loss: 2.6537\n",
      "Epoch: 161/2000... Training loss: 3.0587\n",
      "Epoch: 161/2000... Training loss: 2.7125\n",
      "Epoch: 161/2000... Training loss: 2.7904\n",
      "Epoch: 161/2000... Training loss: 2.8997\n",
      "Epoch: 161/2000... Training loss: 2.7525\n",
      "Epoch: 161/2000... Training loss: 2.6321\n",
      "Epoch: 161/2000... Training loss: 2.7585\n",
      "Epoch: 162/2000... Training loss: 2.7406\n",
      "Epoch: 162/2000... Training loss: 2.8722\n",
      "Epoch: 162/2000... Training loss: 2.7565\n",
      "Epoch: 162/2000... Training loss: 2.5476\n",
      "Epoch: 162/2000... Training loss: 2.8576\n",
      "Epoch: 162/2000... Training loss: 2.8252\n",
      "Epoch: 162/2000... Training loss: 2.9429\n",
      "Epoch: 162/2000... Training loss: 2.7756\n",
      "Epoch: 162/2000... Training loss: 2.6708\n",
      "Epoch: 162/2000... Training loss: 2.5872\n",
      "Epoch: 162/2000... Training loss: 2.7287\n",
      "Epoch: 162/2000... Training loss: 2.5016\n",
      "Epoch: 162/2000... Training loss: 3.0870\n",
      "Epoch: 162/2000... Training loss: 2.8695\n",
      "Epoch: 162/2000... Training loss: 2.6469\n",
      "Epoch: 162/2000... Training loss: 2.8341\n",
      "Epoch: 162/2000... Training loss: 2.8889\n",
      "Epoch: 162/2000... Training loss: 2.9070\n",
      "Epoch: 162/2000... Training loss: 2.8415\n",
      "Epoch: 162/2000... Training loss: 2.7802\n",
      "Epoch: 162/2000... Training loss: 2.9282\n",
      "Epoch: 162/2000... Training loss: 2.7819\n",
      "Epoch: 162/2000... Training loss: 2.8378\n",
      "Epoch: 162/2000... Training loss: 2.7935\n",
      "Epoch: 162/2000... Training loss: 2.6610\n",
      "Epoch: 162/2000... Training loss: 2.7350\n",
      "Epoch: 162/2000... Training loss: 2.6534\n",
      "Epoch: 162/2000... Training loss: 2.7617\n",
      "Epoch: 162/2000... Training loss: 2.6926\n",
      "Epoch: 162/2000... Training loss: 2.7720\n",
      "Epoch: 162/2000... Training loss: 2.9139\n",
      "Epoch: 163/2000... Training loss: 2.6664\n",
      "Epoch: 163/2000... Training loss: 2.6770\n",
      "Epoch: 163/2000... Training loss: 2.6530\n",
      "Epoch: 163/2000... Training loss: 2.7778\n",
      "Epoch: 163/2000... Training loss: 3.0389\n",
      "Epoch: 163/2000... Training loss: 2.7987\n",
      "Epoch: 163/2000... Training loss: 2.5257\n",
      "Epoch: 163/2000... Training loss: 2.7337\n",
      "Epoch: 163/2000... Training loss: 2.7540\n",
      "Epoch: 163/2000... Training loss: 2.6373\n",
      "Epoch: 163/2000... Training loss: 2.6129\n",
      "Epoch: 163/2000... Training loss: 2.7584\n",
      "Epoch: 163/2000... Training loss: 2.8548\n",
      "Epoch: 163/2000... Training loss: 2.8839\n",
      "Epoch: 163/2000... Training loss: 2.9313\n",
      "Epoch: 163/2000... Training loss: 2.6187\n",
      "Epoch: 163/2000... Training loss: 2.9147\n",
      "Epoch: 163/2000... Training loss: 2.8555\n",
      "Epoch: 163/2000... Training loss: 2.8221\n",
      "Epoch: 163/2000... Training loss: 2.8738\n",
      "Epoch: 163/2000... Training loss: 2.6780\n",
      "Epoch: 163/2000... Training loss: 2.5941\n",
      "Epoch: 163/2000... Training loss: 2.6916\n",
      "Epoch: 163/2000... Training loss: 2.8198\n",
      "Epoch: 163/2000... Training loss: 2.6700\n",
      "Epoch: 163/2000... Training loss: 2.6199\n",
      "Epoch: 163/2000... Training loss: 2.7319\n",
      "Epoch: 163/2000... Training loss: 2.9051\n",
      "Epoch: 163/2000... Training loss: 2.8065\n",
      "Epoch: 163/2000... Training loss: 2.9843\n",
      "Epoch: 163/2000... Training loss: 2.7014\n",
      "Epoch: 164/2000... Training loss: 2.7146\n",
      "Epoch: 164/2000... Training loss: 2.6308\n",
      "Epoch: 164/2000... Training loss: 2.8401\n",
      "Epoch: 164/2000... Training loss: 2.6428\n",
      "Epoch: 164/2000... Training loss: 2.7631\n",
      "Epoch: 164/2000... Training loss: 2.7386\n",
      "Epoch: 164/2000... Training loss: 2.8015\n",
      "Epoch: 164/2000... Training loss: 2.6833\n",
      "Epoch: 164/2000... Training loss: 3.0940\n",
      "Epoch: 164/2000... Training loss: 2.4146\n",
      "Epoch: 164/2000... Training loss: 2.7490\n",
      "Epoch: 164/2000... Training loss: 2.8562\n",
      "Epoch: 164/2000... Training loss: 2.8543\n",
      "Epoch: 164/2000... Training loss: 3.0489\n",
      "Epoch: 164/2000... Training loss: 2.9325\n",
      "Epoch: 164/2000... Training loss: 2.7564\n",
      "Epoch: 164/2000... Training loss: 2.7996\n",
      "Epoch: 164/2000... Training loss: 2.9183\n",
      "Epoch: 164/2000... Training loss: 2.4490\n",
      "Epoch: 164/2000... Training loss: 2.7349\n",
      "Epoch: 164/2000... Training loss: 2.8128\n",
      "Epoch: 164/2000... Training loss: 2.8353\n",
      "Epoch: 164/2000... Training loss: 2.8748\n",
      "Epoch: 164/2000... Training loss: 2.6724\n",
      "Epoch: 164/2000... Training loss: 2.7695\n",
      "Epoch: 164/2000... Training loss: 2.7382\n",
      "Epoch: 164/2000... Training loss: 2.8908\n",
      "Epoch: 164/2000... Training loss: 2.8779\n",
      "Epoch: 164/2000... Training loss: 2.9185\n",
      "Epoch: 164/2000... Training loss: 2.8514\n",
      "Epoch: 164/2000... Training loss: 2.9055\n",
      "Epoch: 165/2000... Training loss: 2.5312\n",
      "Epoch: 165/2000... Training loss: 2.7907\n",
      "Epoch: 165/2000... Training loss: 2.7525\n",
      "Epoch: 165/2000... Training loss: 2.8479\n",
      "Epoch: 165/2000... Training loss: 2.5655\n",
      "Epoch: 165/2000... Training loss: 2.8684\n",
      "Epoch: 165/2000... Training loss: 2.9203\n",
      "Epoch: 165/2000... Training loss: 2.8478\n",
      "Epoch: 165/2000... Training loss: 2.8169\n",
      "Epoch: 165/2000... Training loss: 2.7063\n",
      "Epoch: 165/2000... Training loss: 2.5939\n",
      "Epoch: 165/2000... Training loss: 2.7704\n",
      "Epoch: 165/2000... Training loss: 2.7238\n",
      "Epoch: 165/2000... Training loss: 2.5877\n",
      "Epoch: 165/2000... Training loss: 2.4603\n",
      "Epoch: 165/2000... Training loss: 2.8573\n",
      "Epoch: 165/2000... Training loss: 2.9956\n",
      "Epoch: 165/2000... Training loss: 2.6931\n",
      "Epoch: 165/2000... Training loss: 2.5815\n",
      "Epoch: 165/2000... Training loss: 2.8351\n",
      "Epoch: 165/2000... Training loss: 2.5810\n",
      "Epoch: 165/2000... Training loss: 2.6788\n",
      "Epoch: 165/2000... Training loss: 2.7749\n",
      "Epoch: 165/2000... Training loss: 2.7844\n",
      "Epoch: 165/2000... Training loss: 2.4881\n",
      "Epoch: 165/2000... Training loss: 2.9623\n",
      "Epoch: 165/2000... Training loss: 2.7921\n",
      "Epoch: 165/2000... Training loss: 2.7486\n",
      "Epoch: 165/2000... Training loss: 2.5373\n",
      "Epoch: 165/2000... Training loss: 2.7152\n",
      "Epoch: 165/2000... Training loss: 2.6640\n",
      "Epoch: 166/2000... Training loss: 2.9369\n",
      "Epoch: 166/2000... Training loss: 2.6348\n",
      "Epoch: 166/2000... Training loss: 2.7974\n",
      "Epoch: 166/2000... Training loss: 2.6293\n",
      "Epoch: 166/2000... Training loss: 2.6177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166/2000... Training loss: 2.7973\n",
      "Epoch: 166/2000... Training loss: 2.7170\n",
      "Epoch: 166/2000... Training loss: 2.7480\n",
      "Epoch: 166/2000... Training loss: 2.8070\n",
      "Epoch: 166/2000... Training loss: 2.7369\n",
      "Epoch: 166/2000... Training loss: 2.6534\n",
      "Epoch: 166/2000... Training loss: 2.6950\n",
      "Epoch: 166/2000... Training loss: 2.7004\n",
      "Epoch: 166/2000... Training loss: 2.8746\n",
      "Epoch: 166/2000... Training loss: 2.7267\n",
      "Epoch: 166/2000... Training loss: 2.6418\n",
      "Epoch: 166/2000... Training loss: 2.6224\n",
      "Epoch: 166/2000... Training loss: 2.8116\n",
      "Epoch: 166/2000... Training loss: 2.8908\n",
      "Epoch: 166/2000... Training loss: 2.7214\n",
      "Epoch: 166/2000... Training loss: 2.5586\n",
      "Epoch: 166/2000... Training loss: 2.5854\n",
      "Epoch: 166/2000... Training loss: 2.5263\n",
      "Epoch: 166/2000... Training loss: 2.6690\n",
      "Epoch: 166/2000... Training loss: 2.8480\n",
      "Epoch: 166/2000... Training loss: 2.5075\n",
      "Epoch: 166/2000... Training loss: 2.7543\n",
      "Epoch: 166/2000... Training loss: 2.8263\n",
      "Epoch: 166/2000... Training loss: 2.7172\n",
      "Epoch: 166/2000... Training loss: 2.7149\n",
      "Epoch: 166/2000... Training loss: 2.6612\n",
      "Epoch: 167/2000... Training loss: 2.7525\n",
      "Epoch: 167/2000... Training loss: 2.8412\n",
      "Epoch: 167/2000... Training loss: 2.5633\n",
      "Epoch: 167/2000... Training loss: 2.7832\n",
      "Epoch: 167/2000... Training loss: 2.7196\n",
      "Epoch: 167/2000... Training loss: 2.6278\n",
      "Epoch: 167/2000... Training loss: 2.6718\n",
      "Epoch: 167/2000... Training loss: 2.7705\n",
      "Epoch: 167/2000... Training loss: 2.7351\n",
      "Epoch: 167/2000... Training loss: 2.4836\n",
      "Epoch: 167/2000... Training loss: 2.6577\n",
      "Epoch: 167/2000... Training loss: 2.9558\n",
      "Epoch: 167/2000... Training loss: 2.7107\n",
      "Epoch: 167/2000... Training loss: 2.7420\n",
      "Epoch: 167/2000... Training loss: 2.6459\n",
      "Epoch: 167/2000... Training loss: 2.4954\n",
      "Epoch: 167/2000... Training loss: 2.9132\n",
      "Epoch: 167/2000... Training loss: 2.7612\n",
      "Epoch: 167/2000... Training loss: 2.6494\n",
      "Epoch: 167/2000... Training loss: 2.7145\n",
      "Epoch: 167/2000... Training loss: 2.7531\n",
      "Epoch: 167/2000... Training loss: 2.6555\n",
      "Epoch: 167/2000... Training loss: 2.7385\n",
      "Epoch: 167/2000... Training loss: 2.8884\n",
      "Epoch: 167/2000... Training loss: 2.8468\n",
      "Epoch: 167/2000... Training loss: 2.5707\n",
      "Epoch: 167/2000... Training loss: 2.8351\n",
      "Epoch: 167/2000... Training loss: 2.8658\n",
      "Epoch: 167/2000... Training loss: 2.6175\n",
      "Epoch: 167/2000... Training loss: 2.6007\n",
      "Epoch: 167/2000... Training loss: 2.6563\n",
      "Epoch: 168/2000... Training loss: 2.7113\n",
      "Epoch: 168/2000... Training loss: 2.4404\n",
      "Epoch: 168/2000... Training loss: 2.8628\n",
      "Epoch: 168/2000... Training loss: 2.7940\n",
      "Epoch: 168/2000... Training loss: 2.8559\n",
      "Epoch: 168/2000... Training loss: 2.6209\n",
      "Epoch: 168/2000... Training loss: 2.5544\n",
      "Epoch: 168/2000... Training loss: 2.6321\n",
      "Epoch: 168/2000... Training loss: 2.9155\n",
      "Epoch: 168/2000... Training loss: 2.7514\n",
      "Epoch: 168/2000... Training loss: 2.7740\n",
      "Epoch: 168/2000... Training loss: 2.6748\n",
      "Epoch: 168/2000... Training loss: 2.7402\n",
      "Epoch: 168/2000... Training loss: 2.7446\n",
      "Epoch: 168/2000... Training loss: 2.7408\n",
      "Epoch: 168/2000... Training loss: 3.0239\n",
      "Epoch: 168/2000... Training loss: 2.7048\n",
      "Epoch: 168/2000... Training loss: 2.8392\n",
      "Epoch: 168/2000... Training loss: 2.8831\n",
      "Epoch: 168/2000... Training loss: 2.7136\n",
      "Epoch: 168/2000... Training loss: 2.7493\n",
      "Epoch: 168/2000... Training loss: 2.8044\n",
      "Epoch: 168/2000... Training loss: 2.9308\n",
      "Epoch: 168/2000... Training loss: 2.6091\n",
      "Epoch: 168/2000... Training loss: 2.7881\n",
      "Epoch: 168/2000... Training loss: 2.6448\n",
      "Epoch: 168/2000... Training loss: 2.7313\n",
      "Epoch: 168/2000... Training loss: 2.8283\n",
      "Epoch: 168/2000... Training loss: 2.6889\n",
      "Epoch: 168/2000... Training loss: 2.6244\n",
      "Epoch: 168/2000... Training loss: 2.5767\n",
      "Epoch: 169/2000... Training loss: 2.6380\n",
      "Epoch: 169/2000... Training loss: 2.4685\n",
      "Epoch: 169/2000... Training loss: 2.6004\n",
      "Epoch: 169/2000... Training loss: 2.9609\n",
      "Epoch: 169/2000... Training loss: 2.7048\n",
      "Epoch: 169/2000... Training loss: 2.6342\n",
      "Epoch: 169/2000... Training loss: 2.7366\n",
      "Epoch: 169/2000... Training loss: 2.6525\n",
      "Epoch: 169/2000... Training loss: 2.7668\n",
      "Epoch: 169/2000... Training loss: 2.6805\n",
      "Epoch: 169/2000... Training loss: 2.7042\n",
      "Epoch: 169/2000... Training loss: 2.5568\n",
      "Epoch: 169/2000... Training loss: 2.6720\n",
      "Epoch: 169/2000... Training loss: 2.8309\n",
      "Epoch: 169/2000... Training loss: 2.7017\n",
      "Epoch: 169/2000... Training loss: 2.7678\n",
      "Epoch: 169/2000... Training loss: 2.7019\n",
      "Epoch: 169/2000... Training loss: 2.7348\n",
      "Epoch: 169/2000... Training loss: 2.5470\n",
      "Epoch: 169/2000... Training loss: 2.4858\n",
      "Epoch: 169/2000... Training loss: 2.6596\n",
      "Epoch: 169/2000... Training loss: 2.6501\n",
      "Epoch: 169/2000... Training loss: 2.5259\n",
      "Epoch: 169/2000... Training loss: 2.4967\n",
      "Epoch: 169/2000... Training loss: 2.7108\n",
      "Epoch: 169/2000... Training loss: 2.7217\n",
      "Epoch: 169/2000... Training loss: 2.3070\n",
      "Epoch: 169/2000... Training loss: 2.8076\n",
      "Epoch: 169/2000... Training loss: 2.6711\n",
      "Epoch: 169/2000... Training loss: 2.8083\n",
      "Epoch: 169/2000... Training loss: 2.5827\n",
      "Epoch: 170/2000... Training loss: 2.5191\n",
      "Epoch: 170/2000... Training loss: 2.5906\n",
      "Epoch: 170/2000... Training loss: 2.9506\n",
      "Epoch: 170/2000... Training loss: 2.5666\n",
      "Epoch: 170/2000... Training loss: 2.6379\n",
      "Epoch: 170/2000... Training loss: 2.7662\n",
      "Epoch: 170/2000... Training loss: 2.6666\n",
      "Epoch: 170/2000... Training loss: 2.7798\n",
      "Epoch: 170/2000... Training loss: 2.7367\n",
      "Epoch: 170/2000... Training loss: 2.8298\n",
      "Epoch: 170/2000... Training loss: 2.6995\n",
      "Epoch: 170/2000... Training loss: 2.8890\n",
      "Epoch: 170/2000... Training loss: 2.6348\n",
      "Epoch: 170/2000... Training loss: 2.7305\n",
      "Epoch: 170/2000... Training loss: 2.4887\n",
      "Epoch: 170/2000... Training loss: 2.7754\n",
      "Epoch: 170/2000... Training loss: 2.6792\n",
      "Epoch: 170/2000... Training loss: 2.6600\n",
      "Epoch: 170/2000... Training loss: 2.2627\n",
      "Epoch: 170/2000... Training loss: 2.8435\n",
      "Epoch: 170/2000... Training loss: 2.7017\n",
      "Epoch: 170/2000... Training loss: 2.7303\n",
      "Epoch: 170/2000... Training loss: 2.8248\n",
      "Epoch: 170/2000... Training loss: 2.8154\n",
      "Epoch: 170/2000... Training loss: 2.7785\n",
      "Epoch: 170/2000... Training loss: 2.7841\n",
      "Epoch: 170/2000... Training loss: 2.6963\n",
      "Epoch: 170/2000... Training loss: 2.9851\n",
      "Epoch: 170/2000... Training loss: 2.8797\n",
      "Epoch: 170/2000... Training loss: 3.0047\n",
      "Epoch: 170/2000... Training loss: 2.5602\n",
      "Epoch: 171/2000... Training loss: 2.5832\n",
      "Epoch: 171/2000... Training loss: 2.5527\n",
      "Epoch: 171/2000... Training loss: 2.6335\n",
      "Epoch: 171/2000... Training loss: 2.6616\n",
      "Epoch: 171/2000... Training loss: 2.5787\n",
      "Epoch: 171/2000... Training loss: 2.7070\n",
      "Epoch: 171/2000... Training loss: 2.7850\n",
      "Epoch: 171/2000... Training loss: 2.6638\n",
      "Epoch: 171/2000... Training loss: 2.5691\n",
      "Epoch: 171/2000... Training loss: 2.6601\n",
      "Epoch: 171/2000... Training loss: 2.8381\n",
      "Epoch: 171/2000... Training loss: 2.5293\n",
      "Epoch: 171/2000... Training loss: 2.6219\n",
      "Epoch: 171/2000... Training loss: 2.8032\n",
      "Epoch: 171/2000... Training loss: 2.7270\n",
      "Epoch: 171/2000... Training loss: 2.5918\n",
      "Epoch: 171/2000... Training loss: 2.8667\n",
      "Epoch: 171/2000... Training loss: 2.6272\n",
      "Epoch: 171/2000... Training loss: 2.7709\n",
      "Epoch: 171/2000... Training loss: 2.6551\n",
      "Epoch: 171/2000... Training loss: 2.6682\n",
      "Epoch: 171/2000... Training loss: 2.3347\n",
      "Epoch: 171/2000... Training loss: 2.7948\n",
      "Epoch: 171/2000... Training loss: 2.8565\n",
      "Epoch: 171/2000... Training loss: 2.7439\n",
      "Epoch: 171/2000... Training loss: 2.8481\n",
      "Epoch: 171/2000... Training loss: 2.5782\n",
      "Epoch: 171/2000... Training loss: 2.6910\n",
      "Epoch: 171/2000... Training loss: 2.5132\n",
      "Epoch: 171/2000... Training loss: 2.6992\n",
      "Epoch: 171/2000... Training loss: 2.8458\n",
      "Epoch: 172/2000... Training loss: 2.6623\n",
      "Epoch: 172/2000... Training loss: 2.6957\n",
      "Epoch: 172/2000... Training loss: 2.7247\n",
      "Epoch: 172/2000... Training loss: 2.7326\n",
      "Epoch: 172/2000... Training loss: 2.9466\n",
      "Epoch: 172/2000... Training loss: 2.5379\n",
      "Epoch: 172/2000... Training loss: 2.7798\n",
      "Epoch: 172/2000... Training loss: 2.7106\n",
      "Epoch: 172/2000... Training loss: 2.5003\n",
      "Epoch: 172/2000... Training loss: 2.6308\n",
      "Epoch: 172/2000... Training loss: 2.9329\n",
      "Epoch: 172/2000... Training loss: 2.8319\n",
      "Epoch: 172/2000... Training loss: 2.5291\n",
      "Epoch: 172/2000... Training loss: 2.7775\n",
      "Epoch: 172/2000... Training loss: 2.6735\n",
      "Epoch: 172/2000... Training loss: 2.6104\n",
      "Epoch: 172/2000... Training loss: 2.7120\n",
      "Epoch: 172/2000... Training loss: 2.7745\n",
      "Epoch: 172/2000... Training loss: 2.5633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172/2000... Training loss: 2.9017\n",
      "Epoch: 172/2000... Training loss: 2.6168\n",
      "Epoch: 172/2000... Training loss: 2.4460\n",
      "Epoch: 172/2000... Training loss: 2.8555\n",
      "Epoch: 172/2000... Training loss: 2.6477\n",
      "Epoch: 172/2000... Training loss: 2.6366\n",
      "Epoch: 172/2000... Training loss: 2.6362\n",
      "Epoch: 172/2000... Training loss: 2.6303\n",
      "Epoch: 172/2000... Training loss: 2.6263\n",
      "Epoch: 172/2000... Training loss: 2.7078\n",
      "Epoch: 172/2000... Training loss: 2.8429\n",
      "Epoch: 172/2000... Training loss: 2.7640\n",
      "Epoch: 173/2000... Training loss: 2.5474\n",
      "Epoch: 173/2000... Training loss: 2.3104\n",
      "Epoch: 173/2000... Training loss: 2.5708\n",
      "Epoch: 173/2000... Training loss: 2.8083\n",
      "Epoch: 173/2000... Training loss: 2.7431\n",
      "Epoch: 173/2000... Training loss: 2.6834\n",
      "Epoch: 173/2000... Training loss: 2.8161\n",
      "Epoch: 173/2000... Training loss: 2.5734\n",
      "Epoch: 173/2000... Training loss: 2.6174\n",
      "Epoch: 173/2000... Training loss: 2.8891\n",
      "Epoch: 173/2000... Training loss: 2.6354\n",
      "Epoch: 173/2000... Training loss: 2.5114\n",
      "Epoch: 173/2000... Training loss: 2.5484\n",
      "Epoch: 173/2000... Training loss: 2.7443\n",
      "Epoch: 173/2000... Training loss: 2.5373\n",
      "Epoch: 173/2000... Training loss: 2.4866\n",
      "Epoch: 173/2000... Training loss: 2.8136\n",
      "Epoch: 173/2000... Training loss: 2.9580\n",
      "Epoch: 173/2000... Training loss: 2.8565\n",
      "Epoch: 173/2000... Training loss: 2.6529\n",
      "Epoch: 173/2000... Training loss: 2.7251\n",
      "Epoch: 173/2000... Training loss: 2.7744\n",
      "Epoch: 173/2000... Training loss: 2.5531\n",
      "Epoch: 173/2000... Training loss: 2.7543\n",
      "Epoch: 173/2000... Training loss: 2.6522\n",
      "Epoch: 173/2000... Training loss: 2.6729\n",
      "Epoch: 173/2000... Training loss: 2.6718\n",
      "Epoch: 173/2000... Training loss: 2.7176\n",
      "Epoch: 173/2000... Training loss: 2.7953\n",
      "Epoch: 173/2000... Training loss: 2.8525\n",
      "Epoch: 173/2000... Training loss: 2.6688\n",
      "Epoch: 174/2000... Training loss: 2.6894\n",
      "Epoch: 174/2000... Training loss: 2.7806\n",
      "Epoch: 174/2000... Training loss: 2.7360\n",
      "Epoch: 174/2000... Training loss: 2.6929\n",
      "Epoch: 174/2000... Training loss: 2.8557\n",
      "Epoch: 174/2000... Training loss: 2.6954\n",
      "Epoch: 174/2000... Training loss: 2.4351\n",
      "Epoch: 174/2000... Training loss: 2.3756\n",
      "Epoch: 174/2000... Training loss: 2.8427\n",
      "Epoch: 174/2000... Training loss: 2.9039\n",
      "Epoch: 174/2000... Training loss: 2.5630\n",
      "Epoch: 174/2000... Training loss: 2.7569\n",
      "Epoch: 174/2000... Training loss: 2.7675\n",
      "Epoch: 174/2000... Training loss: 2.7004\n",
      "Epoch: 174/2000... Training loss: 2.8971\n",
      "Epoch: 174/2000... Training loss: 2.5610\n",
      "Epoch: 174/2000... Training loss: 2.4742\n",
      "Epoch: 174/2000... Training loss: 2.7103\n",
      "Epoch: 174/2000... Training loss: 2.5281\n",
      "Epoch: 174/2000... Training loss: 2.7325\n",
      "Epoch: 174/2000... Training loss: 2.5796\n",
      "Epoch: 174/2000... Training loss: 2.8025\n",
      "Epoch: 174/2000... Training loss: 2.4273\n",
      "Epoch: 174/2000... Training loss: 2.5271\n",
      "Epoch: 174/2000... Training loss: 2.9491\n",
      "Epoch: 174/2000... Training loss: 2.5703\n",
      "Epoch: 174/2000... Training loss: 2.6624\n",
      "Epoch: 174/2000... Training loss: 2.5479\n",
      "Epoch: 174/2000... Training loss: 2.7334\n",
      "Epoch: 174/2000... Training loss: 2.8590\n",
      "Epoch: 174/2000... Training loss: 2.8795\n",
      "Epoch: 175/2000... Training loss: 2.7372\n",
      "Epoch: 175/2000... Training loss: 2.7385\n",
      "Epoch: 175/2000... Training loss: 2.7820\n",
      "Epoch: 175/2000... Training loss: 2.8519\n",
      "Epoch: 175/2000... Training loss: 2.6881\n",
      "Epoch: 175/2000... Training loss: 2.5629\n",
      "Epoch: 175/2000... Training loss: 2.6181\n",
      "Epoch: 175/2000... Training loss: 2.6438\n",
      "Epoch: 175/2000... Training loss: 2.8847\n",
      "Epoch: 175/2000... Training loss: 3.0217\n",
      "Epoch: 175/2000... Training loss: 2.8598\n",
      "Epoch: 175/2000... Training loss: 2.5530\n",
      "Epoch: 175/2000... Training loss: 2.5177\n",
      "Epoch: 175/2000... Training loss: 2.7937\n",
      "Epoch: 175/2000... Training loss: 2.5783\n",
      "Epoch: 175/2000... Training loss: 2.5905\n",
      "Epoch: 175/2000... Training loss: 2.7186\n",
      "Epoch: 175/2000... Training loss: 2.6458\n",
      "Epoch: 175/2000... Training loss: 2.7244\n",
      "Epoch: 175/2000... Training loss: 2.5975\n",
      "Epoch: 175/2000... Training loss: 2.7916\n",
      "Epoch: 175/2000... Training loss: 2.6182\n",
      "Epoch: 175/2000... Training loss: 2.6502\n",
      "Epoch: 175/2000... Training loss: 2.7566\n",
      "Epoch: 175/2000... Training loss: 2.8045\n",
      "Epoch: 175/2000... Training loss: 2.6488\n",
      "Epoch: 175/2000... Training loss: 2.8111\n",
      "Epoch: 175/2000... Training loss: 2.6794\n",
      "Epoch: 175/2000... Training loss: 2.7783\n",
      "Epoch: 175/2000... Training loss: 2.8493\n",
      "Epoch: 175/2000... Training loss: 2.8781\n",
      "Epoch: 176/2000... Training loss: 2.8011\n",
      "Epoch: 176/2000... Training loss: 2.4831\n",
      "Epoch: 176/2000... Training loss: 2.6443\n",
      "Epoch: 176/2000... Training loss: 2.5245\n",
      "Epoch: 176/2000... Training loss: 2.6699\n",
      "Epoch: 176/2000... Training loss: 2.8093\n",
      "Epoch: 176/2000... Training loss: 2.6965\n",
      "Epoch: 176/2000... Training loss: 2.8536\n",
      "Epoch: 176/2000... Training loss: 2.6635\n",
      "Epoch: 176/2000... Training loss: 2.6206\n",
      "Epoch: 176/2000... Training loss: 2.6875\n",
      "Epoch: 176/2000... Training loss: 2.5904\n",
      "Epoch: 176/2000... Training loss: 2.4717\n",
      "Epoch: 176/2000... Training loss: 2.6398\n",
      "Epoch: 176/2000... Training loss: 2.6292\n",
      "Epoch: 176/2000... Training loss: 2.6874\n",
      "Epoch: 176/2000... Training loss: 2.5691\n",
      "Epoch: 176/2000... Training loss: 2.8011\n",
      "Epoch: 176/2000... Training loss: 2.6786\n",
      "Epoch: 176/2000... Training loss: 2.9166\n",
      "Epoch: 176/2000... Training loss: 2.4383\n",
      "Epoch: 176/2000... Training loss: 2.5670\n",
      "Epoch: 176/2000... Training loss: 2.8392\n",
      "Epoch: 176/2000... Training loss: 2.3956\n",
      "Epoch: 176/2000... Training loss: 2.6144\n",
      "Epoch: 176/2000... Training loss: 2.5524\n",
      "Epoch: 176/2000... Training loss: 2.6762\n",
      "Epoch: 176/2000... Training loss: 2.8809\n",
      "Epoch: 176/2000... Training loss: 2.6664\n",
      "Epoch: 176/2000... Training loss: 2.6169\n",
      "Epoch: 176/2000... Training loss: 2.7518\n",
      "Epoch: 177/2000... Training loss: 2.7091\n",
      "Epoch: 177/2000... Training loss: 2.5314\n",
      "Epoch: 177/2000... Training loss: 2.5627\n",
      "Epoch: 177/2000... Training loss: 2.7323\n",
      "Epoch: 177/2000... Training loss: 2.5827\n",
      "Epoch: 177/2000... Training loss: 2.6048\n",
      "Epoch: 177/2000... Training loss: 2.6897\n",
      "Epoch: 177/2000... Training loss: 2.7088\n",
      "Epoch: 177/2000... Training loss: 2.5693\n",
      "Epoch: 177/2000... Training loss: 2.5719\n",
      "Epoch: 177/2000... Training loss: 2.7273\n",
      "Epoch: 177/2000... Training loss: 2.6731\n",
      "Epoch: 177/2000... Training loss: 2.8487\n",
      "Epoch: 177/2000... Training loss: 3.2196\n",
      "Epoch: 177/2000... Training loss: 2.7445\n",
      "Epoch: 177/2000... Training loss: 2.7998\n",
      "Epoch: 177/2000... Training loss: 2.7345\n",
      "Epoch: 177/2000... Training loss: 2.6999\n",
      "Epoch: 177/2000... Training loss: 2.5595\n",
      "Epoch: 177/2000... Training loss: 2.6850\n",
      "Epoch: 177/2000... Training loss: 2.5361\n",
      "Epoch: 177/2000... Training loss: 2.4769\n",
      "Epoch: 177/2000... Training loss: 2.6493\n",
      "Epoch: 177/2000... Training loss: 2.7381\n",
      "Epoch: 177/2000... Training loss: 2.5328\n",
      "Epoch: 177/2000... Training loss: 2.7732\n",
      "Epoch: 177/2000... Training loss: 2.7185\n",
      "Epoch: 177/2000... Training loss: 2.9758\n",
      "Epoch: 177/2000... Training loss: 2.4141\n",
      "Epoch: 177/2000... Training loss: 2.8056\n",
      "Epoch: 177/2000... Training loss: 2.4590\n",
      "Epoch: 178/2000... Training loss: 2.4477\n",
      "Epoch: 178/2000... Training loss: 2.7544\n",
      "Epoch: 178/2000... Training loss: 2.7529\n",
      "Epoch: 178/2000... Training loss: 2.5899\n",
      "Epoch: 178/2000... Training loss: 2.6717\n",
      "Epoch: 178/2000... Training loss: 2.7502\n",
      "Epoch: 178/2000... Training loss: 2.4339\n",
      "Epoch: 178/2000... Training loss: 2.6293\n",
      "Epoch: 178/2000... Training loss: 3.0145\n",
      "Epoch: 178/2000... Training loss: 2.7985\n",
      "Epoch: 178/2000... Training loss: 2.9038\n",
      "Epoch: 178/2000... Training loss: 2.8772\n",
      "Epoch: 178/2000... Training loss: 2.6396\n",
      "Epoch: 178/2000... Training loss: 2.8884\n",
      "Epoch: 178/2000... Training loss: 2.7431\n",
      "Epoch: 178/2000... Training loss: 2.5286\n",
      "Epoch: 178/2000... Training loss: 2.7827\n",
      "Epoch: 178/2000... Training loss: 2.6489\n",
      "Epoch: 178/2000... Training loss: 2.5141\n",
      "Epoch: 178/2000... Training loss: 2.6690\n",
      "Epoch: 178/2000... Training loss: 2.8392\n",
      "Epoch: 178/2000... Training loss: 2.5962\n",
      "Epoch: 178/2000... Training loss: 2.6673\n",
      "Epoch: 178/2000... Training loss: 2.6192\n",
      "Epoch: 178/2000... Training loss: 2.6499\n",
      "Epoch: 178/2000... Training loss: 2.6460\n",
      "Epoch: 178/2000... Training loss: 2.7180\n",
      "Epoch: 178/2000... Training loss: 2.6475\n",
      "Epoch: 178/2000... Training loss: 2.6306\n",
      "Epoch: 178/2000... Training loss: 2.5270\n",
      "Epoch: 178/2000... Training loss: 2.7398\n",
      "Epoch: 179/2000... Training loss: 2.7181\n",
      "Epoch: 179/2000... Training loss: 2.6848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 179/2000... Training loss: 2.5315\n",
      "Epoch: 179/2000... Training loss: 2.4911\n",
      "Epoch: 179/2000... Training loss: 2.6614\n",
      "Epoch: 179/2000... Training loss: 2.4750\n",
      "Epoch: 179/2000... Training loss: 3.0019\n",
      "Epoch: 179/2000... Training loss: 2.6457\n",
      "Epoch: 179/2000... Training loss: 2.6364\n",
      "Epoch: 179/2000... Training loss: 2.7420\n",
      "Epoch: 179/2000... Training loss: 2.6183\n",
      "Epoch: 179/2000... Training loss: 2.6692\n",
      "Epoch: 179/2000... Training loss: 2.5411\n",
      "Epoch: 179/2000... Training loss: 2.7328\n",
      "Epoch: 179/2000... Training loss: 2.6994\n",
      "Epoch: 179/2000... Training loss: 2.6956\n",
      "Epoch: 179/2000... Training loss: 2.5105\n",
      "Epoch: 179/2000... Training loss: 2.6242\n",
      "Epoch: 179/2000... Training loss: 2.5890\n",
      "Epoch: 179/2000... Training loss: 2.7450\n",
      "Epoch: 179/2000... Training loss: 2.5929\n",
      "Epoch: 179/2000... Training loss: 2.6852\n",
      "Epoch: 179/2000... Training loss: 2.5492\n",
      "Epoch: 179/2000... Training loss: 2.7033\n",
      "Epoch: 179/2000... Training loss: 2.7532\n",
      "Epoch: 179/2000... Training loss: 2.6262\n",
      "Epoch: 179/2000... Training loss: 2.5781\n",
      "Epoch: 179/2000... Training loss: 2.6207\n",
      "Epoch: 179/2000... Training loss: 2.7093\n",
      "Epoch: 179/2000... Training loss: 2.5884\n",
      "Epoch: 179/2000... Training loss: 2.5911\n",
      "Epoch: 180/2000... Training loss: 2.5389\n",
      "Epoch: 180/2000... Training loss: 2.6846\n",
      "Epoch: 180/2000... Training loss: 2.8470\n",
      "Epoch: 180/2000... Training loss: 2.6946\n",
      "Epoch: 180/2000... Training loss: 2.6555\n",
      "Epoch: 180/2000... Training loss: 2.6205\n",
      "Epoch: 180/2000... Training loss: 2.6626\n",
      "Epoch: 180/2000... Training loss: 2.5183\n",
      "Epoch: 180/2000... Training loss: 2.6458\n",
      "Epoch: 180/2000... Training loss: 2.5532\n",
      "Epoch: 180/2000... Training loss: 2.6937\n",
      "Epoch: 180/2000... Training loss: 2.6199\n",
      "Epoch: 180/2000... Training loss: 2.6361\n",
      "Epoch: 180/2000... Training loss: 2.5086\n",
      "Epoch: 180/2000... Training loss: 2.6205\n",
      "Epoch: 180/2000... Training loss: 2.7291\n",
      "Epoch: 180/2000... Training loss: 2.6806\n",
      "Epoch: 180/2000... Training loss: 2.7859\n",
      "Epoch: 180/2000... Training loss: 2.4550\n",
      "Epoch: 180/2000... Training loss: 2.6900\n",
      "Epoch: 180/2000... Training loss: 2.6791\n",
      "Epoch: 180/2000... Training loss: 2.4188\n",
      "Epoch: 180/2000... Training loss: 2.8545\n",
      "Epoch: 180/2000... Training loss: 2.8330\n",
      "Epoch: 180/2000... Training loss: 2.5665\n",
      "Epoch: 180/2000... Training loss: 2.6632\n",
      "Epoch: 180/2000... Training loss: 2.6607\n",
      "Epoch: 180/2000... Training loss: 2.8444\n",
      "Epoch: 180/2000... Training loss: 2.7063\n",
      "Epoch: 180/2000... Training loss: 2.7982\n",
      "Epoch: 180/2000... Training loss: 2.8138\n",
      "Epoch: 181/2000... Training loss: 2.5234\n",
      "Epoch: 181/2000... Training loss: 2.6673\n",
      "Epoch: 181/2000... Training loss: 2.5952\n",
      "Epoch: 181/2000... Training loss: 2.7195\n",
      "Epoch: 181/2000... Training loss: 2.6057\n",
      "Epoch: 181/2000... Training loss: 2.8573\n",
      "Epoch: 181/2000... Training loss: 2.5644\n",
      "Epoch: 181/2000... Training loss: 2.5537\n",
      "Epoch: 181/2000... Training loss: 2.6143\n",
      "Epoch: 181/2000... Training loss: 2.6291\n",
      "Epoch: 181/2000... Training loss: 2.6841\n",
      "Epoch: 181/2000... Training loss: 2.4811\n",
      "Epoch: 181/2000... Training loss: 2.3460\n",
      "Epoch: 181/2000... Training loss: 2.8144\n",
      "Epoch: 181/2000... Training loss: 2.9282\n",
      "Epoch: 181/2000... Training loss: 2.8940\n",
      "Epoch: 181/2000... Training loss: 2.6227\n",
      "Epoch: 181/2000... Training loss: 2.5262\n",
      "Epoch: 181/2000... Training loss: 2.7579\n",
      "Epoch: 181/2000... Training loss: 2.5168\n",
      "Epoch: 181/2000... Training loss: 2.2842\n",
      "Epoch: 181/2000... Training loss: 2.4312\n",
      "Epoch: 181/2000... Training loss: 2.7896\n",
      "Epoch: 181/2000... Training loss: 2.7511\n",
      "Epoch: 181/2000... Training loss: 2.6166\n",
      "Epoch: 181/2000... Training loss: 2.7390\n",
      "Epoch: 181/2000... Training loss: 2.5259\n",
      "Epoch: 181/2000... Training loss: 2.6501\n",
      "Epoch: 181/2000... Training loss: 2.6162\n",
      "Epoch: 181/2000... Training loss: 2.3783\n",
      "Epoch: 181/2000... Training loss: 2.3247\n",
      "Epoch: 182/2000... Training loss: 2.7181\n",
      "Epoch: 182/2000... Training loss: 2.7335\n",
      "Epoch: 182/2000... Training loss: 2.8637\n",
      "Epoch: 182/2000... Training loss: 2.4994\n",
      "Epoch: 182/2000... Training loss: 2.6521\n",
      "Epoch: 182/2000... Training loss: 2.4451\n",
      "Epoch: 182/2000... Training loss: 2.7079\n",
      "Epoch: 182/2000... Training loss: 2.5294\n",
      "Epoch: 182/2000... Training loss: 2.5783\n",
      "Epoch: 182/2000... Training loss: 2.7549\n",
      "Epoch: 182/2000... Training loss: 2.6248\n",
      "Epoch: 182/2000... Training loss: 2.6390\n",
      "Epoch: 182/2000... Training loss: 2.7296\n",
      "Epoch: 182/2000... Training loss: 2.7793\n",
      "Epoch: 182/2000... Training loss: 2.7481\n",
      "Epoch: 182/2000... Training loss: 2.6370\n",
      "Epoch: 182/2000... Training loss: 2.6599\n",
      "Epoch: 182/2000... Training loss: 2.6883\n",
      "Epoch: 182/2000... Training loss: 2.4411\n",
      "Epoch: 182/2000... Training loss: 2.4046\n",
      "Epoch: 182/2000... Training loss: 2.4607\n",
      "Epoch: 182/2000... Training loss: 2.7097\n",
      "Epoch: 182/2000... Training loss: 2.5711\n",
      "Epoch: 182/2000... Training loss: 2.7760\n",
      "Epoch: 182/2000... Training loss: 2.7192\n",
      "Epoch: 182/2000... Training loss: 2.8101\n",
      "Epoch: 182/2000... Training loss: 2.6447\n",
      "Epoch: 182/2000... Training loss: 2.6855\n",
      "Epoch: 182/2000... Training loss: 2.4668\n",
      "Epoch: 182/2000... Training loss: 2.7317\n",
      "Epoch: 182/2000... Training loss: 2.5991\n",
      "Epoch: 183/2000... Training loss: 2.6031\n",
      "Epoch: 183/2000... Training loss: 2.7760\n",
      "Epoch: 183/2000... Training loss: 2.5317\n",
      "Epoch: 183/2000... Training loss: 2.5950\n",
      "Epoch: 183/2000... Training loss: 2.7034\n",
      "Epoch: 183/2000... Training loss: 2.6764\n",
      "Epoch: 183/2000... Training loss: 2.6214\n",
      "Epoch: 183/2000... Training loss: 2.6284\n",
      "Epoch: 183/2000... Training loss: 2.4112\n",
      "Epoch: 183/2000... Training loss: 2.5010\n",
      "Epoch: 183/2000... Training loss: 2.4992\n",
      "Epoch: 183/2000... Training loss: 2.7075\n",
      "Epoch: 183/2000... Training loss: 2.7257\n",
      "Epoch: 183/2000... Training loss: 2.8414\n",
      "Epoch: 183/2000... Training loss: 2.6076\n",
      "Epoch: 183/2000... Training loss: 2.5728\n",
      "Epoch: 183/2000... Training loss: 2.7846\n",
      "Epoch: 183/2000... Training loss: 2.5627\n",
      "Epoch: 183/2000... Training loss: 2.7800\n",
      "Epoch: 183/2000... Training loss: 2.7111\n",
      "Epoch: 183/2000... Training loss: 2.6647\n",
      "Epoch: 183/2000... Training loss: 2.3163\n",
      "Epoch: 183/2000... Training loss: 2.4172\n",
      "Epoch: 183/2000... Training loss: 2.8427\n",
      "Epoch: 183/2000... Training loss: 2.8818\n",
      "Epoch: 183/2000... Training loss: 2.5945\n",
      "Epoch: 183/2000... Training loss: 2.6440\n",
      "Epoch: 183/2000... Training loss: 2.7795\n",
      "Epoch: 183/2000... Training loss: 2.6212\n",
      "Epoch: 183/2000... Training loss: 2.5651\n",
      "Epoch: 183/2000... Training loss: 2.6152\n",
      "Epoch: 184/2000... Training loss: 2.5966\n",
      "Epoch: 184/2000... Training loss: 2.4746\n",
      "Epoch: 184/2000... Training loss: 2.5046\n",
      "Epoch: 184/2000... Training loss: 2.6008\n",
      "Epoch: 184/2000... Training loss: 2.4687\n",
      "Epoch: 184/2000... Training loss: 2.5998\n",
      "Epoch: 184/2000... Training loss: 2.5832\n",
      "Epoch: 184/2000... Training loss: 2.3479\n",
      "Epoch: 184/2000... Training loss: 2.7080\n",
      "Epoch: 184/2000... Training loss: 2.7715\n",
      "Epoch: 184/2000... Training loss: 2.5292\n",
      "Epoch: 184/2000... Training loss: 2.7537\n",
      "Epoch: 184/2000... Training loss: 2.5157\n",
      "Epoch: 184/2000... Training loss: 2.8299\n",
      "Epoch: 184/2000... Training loss: 2.6430\n",
      "Epoch: 184/2000... Training loss: 2.6777\n",
      "Epoch: 184/2000... Training loss: 2.9864\n",
      "Epoch: 184/2000... Training loss: 2.8390\n",
      "Epoch: 184/2000... Training loss: 2.5723\n",
      "Epoch: 184/2000... Training loss: 2.7588\n",
      "Epoch: 184/2000... Training loss: 2.4775\n",
      "Epoch: 184/2000... Training loss: 2.5754\n",
      "Epoch: 184/2000... Training loss: 2.6930\n",
      "Epoch: 184/2000... Training loss: 2.4692\n",
      "Epoch: 184/2000... Training loss: 2.7714\n",
      "Epoch: 184/2000... Training loss: 2.6474\n",
      "Epoch: 184/2000... Training loss: 2.6495\n",
      "Epoch: 184/2000... Training loss: 2.5123\n",
      "Epoch: 184/2000... Training loss: 2.6635\n",
      "Epoch: 184/2000... Training loss: 2.4279\n",
      "Epoch: 184/2000... Training loss: 2.4236\n",
      "Epoch: 185/2000... Training loss: 2.6512\n",
      "Epoch: 185/2000... Training loss: 2.6391\n",
      "Epoch: 185/2000... Training loss: 2.6703\n",
      "Epoch: 185/2000... Training loss: 2.5641\n",
      "Epoch: 185/2000... Training loss: 2.5036\n",
      "Epoch: 185/2000... Training loss: 2.9310\n",
      "Epoch: 185/2000... Training loss: 2.6048\n",
      "Epoch: 185/2000... Training loss: 2.6878\n",
      "Epoch: 185/2000... Training loss: 2.6707\n",
      "Epoch: 185/2000... Training loss: 2.5605\n",
      "Epoch: 185/2000... Training loss: 2.3399\n",
      "Epoch: 185/2000... Training loss: 2.5918\n",
      "Epoch: 185/2000... Training loss: 2.4437\n",
      "Epoch: 185/2000... Training loss: 2.5624\n",
      "Epoch: 185/2000... Training loss: 2.9892\n",
      "Epoch: 185/2000... Training loss: 2.7987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185/2000... Training loss: 2.5223\n",
      "Epoch: 185/2000... Training loss: 2.6877\n",
      "Epoch: 185/2000... Training loss: 2.4285\n",
      "Epoch: 185/2000... Training loss: 2.7747\n",
      "Epoch: 185/2000... Training loss: 2.5612\n",
      "Epoch: 185/2000... Training loss: 2.3461\n",
      "Epoch: 185/2000... Training loss: 2.7122\n",
      "Epoch: 185/2000... Training loss: 2.6976\n",
      "Epoch: 185/2000... Training loss: 2.6808\n",
      "Epoch: 185/2000... Training loss: 2.9545\n",
      "Epoch: 185/2000... Training loss: 2.6397\n",
      "Epoch: 185/2000... Training loss: 2.5747\n",
      "Epoch: 185/2000... Training loss: 2.4468\n",
      "Epoch: 185/2000... Training loss: 2.7556\n",
      "Epoch: 185/2000... Training loss: 2.9482\n",
      "Epoch: 186/2000... Training loss: 2.5294\n",
      "Epoch: 186/2000... Training loss: 2.4400\n",
      "Epoch: 186/2000... Training loss: 2.6302\n",
      "Epoch: 186/2000... Training loss: 2.4941\n",
      "Epoch: 186/2000... Training loss: 2.3718\n",
      "Epoch: 186/2000... Training loss: 2.4526\n",
      "Epoch: 186/2000... Training loss: 2.6213\n",
      "Epoch: 186/2000... Training loss: 2.4801\n",
      "Epoch: 186/2000... Training loss: 2.6264\n",
      "Epoch: 186/2000... Training loss: 2.6937\n",
      "Epoch: 186/2000... Training loss: 2.4990\n",
      "Epoch: 186/2000... Training loss: 2.4977\n",
      "Epoch: 186/2000... Training loss: 2.7980\n",
      "Epoch: 186/2000... Training loss: 2.7675\n",
      "Epoch: 186/2000... Training loss: 2.4768\n",
      "Epoch: 186/2000... Training loss: 2.6120\n",
      "Epoch: 186/2000... Training loss: 2.4780\n",
      "Epoch: 186/2000... Training loss: 2.7638\n",
      "Epoch: 186/2000... Training loss: 2.6770\n",
      "Epoch: 186/2000... Training loss: 2.5263\n",
      "Epoch: 186/2000... Training loss: 2.5543\n",
      "Epoch: 186/2000... Training loss: 2.6959\n",
      "Epoch: 186/2000... Training loss: 2.6785\n",
      "Epoch: 186/2000... Training loss: 2.7152\n",
      "Epoch: 186/2000... Training loss: 2.8787\n",
      "Epoch: 186/2000... Training loss: 2.8279\n",
      "Epoch: 186/2000... Training loss: 2.7052\n",
      "Epoch: 186/2000... Training loss: 2.6565\n",
      "Epoch: 186/2000... Training loss: 2.7066\n",
      "Epoch: 186/2000... Training loss: 2.5061\n",
      "Epoch: 186/2000... Training loss: 2.7334\n",
      "Epoch: 187/2000... Training loss: 2.6548\n",
      "Epoch: 187/2000... Training loss: 2.6545\n",
      "Epoch: 187/2000... Training loss: 2.7731\n",
      "Epoch: 187/2000... Training loss: 2.3709\n",
      "Epoch: 187/2000... Training loss: 2.5387\n",
      "Epoch: 187/2000... Training loss: 2.3906\n",
      "Epoch: 187/2000... Training loss: 2.4987\n",
      "Epoch: 187/2000... Training loss: 2.7075\n",
      "Epoch: 187/2000... Training loss: 2.6394\n",
      "Epoch: 187/2000... Training loss: 2.4594\n",
      "Epoch: 187/2000... Training loss: 2.8244\n",
      "Epoch: 187/2000... Training loss: 2.4655\n",
      "Epoch: 187/2000... Training loss: 2.4602\n",
      "Epoch: 187/2000... Training loss: 2.6832\n",
      "Epoch: 187/2000... Training loss: 2.4956\n",
      "Epoch: 187/2000... Training loss: 2.5968\n",
      "Epoch: 187/2000... Training loss: 2.6088\n",
      "Epoch: 187/2000... Training loss: 2.3698\n",
      "Epoch: 187/2000... Training loss: 2.4077\n",
      "Epoch: 187/2000... Training loss: 2.4674\n",
      "Epoch: 187/2000... Training loss: 2.4091\n",
      "Epoch: 187/2000... Training loss: 2.6870\n",
      "Epoch: 187/2000... Training loss: 2.7259\n",
      "Epoch: 187/2000... Training loss: 2.7035\n",
      "Epoch: 187/2000... Training loss: 2.7232\n",
      "Epoch: 187/2000... Training loss: 3.0886\n",
      "Epoch: 187/2000... Training loss: 2.7479\n",
      "Epoch: 187/2000... Training loss: 2.8777\n",
      "Epoch: 187/2000... Training loss: 2.5000\n",
      "Epoch: 187/2000... Training loss: 2.6529\n",
      "Epoch: 187/2000... Training loss: 2.7294\n",
      "Epoch: 188/2000... Training loss: 2.5375\n",
      "Epoch: 188/2000... Training loss: 2.7503\n",
      "Epoch: 188/2000... Training loss: 2.4543\n",
      "Epoch: 188/2000... Training loss: 2.8027\n",
      "Epoch: 188/2000... Training loss: 2.7287\n",
      "Epoch: 188/2000... Training loss: 2.5475\n",
      "Epoch: 188/2000... Training loss: 2.8594\n",
      "Epoch: 188/2000... Training loss: 2.5743\n",
      "Epoch: 188/2000... Training loss: 2.7840\n",
      "Epoch: 188/2000... Training loss: 2.4552\n",
      "Epoch: 188/2000... Training loss: 2.5912\n",
      "Epoch: 188/2000... Training loss: 2.4808\n",
      "Epoch: 188/2000... Training loss: 2.6653\n",
      "Epoch: 188/2000... Training loss: 2.6245\n",
      "Epoch: 188/2000... Training loss: 2.7549\n",
      "Epoch: 188/2000... Training loss: 2.3960\n",
      "Epoch: 188/2000... Training loss: 2.5500\n",
      "Epoch: 188/2000... Training loss: 2.5982\n",
      "Epoch: 188/2000... Training loss: 2.7464\n",
      "Epoch: 188/2000... Training loss: 2.4486\n",
      "Epoch: 188/2000... Training loss: 2.3757\n",
      "Epoch: 188/2000... Training loss: 2.8617\n",
      "Epoch: 188/2000... Training loss: 2.6512\n",
      "Epoch: 188/2000... Training loss: 2.8165\n",
      "Epoch: 188/2000... Training loss: 2.6396\n",
      "Epoch: 188/2000... Training loss: 2.5433\n",
      "Epoch: 188/2000... Training loss: 2.7915\n",
      "Epoch: 188/2000... Training loss: 2.6557\n",
      "Epoch: 188/2000... Training loss: 2.4817\n",
      "Epoch: 188/2000... Training loss: 2.6125\n",
      "Epoch: 188/2000... Training loss: 2.6569\n",
      "Epoch: 189/2000... Training loss: 2.6155\n",
      "Epoch: 189/2000... Training loss: 2.2253\n",
      "Epoch: 189/2000... Training loss: 2.6254\n",
      "Epoch: 189/2000... Training loss: 2.5234\n",
      "Epoch: 189/2000... Training loss: 2.3875\n",
      "Epoch: 189/2000... Training loss: 2.5249\n",
      "Epoch: 189/2000... Training loss: 2.6452\n",
      "Epoch: 189/2000... Training loss: 2.5678\n",
      "Epoch: 189/2000... Training loss: 2.6346\n",
      "Epoch: 189/2000... Training loss: 2.6535\n",
      "Epoch: 189/2000... Training loss: 2.5632\n",
      "Epoch: 189/2000... Training loss: 2.5309\n",
      "Epoch: 189/2000... Training loss: 2.3663\n",
      "Epoch: 189/2000... Training loss: 2.8435\n",
      "Epoch: 189/2000... Training loss: 2.6759\n",
      "Epoch: 189/2000... Training loss: 2.7122\n",
      "Epoch: 189/2000... Training loss: 2.7004\n",
      "Epoch: 189/2000... Training loss: 2.4979\n",
      "Epoch: 189/2000... Training loss: 2.7559\n",
      "Epoch: 189/2000... Training loss: 2.6206\n",
      "Epoch: 189/2000... Training loss: 2.4965\n",
      "Epoch: 189/2000... Training loss: 2.3746\n",
      "Epoch: 189/2000... Training loss: 2.6057\n",
      "Epoch: 189/2000... Training loss: 2.6005\n",
      "Epoch: 189/2000... Training loss: 2.6032\n",
      "Epoch: 189/2000... Training loss: 2.6021\n",
      "Epoch: 189/2000... Training loss: 2.6175\n",
      "Epoch: 189/2000... Training loss: 2.7129\n",
      "Epoch: 189/2000... Training loss: 2.6496\n",
      "Epoch: 189/2000... Training loss: 2.5506\n",
      "Epoch: 189/2000... Training loss: 2.4962\n",
      "Epoch: 190/2000... Training loss: 2.6836\n",
      "Epoch: 190/2000... Training loss: 2.6720\n",
      "Epoch: 190/2000... Training loss: 2.3511\n",
      "Epoch: 190/2000... Training loss: 2.6310\n",
      "Epoch: 190/2000... Training loss: 2.7411\n",
      "Epoch: 190/2000... Training loss: 2.6567\n",
      "Epoch: 190/2000... Training loss: 2.5803\n",
      "Epoch: 190/2000... Training loss: 2.6054\n",
      "Epoch: 190/2000... Training loss: 2.7801\n",
      "Epoch: 190/2000... Training loss: 2.6704\n",
      "Epoch: 190/2000... Training loss: 2.7043\n",
      "Epoch: 190/2000... Training loss: 2.6828\n",
      "Epoch: 190/2000... Training loss: 2.4592\n",
      "Epoch: 190/2000... Training loss: 2.6478\n",
      "Epoch: 190/2000... Training loss: 2.5174\n",
      "Epoch: 190/2000... Training loss: 2.6634\n",
      "Epoch: 190/2000... Training loss: 2.7001\n",
      "Epoch: 190/2000... Training loss: 2.6838\n",
      "Epoch: 190/2000... Training loss: 2.4003\n",
      "Epoch: 190/2000... Training loss: 2.5090\n",
      "Epoch: 190/2000... Training loss: 2.7512\n",
      "Epoch: 190/2000... Training loss: 2.4727\n",
      "Epoch: 190/2000... Training loss: 2.6269\n",
      "Epoch: 190/2000... Training loss: 2.8691\n",
      "Epoch: 190/2000... Training loss: 2.5640\n",
      "Epoch: 190/2000... Training loss: 2.5219\n",
      "Epoch: 190/2000... Training loss: 2.8444\n",
      "Epoch: 190/2000... Training loss: 2.4238\n",
      "Epoch: 190/2000... Training loss: 2.5709\n",
      "Epoch: 190/2000... Training loss: 2.6195\n",
      "Epoch: 190/2000... Training loss: 2.5707\n",
      "Epoch: 191/2000... Training loss: 2.5424\n",
      "Epoch: 191/2000... Training loss: 2.3661\n",
      "Epoch: 191/2000... Training loss: 2.6177\n",
      "Epoch: 191/2000... Training loss: 2.8190\n",
      "Epoch: 191/2000... Training loss: 2.5431\n",
      "Epoch: 191/2000... Training loss: 2.5703\n",
      "Epoch: 191/2000... Training loss: 2.6494\n",
      "Epoch: 191/2000... Training loss: 2.7521\n",
      "Epoch: 191/2000... Training loss: 2.4209\n",
      "Epoch: 191/2000... Training loss: 2.7170\n",
      "Epoch: 191/2000... Training loss: 2.4682\n",
      "Epoch: 191/2000... Training loss: 2.5394\n",
      "Epoch: 191/2000... Training loss: 2.4409\n",
      "Epoch: 191/2000... Training loss: 2.5229\n",
      "Epoch: 191/2000... Training loss: 2.9731\n",
      "Epoch: 191/2000... Training loss: 2.6237\n",
      "Epoch: 191/2000... Training loss: 2.6375\n",
      "Epoch: 191/2000... Training loss: 2.4408\n",
      "Epoch: 191/2000... Training loss: 2.4787\n",
      "Epoch: 191/2000... Training loss: 2.9292\n",
      "Epoch: 191/2000... Training loss: 2.2359\n",
      "Epoch: 191/2000... Training loss: 2.6308\n",
      "Epoch: 191/2000... Training loss: 2.7925\n",
      "Epoch: 191/2000... Training loss: 2.5345\n",
      "Epoch: 191/2000... Training loss: 2.8735\n",
      "Epoch: 191/2000... Training loss: 2.5618\n",
      "Epoch: 191/2000... Training loss: 2.6668\n",
      "Epoch: 191/2000... Training loss: 2.5050\n",
      "Epoch: 191/2000... Training loss: 2.6867\n",
      "Epoch: 191/2000... Training loss: 2.4600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191/2000... Training loss: 2.7206\n",
      "Epoch: 192/2000... Training loss: 2.4849\n",
      "Epoch: 192/2000... Training loss: 2.7436\n",
      "Epoch: 192/2000... Training loss: 2.7776\n",
      "Epoch: 192/2000... Training loss: 2.5341\n",
      "Epoch: 192/2000... Training loss: 2.4847\n",
      "Epoch: 192/2000... Training loss: 2.6197\n",
      "Epoch: 192/2000... Training loss: 2.3697\n",
      "Epoch: 192/2000... Training loss: 2.6305\n",
      "Epoch: 192/2000... Training loss: 2.5752\n",
      "Epoch: 192/2000... Training loss: 2.4644\n",
      "Epoch: 192/2000... Training loss: 2.5059\n",
      "Epoch: 192/2000... Training loss: 2.7005\n",
      "Epoch: 192/2000... Training loss: 2.6217\n",
      "Epoch: 192/2000... Training loss: 2.5626\n",
      "Epoch: 192/2000... Training loss: 2.8377\n",
      "Epoch: 192/2000... Training loss: 2.7456\n",
      "Epoch: 192/2000... Training loss: 2.4989\n",
      "Epoch: 192/2000... Training loss: 2.6886\n",
      "Epoch: 192/2000... Training loss: 2.7923\n",
      "Epoch: 192/2000... Training loss: 2.7900\n",
      "Epoch: 192/2000... Training loss: 2.4986\n",
      "Epoch: 192/2000... Training loss: 2.5841\n",
      "Epoch: 192/2000... Training loss: 2.6581\n",
      "Epoch: 192/2000... Training loss: 2.5933\n",
      "Epoch: 192/2000... Training loss: 2.4351\n",
      "Epoch: 192/2000... Training loss: 2.6739\n",
      "Epoch: 192/2000... Training loss: 2.7322\n",
      "Epoch: 192/2000... Training loss: 2.6323\n",
      "Epoch: 192/2000... Training loss: 2.6866\n",
      "Epoch: 192/2000... Training loss: 2.5366\n",
      "Epoch: 192/2000... Training loss: 2.7363\n",
      "Epoch: 193/2000... Training loss: 2.5845\n",
      "Epoch: 193/2000... Training loss: 2.6580\n",
      "Epoch: 193/2000... Training loss: 2.5282\n",
      "Epoch: 193/2000... Training loss: 2.8210\n",
      "Epoch: 193/2000... Training loss: 2.5574\n",
      "Epoch: 193/2000... Training loss: 2.4678\n",
      "Epoch: 193/2000... Training loss: 2.5992\n",
      "Epoch: 193/2000... Training loss: 2.5673\n",
      "Epoch: 193/2000... Training loss: 2.4569\n",
      "Epoch: 193/2000... Training loss: 2.6690\n",
      "Epoch: 193/2000... Training loss: 2.5609\n",
      "Epoch: 193/2000... Training loss: 2.6513\n",
      "Epoch: 193/2000... Training loss: 2.7165\n",
      "Epoch: 193/2000... Training loss: 2.6658\n",
      "Epoch: 193/2000... Training loss: 2.5054\n",
      "Epoch: 193/2000... Training loss: 2.4342\n",
      "Epoch: 193/2000... Training loss: 2.6523\n",
      "Epoch: 193/2000... Training loss: 2.6073\n",
      "Epoch: 193/2000... Training loss: 2.6162\n",
      "Epoch: 193/2000... Training loss: 2.7849\n",
      "Epoch: 193/2000... Training loss: 2.6873\n",
      "Epoch: 193/2000... Training loss: 2.5908\n",
      "Epoch: 193/2000... Training loss: 2.5949\n",
      "Epoch: 193/2000... Training loss: 2.5595\n",
      "Epoch: 193/2000... Training loss: 2.7456\n",
      "Epoch: 193/2000... Training loss: 2.6678\n",
      "Epoch: 193/2000... Training loss: 2.5412\n",
      "Epoch: 193/2000... Training loss: 2.5646\n",
      "Epoch: 193/2000... Training loss: 2.4023\n",
      "Epoch: 193/2000... Training loss: 2.4127\n",
      "Epoch: 193/2000... Training loss: 2.4298\n",
      "Epoch: 194/2000... Training loss: 2.5749\n",
      "Epoch: 194/2000... Training loss: 2.6316\n",
      "Epoch: 194/2000... Training loss: 2.6141\n",
      "Epoch: 194/2000... Training loss: 2.3881\n",
      "Epoch: 194/2000... Training loss: 2.7162\n",
      "Epoch: 194/2000... Training loss: 2.5463\n",
      "Epoch: 194/2000... Training loss: 2.6266\n",
      "Epoch: 194/2000... Training loss: 2.8237\n",
      "Epoch: 194/2000... Training loss: 2.7110\n",
      "Epoch: 194/2000... Training loss: 2.8345\n",
      "Epoch: 194/2000... Training loss: 2.4542\n",
      "Epoch: 194/2000... Training loss: 2.7640\n",
      "Epoch: 194/2000... Training loss: 2.5079\n",
      "Epoch: 194/2000... Training loss: 2.6837\n",
      "Epoch: 194/2000... Training loss: 2.5348\n",
      "Epoch: 194/2000... Training loss: 2.4486\n",
      "Epoch: 194/2000... Training loss: 2.5643\n",
      "Epoch: 194/2000... Training loss: 2.7848\n",
      "Epoch: 194/2000... Training loss: 2.3706\n",
      "Epoch: 194/2000... Training loss: 2.8474\n",
      "Epoch: 194/2000... Training loss: 2.6816\n",
      "Epoch: 194/2000... Training loss: 2.5856\n",
      "Epoch: 194/2000... Training loss: 2.4426\n",
      "Epoch: 194/2000... Training loss: 2.7787\n",
      "Epoch: 194/2000... Training loss: 2.4493\n",
      "Epoch: 194/2000... Training loss: 2.5682\n",
      "Epoch: 194/2000... Training loss: 2.9081\n",
      "Epoch: 194/2000... Training loss: 2.9242\n",
      "Epoch: 194/2000... Training loss: 2.7170\n",
      "Epoch: 194/2000... Training loss: 2.6712\n",
      "Epoch: 194/2000... Training loss: 2.4239\n",
      "Epoch: 195/2000... Training loss: 2.4202\n",
      "Epoch: 195/2000... Training loss: 2.8232\n",
      "Epoch: 195/2000... Training loss: 2.5604\n",
      "Epoch: 195/2000... Training loss: 2.4296\n",
      "Epoch: 195/2000... Training loss: 2.3659\n",
      "Epoch: 195/2000... Training loss: 2.4835\n",
      "Epoch: 195/2000... Training loss: 2.4230\n",
      "Epoch: 195/2000... Training loss: 2.5060\n",
      "Epoch: 195/2000... Training loss: 2.4477\n",
      "Epoch: 195/2000... Training loss: 2.8909\n",
      "Epoch: 195/2000... Training loss: 2.8713\n",
      "Epoch: 195/2000... Training loss: 2.6105\n",
      "Epoch: 195/2000... Training loss: 2.7026\n",
      "Epoch: 195/2000... Training loss: 2.4611\n",
      "Epoch: 195/2000... Training loss: 2.5879\n",
      "Epoch: 195/2000... Training loss: 2.5494\n",
      "Epoch: 195/2000... Training loss: 2.3260\n",
      "Epoch: 195/2000... Training loss: 2.6923\n",
      "Epoch: 195/2000... Training loss: 2.6811\n",
      "Epoch: 195/2000... Training loss: 2.4361\n",
      "Epoch: 195/2000... Training loss: 2.7128\n",
      "Epoch: 195/2000... Training loss: 2.5599\n",
      "Epoch: 195/2000... Training loss: 2.6751\n",
      "Epoch: 195/2000... Training loss: 2.5563\n",
      "Epoch: 195/2000... Training loss: 2.6458\n",
      "Epoch: 195/2000... Training loss: 2.7668\n",
      "Epoch: 195/2000... Training loss: 2.6083\n",
      "Epoch: 195/2000... Training loss: 2.7775\n",
      "Epoch: 195/2000... Training loss: 2.9766\n",
      "Epoch: 195/2000... Training loss: 2.5258\n",
      "Epoch: 195/2000... Training loss: 2.4349\n",
      "Epoch: 196/2000... Training loss: 2.5012\n",
      "Epoch: 196/2000... Training loss: 2.5683\n",
      "Epoch: 196/2000... Training loss: 2.6170\n",
      "Epoch: 196/2000... Training loss: 2.5501\n",
      "Epoch: 196/2000... Training loss: 2.7510\n",
      "Epoch: 196/2000... Training loss: 2.5047\n",
      "Epoch: 196/2000... Training loss: 2.5839\n",
      "Epoch: 196/2000... Training loss: 2.5738\n",
      "Epoch: 196/2000... Training loss: 2.7386\n",
      "Epoch: 196/2000... Training loss: 2.5516\n",
      "Epoch: 196/2000... Training loss: 2.5773\n",
      "Epoch: 196/2000... Training loss: 2.7037\n",
      "Epoch: 196/2000... Training loss: 2.6837\n",
      "Epoch: 196/2000... Training loss: 2.5944\n",
      "Epoch: 196/2000... Training loss: 2.6565\n",
      "Epoch: 196/2000... Training loss: 2.5295\n",
      "Epoch: 196/2000... Training loss: 2.5543\n",
      "Epoch: 196/2000... Training loss: 2.5932\n",
      "Epoch: 196/2000... Training loss: 2.5703\n",
      "Epoch: 196/2000... Training loss: 2.4641\n",
      "Epoch: 196/2000... Training loss: 2.3124\n",
      "Epoch: 196/2000... Training loss: 2.6759\n",
      "Epoch: 196/2000... Training loss: 2.6621\n",
      "Epoch: 196/2000... Training loss: 2.4287\n",
      "Epoch: 196/2000... Training loss: 2.6031\n",
      "Epoch: 196/2000... Training loss: 2.6555\n",
      "Epoch: 196/2000... Training loss: 2.4271\n",
      "Epoch: 196/2000... Training loss: 2.7223\n",
      "Epoch: 196/2000... Training loss: 2.2454\n",
      "Epoch: 196/2000... Training loss: 2.5769\n",
      "Epoch: 196/2000... Training loss: 2.6797\n",
      "Epoch: 197/2000... Training loss: 2.5641\n",
      "Epoch: 197/2000... Training loss: 2.3337\n",
      "Epoch: 197/2000... Training loss: 2.6802\n",
      "Epoch: 197/2000... Training loss: 2.6283\n",
      "Epoch: 197/2000... Training loss: 2.4046\n",
      "Epoch: 197/2000... Training loss: 2.5278\n",
      "Epoch: 197/2000... Training loss: 2.6134\n",
      "Epoch: 197/2000... Training loss: 2.7811\n",
      "Epoch: 197/2000... Training loss: 2.6423\n",
      "Epoch: 197/2000... Training loss: 2.6929\n",
      "Epoch: 197/2000... Training loss: 2.5028\n",
      "Epoch: 197/2000... Training loss: 2.3784\n",
      "Epoch: 197/2000... Training loss: 2.4706\n",
      "Epoch: 197/2000... Training loss: 2.4804\n",
      "Epoch: 197/2000... Training loss: 2.4974\n",
      "Epoch: 197/2000... Training loss: 2.7129\n",
      "Epoch: 197/2000... Training loss: 2.2327\n",
      "Epoch: 197/2000... Training loss: 2.7513\n",
      "Epoch: 197/2000... Training loss: 2.3595\n",
      "Epoch: 197/2000... Training loss: 2.4519\n",
      "Epoch: 197/2000... Training loss: 2.3299\n",
      "Epoch: 197/2000... Training loss: 2.5209\n",
      "Epoch: 197/2000... Training loss: 2.5172\n",
      "Epoch: 197/2000... Training loss: 2.4848\n",
      "Epoch: 197/2000... Training loss: 2.6763\n",
      "Epoch: 197/2000... Training loss: 2.5633\n",
      "Epoch: 197/2000... Training loss: 2.6619\n",
      "Epoch: 197/2000... Training loss: 2.6012\n",
      "Epoch: 197/2000... Training loss: 2.4879\n",
      "Epoch: 197/2000... Training loss: 2.5376\n",
      "Epoch: 197/2000... Training loss: 2.5537\n",
      "Epoch: 198/2000... Training loss: 2.3995\n",
      "Epoch: 198/2000... Training loss: 2.5804\n",
      "Epoch: 198/2000... Training loss: 2.6064\n",
      "Epoch: 198/2000... Training loss: 2.4428\n",
      "Epoch: 198/2000... Training loss: 2.5934\n",
      "Epoch: 198/2000... Training loss: 2.5945\n",
      "Epoch: 198/2000... Training loss: 2.5260\n",
      "Epoch: 198/2000... Training loss: 2.6422\n",
      "Epoch: 198/2000... Training loss: 2.5388\n",
      "Epoch: 198/2000... Training loss: 3.0436\n",
      "Epoch: 198/2000... Training loss: 2.5616\n",
      "Epoch: 198/2000... Training loss: 2.3262\n",
      "Epoch: 198/2000... Training loss: 2.6445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198/2000... Training loss: 2.5002\n",
      "Epoch: 198/2000... Training loss: 2.8589\n",
      "Epoch: 198/2000... Training loss: 2.3281\n",
      "Epoch: 198/2000... Training loss: 2.3757\n",
      "Epoch: 198/2000... Training loss: 2.7320\n",
      "Epoch: 198/2000... Training loss: 2.4365\n",
      "Epoch: 198/2000... Training loss: 2.4608\n",
      "Epoch: 198/2000... Training loss: 2.5362\n",
      "Epoch: 198/2000... Training loss: 2.6383\n",
      "Epoch: 198/2000... Training loss: 2.4044\n",
      "Epoch: 198/2000... Training loss: 2.5154\n",
      "Epoch: 198/2000... Training loss: 2.5780\n",
      "Epoch: 198/2000... Training loss: 2.5994\n",
      "Epoch: 198/2000... Training loss: 2.6730\n",
      "Epoch: 198/2000... Training loss: 2.6548\n",
      "Epoch: 198/2000... Training loss: 2.4814\n",
      "Epoch: 198/2000... Training loss: 2.7183\n",
      "Epoch: 198/2000... Training loss: 2.5698\n",
      "Epoch: 199/2000... Training loss: 2.7848\n",
      "Epoch: 199/2000... Training loss: 2.6776\n",
      "Epoch: 199/2000... Training loss: 2.7863\n",
      "Epoch: 199/2000... Training loss: 2.4797\n",
      "Epoch: 199/2000... Training loss: 2.5150\n",
      "Epoch: 199/2000... Training loss: 2.5627\n",
      "Epoch: 199/2000... Training loss: 2.3811\n",
      "Epoch: 199/2000... Training loss: 2.4391\n",
      "Epoch: 199/2000... Training loss: 2.7010\n",
      "Epoch: 199/2000... Training loss: 2.6667\n",
      "Epoch: 199/2000... Training loss: 2.3317\n",
      "Epoch: 199/2000... Training loss: 2.8010\n",
      "Epoch: 199/2000... Training loss: 2.4436\n",
      "Epoch: 199/2000... Training loss: 2.6722\n",
      "Epoch: 199/2000... Training loss: 2.6197\n",
      "Epoch: 199/2000... Training loss: 2.5320\n",
      "Epoch: 199/2000... Training loss: 2.4180\n",
      "Epoch: 199/2000... Training loss: 2.5932\n",
      "Epoch: 199/2000... Training loss: 2.5170\n",
      "Epoch: 199/2000... Training loss: 2.8185\n",
      "Epoch: 199/2000... Training loss: 2.5035\n",
      "Epoch: 199/2000... Training loss: 2.6640\n",
      "Epoch: 199/2000... Training loss: 2.8074\n",
      "Epoch: 199/2000... Training loss: 2.4213\n",
      "Epoch: 199/2000... Training loss: 2.7075\n",
      "Epoch: 199/2000... Training loss: 2.6788\n",
      "Epoch: 199/2000... Training loss: 2.3190\n",
      "Epoch: 199/2000... Training loss: 2.5413\n",
      "Epoch: 199/2000... Training loss: 2.5060\n",
      "Epoch: 199/2000... Training loss: 2.6162\n",
      "Epoch: 199/2000... Training loss: 2.5754\n",
      "Epoch: 200/2000... Training loss: 2.5004\n",
      "Epoch: 200/2000... Training loss: 2.4891\n",
      "Epoch: 200/2000... Training loss: 2.5134\n",
      "Epoch: 200/2000... Training loss: 2.2561\n",
      "Epoch: 200/2000... Training loss: 2.6629\n",
      "Epoch: 200/2000... Training loss: 2.5152\n",
      "Epoch: 200/2000... Training loss: 2.4641\n",
      "Epoch: 200/2000... Training loss: 2.3760\n",
      "Epoch: 200/2000... Training loss: 2.2682\n",
      "Epoch: 200/2000... Training loss: 2.2223\n",
      "Epoch: 200/2000... Training loss: 2.5606\n",
      "Epoch: 200/2000... Training loss: 2.4557\n",
      "Epoch: 200/2000... Training loss: 2.5139\n",
      "Epoch: 200/2000... Training loss: 2.4984\n",
      "Epoch: 200/2000... Training loss: 2.4161\n",
      "Epoch: 200/2000... Training loss: 2.4372\n",
      "Epoch: 200/2000... Training loss: 2.6260\n",
      "Epoch: 200/2000... Training loss: 2.7139\n",
      "Epoch: 200/2000... Training loss: 2.5654\n",
      "Epoch: 200/2000... Training loss: 2.6864\n",
      "Epoch: 200/2000... Training loss: 2.6964\n",
      "Epoch: 200/2000... Training loss: 2.6044\n",
      "Epoch: 200/2000... Training loss: 2.3670\n",
      "Epoch: 200/2000... Training loss: 2.2705\n",
      "Epoch: 200/2000... Training loss: 2.5622\n",
      "Epoch: 200/2000... Training loss: 2.5937\n",
      "Epoch: 200/2000... Training loss: 2.7892\n",
      "Epoch: 200/2000... Training loss: 2.2649\n",
      "Epoch: 200/2000... Training loss: 2.5262\n",
      "Epoch: 200/2000... Training loss: 2.6657\n",
      "Epoch: 200/2000... Training loss: 2.4013\n",
      "Epoch: 201/2000... Training loss: 2.5034\n",
      "Epoch: 201/2000... Training loss: 2.4933\n",
      "Epoch: 201/2000... Training loss: 2.4114\n",
      "Epoch: 201/2000... Training loss: 2.5511\n",
      "Epoch: 201/2000... Training loss: 2.4336\n",
      "Epoch: 201/2000... Training loss: 2.5124\n",
      "Epoch: 201/2000... Training loss: 2.4813\n",
      "Epoch: 201/2000... Training loss: 2.5549\n",
      "Epoch: 201/2000... Training loss: 2.8017\n",
      "Epoch: 201/2000... Training loss: 2.7128\n",
      "Epoch: 201/2000... Training loss: 2.5753\n",
      "Epoch: 201/2000... Training loss: 2.5823\n",
      "Epoch: 201/2000... Training loss: 2.4830\n",
      "Epoch: 201/2000... Training loss: 2.5974\n",
      "Epoch: 201/2000... Training loss: 2.6957\n",
      "Epoch: 201/2000... Training loss: 2.6198\n",
      "Epoch: 201/2000... Training loss: 2.3184\n",
      "Epoch: 201/2000... Training loss: 2.7795\n",
      "Epoch: 201/2000... Training loss: 2.3930\n",
      "Epoch: 201/2000... Training loss: 2.6478\n",
      "Epoch: 201/2000... Training loss: 2.4179\n",
      "Epoch: 201/2000... Training loss: 2.2746\n",
      "Epoch: 201/2000... Training loss: 2.7603\n",
      "Epoch: 201/2000... Training loss: 2.3575\n",
      "Epoch: 201/2000... Training loss: 2.6783\n",
      "Epoch: 201/2000... Training loss: 2.6459\n",
      "Epoch: 201/2000... Training loss: 2.6182\n",
      "Epoch: 201/2000... Training loss: 2.4980\n",
      "Epoch: 201/2000... Training loss: 2.4338\n",
      "Epoch: 201/2000... Training loss: 2.3956\n",
      "Epoch: 201/2000... Training loss: 2.5534\n",
      "Epoch: 202/2000... Training loss: 2.4109\n",
      "Epoch: 202/2000... Training loss: 2.5344\n",
      "Epoch: 202/2000... Training loss: 2.6400\n",
      "Epoch: 202/2000... Training loss: 2.5020\n",
      "Epoch: 202/2000... Training loss: 2.5946\n",
      "Epoch: 202/2000... Training loss: 2.4419\n",
      "Epoch: 202/2000... Training loss: 2.4796\n",
      "Epoch: 202/2000... Training loss: 2.6323\n",
      "Epoch: 202/2000... Training loss: 2.6147\n",
      "Epoch: 202/2000... Training loss: 2.5397\n",
      "Epoch: 202/2000... Training loss: 2.7871\n",
      "Epoch: 202/2000... Training loss: 2.6264\n",
      "Epoch: 202/2000... Training loss: 2.4013\n",
      "Epoch: 202/2000... Training loss: 2.6759\n",
      "Epoch: 202/2000... Training loss: 2.6427\n",
      "Epoch: 202/2000... Training loss: 2.6355\n",
      "Epoch: 202/2000... Training loss: 2.5394\n",
      "Epoch: 202/2000... Training loss: 2.6352\n",
      "Epoch: 202/2000... Training loss: 2.3381\n",
      "Epoch: 202/2000... Training loss: 2.6352\n",
      "Epoch: 202/2000... Training loss: 2.6246\n",
      "Epoch: 202/2000... Training loss: 2.6711\n",
      "Epoch: 202/2000... Training loss: 2.4975\n",
      "Epoch: 202/2000... Training loss: 2.5855\n",
      "Epoch: 202/2000... Training loss: 2.4660\n",
      "Epoch: 202/2000... Training loss: 2.7179\n",
      "Epoch: 202/2000... Training loss: 2.8259\n",
      "Epoch: 202/2000... Training loss: 2.5350\n",
      "Epoch: 202/2000... Training loss: 2.4830\n",
      "Epoch: 202/2000... Training loss: 2.5009\n",
      "Epoch: 202/2000... Training loss: 2.5422\n",
      "Epoch: 203/2000... Training loss: 2.1655\n",
      "Epoch: 203/2000... Training loss: 2.4754\n",
      "Epoch: 203/2000... Training loss: 2.4874\n",
      "Epoch: 203/2000... Training loss: 2.4182\n",
      "Epoch: 203/2000... Training loss: 2.4825\n",
      "Epoch: 203/2000... Training loss: 2.6495\n",
      "Epoch: 203/2000... Training loss: 2.5542\n",
      "Epoch: 203/2000... Training loss: 2.6206\n",
      "Epoch: 203/2000... Training loss: 2.5755\n",
      "Epoch: 203/2000... Training loss: 2.7314\n",
      "Epoch: 203/2000... Training loss: 2.6006\n",
      "Epoch: 203/2000... Training loss: 2.5132\n",
      "Epoch: 203/2000... Training loss: 2.3954\n",
      "Epoch: 203/2000... Training loss: 2.7531\n",
      "Epoch: 203/2000... Training loss: 2.5961\n",
      "Epoch: 203/2000... Training loss: 2.6379\n",
      "Epoch: 203/2000... Training loss: 2.6795\n",
      "Epoch: 203/2000... Training loss: 2.6103\n",
      "Epoch: 203/2000... Training loss: 2.5381\n",
      "Epoch: 203/2000... Training loss: 2.5791\n",
      "Epoch: 203/2000... Training loss: 2.6532\n",
      "Epoch: 203/2000... Training loss: 2.5043\n",
      "Epoch: 203/2000... Training loss: 2.4859\n",
      "Epoch: 203/2000... Training loss: 2.6563\n",
      "Epoch: 203/2000... Training loss: 2.6807\n",
      "Epoch: 203/2000... Training loss: 2.3827\n",
      "Epoch: 203/2000... Training loss: 2.4777\n",
      "Epoch: 203/2000... Training loss: 2.6633\n",
      "Epoch: 203/2000... Training loss: 2.6804\n",
      "Epoch: 203/2000... Training loss: 2.8986\n",
      "Epoch: 203/2000... Training loss: 2.3239\n",
      "Epoch: 204/2000... Training loss: 2.4806\n",
      "Epoch: 204/2000... Training loss: 2.5630\n",
      "Epoch: 204/2000... Training loss: 2.3111\n",
      "Epoch: 204/2000... Training loss: 2.5499\n",
      "Epoch: 204/2000... Training loss: 2.2232\n",
      "Epoch: 204/2000... Training loss: 2.5595\n",
      "Epoch: 204/2000... Training loss: 2.5634\n",
      "Epoch: 204/2000... Training loss: 2.5181\n",
      "Epoch: 204/2000... Training loss: 2.5973\n",
      "Epoch: 204/2000... Training loss: 2.6784\n",
      "Epoch: 204/2000... Training loss: 2.4400\n",
      "Epoch: 204/2000... Training loss: 2.6674\n",
      "Epoch: 204/2000... Training loss: 2.4000\n",
      "Epoch: 204/2000... Training loss: 2.7168\n",
      "Epoch: 204/2000... Training loss: 2.7030\n",
      "Epoch: 204/2000... Training loss: 2.3932\n",
      "Epoch: 204/2000... Training loss: 2.4917\n",
      "Epoch: 204/2000... Training loss: 2.3531\n",
      "Epoch: 204/2000... Training loss: 2.6383\n",
      "Epoch: 204/2000... Training loss: 2.4426\n",
      "Epoch: 204/2000... Training loss: 2.5691\n",
      "Epoch: 204/2000... Training loss: 2.5214\n",
      "Epoch: 204/2000... Training loss: 2.8199\n",
      "Epoch: 204/2000... Training loss: 2.6478\n",
      "Epoch: 204/2000... Training loss: 2.6229\n",
      "Epoch: 204/2000... Training loss: 2.5963\n",
      "Epoch: 204/2000... Training loss: 2.2455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204/2000... Training loss: 2.6725\n",
      "Epoch: 204/2000... Training loss: 2.4718\n",
      "Epoch: 204/2000... Training loss: 2.6896\n",
      "Epoch: 204/2000... Training loss: 2.8207\n",
      "Epoch: 205/2000... Training loss: 2.4214\n",
      "Epoch: 205/2000... Training loss: 2.5979\n",
      "Epoch: 205/2000... Training loss: 2.5205\n",
      "Epoch: 205/2000... Training loss: 2.2726\n",
      "Epoch: 205/2000... Training loss: 2.5910\n",
      "Epoch: 205/2000... Training loss: 2.2710\n",
      "Epoch: 205/2000... Training loss: 2.5647\n",
      "Epoch: 205/2000... Training loss: 2.6550\n",
      "Epoch: 205/2000... Training loss: 2.8618\n",
      "Epoch: 205/2000... Training loss: 2.8743\n",
      "Epoch: 205/2000... Training loss: 2.4310\n",
      "Epoch: 205/2000... Training loss: 2.5220\n",
      "Epoch: 205/2000... Training loss: 2.5460\n",
      "Epoch: 205/2000... Training loss: 2.6477\n",
      "Epoch: 205/2000... Training loss: 2.1665\n",
      "Epoch: 205/2000... Training loss: 2.6557\n",
      "Epoch: 205/2000... Training loss: 2.3613\n",
      "Epoch: 205/2000... Training loss: 2.5777\n",
      "Epoch: 205/2000... Training loss: 2.6415\n",
      "Epoch: 205/2000... Training loss: 2.5714\n",
      "Epoch: 205/2000... Training loss: 2.2015\n",
      "Epoch: 205/2000... Training loss: 2.4073\n",
      "Epoch: 205/2000... Training loss: 2.6135\n",
      "Epoch: 205/2000... Training loss: 2.4151\n",
      "Epoch: 205/2000... Training loss: 2.6296\n",
      "Epoch: 205/2000... Training loss: 2.7176\n",
      "Epoch: 205/2000... Training loss: 2.6077\n",
      "Epoch: 205/2000... Training loss: 2.6126\n",
      "Epoch: 205/2000... Training loss: 2.5170\n",
      "Epoch: 205/2000... Training loss: 2.5082\n",
      "Epoch: 205/2000... Training loss: 2.8219\n",
      "Epoch: 206/2000... Training loss: 2.7089\n",
      "Epoch: 206/2000... Training loss: 2.5106\n",
      "Epoch: 206/2000... Training loss: 2.5290\n",
      "Epoch: 206/2000... Training loss: 2.3480\n",
      "Epoch: 206/2000... Training loss: 2.5777\n",
      "Epoch: 206/2000... Training loss: 2.2926\n",
      "Epoch: 206/2000... Training loss: 2.6404\n",
      "Epoch: 206/2000... Training loss: 2.3575\n",
      "Epoch: 206/2000... Training loss: 2.5255\n",
      "Epoch: 206/2000... Training loss: 2.7317\n",
      "Epoch: 206/2000... Training loss: 2.4774\n",
      "Epoch: 206/2000... Training loss: 2.6036\n",
      "Epoch: 206/2000... Training loss: 2.5793\n",
      "Epoch: 206/2000... Training loss: 2.4678\n",
      "Epoch: 206/2000... Training loss: 2.6402\n",
      "Epoch: 206/2000... Training loss: 2.7579\n",
      "Epoch: 206/2000... Training loss: 2.5608\n",
      "Epoch: 206/2000... Training loss: 2.4451\n",
      "Epoch: 206/2000... Training loss: 2.4364\n",
      "Epoch: 206/2000... Training loss: 2.9137\n",
      "Epoch: 206/2000... Training loss: 2.5708\n",
      "Epoch: 206/2000... Training loss: 2.5774\n",
      "Epoch: 206/2000... Training loss: 2.6094\n",
      "Epoch: 206/2000... Training loss: 2.5895\n",
      "Epoch: 206/2000... Training loss: 2.5542\n",
      "Epoch: 206/2000... Training loss: 2.2938\n",
      "Epoch: 206/2000... Training loss: 2.5823\n",
      "Epoch: 206/2000... Training loss: 2.7764\n",
      "Epoch: 206/2000... Training loss: 2.7374\n",
      "Epoch: 206/2000... Training loss: 2.5797\n",
      "Epoch: 206/2000... Training loss: 2.5789\n",
      "Epoch: 207/2000... Training loss: 2.2443\n",
      "Epoch: 207/2000... Training loss: 2.4078\n",
      "Epoch: 207/2000... Training loss: 2.4710\n",
      "Epoch: 207/2000... Training loss: 2.5238\n",
      "Epoch: 207/2000... Training loss: 2.4542\n",
      "Epoch: 207/2000... Training loss: 2.3262\n",
      "Epoch: 207/2000... Training loss: 2.6370\n",
      "Epoch: 207/2000... Training loss: 2.3246\n",
      "Epoch: 207/2000... Training loss: 2.4581\n",
      "Epoch: 207/2000... Training loss: 2.6124\n",
      "Epoch: 207/2000... Training loss: 2.3906\n",
      "Epoch: 207/2000... Training loss: 2.3657\n",
      "Epoch: 207/2000... Training loss: 2.3507\n",
      "Epoch: 207/2000... Training loss: 2.5789\n",
      "Epoch: 207/2000... Training loss: 2.7313\n",
      "Epoch: 207/2000... Training loss: 2.8050\n",
      "Epoch: 207/2000... Training loss: 2.5677\n",
      "Epoch: 207/2000... Training loss: 2.3264\n",
      "Epoch: 207/2000... Training loss: 2.7116\n",
      "Epoch: 207/2000... Training loss: 2.3971\n",
      "Epoch: 207/2000... Training loss: 2.5423\n",
      "Epoch: 207/2000... Training loss: 2.4969\n",
      "Epoch: 207/2000... Training loss: 2.5529\n",
      "Epoch: 207/2000... Training loss: 2.6888\n",
      "Epoch: 207/2000... Training loss: 2.6088\n",
      "Epoch: 207/2000... Training loss: 2.3707\n",
      "Epoch: 207/2000... Training loss: 2.4047\n",
      "Epoch: 207/2000... Training loss: 2.6262\n",
      "Epoch: 207/2000... Training loss: 2.5052\n",
      "Epoch: 207/2000... Training loss: 2.3729\n",
      "Epoch: 207/2000... Training loss: 2.7791\n",
      "Epoch: 208/2000... Training loss: 2.5358\n",
      "Epoch: 208/2000... Training loss: 2.8680\n",
      "Epoch: 208/2000... Training loss: 2.5388\n",
      "Epoch: 208/2000... Training loss: 2.5655\n",
      "Epoch: 208/2000... Training loss: 2.5247\n",
      "Epoch: 208/2000... Training loss: 2.5048\n",
      "Epoch: 208/2000... Training loss: 2.6098\n",
      "Epoch: 208/2000... Training loss: 2.2966\n",
      "Epoch: 208/2000... Training loss: 2.5067\n",
      "Epoch: 208/2000... Training loss: 2.7939\n",
      "Epoch: 208/2000... Training loss: 2.4659\n",
      "Epoch: 208/2000... Training loss: 2.4724\n",
      "Epoch: 208/2000... Training loss: 2.2366\n",
      "Epoch: 208/2000... Training loss: 2.5316\n",
      "Epoch: 208/2000... Training loss: 2.3366\n",
      "Epoch: 208/2000... Training loss: 2.6006\n",
      "Epoch: 208/2000... Training loss: 2.4623\n",
      "Epoch: 208/2000... Training loss: 2.4847\n",
      "Epoch: 208/2000... Training loss: 2.4766\n",
      "Epoch: 208/2000... Training loss: 2.3524\n",
      "Epoch: 208/2000... Training loss: 2.3106\n",
      "Epoch: 208/2000... Training loss: 2.2325\n",
      "Epoch: 208/2000... Training loss: 2.4708\n",
      "Epoch: 208/2000... Training loss: 2.6352\n",
      "Epoch: 208/2000... Training loss: 2.5537\n",
      "Epoch: 208/2000... Training loss: 2.4870\n",
      "Epoch: 208/2000... Training loss: 2.7578\n",
      "Epoch: 208/2000... Training loss: 2.4847\n",
      "Epoch: 208/2000... Training loss: 2.6048\n",
      "Epoch: 208/2000... Training loss: 2.4714\n",
      "Epoch: 208/2000... Training loss: 2.5878\n",
      "Epoch: 209/2000... Training loss: 2.6436\n",
      "Epoch: 209/2000... Training loss: 2.7737\n",
      "Epoch: 209/2000... Training loss: 2.4489\n",
      "Epoch: 209/2000... Training loss: 2.3857\n",
      "Epoch: 209/2000... Training loss: 2.4862\n",
      "Epoch: 209/2000... Training loss: 2.3809\n",
      "Epoch: 209/2000... Training loss: 2.2670\n",
      "Epoch: 209/2000... Training loss: 2.5536\n",
      "Epoch: 209/2000... Training loss: 2.1823\n",
      "Epoch: 209/2000... Training loss: 2.6923\n",
      "Epoch: 209/2000... Training loss: 2.3981\n",
      "Epoch: 209/2000... Training loss: 2.2160\n",
      "Epoch: 209/2000... Training loss: 2.4284\n",
      "Epoch: 209/2000... Training loss: 2.4883\n",
      "Epoch: 209/2000... Training loss: 2.5964\n",
      "Epoch: 209/2000... Training loss: 2.7118\n",
      "Epoch: 209/2000... Training loss: 2.5180\n",
      "Epoch: 209/2000... Training loss: 2.5161\n",
      "Epoch: 209/2000... Training loss: 2.2768\n",
      "Epoch: 209/2000... Training loss: 2.6972\n",
      "Epoch: 209/2000... Training loss: 2.5374\n",
      "Epoch: 209/2000... Training loss: 2.4094\n",
      "Epoch: 209/2000... Training loss: 2.6341\n",
      "Epoch: 209/2000... Training loss: 2.6528\n",
      "Epoch: 209/2000... Training loss: 2.5357\n",
      "Epoch: 209/2000... Training loss: 2.6037\n",
      "Epoch: 209/2000... Training loss: 2.3388\n",
      "Epoch: 209/2000... Training loss: 2.6084\n",
      "Epoch: 209/2000... Training loss: 2.6053\n",
      "Epoch: 209/2000... Training loss: 2.4004\n",
      "Epoch: 209/2000... Training loss: 2.5994\n",
      "Epoch: 210/2000... Training loss: 2.3727\n",
      "Epoch: 210/2000... Training loss: 2.5472\n",
      "Epoch: 210/2000... Training loss: 2.1756\n",
      "Epoch: 210/2000... Training loss: 2.4308\n",
      "Epoch: 210/2000... Training loss: 2.5301\n",
      "Epoch: 210/2000... Training loss: 2.7143\n",
      "Epoch: 210/2000... Training loss: 2.5253\n",
      "Epoch: 210/2000... Training loss: 2.6160\n",
      "Epoch: 210/2000... Training loss: 2.5489\n",
      "Epoch: 210/2000... Training loss: 2.6944\n",
      "Epoch: 210/2000... Training loss: 2.6183\n",
      "Epoch: 210/2000... Training loss: 2.4704\n",
      "Epoch: 210/2000... Training loss: 2.5077\n",
      "Epoch: 210/2000... Training loss: 2.5216\n",
      "Epoch: 210/2000... Training loss: 2.4720\n",
      "Epoch: 210/2000... Training loss: 2.6057\n",
      "Epoch: 210/2000... Training loss: 2.5359\n",
      "Epoch: 210/2000... Training loss: 2.4754\n",
      "Epoch: 210/2000... Training loss: 2.4960\n",
      "Epoch: 210/2000... Training loss: 2.7143\n",
      "Epoch: 210/2000... Training loss: 2.5662\n",
      "Epoch: 210/2000... Training loss: 2.9129\n",
      "Epoch: 210/2000... Training loss: 2.7076\n",
      "Epoch: 210/2000... Training loss: 2.5632\n",
      "Epoch: 210/2000... Training loss: 2.6203\n",
      "Epoch: 210/2000... Training loss: 2.5203\n",
      "Epoch: 210/2000... Training loss: 2.7000\n",
      "Epoch: 210/2000... Training loss: 2.4703\n",
      "Epoch: 210/2000... Training loss: 2.4371\n",
      "Epoch: 210/2000... Training loss: 2.6248\n",
      "Epoch: 210/2000... Training loss: 2.3346\n",
      "Epoch: 211/2000... Training loss: 2.2601\n",
      "Epoch: 211/2000... Training loss: 2.3716\n",
      "Epoch: 211/2000... Training loss: 2.4148\n",
      "Epoch: 211/2000... Training loss: 2.4635\n",
      "Epoch: 211/2000... Training loss: 2.5284\n",
      "Epoch: 211/2000... Training loss: 2.3859\n",
      "Epoch: 211/2000... Training loss: 2.7052\n",
      "Epoch: 211/2000... Training loss: 2.6461\n",
      "Epoch: 211/2000... Training loss: 2.5354\n",
      "Epoch: 211/2000... Training loss: 2.4401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211/2000... Training loss: 2.2690\n",
      "Epoch: 211/2000... Training loss: 2.3061\n",
      "Epoch: 211/2000... Training loss: 2.5036\n",
      "Epoch: 211/2000... Training loss: 2.6099\n",
      "Epoch: 211/2000... Training loss: 2.5914\n",
      "Epoch: 211/2000... Training loss: 2.5401\n",
      "Epoch: 211/2000... Training loss: 2.6185\n",
      "Epoch: 211/2000... Training loss: 2.4540\n",
      "Epoch: 211/2000... Training loss: 2.3568\n",
      "Epoch: 211/2000... Training loss: 2.4625\n",
      "Epoch: 211/2000... Training loss: 2.5979\n",
      "Epoch: 211/2000... Training loss: 2.5404\n",
      "Epoch: 211/2000... Training loss: 2.2698\n",
      "Epoch: 211/2000... Training loss: 2.6613\n",
      "Epoch: 211/2000... Training loss: 2.4515\n",
      "Epoch: 211/2000... Training loss: 2.2784\n",
      "Epoch: 211/2000... Training loss: 2.3983\n",
      "Epoch: 211/2000... Training loss: 2.5910\n",
      "Epoch: 211/2000... Training loss: 2.5324\n",
      "Epoch: 211/2000... Training loss: 2.7460\n",
      "Epoch: 211/2000... Training loss: 2.4925\n",
      "Epoch: 212/2000... Training loss: 2.3739\n",
      "Epoch: 212/2000... Training loss: 2.4318\n",
      "Epoch: 212/2000... Training loss: 2.4624\n",
      "Epoch: 212/2000... Training loss: 2.4354\n",
      "Epoch: 212/2000... Training loss: 2.2974\n",
      "Epoch: 212/2000... Training loss: 2.3466\n",
      "Epoch: 212/2000... Training loss: 2.3091\n",
      "Epoch: 212/2000... Training loss: 2.4355\n",
      "Epoch: 212/2000... Training loss: 2.5258\n",
      "Epoch: 212/2000... Training loss: 2.5282\n",
      "Epoch: 212/2000... Training loss: 2.6975\n",
      "Epoch: 212/2000... Training loss: 2.4651\n",
      "Epoch: 212/2000... Training loss: 2.5054\n",
      "Epoch: 212/2000... Training loss: 2.7118\n",
      "Epoch: 212/2000... Training loss: 2.4935\n",
      "Epoch: 212/2000... Training loss: 2.2820\n",
      "Epoch: 212/2000... Training loss: 2.2246\n",
      "Epoch: 212/2000... Training loss: 2.6442\n",
      "Epoch: 212/2000... Training loss: 2.1924\n",
      "Epoch: 212/2000... Training loss: 2.5440\n",
      "Epoch: 212/2000... Training loss: 2.3960\n",
      "Epoch: 212/2000... Training loss: 2.6173\n",
      "Epoch: 212/2000... Training loss: 2.4085\n",
      "Epoch: 212/2000... Training loss: 2.5441\n",
      "Epoch: 212/2000... Training loss: 2.4040\n",
      "Epoch: 212/2000... Training loss: 2.3722\n",
      "Epoch: 212/2000... Training loss: 2.4484\n",
      "Epoch: 212/2000... Training loss: 2.5731\n",
      "Epoch: 212/2000... Training loss: 2.4447\n",
      "Epoch: 212/2000... Training loss: 2.4737\n",
      "Epoch: 212/2000... Training loss: 2.4554\n",
      "Epoch: 213/2000... Training loss: 2.3875\n",
      "Epoch: 213/2000... Training loss: 2.3504\n",
      "Epoch: 213/2000... Training loss: 2.5710\n",
      "Epoch: 213/2000... Training loss: 2.6703\n",
      "Epoch: 213/2000... Training loss: 2.5576\n",
      "Epoch: 213/2000... Training loss: 2.4017\n",
      "Epoch: 213/2000... Training loss: 2.1231\n",
      "Epoch: 213/2000... Training loss: 2.4148\n",
      "Epoch: 213/2000... Training loss: 2.3609\n",
      "Epoch: 213/2000... Training loss: 2.4789\n",
      "Epoch: 213/2000... Training loss: 2.3992\n",
      "Epoch: 213/2000... Training loss: 2.1520\n",
      "Epoch: 213/2000... Training loss: 2.5256\n",
      "Epoch: 213/2000... Training loss: 2.4696\n",
      "Epoch: 213/2000... Training loss: 2.7795\n",
      "Epoch: 213/2000... Training loss: 2.4327\n",
      "Epoch: 213/2000... Training loss: 2.6031\n",
      "Epoch: 213/2000... Training loss: 2.3505\n",
      "Epoch: 213/2000... Training loss: 2.3255\n",
      "Epoch: 213/2000... Training loss: 2.5041\n",
      "Epoch: 213/2000... Training loss: 2.6201\n",
      "Epoch: 213/2000... Training loss: 2.3770\n",
      "Epoch: 213/2000... Training loss: 2.6176\n",
      "Epoch: 213/2000... Training loss: 2.5162\n",
      "Epoch: 213/2000... Training loss: 2.5706\n",
      "Epoch: 213/2000... Training loss: 2.4130\n",
      "Epoch: 213/2000... Training loss: 2.5172\n",
      "Epoch: 213/2000... Training loss: 2.4365\n",
      "Epoch: 213/2000... Training loss: 2.4939\n",
      "Epoch: 213/2000... Training loss: 2.4672\n",
      "Epoch: 213/2000... Training loss: 2.6709\n",
      "Epoch: 214/2000... Training loss: 2.4449\n",
      "Epoch: 214/2000... Training loss: 2.4210\n",
      "Epoch: 214/2000... Training loss: 2.3566\n",
      "Epoch: 214/2000... Training loss: 2.4876\n",
      "Epoch: 214/2000... Training loss: 2.4123\n",
      "Epoch: 214/2000... Training loss: 2.3815\n",
      "Epoch: 214/2000... Training loss: 2.4464\n",
      "Epoch: 214/2000... Training loss: 2.4675\n",
      "Epoch: 214/2000... Training loss: 2.2272\n",
      "Epoch: 214/2000... Training loss: 2.6612\n",
      "Epoch: 214/2000... Training loss: 2.5316\n",
      "Epoch: 214/2000... Training loss: 2.5317\n",
      "Epoch: 214/2000... Training loss: 2.3275\n",
      "Epoch: 214/2000... Training loss: 2.6888\n",
      "Epoch: 214/2000... Training loss: 2.7032\n",
      "Epoch: 214/2000... Training loss: 2.3884\n",
      "Epoch: 214/2000... Training loss: 2.6971\n",
      "Epoch: 214/2000... Training loss: 2.4742\n",
      "Epoch: 214/2000... Training loss: 2.3312\n",
      "Epoch: 214/2000... Training loss: 2.6328\n",
      "Epoch: 214/2000... Training loss: 2.1553\n",
      "Epoch: 214/2000... Training loss: 2.4143\n",
      "Epoch: 214/2000... Training loss: 2.2652\n",
      "Epoch: 214/2000... Training loss: 2.4787\n",
      "Epoch: 214/2000... Training loss: 2.4396\n",
      "Epoch: 214/2000... Training loss: 2.8009\n",
      "Epoch: 214/2000... Training loss: 2.4981\n",
      "Epoch: 214/2000... Training loss: 2.6442\n",
      "Epoch: 214/2000... Training loss: 2.4969\n",
      "Epoch: 214/2000... Training loss: 2.5990\n",
      "Epoch: 214/2000... Training loss: 2.6234\n",
      "Epoch: 215/2000... Training loss: 2.5864\n",
      "Epoch: 215/2000... Training loss: 2.6136\n",
      "Epoch: 215/2000... Training loss: 2.2730\n",
      "Epoch: 215/2000... Training loss: 2.4954\n",
      "Epoch: 215/2000... Training loss: 2.3990\n",
      "Epoch: 215/2000... Training loss: 2.6365\n",
      "Epoch: 215/2000... Training loss: 2.4942\n",
      "Epoch: 215/2000... Training loss: 2.4833\n",
      "Epoch: 215/2000... Training loss: 2.5214\n",
      "Epoch: 215/2000... Training loss: 2.4277\n",
      "Epoch: 215/2000... Training loss: 2.5709\n",
      "Epoch: 215/2000... Training loss: 2.4880\n",
      "Epoch: 215/2000... Training loss: 2.3671\n",
      "Epoch: 215/2000... Training loss: 2.4339\n",
      "Epoch: 215/2000... Training loss: 2.3876\n",
      "Epoch: 215/2000... Training loss: 2.1236\n",
      "Epoch: 215/2000... Training loss: 2.6087\n",
      "Epoch: 215/2000... Training loss: 2.7877\n",
      "Epoch: 215/2000... Training loss: 2.3344\n",
      "Epoch: 215/2000... Training loss: 2.4546\n",
      "Epoch: 215/2000... Training loss: 2.3077\n",
      "Epoch: 215/2000... Training loss: 2.5683\n",
      "Epoch: 215/2000... Training loss: 2.6432\n",
      "Epoch: 215/2000... Training loss: 2.2989\n",
      "Epoch: 215/2000... Training loss: 2.4112\n",
      "Epoch: 215/2000... Training loss: 2.2481\n",
      "Epoch: 215/2000... Training loss: 2.4075\n",
      "Epoch: 215/2000... Training loss: 2.3754\n",
      "Epoch: 215/2000... Training loss: 2.3638\n",
      "Epoch: 215/2000... Training loss: 2.4617\n",
      "Epoch: 215/2000... Training loss: 2.4724\n",
      "Epoch: 216/2000... Training loss: 2.4155\n",
      "Epoch: 216/2000... Training loss: 2.3811\n",
      "Epoch: 216/2000... Training loss: 2.4942\n",
      "Epoch: 216/2000... Training loss: 2.0764\n",
      "Epoch: 216/2000... Training loss: 2.4272\n",
      "Epoch: 216/2000... Training loss: 2.2204\n",
      "Epoch: 216/2000... Training loss: 2.5383\n",
      "Epoch: 216/2000... Training loss: 2.5147\n",
      "Epoch: 216/2000... Training loss: 2.1946\n",
      "Epoch: 216/2000... Training loss: 2.5049\n",
      "Epoch: 216/2000... Training loss: 2.3189\n",
      "Epoch: 216/2000... Training loss: 2.4452\n",
      "Epoch: 216/2000... Training loss: 2.4363\n",
      "Epoch: 216/2000... Training loss: 2.3417\n",
      "Epoch: 216/2000... Training loss: 2.4514\n",
      "Epoch: 216/2000... Training loss: 2.2248\n",
      "Epoch: 216/2000... Training loss: 2.4410\n",
      "Epoch: 216/2000... Training loss: 2.7059\n",
      "Epoch: 216/2000... Training loss: 2.5241\n",
      "Epoch: 216/2000... Training loss: 2.5234\n",
      "Epoch: 216/2000... Training loss: 2.4037\n",
      "Epoch: 216/2000... Training loss: 2.3923\n",
      "Epoch: 216/2000... Training loss: 2.4516\n",
      "Epoch: 216/2000... Training loss: 2.7716\n",
      "Epoch: 216/2000... Training loss: 2.5970\n",
      "Epoch: 216/2000... Training loss: 2.4897\n",
      "Epoch: 216/2000... Training loss: 2.4420\n",
      "Epoch: 216/2000... Training loss: 2.5504\n",
      "Epoch: 216/2000... Training loss: 2.7260\n",
      "Epoch: 216/2000... Training loss: 2.5391\n",
      "Epoch: 216/2000... Training loss: 2.4614\n",
      "Epoch: 217/2000... Training loss: 2.6490\n",
      "Epoch: 217/2000... Training loss: 2.3646\n",
      "Epoch: 217/2000... Training loss: 2.5361\n",
      "Epoch: 217/2000... Training loss: 2.4759\n",
      "Epoch: 217/2000... Training loss: 2.4323\n",
      "Epoch: 217/2000... Training loss: 2.5177\n",
      "Epoch: 217/2000... Training loss: 2.4647\n",
      "Epoch: 217/2000... Training loss: 2.4238\n",
      "Epoch: 217/2000... Training loss: 2.6765\n",
      "Epoch: 217/2000... Training loss: 2.7074\n",
      "Epoch: 217/2000... Training loss: 2.6734\n",
      "Epoch: 217/2000... Training loss: 2.7678\n",
      "Epoch: 217/2000... Training loss: 2.6147\n",
      "Epoch: 217/2000... Training loss: 2.6765\n",
      "Epoch: 217/2000... Training loss: 2.7723\n",
      "Epoch: 217/2000... Training loss: 2.3201\n",
      "Epoch: 217/2000... Training loss: 2.4864\n",
      "Epoch: 217/2000... Training loss: 2.3013\n",
      "Epoch: 217/2000... Training loss: 2.3840\n",
      "Epoch: 217/2000... Training loss: 2.4645\n",
      "Epoch: 217/2000... Training loss: 2.4976\n",
      "Epoch: 217/2000... Training loss: 2.4474\n",
      "Epoch: 217/2000... Training loss: 2.1367\n",
      "Epoch: 217/2000... Training loss: 2.4713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 217/2000... Training loss: 2.4343\n",
      "Epoch: 217/2000... Training loss: 2.3575\n",
      "Epoch: 217/2000... Training loss: 2.5525\n",
      "Epoch: 217/2000... Training loss: 2.3445\n",
      "Epoch: 217/2000... Training loss: 2.3941\n",
      "Epoch: 217/2000... Training loss: 2.4189\n",
      "Epoch: 217/2000... Training loss: 2.3627\n",
      "Epoch: 218/2000... Training loss: 2.6003\n",
      "Epoch: 218/2000... Training loss: 2.5116\n",
      "Epoch: 218/2000... Training loss: 2.3746\n",
      "Epoch: 218/2000... Training loss: 2.5042\n",
      "Epoch: 218/2000... Training loss: 2.7903\n",
      "Epoch: 218/2000... Training loss: 2.6922\n",
      "Epoch: 218/2000... Training loss: 2.4019\n",
      "Epoch: 218/2000... Training loss: 2.4767\n",
      "Epoch: 218/2000... Training loss: 2.4626\n",
      "Epoch: 218/2000... Training loss: 2.3777\n",
      "Epoch: 218/2000... Training loss: 2.4646\n",
      "Epoch: 218/2000... Training loss: 2.3111\n",
      "Epoch: 218/2000... Training loss: 2.3707\n",
      "Epoch: 218/2000... Training loss: 2.4523\n",
      "Epoch: 218/2000... Training loss: 2.5884\n",
      "Epoch: 218/2000... Training loss: 2.4497\n",
      "Epoch: 218/2000... Training loss: 2.5072\n",
      "Epoch: 218/2000... Training loss: 2.2247\n",
      "Epoch: 218/2000... Training loss: 2.3477\n",
      "Epoch: 218/2000... Training loss: 2.7333\n",
      "Epoch: 218/2000... Training loss: 2.1690\n",
      "Epoch: 218/2000... Training loss: 2.5219\n",
      "Epoch: 218/2000... Training loss: 2.5843\n",
      "Epoch: 218/2000... Training loss: 2.7044\n",
      "Epoch: 218/2000... Training loss: 2.3210\n",
      "Epoch: 218/2000... Training loss: 2.7237\n",
      "Epoch: 218/2000... Training loss: 2.3819\n",
      "Epoch: 218/2000... Training loss: 2.7029\n",
      "Epoch: 218/2000... Training loss: 2.4817\n",
      "Epoch: 218/2000... Training loss: 2.3172\n",
      "Epoch: 218/2000... Training loss: 2.5285\n",
      "Epoch: 219/2000... Training loss: 2.5114\n",
      "Epoch: 219/2000... Training loss: 2.6529\n",
      "Epoch: 219/2000... Training loss: 2.2887\n",
      "Epoch: 219/2000... Training loss: 2.4662\n",
      "Epoch: 219/2000... Training loss: 2.3226\n",
      "Epoch: 219/2000... Training loss: 2.4859\n",
      "Epoch: 219/2000... Training loss: 2.5400\n",
      "Epoch: 219/2000... Training loss: 2.3375\n",
      "Epoch: 219/2000... Training loss: 2.2926\n",
      "Epoch: 219/2000... Training loss: 2.7652\n",
      "Epoch: 219/2000... Training loss: 2.5734\n",
      "Epoch: 219/2000... Training loss: 2.2143\n",
      "Epoch: 219/2000... Training loss: 2.3875\n",
      "Epoch: 219/2000... Training loss: 2.5887\n",
      "Epoch: 219/2000... Training loss: 2.5215\n",
      "Epoch: 219/2000... Training loss: 2.3250\n",
      "Epoch: 219/2000... Training loss: 2.3327\n",
      "Epoch: 219/2000... Training loss: 2.2363\n",
      "Epoch: 219/2000... Training loss: 2.4863\n",
      "Epoch: 219/2000... Training loss: 2.6708\n",
      "Epoch: 219/2000... Training loss: 2.4745\n",
      "Epoch: 219/2000... Training loss: 2.2105\n",
      "Epoch: 219/2000... Training loss: 2.7344\n",
      "Epoch: 219/2000... Training loss: 2.3561\n",
      "Epoch: 219/2000... Training loss: 2.6845\n",
      "Epoch: 219/2000... Training loss: 2.5182\n",
      "Epoch: 219/2000... Training loss: 2.3347\n",
      "Epoch: 219/2000... Training loss: 2.3754\n",
      "Epoch: 219/2000... Training loss: 2.6775\n",
      "Epoch: 219/2000... Training loss: 2.3438\n",
      "Epoch: 219/2000... Training loss: 2.2548\n",
      "Epoch: 220/2000... Training loss: 2.4529\n",
      "Epoch: 220/2000... Training loss: 2.5071\n",
      "Epoch: 220/2000... Training loss: 2.4236\n",
      "Epoch: 220/2000... Training loss: 2.4052\n",
      "Epoch: 220/2000... Training loss: 2.4915\n",
      "Epoch: 220/2000... Training loss: 2.3622\n",
      "Epoch: 220/2000... Training loss: 2.2827\n",
      "Epoch: 220/2000... Training loss: 2.5486\n",
      "Epoch: 220/2000... Training loss: 2.4039\n",
      "Epoch: 220/2000... Training loss: 2.3523\n",
      "Epoch: 220/2000... Training loss: 2.4996\n",
      "Epoch: 220/2000... Training loss: 2.4514\n",
      "Epoch: 220/2000... Training loss: 2.3512\n",
      "Epoch: 220/2000... Training loss: 2.5526\n",
      "Epoch: 220/2000... Training loss: 2.8346\n",
      "Epoch: 220/2000... Training loss: 2.2720\n",
      "Epoch: 220/2000... Training loss: 2.3668\n",
      "Epoch: 220/2000... Training loss: 2.5120\n",
      "Epoch: 220/2000... Training loss: 2.3832\n",
      "Epoch: 220/2000... Training loss: 2.6055\n",
      "Epoch: 220/2000... Training loss: 2.4913\n",
      "Epoch: 220/2000... Training loss: 2.5648\n",
      "Epoch: 220/2000... Training loss: 2.5431\n",
      "Epoch: 220/2000... Training loss: 2.4867\n",
      "Epoch: 220/2000... Training loss: 2.5161\n",
      "Epoch: 220/2000... Training loss: 2.2873\n",
      "Epoch: 220/2000... Training loss: 2.4875\n",
      "Epoch: 220/2000... Training loss: 2.5237\n",
      "Epoch: 220/2000... Training loss: 2.6363\n",
      "Epoch: 220/2000... Training loss: 2.3247\n",
      "Epoch: 220/2000... Training loss: 2.4808\n",
      "Epoch: 221/2000... Training loss: 2.4918\n",
      "Epoch: 221/2000... Training loss: 2.4786\n",
      "Epoch: 221/2000... Training loss: 2.2600\n",
      "Epoch: 221/2000... Training loss: 2.7056\n",
      "Epoch: 221/2000... Training loss: 2.5934\n",
      "Epoch: 221/2000... Training loss: 2.3579\n",
      "Epoch: 221/2000... Training loss: 2.2580\n",
      "Epoch: 221/2000... Training loss: 2.2023\n",
      "Epoch: 221/2000... Training loss: 2.2940\n",
      "Epoch: 221/2000... Training loss: 2.5273\n",
      "Epoch: 221/2000... Training loss: 2.0818\n",
      "Epoch: 221/2000... Training loss: 2.3466\n",
      "Epoch: 221/2000... Training loss: 2.4701\n",
      "Epoch: 221/2000... Training loss: 2.4847\n",
      "Epoch: 221/2000... Training loss: 2.3203\n",
      "Epoch: 221/2000... Training loss: 2.4684\n",
      "Epoch: 221/2000... Training loss: 2.6300\n",
      "Epoch: 221/2000... Training loss: 2.6587\n",
      "Epoch: 221/2000... Training loss: 2.2884\n",
      "Epoch: 221/2000... Training loss: 2.7763\n",
      "Epoch: 221/2000... Training loss: 2.4638\n",
      "Epoch: 221/2000... Training loss: 2.4113\n",
      "Epoch: 221/2000... Training loss: 2.5519\n",
      "Epoch: 221/2000... Training loss: 2.6016\n",
      "Epoch: 221/2000... Training loss: 2.3320\n",
      "Epoch: 221/2000... Training loss: 2.3305\n",
      "Epoch: 221/2000... Training loss: 2.4845\n",
      "Epoch: 221/2000... Training loss: 2.6277\n",
      "Epoch: 221/2000... Training loss: 2.4387\n",
      "Epoch: 221/2000... Training loss: 2.4097\n",
      "Epoch: 221/2000... Training loss: 2.3547\n",
      "Epoch: 222/2000... Training loss: 2.3199\n",
      "Epoch: 222/2000... Training loss: 2.2609\n",
      "Epoch: 222/2000... Training loss: 2.3753\n",
      "Epoch: 222/2000... Training loss: 2.3752\n",
      "Epoch: 222/2000... Training loss: 2.4493\n",
      "Epoch: 222/2000... Training loss: 2.2531\n",
      "Epoch: 222/2000... Training loss: 2.1697\n",
      "Epoch: 222/2000... Training loss: 2.3339\n",
      "Epoch: 222/2000... Training loss: 2.2515\n",
      "Epoch: 222/2000... Training loss: 2.4305\n",
      "Epoch: 222/2000... Training loss: 2.6039\n",
      "Epoch: 222/2000... Training loss: 2.4103\n",
      "Epoch: 222/2000... Training loss: 2.3372\n",
      "Epoch: 222/2000... Training loss: 2.4325\n",
      "Epoch: 222/2000... Training loss: 2.2484\n",
      "Epoch: 222/2000... Training loss: 2.4248\n",
      "Epoch: 222/2000... Training loss: 2.6848\n",
      "Epoch: 222/2000... Training loss: 2.4607\n",
      "Epoch: 222/2000... Training loss: 2.4435\n",
      "Epoch: 222/2000... Training loss: 2.3730\n",
      "Epoch: 222/2000... Training loss: 2.6630\n",
      "Epoch: 222/2000... Training loss: 2.4159\n",
      "Epoch: 222/2000... Training loss: 2.5600\n",
      "Epoch: 222/2000... Training loss: 2.3684\n",
      "Epoch: 222/2000... Training loss: 2.4657\n",
      "Epoch: 222/2000... Training loss: 2.3611\n",
      "Epoch: 222/2000... Training loss: 2.5006\n",
      "Epoch: 222/2000... Training loss: 2.3807\n",
      "Epoch: 222/2000... Training loss: 2.3921\n",
      "Epoch: 222/2000... Training loss: 2.6445\n",
      "Epoch: 222/2000... Training loss: 2.2741\n",
      "Epoch: 223/2000... Training loss: 2.2312\n",
      "Epoch: 223/2000... Training loss: 2.3479\n",
      "Epoch: 223/2000... Training loss: 2.2858\n",
      "Epoch: 223/2000... Training loss: 2.4289\n",
      "Epoch: 223/2000... Training loss: 2.3607\n",
      "Epoch: 223/2000... Training loss: 2.2751\n",
      "Epoch: 223/2000... Training loss: 2.3341\n",
      "Epoch: 223/2000... Training loss: 2.3512\n",
      "Epoch: 223/2000... Training loss: 2.5041\n",
      "Epoch: 223/2000... Training loss: 2.3288\n",
      "Epoch: 223/2000... Training loss: 2.6193\n",
      "Epoch: 223/2000... Training loss: 2.4333\n",
      "Epoch: 223/2000... Training loss: 2.5946\n",
      "Epoch: 223/2000... Training loss: 2.4131\n",
      "Epoch: 223/2000... Training loss: 2.4443\n",
      "Epoch: 223/2000... Training loss: 2.4346\n",
      "Epoch: 223/2000... Training loss: 2.3279\n",
      "Epoch: 223/2000... Training loss: 2.3710\n",
      "Epoch: 223/2000... Training loss: 2.4438\n",
      "Epoch: 223/2000... Training loss: 2.6814\n",
      "Epoch: 223/2000... Training loss: 2.6066\n",
      "Epoch: 223/2000... Training loss: 2.5074\n",
      "Epoch: 223/2000... Training loss: 2.4584\n",
      "Epoch: 223/2000... Training loss: 2.3979\n",
      "Epoch: 223/2000... Training loss: 2.6452\n",
      "Epoch: 223/2000... Training loss: 2.4109\n",
      "Epoch: 223/2000... Training loss: 2.3384\n",
      "Epoch: 223/2000... Training loss: 2.3949\n",
      "Epoch: 223/2000... Training loss: 2.4508\n",
      "Epoch: 223/2000... Training loss: 2.4507\n",
      "Epoch: 223/2000... Training loss: 2.5909\n",
      "Epoch: 224/2000... Training loss: 2.3994\n",
      "Epoch: 224/2000... Training loss: 2.4331\n",
      "Epoch: 224/2000... Training loss: 2.4570\n",
      "Epoch: 224/2000... Training loss: 2.4675\n",
      "Epoch: 224/2000... Training loss: 2.4051\n",
      "Epoch: 224/2000... Training loss: 2.2898\n",
      "Epoch: 224/2000... Training loss: 2.3630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224/2000... Training loss: 2.5424\n",
      "Epoch: 224/2000... Training loss: 2.3527\n",
      "Epoch: 224/2000... Training loss: 2.4725\n",
      "Epoch: 224/2000... Training loss: 2.5906\n",
      "Epoch: 224/2000... Training loss: 2.3078\n",
      "Epoch: 224/2000... Training loss: 2.4422\n",
      "Epoch: 224/2000... Training loss: 2.5695\n",
      "Epoch: 224/2000... Training loss: 2.4217\n",
      "Epoch: 224/2000... Training loss: 2.4506\n",
      "Epoch: 224/2000... Training loss: 2.3574\n",
      "Epoch: 224/2000... Training loss: 2.4608\n",
      "Epoch: 224/2000... Training loss: 2.2648\n",
      "Epoch: 224/2000... Training loss: 2.6534\n",
      "Epoch: 224/2000... Training loss: 2.1585\n",
      "Epoch: 224/2000... Training loss: 2.3319\n",
      "Epoch: 224/2000... Training loss: 2.6820\n",
      "Epoch: 224/2000... Training loss: 2.3570\n",
      "Epoch: 224/2000... Training loss: 2.6450\n",
      "Epoch: 224/2000... Training loss: 2.5940\n",
      "Epoch: 224/2000... Training loss: 2.5714\n",
      "Epoch: 224/2000... Training loss: 2.5162\n",
      "Epoch: 224/2000... Training loss: 2.5895\n",
      "Epoch: 224/2000... Training loss: 2.5445\n",
      "Epoch: 224/2000... Training loss: 2.2659\n",
      "Epoch: 225/2000... Training loss: 2.3525\n",
      "Epoch: 225/2000... Training loss: 2.3646\n",
      "Epoch: 225/2000... Training loss: 2.2946\n",
      "Epoch: 225/2000... Training loss: 2.7507\n",
      "Epoch: 225/2000... Training loss: 2.3597\n",
      "Epoch: 225/2000... Training loss: 2.4361\n",
      "Epoch: 225/2000... Training loss: 2.4904\n",
      "Epoch: 225/2000... Training loss: 2.4796\n",
      "Epoch: 225/2000... Training loss: 2.2521\n",
      "Epoch: 225/2000... Training loss: 2.7536\n",
      "Epoch: 225/2000... Training loss: 2.3810\n",
      "Epoch: 225/2000... Training loss: 2.4490\n",
      "Epoch: 225/2000... Training loss: 2.2821\n",
      "Epoch: 225/2000... Training loss: 2.7255\n",
      "Epoch: 225/2000... Training loss: 2.2039\n",
      "Epoch: 225/2000... Training loss: 2.2713\n",
      "Epoch: 225/2000... Training loss: 2.4170\n",
      "Epoch: 225/2000... Training loss: 2.4556\n",
      "Epoch: 225/2000... Training loss: 2.4546\n",
      "Epoch: 225/2000... Training loss: 2.5383\n",
      "Epoch: 225/2000... Training loss: 2.3643\n",
      "Epoch: 225/2000... Training loss: 2.4283\n",
      "Epoch: 225/2000... Training loss: 2.4210\n",
      "Epoch: 225/2000... Training loss: 2.3435\n",
      "Epoch: 225/2000... Training loss: 2.7937\n",
      "Epoch: 225/2000... Training loss: 2.2381\n",
      "Epoch: 225/2000... Training loss: 2.4789\n",
      "Epoch: 225/2000... Training loss: 2.4438\n",
      "Epoch: 225/2000... Training loss: 2.5499\n",
      "Epoch: 225/2000... Training loss: 2.6931\n",
      "Epoch: 225/2000... Training loss: 2.4889\n",
      "Epoch: 226/2000... Training loss: 2.1028\n",
      "Epoch: 226/2000... Training loss: 2.5042\n",
      "Epoch: 226/2000... Training loss: 2.4516\n",
      "Epoch: 226/2000... Training loss: 2.2299\n",
      "Epoch: 226/2000... Training loss: 2.5340\n",
      "Epoch: 226/2000... Training loss: 2.5292\n",
      "Epoch: 226/2000... Training loss: 2.3849\n",
      "Epoch: 226/2000... Training loss: 2.5394\n",
      "Epoch: 226/2000... Training loss: 2.3258\n",
      "Epoch: 226/2000... Training loss: 2.6286\n",
      "Epoch: 226/2000... Training loss: 2.5178\n",
      "Epoch: 226/2000... Training loss: 2.3371\n",
      "Epoch: 226/2000... Training loss: 2.4863\n",
      "Epoch: 226/2000... Training loss: 2.4282\n",
      "Epoch: 226/2000... Training loss: 2.2794\n",
      "Epoch: 226/2000... Training loss: 2.3798\n",
      "Epoch: 226/2000... Training loss: 2.1138\n",
      "Epoch: 226/2000... Training loss: 2.3707\n",
      "Epoch: 226/2000... Training loss: 2.2678\n",
      "Epoch: 226/2000... Training loss: 2.3508\n",
      "Epoch: 226/2000... Training loss: 2.7713\n",
      "Epoch: 226/2000... Training loss: 2.0533\n",
      "Epoch: 226/2000... Training loss: 2.6936\n",
      "Epoch: 226/2000... Training loss: 2.4117\n",
      "Epoch: 226/2000... Training loss: 2.6535\n",
      "Epoch: 226/2000... Training loss: 2.2158\n",
      "Epoch: 226/2000... Training loss: 2.5993\n",
      "Epoch: 226/2000... Training loss: 2.4648\n",
      "Epoch: 226/2000... Training loss: 2.4785\n",
      "Epoch: 226/2000... Training loss: 2.5488\n",
      "Epoch: 226/2000... Training loss: 2.4594\n",
      "Epoch: 227/2000... Training loss: 2.1467\n",
      "Epoch: 227/2000... Training loss: 2.5962\n",
      "Epoch: 227/2000... Training loss: 2.1960\n",
      "Epoch: 227/2000... Training loss: 2.2629\n",
      "Epoch: 227/2000... Training loss: 2.2155\n",
      "Epoch: 227/2000... Training loss: 2.2760\n",
      "Epoch: 227/2000... Training loss: 2.3982\n",
      "Epoch: 227/2000... Training loss: 2.3973\n",
      "Epoch: 227/2000... Training loss: 2.4364\n",
      "Epoch: 227/2000... Training loss: 2.4396\n",
      "Epoch: 227/2000... Training loss: 2.4356\n",
      "Epoch: 227/2000... Training loss: 2.4438\n",
      "Epoch: 227/2000... Training loss: 2.4707\n",
      "Epoch: 227/2000... Training loss: 2.4464\n",
      "Epoch: 227/2000... Training loss: 2.7296\n",
      "Epoch: 227/2000... Training loss: 2.4992\n",
      "Epoch: 227/2000... Training loss: 2.4089\n",
      "Epoch: 227/2000... Training loss: 2.5066\n",
      "Epoch: 227/2000... Training loss: 2.4677\n",
      "Epoch: 227/2000... Training loss: 2.6135\n",
      "Epoch: 227/2000... Training loss: 2.4053\n",
      "Epoch: 227/2000... Training loss: 2.0909\n",
      "Epoch: 227/2000... Training loss: 2.5906\n",
      "Epoch: 227/2000... Training loss: 2.3791\n",
      "Epoch: 227/2000... Training loss: 2.2996\n",
      "Epoch: 227/2000... Training loss: 2.5899\n",
      "Epoch: 227/2000... Training loss: 2.5009\n",
      "Epoch: 227/2000... Training loss: 2.1720\n",
      "Epoch: 227/2000... Training loss: 2.3394\n",
      "Epoch: 227/2000... Training loss: 2.2904\n",
      "Epoch: 227/2000... Training loss: 2.6582\n",
      "Epoch: 228/2000... Training loss: 2.2363\n",
      "Epoch: 228/2000... Training loss: 2.5523\n",
      "Epoch: 228/2000... Training loss: 2.2444\n",
      "Epoch: 228/2000... Training loss: 2.1543\n",
      "Epoch: 228/2000... Training loss: 2.4154\n",
      "Epoch: 228/2000... Training loss: 2.2823\n",
      "Epoch: 228/2000... Training loss: 2.2109\n",
      "Epoch: 228/2000... Training loss: 2.2616\n",
      "Epoch: 228/2000... Training loss: 2.7819\n",
      "Epoch: 228/2000... Training loss: 2.5998\n",
      "Epoch: 228/2000... Training loss: 2.4083\n",
      "Epoch: 228/2000... Training loss: 2.3910\n",
      "Epoch: 228/2000... Training loss: 2.2140\n",
      "Epoch: 228/2000... Training loss: 2.6681\n",
      "Epoch: 228/2000... Training loss: 2.3189\n",
      "Epoch: 228/2000... Training loss: 2.0787\n",
      "Epoch: 228/2000... Training loss: 2.6448\n",
      "Epoch: 228/2000... Training loss: 2.2741\n",
      "Epoch: 228/2000... Training loss: 2.4056\n",
      "Epoch: 228/2000... Training loss: 2.5771\n",
      "Epoch: 228/2000... Training loss: 2.3793\n",
      "Epoch: 228/2000... Training loss: 2.3077\n",
      "Epoch: 228/2000... Training loss: 2.3445\n",
      "Epoch: 228/2000... Training loss: 2.6136\n",
      "Epoch: 228/2000... Training loss: 2.5834\n",
      "Epoch: 228/2000... Training loss: 2.3358\n",
      "Epoch: 228/2000... Training loss: 2.5685\n",
      "Epoch: 228/2000... Training loss: 2.2616\n",
      "Epoch: 228/2000... Training loss: 2.3660\n",
      "Epoch: 228/2000... Training loss: 2.3127\n",
      "Epoch: 228/2000... Training loss: 2.4516\n",
      "Epoch: 229/2000... Training loss: 2.6735\n",
      "Epoch: 229/2000... Training loss: 2.5155\n",
      "Epoch: 229/2000... Training loss: 2.4359\n",
      "Epoch: 229/2000... Training loss: 2.1627\n",
      "Epoch: 229/2000... Training loss: 2.2376\n",
      "Epoch: 229/2000... Training loss: 2.2485\n",
      "Epoch: 229/2000... Training loss: 2.4982\n",
      "Epoch: 229/2000... Training loss: 2.4129\n",
      "Epoch: 229/2000... Training loss: 2.5117\n",
      "Epoch: 229/2000... Training loss: 2.4975\n",
      "Epoch: 229/2000... Training loss: 2.3045\n",
      "Epoch: 229/2000... Training loss: 2.4328\n",
      "Epoch: 229/2000... Training loss: 2.1086\n",
      "Epoch: 229/2000... Training loss: 2.4294\n",
      "Epoch: 229/2000... Training loss: 2.4839\n",
      "Epoch: 229/2000... Training loss: 2.5156\n",
      "Epoch: 229/2000... Training loss: 2.5433\n",
      "Epoch: 229/2000... Training loss: 2.1689\n",
      "Epoch: 229/2000... Training loss: 2.4472\n",
      "Epoch: 229/2000... Training loss: 2.3158\n",
      "Epoch: 229/2000... Training loss: 2.2442\n",
      "Epoch: 229/2000... Training loss: 2.3362\n",
      "Epoch: 229/2000... Training loss: 2.3752\n",
      "Epoch: 229/2000... Training loss: 2.5436\n",
      "Epoch: 229/2000... Training loss: 2.4596\n",
      "Epoch: 229/2000... Training loss: 2.2320\n",
      "Epoch: 229/2000... Training loss: 2.2983\n",
      "Epoch: 229/2000... Training loss: 2.3583\n",
      "Epoch: 229/2000... Training loss: 2.0893\n",
      "Epoch: 229/2000... Training loss: 2.3455\n",
      "Epoch: 229/2000... Training loss: 2.5122\n",
      "Epoch: 230/2000... Training loss: 2.1845\n",
      "Epoch: 230/2000... Training loss: 2.4000\n",
      "Epoch: 230/2000... Training loss: 2.2689\n",
      "Epoch: 230/2000... Training loss: 2.5775\n",
      "Epoch: 230/2000... Training loss: 2.1920\n",
      "Epoch: 230/2000... Training loss: 2.3830\n",
      "Epoch: 230/2000... Training loss: 2.5280\n",
      "Epoch: 230/2000... Training loss: 2.5281\n",
      "Epoch: 230/2000... Training loss: 2.4303\n",
      "Epoch: 230/2000... Training loss: 2.5143\n",
      "Epoch: 230/2000... Training loss: 2.5197\n",
      "Epoch: 230/2000... Training loss: 2.1659\n",
      "Epoch: 230/2000... Training loss: 2.4941\n",
      "Epoch: 230/2000... Training loss: 2.3610\n",
      "Epoch: 230/2000... Training loss: 2.3152\n",
      "Epoch: 230/2000... Training loss: 2.5815\n",
      "Epoch: 230/2000... Training loss: 2.4235\n",
      "Epoch: 230/2000... Training loss: 2.4656\n",
      "Epoch: 230/2000... Training loss: 2.3878\n",
      "Epoch: 230/2000... Training loss: 2.6760\n",
      "Epoch: 230/2000... Training loss: 2.4913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 230/2000... Training loss: 2.5559\n",
      "Epoch: 230/2000... Training loss: 2.5388\n",
      "Epoch: 230/2000... Training loss: 2.4675\n",
      "Epoch: 230/2000... Training loss: 2.7471\n",
      "Epoch: 230/2000... Training loss: 2.2338\n",
      "Epoch: 230/2000... Training loss: 2.5797\n",
      "Epoch: 230/2000... Training loss: 2.5093\n",
      "Epoch: 230/2000... Training loss: 2.2529\n",
      "Epoch: 230/2000... Training loss: 2.4005\n",
      "Epoch: 230/2000... Training loss: 2.6155\n",
      "Epoch: 231/2000... Training loss: 2.3905\n",
      "Epoch: 231/2000... Training loss: 2.3555\n",
      "Epoch: 231/2000... Training loss: 2.2239\n",
      "Epoch: 231/2000... Training loss: 2.3503\n",
      "Epoch: 231/2000... Training loss: 2.3572\n",
      "Epoch: 231/2000... Training loss: 2.3728\n",
      "Epoch: 231/2000... Training loss: 2.3475\n",
      "Epoch: 231/2000... Training loss: 2.3226\n",
      "Epoch: 231/2000... Training loss: 2.2937\n",
      "Epoch: 231/2000... Training loss: 2.5665\n",
      "Epoch: 231/2000... Training loss: 2.7137\n",
      "Epoch: 231/2000... Training loss: 2.5905\n",
      "Epoch: 231/2000... Training loss: 2.3605\n",
      "Epoch: 231/2000... Training loss: 2.5464\n",
      "Epoch: 231/2000... Training loss: 2.3044\n",
      "Epoch: 231/2000... Training loss: 2.5662\n",
      "Epoch: 231/2000... Training loss: 2.7782\n",
      "Epoch: 231/2000... Training loss: 2.3994\n",
      "Epoch: 231/2000... Training loss: 2.5521\n",
      "Epoch: 231/2000... Training loss: 2.5200\n",
      "Epoch: 231/2000... Training loss: 2.6549\n",
      "Epoch: 231/2000... Training loss: 2.1353\n",
      "Epoch: 231/2000... Training loss: 2.2980\n",
      "Epoch: 231/2000... Training loss: 2.4072\n",
      "Epoch: 231/2000... Training loss: 2.6299\n",
      "Epoch: 231/2000... Training loss: 2.3033\n",
      "Epoch: 231/2000... Training loss: 2.4745\n",
      "Epoch: 231/2000... Training loss: 2.3198\n",
      "Epoch: 231/2000... Training loss: 2.3750\n",
      "Epoch: 231/2000... Training loss: 2.5034\n",
      "Epoch: 231/2000... Training loss: 2.2014\n",
      "Epoch: 232/2000... Training loss: 2.3235\n",
      "Epoch: 232/2000... Training loss: 2.3122\n",
      "Epoch: 232/2000... Training loss: 2.0518\n",
      "Epoch: 232/2000... Training loss: 2.3069\n",
      "Epoch: 232/2000... Training loss: 2.4745\n",
      "Epoch: 232/2000... Training loss: 2.4672\n",
      "Epoch: 232/2000... Training loss: 2.3321\n",
      "Epoch: 232/2000... Training loss: 2.4927\n",
      "Epoch: 232/2000... Training loss: 2.1699\n",
      "Epoch: 232/2000... Training loss: 2.4830\n",
      "Epoch: 232/2000... Training loss: 2.3478\n",
      "Epoch: 232/2000... Training loss: 2.3018\n",
      "Epoch: 232/2000... Training loss: 2.6436\n",
      "Epoch: 232/2000... Training loss: 2.5806\n",
      "Epoch: 232/2000... Training loss: 2.1980\n",
      "Epoch: 232/2000... Training loss: 2.6115\n",
      "Epoch: 232/2000... Training loss: 2.4173\n",
      "Epoch: 232/2000... Training loss: 2.5649\n",
      "Epoch: 232/2000... Training loss: 2.2621\n",
      "Epoch: 232/2000... Training loss: 2.5849\n",
      "Epoch: 232/2000... Training loss: 2.5119\n",
      "Epoch: 232/2000... Training loss: 2.4499\n",
      "Epoch: 232/2000... Training loss: 2.4312\n",
      "Epoch: 232/2000... Training loss: 2.2916\n",
      "Epoch: 232/2000... Training loss: 2.3813\n",
      "Epoch: 232/2000... Training loss: 2.5976\n",
      "Epoch: 232/2000... Training loss: 2.4578\n",
      "Epoch: 232/2000... Training loss: 2.5532\n",
      "Epoch: 232/2000... Training loss: 2.5107\n",
      "Epoch: 232/2000... Training loss: 2.3232\n",
      "Epoch: 232/2000... Training loss: 2.6357\n",
      "Epoch: 233/2000... Training loss: 2.4432\n",
      "Epoch: 233/2000... Training loss: 2.3826\n",
      "Epoch: 233/2000... Training loss: 2.4135\n",
      "Epoch: 233/2000... Training loss: 2.3389\n",
      "Epoch: 233/2000... Training loss: 2.5180\n",
      "Epoch: 233/2000... Training loss: 2.3970\n",
      "Epoch: 233/2000... Training loss: 2.1412\n",
      "Epoch: 233/2000... Training loss: 2.5020\n",
      "Epoch: 233/2000... Training loss: 2.4705\n",
      "Epoch: 233/2000... Training loss: 2.3020\n",
      "Epoch: 233/2000... Training loss: 2.3450\n",
      "Epoch: 233/2000... Training loss: 2.5769\n",
      "Epoch: 233/2000... Training loss: 2.2628\n",
      "Epoch: 233/2000... Training loss: 2.4267\n",
      "Epoch: 233/2000... Training loss: 2.2487\n",
      "Epoch: 233/2000... Training loss: 2.3137\n",
      "Epoch: 233/2000... Training loss: 2.1508\n",
      "Epoch: 233/2000... Training loss: 2.6623\n",
      "Epoch: 233/2000... Training loss: 2.5109\n",
      "Epoch: 233/2000... Training loss: 2.6493\n",
      "Epoch: 233/2000... Training loss: 2.4917\n",
      "Epoch: 233/2000... Training loss: 2.1544\n",
      "Epoch: 233/2000... Training loss: 2.3686\n",
      "Epoch: 233/2000... Training loss: 2.3736\n",
      "Epoch: 233/2000... Training loss: 2.6015\n",
      "Epoch: 233/2000... Training loss: 2.2703\n",
      "Epoch: 233/2000... Training loss: 2.4745\n",
      "Epoch: 233/2000... Training loss: 2.2478\n",
      "Epoch: 233/2000... Training loss: 2.3897\n",
      "Epoch: 233/2000... Training loss: 2.7038\n",
      "Epoch: 233/2000... Training loss: 2.3225\n",
      "Epoch: 234/2000... Training loss: 2.2253\n",
      "Epoch: 234/2000... Training loss: 2.3048\n",
      "Epoch: 234/2000... Training loss: 2.5514\n",
      "Epoch: 234/2000... Training loss: 2.6276\n",
      "Epoch: 234/2000... Training loss: 2.3374\n",
      "Epoch: 234/2000... Training loss: 2.4869\n",
      "Epoch: 234/2000... Training loss: 2.4181\n",
      "Epoch: 234/2000... Training loss: 2.2124\n",
      "Epoch: 234/2000... Training loss: 2.4637\n",
      "Epoch: 234/2000... Training loss: 2.3049\n",
      "Epoch: 234/2000... Training loss: 2.5284\n",
      "Epoch: 234/2000... Training loss: 2.2329\n",
      "Epoch: 234/2000... Training loss: 2.1156\n",
      "Epoch: 234/2000... Training loss: 2.4499\n",
      "Epoch: 234/2000... Training loss: 2.5140\n",
      "Epoch: 234/2000... Training loss: 2.4769\n",
      "Epoch: 234/2000... Training loss: 2.5841\n",
      "Epoch: 234/2000... Training loss: 2.4296\n",
      "Epoch: 234/2000... Training loss: 2.2869\n",
      "Epoch: 234/2000... Training loss: 2.4555\n",
      "Epoch: 234/2000... Training loss: 2.2568\n",
      "Epoch: 234/2000... Training loss: 2.2966\n",
      "Epoch: 234/2000... Training loss: 2.4088\n",
      "Epoch: 234/2000... Training loss: 2.3857\n",
      "Epoch: 234/2000... Training loss: 2.5468\n",
      "Epoch: 234/2000... Training loss: 2.3987\n",
      "Epoch: 234/2000... Training loss: 2.2827\n",
      "Epoch: 234/2000... Training loss: 2.3822\n",
      "Epoch: 234/2000... Training loss: 2.5274\n",
      "Epoch: 234/2000... Training loss: 2.4848\n",
      "Epoch: 234/2000... Training loss: 2.4228\n",
      "Epoch: 235/2000... Training loss: 2.5289\n",
      "Epoch: 235/2000... Training loss: 2.3213\n",
      "Epoch: 235/2000... Training loss: 2.4681\n",
      "Epoch: 235/2000... Training loss: 2.1988\n",
      "Epoch: 235/2000... Training loss: 2.2592\n",
      "Epoch: 235/2000... Training loss: 2.4193\n",
      "Epoch: 235/2000... Training loss: 2.5594\n",
      "Epoch: 235/2000... Training loss: 2.2289\n",
      "Epoch: 235/2000... Training loss: 2.3274\n",
      "Epoch: 235/2000... Training loss: 2.6857\n",
      "Epoch: 235/2000... Training loss: 2.4069\n",
      "Epoch: 235/2000... Training loss: 2.7504\n",
      "Epoch: 235/2000... Training loss: 2.3646\n",
      "Epoch: 235/2000... Training loss: 2.2994\n",
      "Epoch: 235/2000... Training loss: 2.2867\n",
      "Epoch: 235/2000... Training loss: 2.3242\n",
      "Epoch: 235/2000... Training loss: 2.2717\n",
      "Epoch: 235/2000... Training loss: 2.3793\n",
      "Epoch: 235/2000... Training loss: 2.3140\n",
      "Epoch: 235/2000... Training loss: 2.6979\n",
      "Epoch: 235/2000... Training loss: 2.5999\n",
      "Epoch: 235/2000... Training loss: 2.1847\n",
      "Epoch: 235/2000... Training loss: 2.1785\n",
      "Epoch: 235/2000... Training loss: 2.3144\n",
      "Epoch: 235/2000... Training loss: 2.2190\n",
      "Epoch: 235/2000... Training loss: 2.4149\n",
      "Epoch: 235/2000... Training loss: 2.3004\n",
      "Epoch: 235/2000... Training loss: 2.6248\n",
      "Epoch: 235/2000... Training loss: 2.3577\n",
      "Epoch: 235/2000... Training loss: 2.3806\n",
      "Epoch: 235/2000... Training loss: 2.6072\n",
      "Epoch: 236/2000... Training loss: 2.4437\n",
      "Epoch: 236/2000... Training loss: 2.5132\n",
      "Epoch: 236/2000... Training loss: 2.3407\n",
      "Epoch: 236/2000... Training loss: 2.5580\n",
      "Epoch: 236/2000... Training loss: 2.3505\n",
      "Epoch: 236/2000... Training loss: 2.1704\n",
      "Epoch: 236/2000... Training loss: 2.5091\n",
      "Epoch: 236/2000... Training loss: 2.1459\n",
      "Epoch: 236/2000... Training loss: 2.4982\n",
      "Epoch: 236/2000... Training loss: 2.2990\n",
      "Epoch: 236/2000... Training loss: 2.2328\n",
      "Epoch: 236/2000... Training loss: 2.2769\n",
      "Epoch: 236/2000... Training loss: 2.0793\n",
      "Epoch: 236/2000... Training loss: 2.3203\n",
      "Epoch: 236/2000... Training loss: 2.3977\n",
      "Epoch: 236/2000... Training loss: 2.2158\n",
      "Epoch: 236/2000... Training loss: 2.5772\n",
      "Epoch: 236/2000... Training loss: 2.3546\n",
      "Epoch: 236/2000... Training loss: 2.2854\n",
      "Epoch: 236/2000... Training loss: 2.5653\n",
      "Epoch: 236/2000... Training loss: 2.3184\n",
      "Epoch: 236/2000... Training loss: 2.4104\n",
      "Epoch: 236/2000... Training loss: 2.5247\n",
      "Epoch: 236/2000... Training loss: 2.3990\n",
      "Epoch: 236/2000... Training loss: 2.5803\n",
      "Epoch: 236/2000... Training loss: 2.4507\n",
      "Epoch: 236/2000... Training loss: 2.3334\n",
      "Epoch: 236/2000... Training loss: 2.3769\n",
      "Epoch: 236/2000... Training loss: 2.1559\n",
      "Epoch: 236/2000... Training loss: 2.1168\n",
      "Epoch: 236/2000... Training loss: 2.2849\n",
      "Epoch: 237/2000... Training loss: 2.1333\n",
      "Epoch: 237/2000... Training loss: 2.1951\n",
      "Epoch: 237/2000... Training loss: 2.5519\n",
      "Epoch: 237/2000... Training loss: 2.6522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 237/2000... Training loss: 2.1904\n",
      "Epoch: 237/2000... Training loss: 2.7121\n",
      "Epoch: 237/2000... Training loss: 2.5655\n",
      "Epoch: 237/2000... Training loss: 2.2118\n",
      "Epoch: 237/2000... Training loss: 2.5552\n",
      "Epoch: 237/2000... Training loss: 2.3198\n",
      "Epoch: 237/2000... Training loss: 2.1416\n",
      "Epoch: 237/2000... Training loss: 2.3521\n",
      "Epoch: 237/2000... Training loss: 2.3551\n",
      "Epoch: 237/2000... Training loss: 2.5874\n",
      "Epoch: 237/2000... Training loss: 2.2592\n",
      "Epoch: 237/2000... Training loss: 2.3908\n",
      "Epoch: 237/2000... Training loss: 2.3932\n",
      "Epoch: 237/2000... Training loss: 2.3097\n",
      "Epoch: 237/2000... Training loss: 2.4077\n",
      "Epoch: 237/2000... Training loss: 2.4881\n",
      "Epoch: 237/2000... Training loss: 2.2435\n",
      "Epoch: 237/2000... Training loss: 2.2598\n",
      "Epoch: 237/2000... Training loss: 2.4994\n",
      "Epoch: 237/2000... Training loss: 2.6329\n",
      "Epoch: 237/2000... Training loss: 2.3417\n",
      "Epoch: 237/2000... Training loss: 2.5114\n",
      "Epoch: 237/2000... Training loss: 2.3644\n",
      "Epoch: 237/2000... Training loss: 2.4465\n",
      "Epoch: 237/2000... Training loss: 2.1726\n",
      "Epoch: 237/2000... Training loss: 2.2396\n",
      "Epoch: 237/2000... Training loss: 2.2378\n",
      "Epoch: 238/2000... Training loss: 2.1297\n",
      "Epoch: 238/2000... Training loss: 2.2336\n",
      "Epoch: 238/2000... Training loss: 2.2342\n",
      "Epoch: 238/2000... Training loss: 2.4124\n",
      "Epoch: 238/2000... Training loss: 2.4505\n",
      "Epoch: 238/2000... Training loss: 2.1771\n",
      "Epoch: 238/2000... Training loss: 2.3182\n",
      "Epoch: 238/2000... Training loss: 2.0828\n",
      "Epoch: 238/2000... Training loss: 2.2594\n",
      "Epoch: 238/2000... Training loss: 2.3603\n",
      "Epoch: 238/2000... Training loss: 2.3599\n",
      "Epoch: 238/2000... Training loss: 2.3725\n",
      "Epoch: 238/2000... Training loss: 2.2451\n",
      "Epoch: 238/2000... Training loss: 2.3519\n",
      "Epoch: 238/2000... Training loss: 2.3413\n",
      "Epoch: 238/2000... Training loss: 2.3349\n",
      "Epoch: 238/2000... Training loss: 2.4164\n",
      "Epoch: 238/2000... Training loss: 2.5051\n",
      "Epoch: 238/2000... Training loss: 2.4474\n",
      "Epoch: 238/2000... Training loss: 2.4393\n",
      "Epoch: 238/2000... Training loss: 2.2281\n",
      "Epoch: 238/2000... Training loss: 2.4159\n",
      "Epoch: 238/2000... Training loss: 2.3822\n",
      "Epoch: 238/2000... Training loss: 2.1261\n",
      "Epoch: 238/2000... Training loss: 2.5814\n",
      "Epoch: 238/2000... Training loss: 2.2824\n",
      "Epoch: 238/2000... Training loss: 2.3993\n",
      "Epoch: 238/2000... Training loss: 2.4158\n",
      "Epoch: 238/2000... Training loss: 2.3885\n",
      "Epoch: 238/2000... Training loss: 2.2270\n",
      "Epoch: 238/2000... Training loss: 2.3052\n",
      "Epoch: 239/2000... Training loss: 2.2886\n",
      "Epoch: 239/2000... Training loss: 2.3787\n",
      "Epoch: 239/2000... Training loss: 2.3143\n",
      "Epoch: 239/2000... Training loss: 2.4838\n",
      "Epoch: 239/2000... Training loss: 2.4655\n",
      "Epoch: 239/2000... Training loss: 2.2750\n",
      "Epoch: 239/2000... Training loss: 2.2416\n",
      "Epoch: 239/2000... Training loss: 2.3265\n",
      "Epoch: 239/2000... Training loss: 2.2825\n",
      "Epoch: 239/2000... Training loss: 2.2794\n",
      "Epoch: 239/2000... Training loss: 2.3587\n",
      "Epoch: 239/2000... Training loss: 2.2463\n",
      "Epoch: 239/2000... Training loss: 2.4021\n",
      "Epoch: 239/2000... Training loss: 2.3153\n",
      "Epoch: 239/2000... Training loss: 2.3924\n",
      "Epoch: 239/2000... Training loss: 2.4963\n",
      "Epoch: 239/2000... Training loss: 2.3504\n",
      "Epoch: 239/2000... Training loss: 2.4632\n",
      "Epoch: 239/2000... Training loss: 2.3902\n",
      "Epoch: 239/2000... Training loss: 2.1504\n",
      "Epoch: 239/2000... Training loss: 2.1991\n",
      "Epoch: 239/2000... Training loss: 2.5667\n",
      "Epoch: 239/2000... Training loss: 2.3161\n",
      "Epoch: 239/2000... Training loss: 2.1680\n",
      "Epoch: 239/2000... Training loss: 2.3554\n",
      "Epoch: 239/2000... Training loss: 2.2034\n",
      "Epoch: 239/2000... Training loss: 2.4609\n",
      "Epoch: 239/2000... Training loss: 2.4825\n",
      "Epoch: 239/2000... Training loss: 2.2955\n",
      "Epoch: 239/2000... Training loss: 2.4738\n",
      "Epoch: 239/2000... Training loss: 2.3291\n",
      "Epoch: 240/2000... Training loss: 2.2546\n",
      "Epoch: 240/2000... Training loss: 2.6421\n",
      "Epoch: 240/2000... Training loss: 2.4972\n",
      "Epoch: 240/2000... Training loss: 2.2748\n",
      "Epoch: 240/2000... Training loss: 2.3100\n",
      "Epoch: 240/2000... Training loss: 2.2535\n",
      "Epoch: 240/2000... Training loss: 2.5588\n",
      "Epoch: 240/2000... Training loss: 2.3998\n",
      "Epoch: 240/2000... Training loss: 2.0492\n",
      "Epoch: 240/2000... Training loss: 2.4458\n",
      "Epoch: 240/2000... Training loss: 2.2866\n",
      "Epoch: 240/2000... Training loss: 2.2313\n",
      "Epoch: 240/2000... Training loss: 2.5103\n",
      "Epoch: 240/2000... Training loss: 2.2580\n",
      "Epoch: 240/2000... Training loss: 2.6043\n",
      "Epoch: 240/2000... Training loss: 2.2637\n",
      "Epoch: 240/2000... Training loss: 2.3888\n",
      "Epoch: 240/2000... Training loss: 2.1348\n",
      "Epoch: 240/2000... Training loss: 2.0485\n",
      "Epoch: 240/2000... Training loss: 2.2615\n",
      "Epoch: 240/2000... Training loss: 2.4508\n",
      "Epoch: 240/2000... Training loss: 2.4000\n",
      "Epoch: 240/2000... Training loss: 2.2569\n",
      "Epoch: 240/2000... Training loss: 2.4397\n",
      "Epoch: 240/2000... Training loss: 2.3417\n",
      "Epoch: 240/2000... Training loss: 2.3108\n",
      "Epoch: 240/2000... Training loss: 2.3592\n",
      "Epoch: 240/2000... Training loss: 2.4168\n",
      "Epoch: 240/2000... Training loss: 2.3760\n",
      "Epoch: 240/2000... Training loss: 2.2754\n",
      "Epoch: 240/2000... Training loss: 2.4537\n",
      "Epoch: 241/2000... Training loss: 2.4398\n",
      "Epoch: 241/2000... Training loss: 2.4825\n",
      "Epoch: 241/2000... Training loss: 2.2068\n",
      "Epoch: 241/2000... Training loss: 2.1225\n",
      "Epoch: 241/2000... Training loss: 2.6839\n",
      "Epoch: 241/2000... Training loss: 2.2622\n",
      "Epoch: 241/2000... Training loss: 2.3981\n",
      "Epoch: 241/2000... Training loss: 2.5994\n",
      "Epoch: 241/2000... Training loss: 2.3517\n",
      "Epoch: 241/2000... Training loss: 2.2819\n",
      "Epoch: 241/2000... Training loss: 2.2229\n",
      "Epoch: 241/2000... Training loss: 2.3990\n",
      "Epoch: 241/2000... Training loss: 2.5066\n",
      "Epoch: 241/2000... Training loss: 2.3412\n",
      "Epoch: 241/2000... Training loss: 2.4383\n",
      "Epoch: 241/2000... Training loss: 2.5300\n",
      "Epoch: 241/2000... Training loss: 2.3833\n",
      "Epoch: 241/2000... Training loss: 2.3198\n",
      "Epoch: 241/2000... Training loss: 2.4018\n",
      "Epoch: 241/2000... Training loss: 2.4143\n",
      "Epoch: 241/2000... Training loss: 2.5373\n",
      "Epoch: 241/2000... Training loss: 2.4074\n",
      "Epoch: 241/2000... Training loss: 2.2210\n",
      "Epoch: 241/2000... Training loss: 2.4913\n",
      "Epoch: 241/2000... Training loss: 2.4582\n",
      "Epoch: 241/2000... Training loss: 2.2926\n",
      "Epoch: 241/2000... Training loss: 2.4499\n",
      "Epoch: 241/2000... Training loss: 2.3446\n",
      "Epoch: 241/2000... Training loss: 2.4533\n",
      "Epoch: 241/2000... Training loss: 2.4581\n",
      "Epoch: 241/2000... Training loss: 2.1580\n",
      "Epoch: 242/2000... Training loss: 2.2195\n",
      "Epoch: 242/2000... Training loss: 2.3856\n",
      "Epoch: 242/2000... Training loss: 2.4641\n",
      "Epoch: 242/2000... Training loss: 2.1176\n",
      "Epoch: 242/2000... Training loss: 2.3495\n",
      "Epoch: 242/2000... Training loss: 2.4929\n",
      "Epoch: 242/2000... Training loss: 2.3585\n",
      "Epoch: 242/2000... Training loss: 2.3374\n",
      "Epoch: 242/2000... Training loss: 2.2357\n",
      "Epoch: 242/2000... Training loss: 2.3804\n",
      "Epoch: 242/2000... Training loss: 2.3580\n",
      "Epoch: 242/2000... Training loss: 2.1932\n",
      "Epoch: 242/2000... Training loss: 2.5570\n",
      "Epoch: 242/2000... Training loss: 2.5065\n",
      "Epoch: 242/2000... Training loss: 2.1226\n",
      "Epoch: 242/2000... Training loss: 2.3201\n",
      "Epoch: 242/2000... Training loss: 2.2210\n",
      "Epoch: 242/2000... Training loss: 2.4649\n",
      "Epoch: 242/2000... Training loss: 2.4041\n",
      "Epoch: 242/2000... Training loss: 2.1402\n",
      "Epoch: 242/2000... Training loss: 2.2592\n",
      "Epoch: 242/2000... Training loss: 2.4036\n",
      "Epoch: 242/2000... Training loss: 2.2124\n",
      "Epoch: 242/2000... Training loss: 2.4929\n",
      "Epoch: 242/2000... Training loss: 2.3585\n",
      "Epoch: 242/2000... Training loss: 2.6522\n",
      "Epoch: 242/2000... Training loss: 2.0614\n",
      "Epoch: 242/2000... Training loss: 2.2373\n",
      "Epoch: 242/2000... Training loss: 2.3448\n",
      "Epoch: 242/2000... Training loss: 2.3866\n",
      "Epoch: 242/2000... Training loss: 2.1873\n",
      "Epoch: 243/2000... Training loss: 2.2889\n",
      "Epoch: 243/2000... Training loss: 2.2817\n",
      "Epoch: 243/2000... Training loss: 2.1841\n",
      "Epoch: 243/2000... Training loss: 2.2526\n",
      "Epoch: 243/2000... Training loss: 2.2087\n",
      "Epoch: 243/2000... Training loss: 2.2732\n",
      "Epoch: 243/2000... Training loss: 2.1551\n",
      "Epoch: 243/2000... Training loss: 2.3272\n",
      "Epoch: 243/2000... Training loss: 2.4443\n",
      "Epoch: 243/2000... Training loss: 2.4345\n",
      "Epoch: 243/2000... Training loss: 2.4053\n",
      "Epoch: 243/2000... Training loss: 2.3754\n",
      "Epoch: 243/2000... Training loss: 2.0178\n",
      "Epoch: 243/2000... Training loss: 2.0711\n",
      "Epoch: 243/2000... Training loss: 2.2354\n",
      "Epoch: 243/2000... Training loss: 2.3305\n",
      "Epoch: 243/2000... Training loss: 2.1775\n",
      "Epoch: 243/2000... Training loss: 2.3022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243/2000... Training loss: 2.0330\n",
      "Epoch: 243/2000... Training loss: 2.2645\n",
      "Epoch: 243/2000... Training loss: 2.4122\n",
      "Epoch: 243/2000... Training loss: 2.1999\n",
      "Epoch: 243/2000... Training loss: 2.5367\n",
      "Epoch: 243/2000... Training loss: 2.4538\n",
      "Epoch: 243/2000... Training loss: 2.4296\n",
      "Epoch: 243/2000... Training loss: 2.3142\n",
      "Epoch: 243/2000... Training loss: 2.5026\n",
      "Epoch: 243/2000... Training loss: 2.3072\n",
      "Epoch: 243/2000... Training loss: 2.3745\n",
      "Epoch: 243/2000... Training loss: 2.2948\n",
      "Epoch: 243/2000... Training loss: 2.3478\n",
      "Epoch: 244/2000... Training loss: 2.3145\n",
      "Epoch: 244/2000... Training loss: 2.2949\n",
      "Epoch: 244/2000... Training loss: 2.2201\n",
      "Epoch: 244/2000... Training loss: 2.2749\n",
      "Epoch: 244/2000... Training loss: 2.3480\n",
      "Epoch: 244/2000... Training loss: 2.3657\n",
      "Epoch: 244/2000... Training loss: 2.2299\n",
      "Epoch: 244/2000... Training loss: 2.2910\n",
      "Epoch: 244/2000... Training loss: 2.1469\n",
      "Epoch: 244/2000... Training loss: 2.4168\n",
      "Epoch: 244/2000... Training loss: 2.4559\n",
      "Epoch: 244/2000... Training loss: 2.1976\n",
      "Epoch: 244/2000... Training loss: 2.3567\n",
      "Epoch: 244/2000... Training loss: 2.5320\n",
      "Epoch: 244/2000... Training loss: 2.3728\n",
      "Epoch: 244/2000... Training loss: 2.1851\n",
      "Epoch: 244/2000... Training loss: 2.3873\n",
      "Epoch: 244/2000... Training loss: 2.2661\n",
      "Epoch: 244/2000... Training loss: 2.0877\n",
      "Epoch: 244/2000... Training loss: 2.2962\n",
      "Epoch: 244/2000... Training loss: 2.1971\n",
      "Epoch: 244/2000... Training loss: 2.0730\n",
      "Epoch: 244/2000... Training loss: 2.5283\n",
      "Epoch: 244/2000... Training loss: 2.3381\n",
      "Epoch: 244/2000... Training loss: 2.3093\n",
      "Epoch: 244/2000... Training loss: 2.3924\n",
      "Epoch: 244/2000... Training loss: 2.3903\n",
      "Epoch: 244/2000... Training loss: 2.6489\n",
      "Epoch: 244/2000... Training loss: 2.4392\n",
      "Epoch: 244/2000... Training loss: 2.3490\n",
      "Epoch: 244/2000... Training loss: 2.3256\n",
      "Epoch: 245/2000... Training loss: 2.0794\n",
      "Epoch: 245/2000... Training loss: 2.0370\n",
      "Epoch: 245/2000... Training loss: 2.0731\n",
      "Epoch: 245/2000... Training loss: 2.3905\n",
      "Epoch: 245/2000... Training loss: 2.3986\n",
      "Epoch: 245/2000... Training loss: 2.6489\n",
      "Epoch: 245/2000... Training loss: 2.3739\n",
      "Epoch: 245/2000... Training loss: 2.1915\n",
      "Epoch: 245/2000... Training loss: 2.3618\n",
      "Epoch: 245/2000... Training loss: 2.2911\n",
      "Epoch: 245/2000... Training loss: 2.4103\n",
      "Epoch: 245/2000... Training loss: 2.0877\n",
      "Epoch: 245/2000... Training loss: 2.1548\n",
      "Epoch: 245/2000... Training loss: 2.6687\n",
      "Epoch: 245/2000... Training loss: 2.3385\n",
      "Epoch: 245/2000... Training loss: 2.2499\n",
      "Epoch: 245/2000... Training loss: 2.4915\n",
      "Epoch: 245/2000... Training loss: 2.2460\n",
      "Epoch: 245/2000... Training loss: 2.4437\n",
      "Epoch: 245/2000... Training loss: 2.6703\n",
      "Epoch: 245/2000... Training loss: 2.2779\n",
      "Epoch: 245/2000... Training loss: 2.3471\n",
      "Epoch: 245/2000... Training loss: 2.4174\n",
      "Epoch: 245/2000... Training loss: 2.4429\n",
      "Epoch: 245/2000... Training loss: 2.4052\n",
      "Epoch: 245/2000... Training loss: 2.4383\n",
      "Epoch: 245/2000... Training loss: 2.1949\n",
      "Epoch: 245/2000... Training loss: 2.4929\n",
      "Epoch: 245/2000... Training loss: 2.3140\n",
      "Epoch: 245/2000... Training loss: 2.3619\n",
      "Epoch: 245/2000... Training loss: 2.5151\n",
      "Epoch: 246/2000... Training loss: 2.2128\n",
      "Epoch: 246/2000... Training loss: 2.2191\n",
      "Epoch: 246/2000... Training loss: 2.2808\n",
      "Epoch: 246/2000... Training loss: 2.2790\n",
      "Epoch: 246/2000... Training loss: 2.0156\n",
      "Epoch: 246/2000... Training loss: 2.3180\n",
      "Epoch: 246/2000... Training loss: 2.1424\n",
      "Epoch: 246/2000... Training loss: 2.0978\n",
      "Epoch: 246/2000... Training loss: 2.3102\n",
      "Epoch: 246/2000... Training loss: 2.4223\n",
      "Epoch: 246/2000... Training loss: 2.1639\n",
      "Epoch: 246/2000... Training loss: 2.3933\n",
      "Epoch: 246/2000... Training loss: 2.5269\n",
      "Epoch: 246/2000... Training loss: 2.2428\n",
      "Epoch: 246/2000... Training loss: 2.1167\n",
      "Epoch: 246/2000... Training loss: 2.2324\n",
      "Epoch: 246/2000... Training loss: 2.1412\n",
      "Epoch: 246/2000... Training loss: 2.5303\n",
      "Epoch: 246/2000... Training loss: 2.3465\n",
      "Epoch: 246/2000... Training loss: 2.4338\n",
      "Epoch: 246/2000... Training loss: 2.3896\n",
      "Epoch: 246/2000... Training loss: 2.2081\n",
      "Epoch: 246/2000... Training loss: 2.3922\n",
      "Epoch: 246/2000... Training loss: 2.1435\n",
      "Epoch: 246/2000... Training loss: 2.4178\n",
      "Epoch: 246/2000... Training loss: 2.2805\n",
      "Epoch: 246/2000... Training loss: 2.5199\n",
      "Epoch: 246/2000... Training loss: 2.4131\n",
      "Epoch: 246/2000... Training loss: 2.3281\n",
      "Epoch: 246/2000... Training loss: 2.3433\n",
      "Epoch: 246/2000... Training loss: 2.2694\n",
      "Epoch: 247/2000... Training loss: 2.4698\n",
      "Epoch: 247/2000... Training loss: 2.0251\n",
      "Epoch: 247/2000... Training loss: 2.4066\n",
      "Epoch: 247/2000... Training loss: 2.3460\n",
      "Epoch: 247/2000... Training loss: 2.3940\n",
      "Epoch: 247/2000... Training loss: 2.3081\n",
      "Epoch: 247/2000... Training loss: 2.4759\n",
      "Epoch: 247/2000... Training loss: 2.2354\n",
      "Epoch: 247/2000... Training loss: 2.2324\n",
      "Epoch: 247/2000... Training loss: 2.3255\n",
      "Epoch: 247/2000... Training loss: 2.2721\n",
      "Epoch: 247/2000... Training loss: 2.4606\n",
      "Epoch: 247/2000... Training loss: 2.4952\n",
      "Epoch: 247/2000... Training loss: 2.4814\n",
      "Epoch: 247/2000... Training loss: 2.5772\n",
      "Epoch: 247/2000... Training loss: 2.3064\n",
      "Epoch: 247/2000... Training loss: 2.2826\n",
      "Epoch: 247/2000... Training loss: 2.5681\n",
      "Epoch: 247/2000... Training loss: 2.2421\n",
      "Epoch: 247/2000... Training loss: 2.0903\n",
      "Epoch: 247/2000... Training loss: 2.3005\n",
      "Epoch: 247/2000... Training loss: 2.5343\n",
      "Epoch: 247/2000... Training loss: 2.2991\n",
      "Epoch: 247/2000... Training loss: 2.3389\n",
      "Epoch: 247/2000... Training loss: 2.4676\n",
      "Epoch: 247/2000... Training loss: 2.3323\n",
      "Epoch: 247/2000... Training loss: 2.4607\n",
      "Epoch: 247/2000... Training loss: 2.3462\n",
      "Epoch: 247/2000... Training loss: 2.2650\n",
      "Epoch: 247/2000... Training loss: 2.3006\n",
      "Epoch: 247/2000... Training loss: 2.3689\n",
      "Epoch: 248/2000... Training loss: 2.3323\n",
      "Epoch: 248/2000... Training loss: 2.4797\n",
      "Epoch: 248/2000... Training loss: 2.3945\n",
      "Epoch: 248/2000... Training loss: 2.2955\n",
      "Epoch: 248/2000... Training loss: 2.1574\n",
      "Epoch: 248/2000... Training loss: 2.3071\n",
      "Epoch: 248/2000... Training loss: 2.0586\n",
      "Epoch: 248/2000... Training loss: 2.3886\n",
      "Epoch: 248/2000... Training loss: 2.3246\n",
      "Epoch: 248/2000... Training loss: 2.4332\n",
      "Epoch: 248/2000... Training loss: 2.4860\n",
      "Epoch: 248/2000... Training loss: 2.1868\n",
      "Epoch: 248/2000... Training loss: 2.2901\n",
      "Epoch: 248/2000... Training loss: 2.4651\n",
      "Epoch: 248/2000... Training loss: 2.3159\n",
      "Epoch: 248/2000... Training loss: 2.0417\n",
      "Epoch: 248/2000... Training loss: 2.2491\n",
      "Epoch: 248/2000... Training loss: 2.5418\n",
      "Epoch: 248/2000... Training loss: 2.2057\n",
      "Epoch: 248/2000... Training loss: 2.3705\n",
      "Epoch: 248/2000... Training loss: 2.3334\n",
      "Epoch: 248/2000... Training loss: 2.3666\n",
      "Epoch: 248/2000... Training loss: 2.4606\n",
      "Epoch: 248/2000... Training loss: 2.3895\n",
      "Epoch: 248/2000... Training loss: 2.3030\n",
      "Epoch: 248/2000... Training loss: 2.3445\n",
      "Epoch: 248/2000... Training loss: 2.3103\n",
      "Epoch: 248/2000... Training loss: 2.5136\n",
      "Epoch: 248/2000... Training loss: 2.2652\n",
      "Epoch: 248/2000... Training loss: 2.1846\n",
      "Epoch: 248/2000... Training loss: 2.3639\n",
      "Epoch: 249/2000... Training loss: 2.2558\n",
      "Epoch: 249/2000... Training loss: 2.2025\n",
      "Epoch: 249/2000... Training loss: 2.5442\n",
      "Epoch: 249/2000... Training loss: 2.1402\n",
      "Epoch: 249/2000... Training loss: 2.1133\n",
      "Epoch: 249/2000... Training loss: 2.3490\n",
      "Epoch: 249/2000... Training loss: 2.3711\n",
      "Epoch: 249/2000... Training loss: 2.1243\n",
      "Epoch: 249/2000... Training loss: 2.2634\n",
      "Epoch: 249/2000... Training loss: 2.4842\n",
      "Epoch: 249/2000... Training loss: 2.1588\n",
      "Epoch: 249/2000... Training loss: 2.2811\n",
      "Epoch: 249/2000... Training loss: 2.2267\n",
      "Epoch: 249/2000... Training loss: 2.2356\n",
      "Epoch: 249/2000... Training loss: 2.2289\n",
      "Epoch: 249/2000... Training loss: 2.2078\n",
      "Epoch: 249/2000... Training loss: 2.3529\n",
      "Epoch: 249/2000... Training loss: 2.4479\n",
      "Epoch: 249/2000... Training loss: 2.2560\n",
      "Epoch: 249/2000... Training loss: 2.5164\n",
      "Epoch: 249/2000... Training loss: 2.5145\n",
      "Epoch: 249/2000... Training loss: 2.3397\n",
      "Epoch: 249/2000... Training loss: 2.2982\n",
      "Epoch: 249/2000... Training loss: 2.4588\n",
      "Epoch: 249/2000... Training loss: 2.5699\n",
      "Epoch: 249/2000... Training loss: 2.2688\n",
      "Epoch: 249/2000... Training loss: 2.1913\n",
      "Epoch: 249/2000... Training loss: 2.3020\n",
      "Epoch: 249/2000... Training loss: 2.3705\n",
      "Epoch: 249/2000... Training loss: 2.1896\n",
      "Epoch: 249/2000... Training loss: 2.5704\n",
      "Epoch: 250/2000... Training loss: 2.1760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250/2000... Training loss: 2.3833\n",
      "Epoch: 250/2000... Training loss: 2.3016\n",
      "Epoch: 250/2000... Training loss: 2.1563\n",
      "Epoch: 250/2000... Training loss: 2.1683\n",
      "Epoch: 250/2000... Training loss: 2.1161\n",
      "Epoch: 250/2000... Training loss: 2.0980\n",
      "Epoch: 250/2000... Training loss: 2.2718\n",
      "Epoch: 250/2000... Training loss: 2.3208\n",
      "Epoch: 250/2000... Training loss: 2.2852\n",
      "Epoch: 250/2000... Training loss: 2.4580\n",
      "Epoch: 250/2000... Training loss: 2.3358\n",
      "Epoch: 250/2000... Training loss: 2.1694\n",
      "Epoch: 250/2000... Training loss: 2.3427\n",
      "Epoch: 250/2000... Training loss: 2.2088\n",
      "Epoch: 250/2000... Training loss: 2.2121\n",
      "Epoch: 250/2000... Training loss: 2.4872\n",
      "Epoch: 250/2000... Training loss: 2.5727\n",
      "Epoch: 250/2000... Training loss: 2.2612\n",
      "Epoch: 250/2000... Training loss: 2.5353\n",
      "Epoch: 250/2000... Training loss: 2.4335\n",
      "Epoch: 250/2000... Training loss: 2.2817\n",
      "Epoch: 250/2000... Training loss: 2.3369\n",
      "Epoch: 250/2000... Training loss: 2.4779\n",
      "Epoch: 250/2000... Training loss: 2.2626\n",
      "Epoch: 250/2000... Training loss: 2.3034\n",
      "Epoch: 250/2000... Training loss: 2.2590\n",
      "Epoch: 250/2000... Training loss: 2.5308\n",
      "Epoch: 250/2000... Training loss: 2.2896\n",
      "Epoch: 250/2000... Training loss: 2.2867\n",
      "Epoch: 250/2000... Training loss: 2.2385\n",
      "Epoch: 251/2000... Training loss: 2.1018\n",
      "Epoch: 251/2000... Training loss: 2.1699\n",
      "Epoch: 251/2000... Training loss: 2.1808\n",
      "Epoch: 251/2000... Training loss: 2.5321\n",
      "Epoch: 251/2000... Training loss: 2.3327\n",
      "Epoch: 251/2000... Training loss: 2.1582\n",
      "Epoch: 251/2000... Training loss: 2.0252\n",
      "Epoch: 251/2000... Training loss: 2.1448\n",
      "Epoch: 251/2000... Training loss: 2.2804\n",
      "Epoch: 251/2000... Training loss: 2.5059\n",
      "Epoch: 251/2000... Training loss: 2.1433\n",
      "Epoch: 251/2000... Training loss: 2.1905\n",
      "Epoch: 251/2000... Training loss: 2.4159\n",
      "Epoch: 251/2000... Training loss: 2.3631\n",
      "Epoch: 251/2000... Training loss: 2.3484\n",
      "Epoch: 251/2000... Training loss: 2.3682\n",
      "Epoch: 251/2000... Training loss: 2.5270\n",
      "Epoch: 251/2000... Training loss: 2.4813\n",
      "Epoch: 251/2000... Training loss: 2.2236\n",
      "Epoch: 251/2000... Training loss: 2.3352\n",
      "Epoch: 251/2000... Training loss: 2.4525\n",
      "Epoch: 251/2000... Training loss: 2.3777\n",
      "Epoch: 251/2000... Training loss: 2.4754\n",
      "Epoch: 251/2000... Training loss: 2.4604\n",
      "Epoch: 251/2000... Training loss: 2.4296\n",
      "Epoch: 251/2000... Training loss: 2.4678\n",
      "Epoch: 251/2000... Training loss: 2.1712\n",
      "Epoch: 251/2000... Training loss: 2.3655\n",
      "Epoch: 251/2000... Training loss: 2.1978\n",
      "Epoch: 251/2000... Training loss: 2.1650\n",
      "Epoch: 251/2000... Training loss: 2.2710\n",
      "Epoch: 252/2000... Training loss: 2.0080\n",
      "Epoch: 252/2000... Training loss: 2.4557\n",
      "Epoch: 252/2000... Training loss: 2.4132\n",
      "Epoch: 252/2000... Training loss: 2.0661\n",
      "Epoch: 252/2000... Training loss: 2.1751\n",
      "Epoch: 252/2000... Training loss: 2.5293\n",
      "Epoch: 252/2000... Training loss: 2.2068\n",
      "Epoch: 252/2000... Training loss: 2.4939\n",
      "Epoch: 252/2000... Training loss: 2.4323\n",
      "Epoch: 252/2000... Training loss: 2.5581\n",
      "Epoch: 252/2000... Training loss: 2.2250\n",
      "Epoch: 252/2000... Training loss: 2.1149\n",
      "Epoch: 252/2000... Training loss: 2.3098\n",
      "Epoch: 252/2000... Training loss: 2.3665\n",
      "Epoch: 252/2000... Training loss: 2.2622\n",
      "Epoch: 252/2000... Training loss: 2.3695\n",
      "Epoch: 252/2000... Training loss: 2.3095\n",
      "Epoch: 252/2000... Training loss: 2.2332\n",
      "Epoch: 252/2000... Training loss: 2.3548\n",
      "Epoch: 252/2000... Training loss: 2.3797\n",
      "Epoch: 252/2000... Training loss: 2.2101\n",
      "Epoch: 252/2000... Training loss: 2.2776\n",
      "Epoch: 252/2000... Training loss: 2.3727\n",
      "Epoch: 252/2000... Training loss: 2.2545\n",
      "Epoch: 252/2000... Training loss: 2.3040\n",
      "Epoch: 252/2000... Training loss: 2.1880\n",
      "Epoch: 252/2000... Training loss: 2.2977\n",
      "Epoch: 252/2000... Training loss: 2.3344\n",
      "Epoch: 252/2000... Training loss: 2.1810\n",
      "Epoch: 252/2000... Training loss: 2.2993\n",
      "Epoch: 252/2000... Training loss: 2.5186\n",
      "Epoch: 253/2000... Training loss: 1.9764\n",
      "Epoch: 253/2000... Training loss: 2.2362\n",
      "Epoch: 253/2000... Training loss: 2.2637\n",
      "Epoch: 253/2000... Training loss: 2.1552\n",
      "Epoch: 253/2000... Training loss: 2.1258\n",
      "Epoch: 253/2000... Training loss: 2.2666\n",
      "Epoch: 253/2000... Training loss: 2.2459\n",
      "Epoch: 253/2000... Training loss: 2.2340\n",
      "Epoch: 253/2000... Training loss: 2.1615\n",
      "Epoch: 253/2000... Training loss: 2.3431\n",
      "Epoch: 253/2000... Training loss: 2.4426\n",
      "Epoch: 253/2000... Training loss: 2.2967\n",
      "Epoch: 253/2000... Training loss: 2.4146\n",
      "Epoch: 253/2000... Training loss: 2.2573\n",
      "Epoch: 253/2000... Training loss: 2.2599\n",
      "Epoch: 253/2000... Training loss: 2.5748\n",
      "Epoch: 253/2000... Training loss: 2.2596\n",
      "Epoch: 253/2000... Training loss: 2.1561\n",
      "Epoch: 253/2000... Training loss: 2.4493\n",
      "Epoch: 253/2000... Training loss: 2.6073\n",
      "Epoch: 253/2000... Training loss: 2.2681\n",
      "Epoch: 253/2000... Training loss: 2.1618\n",
      "Epoch: 253/2000... Training loss: 2.4039\n",
      "Epoch: 253/2000... Training loss: 2.6911\n",
      "Epoch: 253/2000... Training loss: 2.4236\n",
      "Epoch: 253/2000... Training loss: 2.0697\n",
      "Epoch: 253/2000... Training loss: 2.3093\n",
      "Epoch: 253/2000... Training loss: 2.3073\n",
      "Epoch: 253/2000... Training loss: 2.1835\n",
      "Epoch: 253/2000... Training loss: 2.2274\n",
      "Epoch: 253/2000... Training loss: 1.9137\n",
      "Epoch: 254/2000... Training loss: 2.1968\n",
      "Epoch: 254/2000... Training loss: 2.2256\n",
      "Epoch: 254/2000... Training loss: 2.2994\n",
      "Epoch: 254/2000... Training loss: 2.1847\n",
      "Epoch: 254/2000... Training loss: 2.2250\n",
      "Epoch: 254/2000... Training loss: 2.2716\n",
      "Epoch: 254/2000... Training loss: 2.3396\n",
      "Epoch: 254/2000... Training loss: 2.0305\n",
      "Epoch: 254/2000... Training loss: 2.3388\n",
      "Epoch: 254/2000... Training loss: 2.1850\n",
      "Epoch: 254/2000... Training loss: 2.3615\n",
      "Epoch: 254/2000... Training loss: 2.2745\n",
      "Epoch: 254/2000... Training loss: 2.2277\n",
      "Epoch: 254/2000... Training loss: 2.2728\n",
      "Epoch: 254/2000... Training loss: 2.5199\n",
      "Epoch: 254/2000... Training loss: 2.3653\n",
      "Epoch: 254/2000... Training loss: 2.1488\n",
      "Epoch: 254/2000... Training loss: 2.2611\n",
      "Epoch: 254/2000... Training loss: 2.0534\n",
      "Epoch: 254/2000... Training loss: 2.6207\n",
      "Epoch: 254/2000... Training loss: 2.1858\n",
      "Epoch: 254/2000... Training loss: 2.0899\n",
      "Epoch: 254/2000... Training loss: 2.4667\n",
      "Epoch: 254/2000... Training loss: 2.2633\n",
      "Epoch: 254/2000... Training loss: 2.3038\n",
      "Epoch: 254/2000... Training loss: 2.1047\n",
      "Epoch: 254/2000... Training loss: 2.3164\n",
      "Epoch: 254/2000... Training loss: 2.6556\n",
      "Epoch: 254/2000... Training loss: 2.3326\n",
      "Epoch: 254/2000... Training loss: 2.1783\n",
      "Epoch: 254/2000... Training loss: 2.1868\n",
      "Epoch: 255/2000... Training loss: 2.2643\n",
      "Epoch: 255/2000... Training loss: 2.2117\n",
      "Epoch: 255/2000... Training loss: 2.0617\n",
      "Epoch: 255/2000... Training loss: 2.2074\n",
      "Epoch: 255/2000... Training loss: 1.9564\n",
      "Epoch: 255/2000... Training loss: 2.4178\n",
      "Epoch: 255/2000... Training loss: 2.4452\n",
      "Epoch: 255/2000... Training loss: 2.0400\n",
      "Epoch: 255/2000... Training loss: 2.3580\n",
      "Epoch: 255/2000... Training loss: 2.3334\n",
      "Epoch: 255/2000... Training loss: 2.2867\n",
      "Epoch: 255/2000... Training loss: 2.4278\n",
      "Epoch: 255/2000... Training loss: 2.2057\n",
      "Epoch: 255/2000... Training loss: 2.2491\n",
      "Epoch: 255/2000... Training loss: 2.3010\n",
      "Epoch: 255/2000... Training loss: 2.1527\n",
      "Epoch: 255/2000... Training loss: 2.3944\n",
      "Epoch: 255/2000... Training loss: 2.3379\n",
      "Epoch: 255/2000... Training loss: 2.1287\n",
      "Epoch: 255/2000... Training loss: 2.3503\n",
      "Epoch: 255/2000... Training loss: 2.1193\n",
      "Epoch: 255/2000... Training loss: 2.3590\n",
      "Epoch: 255/2000... Training loss: 2.1341\n",
      "Epoch: 255/2000... Training loss: 2.4359\n",
      "Epoch: 255/2000... Training loss: 2.1423\n",
      "Epoch: 255/2000... Training loss: 2.2244\n",
      "Epoch: 255/2000... Training loss: 2.1581\n",
      "Epoch: 255/2000... Training loss: 2.5608\n",
      "Epoch: 255/2000... Training loss: 2.3317\n",
      "Epoch: 255/2000... Training loss: 2.0463\n",
      "Epoch: 255/2000... Training loss: 2.2090\n",
      "Epoch: 256/2000... Training loss: 2.3794\n",
      "Epoch: 256/2000... Training loss: 2.1771\n",
      "Epoch: 256/2000... Training loss: 2.2022\n",
      "Epoch: 256/2000... Training loss: 2.1371\n",
      "Epoch: 256/2000... Training loss: 2.0538\n",
      "Epoch: 256/2000... Training loss: 2.0309\n",
      "Epoch: 256/2000... Training loss: 2.0568\n",
      "Epoch: 256/2000... Training loss: 2.2653\n",
      "Epoch: 256/2000... Training loss: 2.4237\n",
      "Epoch: 256/2000... Training loss: 2.2898\n",
      "Epoch: 256/2000... Training loss: 2.4913\n",
      "Epoch: 256/2000... Training loss: 2.3909\n",
      "Epoch: 256/2000... Training loss: 2.4395\n",
      "Epoch: 256/2000... Training loss: 2.3007\n",
      "Epoch: 256/2000... Training loss: 2.2591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 256/2000... Training loss: 2.1789\n",
      "Epoch: 256/2000... Training loss: 2.2415\n",
      "Epoch: 256/2000... Training loss: 2.3049\n",
      "Epoch: 256/2000... Training loss: 2.4551\n",
      "Epoch: 256/2000... Training loss: 2.2998\n",
      "Epoch: 256/2000... Training loss: 2.3399\n",
      "Epoch: 256/2000... Training loss: 2.4138\n",
      "Epoch: 256/2000... Training loss: 2.3740\n",
      "Epoch: 256/2000... Training loss: 2.2363\n",
      "Epoch: 256/2000... Training loss: 2.4075\n",
      "Epoch: 256/2000... Training loss: 2.2507\n",
      "Epoch: 256/2000... Training loss: 2.2955\n",
      "Epoch: 256/2000... Training loss: 2.3096\n",
      "Epoch: 256/2000... Training loss: 2.3613\n",
      "Epoch: 256/2000... Training loss: 2.3337\n",
      "Epoch: 256/2000... Training loss: 2.4526\n",
      "Epoch: 257/2000... Training loss: 2.1659\n",
      "Epoch: 257/2000... Training loss: 2.3743\n",
      "Epoch: 257/2000... Training loss: 2.5784\n",
      "Epoch: 257/2000... Training loss: 2.4133\n",
      "Epoch: 257/2000... Training loss: 1.9662\n",
      "Epoch: 257/2000... Training loss: 2.1630\n",
      "Epoch: 257/2000... Training loss: 2.0536\n",
      "Epoch: 257/2000... Training loss: 2.0029\n",
      "Epoch: 257/2000... Training loss: 2.2040\n",
      "Epoch: 257/2000... Training loss: 2.4250\n",
      "Epoch: 257/2000... Training loss: 2.2820\n",
      "Epoch: 257/2000... Training loss: 2.2011\n",
      "Epoch: 257/2000... Training loss: 2.0092\n",
      "Epoch: 257/2000... Training loss: 2.2621\n",
      "Epoch: 257/2000... Training loss: 2.2390\n",
      "Epoch: 257/2000... Training loss: 2.2424\n",
      "Epoch: 257/2000... Training loss: 2.3963\n",
      "Epoch: 257/2000... Training loss: 2.5673\n",
      "Epoch: 257/2000... Training loss: 2.4618\n",
      "Epoch: 257/2000... Training loss: 2.2113\n",
      "Epoch: 257/2000... Training loss: 2.3174\n",
      "Epoch: 257/2000... Training loss: 2.3598\n",
      "Epoch: 257/2000... Training loss: 2.4679\n",
      "Epoch: 257/2000... Training loss: 2.2679\n",
      "Epoch: 257/2000... Training loss: 2.3475\n",
      "Epoch: 257/2000... Training loss: 2.1887\n",
      "Epoch: 257/2000... Training loss: 2.2650\n",
      "Epoch: 257/2000... Training loss: 2.2434\n",
      "Epoch: 257/2000... Training loss: 2.1701\n",
      "Epoch: 257/2000... Training loss: 2.5148\n",
      "Epoch: 257/2000... Training loss: 2.7033\n",
      "Epoch: 258/2000... Training loss: 2.2266\n",
      "Epoch: 258/2000... Training loss: 2.1822\n",
      "Epoch: 258/2000... Training loss: 2.3026\n",
      "Epoch: 258/2000... Training loss: 2.1231\n",
      "Epoch: 258/2000... Training loss: 2.0795\n",
      "Epoch: 258/2000... Training loss: 2.3518\n",
      "Epoch: 258/2000... Training loss: 2.2644\n",
      "Epoch: 258/2000... Training loss: 2.3316\n",
      "Epoch: 258/2000... Training loss: 2.3314\n",
      "Epoch: 258/2000... Training loss: 2.3515\n",
      "Epoch: 258/2000... Training loss: 2.2199\n",
      "Epoch: 258/2000... Training loss: 2.3738\n",
      "Epoch: 258/2000... Training loss: 2.0819\n",
      "Epoch: 258/2000... Training loss: 2.5150\n",
      "Epoch: 258/2000... Training loss: 2.2639\n",
      "Epoch: 258/2000... Training loss: 2.0463\n",
      "Epoch: 258/2000... Training loss: 2.1951\n",
      "Epoch: 258/2000... Training loss: 2.2982\n",
      "Epoch: 258/2000... Training loss: 2.3994\n",
      "Epoch: 258/2000... Training loss: 2.2708\n",
      "Epoch: 258/2000... Training loss: 2.3697\n",
      "Epoch: 258/2000... Training loss: 2.1954\n",
      "Epoch: 258/2000... Training loss: 2.2809\n",
      "Epoch: 258/2000... Training loss: 2.3016\n",
      "Epoch: 258/2000... Training loss: 2.4192\n",
      "Epoch: 258/2000... Training loss: 2.2802\n",
      "Epoch: 258/2000... Training loss: 2.4362\n",
      "Epoch: 258/2000... Training loss: 2.2840\n",
      "Epoch: 258/2000... Training loss: 2.4221\n",
      "Epoch: 258/2000... Training loss: 2.0473\n",
      "Epoch: 258/2000... Training loss: 2.1361\n",
      "Epoch: 259/2000... Training loss: 2.2563\n",
      "Epoch: 259/2000... Training loss: 2.0687\n",
      "Epoch: 259/2000... Training loss: 2.1496\n",
      "Epoch: 259/2000... Training loss: 1.9987\n",
      "Epoch: 259/2000... Training loss: 2.3289\n",
      "Epoch: 259/2000... Training loss: 2.2673\n",
      "Epoch: 259/2000... Training loss: 2.0932\n",
      "Epoch: 259/2000... Training loss: 2.2146\n",
      "Epoch: 259/2000... Training loss: 2.2204\n",
      "Epoch: 259/2000... Training loss: 2.3899\n",
      "Epoch: 259/2000... Training loss: 2.3015\n",
      "Epoch: 259/2000... Training loss: 2.0855\n",
      "Epoch: 259/2000... Training loss: 2.2618\n",
      "Epoch: 259/2000... Training loss: 2.3307\n",
      "Epoch: 259/2000... Training loss: 2.2139\n",
      "Epoch: 259/2000... Training loss: 2.3672\n",
      "Epoch: 259/2000... Training loss: 2.3147\n",
      "Epoch: 259/2000... Training loss: 2.3464\n",
      "Epoch: 259/2000... Training loss: 2.4147\n",
      "Epoch: 259/2000... Training loss: 2.3548\n",
      "Epoch: 259/2000... Training loss: 2.3504\n",
      "Epoch: 259/2000... Training loss: 1.9380\n",
      "Epoch: 259/2000... Training loss: 2.3794\n",
      "Epoch: 259/2000... Training loss: 2.2988\n",
      "Epoch: 259/2000... Training loss: 2.0697\n",
      "Epoch: 259/2000... Training loss: 2.1038\n",
      "Epoch: 259/2000... Training loss: 2.1640\n",
      "Epoch: 259/2000... Training loss: 2.3648\n",
      "Epoch: 259/2000... Training loss: 2.2754\n",
      "Epoch: 259/2000... Training loss: 2.3041\n",
      "Epoch: 259/2000... Training loss: 2.2586\n",
      "Epoch: 260/2000... Training loss: 2.3706\n",
      "Epoch: 260/2000... Training loss: 2.0281\n",
      "Epoch: 260/2000... Training loss: 2.1761\n",
      "Epoch: 260/2000... Training loss: 2.1146\n",
      "Epoch: 260/2000... Training loss: 2.3561\n",
      "Epoch: 260/2000... Training loss: 2.2153\n",
      "Epoch: 260/2000... Training loss: 2.1818\n",
      "Epoch: 260/2000... Training loss: 2.2071\n",
      "Epoch: 260/2000... Training loss: 2.2314\n",
      "Epoch: 260/2000... Training loss: 2.2822\n",
      "Epoch: 260/2000... Training loss: 2.2754\n",
      "Epoch: 260/2000... Training loss: 2.2900\n",
      "Epoch: 260/2000... Training loss: 2.3576\n",
      "Epoch: 260/2000... Training loss: 2.2637\n",
      "Epoch: 260/2000... Training loss: 2.3087\n",
      "Epoch: 260/2000... Training loss: 2.1828\n",
      "Epoch: 260/2000... Training loss: 2.5063\n",
      "Epoch: 260/2000... Training loss: 2.0336\n",
      "Epoch: 260/2000... Training loss: 2.3707\n",
      "Epoch: 260/2000... Training loss: 2.3327\n",
      "Epoch: 260/2000... Training loss: 2.2318\n",
      "Epoch: 260/2000... Training loss: 2.0255\n",
      "Epoch: 260/2000... Training loss: 2.3050\n",
      "Epoch: 260/2000... Training loss: 2.1716\n",
      "Epoch: 260/2000... Training loss: 2.3373\n",
      "Epoch: 260/2000... Training loss: 2.3091\n",
      "Epoch: 260/2000... Training loss: 2.1138\n",
      "Epoch: 260/2000... Training loss: 2.3196\n",
      "Epoch: 260/2000... Training loss: 2.4784\n",
      "Epoch: 260/2000... Training loss: 2.0506\n",
      "Epoch: 260/2000... Training loss: 2.1363\n",
      "Epoch: 261/2000... Training loss: 2.4780\n",
      "Epoch: 261/2000... Training loss: 2.2075\n",
      "Epoch: 261/2000... Training loss: 2.0667\n",
      "Epoch: 261/2000... Training loss: 2.1571\n",
      "Epoch: 261/2000... Training loss: 2.4521\n",
      "Epoch: 261/2000... Training loss: 2.0322\n",
      "Epoch: 261/2000... Training loss: 2.0153\n",
      "Epoch: 261/2000... Training loss: 2.1683\n",
      "Epoch: 261/2000... Training loss: 2.1760\n",
      "Epoch: 261/2000... Training loss: 2.3390\n",
      "Epoch: 261/2000... Training loss: 2.2959\n",
      "Epoch: 261/2000... Training loss: 2.2335\n",
      "Epoch: 261/2000... Training loss: 2.3473\n",
      "Epoch: 261/2000... Training loss: 2.3086\n",
      "Epoch: 261/2000... Training loss: 2.3518\n",
      "Epoch: 261/2000... Training loss: 2.4896\n",
      "Epoch: 261/2000... Training loss: 2.3611\n",
      "Epoch: 261/2000... Training loss: 2.3527\n",
      "Epoch: 261/2000... Training loss: 2.3560\n",
      "Epoch: 261/2000... Training loss: 2.3009\n",
      "Epoch: 261/2000... Training loss: 2.2410\n",
      "Epoch: 261/2000... Training loss: 1.9908\n",
      "Epoch: 261/2000... Training loss: 2.3889\n",
      "Epoch: 261/2000... Training loss: 2.2556\n",
      "Epoch: 261/2000... Training loss: 2.4321\n",
      "Epoch: 261/2000... Training loss: 2.2359\n",
      "Epoch: 261/2000... Training loss: 2.2253\n",
      "Epoch: 261/2000... Training loss: 2.6147\n",
      "Epoch: 261/2000... Training loss: 2.1816\n",
      "Epoch: 261/2000... Training loss: 2.2385\n",
      "Epoch: 261/2000... Training loss: 2.1601\n",
      "Epoch: 262/2000... Training loss: 2.1970\n",
      "Epoch: 262/2000... Training loss: 2.2665\n",
      "Epoch: 262/2000... Training loss: 1.8851\n",
      "Epoch: 262/2000... Training loss: 2.2302\n",
      "Epoch: 262/2000... Training loss: 2.4017\n",
      "Epoch: 262/2000... Training loss: 2.2664\n",
      "Epoch: 262/2000... Training loss: 2.0852\n",
      "Epoch: 262/2000... Training loss: 2.4259\n",
      "Epoch: 262/2000... Training loss: 2.3409\n",
      "Epoch: 262/2000... Training loss: 2.2636\n",
      "Epoch: 262/2000... Training loss: 2.2349\n",
      "Epoch: 262/2000... Training loss: 2.2736\n",
      "Epoch: 262/2000... Training loss: 2.4071\n",
      "Epoch: 262/2000... Training loss: 2.4421\n",
      "Epoch: 262/2000... Training loss: 2.3650\n",
      "Epoch: 262/2000... Training loss: 2.1819\n",
      "Epoch: 262/2000... Training loss: 2.1713\n",
      "Epoch: 262/2000... Training loss: 2.3448\n",
      "Epoch: 262/2000... Training loss: 2.2750\n",
      "Epoch: 262/2000... Training loss: 2.2464\n",
      "Epoch: 262/2000... Training loss: 2.2876\n",
      "Epoch: 262/2000... Training loss: 1.9601\n",
      "Epoch: 262/2000... Training loss: 2.3825\n",
      "Epoch: 262/2000... Training loss: 2.2904\n",
      "Epoch: 262/2000... Training loss: 2.1526\n",
      "Epoch: 262/2000... Training loss: 2.3071\n",
      "Epoch: 262/2000... Training loss: 2.1631\n",
      "Epoch: 262/2000... Training loss: 2.2489\n",
      "Epoch: 262/2000... Training loss: 2.3602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 262/2000... Training loss: 2.1780\n",
      "Epoch: 262/2000... Training loss: 2.1800\n",
      "Epoch: 263/2000... Training loss: 2.1560\n",
      "Epoch: 263/2000... Training loss: 2.2578\n",
      "Epoch: 263/2000... Training loss: 2.0451\n",
      "Epoch: 263/2000... Training loss: 2.2507\n",
      "Epoch: 263/2000... Training loss: 2.1544\n",
      "Epoch: 263/2000... Training loss: 2.5113\n",
      "Epoch: 263/2000... Training loss: 2.1461\n",
      "Epoch: 263/2000... Training loss: 2.3697\n",
      "Epoch: 263/2000... Training loss: 1.9625\n",
      "Epoch: 263/2000... Training loss: 2.2610\n",
      "Epoch: 263/2000... Training loss: 2.1241\n",
      "Epoch: 263/2000... Training loss: 2.0911\n",
      "Epoch: 263/2000... Training loss: 2.1277\n",
      "Epoch: 263/2000... Training loss: 2.2167\n",
      "Epoch: 263/2000... Training loss: 2.4061\n",
      "Epoch: 263/2000... Training loss: 2.2216\n",
      "Epoch: 263/2000... Training loss: 2.3769\n",
      "Epoch: 263/2000... Training loss: 2.1483\n",
      "Epoch: 263/2000... Training loss: 2.5417\n",
      "Epoch: 263/2000... Training loss: 2.2016\n",
      "Epoch: 263/2000... Training loss: 2.0930\n",
      "Epoch: 263/2000... Training loss: 2.3909\n",
      "Epoch: 263/2000... Training loss: 2.1869\n",
      "Epoch: 263/2000... Training loss: 2.3695\n",
      "Epoch: 263/2000... Training loss: 2.1228\n",
      "Epoch: 263/2000... Training loss: 2.2892\n",
      "Epoch: 263/2000... Training loss: 2.4821\n",
      "Epoch: 263/2000... Training loss: 2.2452\n",
      "Epoch: 263/2000... Training loss: 2.3550\n",
      "Epoch: 263/2000... Training loss: 2.2930\n",
      "Epoch: 263/2000... Training loss: 2.1455\n",
      "Epoch: 264/2000... Training loss: 2.1877\n",
      "Epoch: 264/2000... Training loss: 2.1973\n",
      "Epoch: 264/2000... Training loss: 2.3036\n",
      "Epoch: 264/2000... Training loss: 2.2404\n",
      "Epoch: 264/2000... Training loss: 1.9745\n",
      "Epoch: 264/2000... Training loss: 2.2655\n",
      "Epoch: 264/2000... Training loss: 2.4644\n",
      "Epoch: 264/2000... Training loss: 2.3923\n",
      "Epoch: 264/2000... Training loss: 2.4534\n",
      "Epoch: 264/2000... Training loss: 2.1700\n",
      "Epoch: 264/2000... Training loss: 2.4110\n",
      "Epoch: 264/2000... Training loss: 2.0700\n",
      "Epoch: 264/2000... Training loss: 2.2453\n",
      "Epoch: 264/2000... Training loss: 2.1437\n",
      "Epoch: 264/2000... Training loss: 2.2845\n",
      "Epoch: 264/2000... Training loss: 2.0592\n",
      "Epoch: 264/2000... Training loss: 2.4437\n",
      "Epoch: 264/2000... Training loss: 2.3087\n",
      "Epoch: 264/2000... Training loss: 2.1635\n",
      "Epoch: 264/2000... Training loss: 2.1772\n",
      "Epoch: 264/2000... Training loss: 2.1324\n",
      "Epoch: 264/2000... Training loss: 2.3405\n",
      "Epoch: 264/2000... Training loss: 2.1924\n",
      "Epoch: 264/2000... Training loss: 2.0900\n",
      "Epoch: 264/2000... Training loss: 1.9680\n",
      "Epoch: 264/2000... Training loss: 2.1356\n",
      "Epoch: 264/2000... Training loss: 2.1237\n",
      "Epoch: 264/2000... Training loss: 2.3180\n",
      "Epoch: 264/2000... Training loss: 2.4569\n",
      "Epoch: 264/2000... Training loss: 2.2541\n",
      "Epoch: 264/2000... Training loss: 2.1990\n",
      "Epoch: 265/2000... Training loss: 1.9259\n",
      "Epoch: 265/2000... Training loss: 2.0709\n",
      "Epoch: 265/2000... Training loss: 2.2707\n",
      "Epoch: 265/2000... Training loss: 2.2423\n",
      "Epoch: 265/2000... Training loss: 2.2053\n",
      "Epoch: 265/2000... Training loss: 2.1353\n",
      "Epoch: 265/2000... Training loss: 2.3340\n",
      "Epoch: 265/2000... Training loss: 2.0991\n",
      "Epoch: 265/2000... Training loss: 2.0721\n",
      "Epoch: 265/2000... Training loss: 2.1669\n",
      "Epoch: 265/2000... Training loss: 2.1597\n",
      "Epoch: 265/2000... Training loss: 2.1642\n",
      "Epoch: 265/2000... Training loss: 2.1891\n",
      "Epoch: 265/2000... Training loss: 2.0499\n",
      "Epoch: 265/2000... Training loss: 2.3904\n",
      "Epoch: 265/2000... Training loss: 2.3968\n",
      "Epoch: 265/2000... Training loss: 2.4241\n",
      "Epoch: 265/2000... Training loss: 2.3524\n",
      "Epoch: 265/2000... Training loss: 2.3775\n",
      "Epoch: 265/2000... Training loss: 2.1791\n",
      "Epoch: 265/2000... Training loss: 2.1399\n",
      "Epoch: 265/2000... Training loss: 2.4004\n",
      "Epoch: 265/2000... Training loss: 2.2977\n",
      "Epoch: 265/2000... Training loss: 2.1501\n",
      "Epoch: 265/2000... Training loss: 2.4399\n",
      "Epoch: 265/2000... Training loss: 2.0844\n",
      "Epoch: 265/2000... Training loss: 2.1891\n",
      "Epoch: 265/2000... Training loss: 2.2386\n",
      "Epoch: 265/2000... Training loss: 2.1534\n",
      "Epoch: 265/2000... Training loss: 2.3254\n",
      "Epoch: 265/2000... Training loss: 2.1003\n",
      "Epoch: 266/2000... Training loss: 2.0237\n",
      "Epoch: 266/2000... Training loss: 2.2775\n",
      "Epoch: 266/2000... Training loss: 2.0151\n",
      "Epoch: 266/2000... Training loss: 2.2497\n",
      "Epoch: 266/2000... Training loss: 2.0645\n",
      "Epoch: 266/2000... Training loss: 2.2354\n",
      "Epoch: 266/2000... Training loss: 2.0441\n",
      "Epoch: 266/2000... Training loss: 2.2739\n",
      "Epoch: 266/2000... Training loss: 2.2406\n",
      "Epoch: 266/2000... Training loss: 2.4164\n",
      "Epoch: 266/2000... Training loss: 2.3489\n",
      "Epoch: 266/2000... Training loss: 2.3316\n",
      "Epoch: 266/2000... Training loss: 2.3072\n",
      "Epoch: 266/2000... Training loss: 2.2290\n",
      "Epoch: 266/2000... Training loss: 2.1153\n",
      "Epoch: 266/2000... Training loss: 2.1862\n",
      "Epoch: 266/2000... Training loss: 2.0749\n",
      "Epoch: 266/2000... Training loss: 2.2415\n",
      "Epoch: 266/2000... Training loss: 2.4049\n",
      "Epoch: 266/2000... Training loss: 2.3309\n",
      "Epoch: 266/2000... Training loss: 2.2395\n",
      "Epoch: 266/2000... Training loss: 2.1766\n",
      "Epoch: 266/2000... Training loss: 2.2716\n",
      "Epoch: 266/2000... Training loss: 2.0519\n",
      "Epoch: 266/2000... Training loss: 2.2860\n",
      "Epoch: 266/2000... Training loss: 2.3493\n",
      "Epoch: 266/2000... Training loss: 2.3117\n",
      "Epoch: 266/2000... Training loss: 2.1942\n",
      "Epoch: 266/2000... Training loss: 2.0870\n",
      "Epoch: 266/2000... Training loss: 2.2460\n",
      "Epoch: 266/2000... Training loss: 2.2517\n",
      "Epoch: 267/2000... Training loss: 1.9909\n",
      "Epoch: 267/2000... Training loss: 2.4141\n",
      "Epoch: 267/2000... Training loss: 2.1827\n",
      "Epoch: 267/2000... Training loss: 2.3669\n",
      "Epoch: 267/2000... Training loss: 2.1227\n",
      "Epoch: 267/2000... Training loss: 2.2772\n",
      "Epoch: 267/2000... Training loss: 2.2855\n",
      "Epoch: 267/2000... Training loss: 2.4671\n",
      "Epoch: 267/2000... Training loss: 2.2810\n",
      "Epoch: 267/2000... Training loss: 2.1879\n",
      "Epoch: 267/2000... Training loss: 2.2666\n",
      "Epoch: 267/2000... Training loss: 2.2073\n",
      "Epoch: 267/2000... Training loss: 2.0527\n",
      "Epoch: 267/2000... Training loss: 2.0833\n",
      "Epoch: 267/2000... Training loss: 2.3793\n",
      "Epoch: 267/2000... Training loss: 2.0353\n",
      "Epoch: 267/2000... Training loss: 2.5619\n",
      "Epoch: 267/2000... Training loss: 1.9936\n",
      "Epoch: 267/2000... Training loss: 2.3609\n",
      "Epoch: 267/2000... Training loss: 2.0805\n",
      "Epoch: 267/2000... Training loss: 2.2825\n",
      "Epoch: 267/2000... Training loss: 2.5095\n",
      "Epoch: 267/2000... Training loss: 2.1947\n",
      "Epoch: 267/2000... Training loss: 2.3824\n",
      "Epoch: 267/2000... Training loss: 2.2332\n",
      "Epoch: 267/2000... Training loss: 2.3555\n",
      "Epoch: 267/2000... Training loss: 2.4105\n",
      "Epoch: 267/2000... Training loss: 2.1964\n",
      "Epoch: 267/2000... Training loss: 2.2514\n",
      "Epoch: 267/2000... Training loss: 2.3580\n",
      "Epoch: 267/2000... Training loss: 2.2448\n",
      "Epoch: 268/2000... Training loss: 2.1845\n",
      "Epoch: 268/2000... Training loss: 2.2426\n",
      "Epoch: 268/2000... Training loss: 2.4863\n",
      "Epoch: 268/2000... Training loss: 2.2167\n",
      "Epoch: 268/2000... Training loss: 1.9799\n",
      "Epoch: 268/2000... Training loss: 2.1686\n",
      "Epoch: 268/2000... Training loss: 2.1935\n",
      "Epoch: 268/2000... Training loss: 1.9651\n",
      "Epoch: 268/2000... Training loss: 2.1958\n",
      "Epoch: 268/2000... Training loss: 2.1975\n",
      "Epoch: 268/2000... Training loss: 2.2371\n",
      "Epoch: 268/2000... Training loss: 2.2572\n",
      "Epoch: 268/2000... Training loss: 2.2398\n",
      "Epoch: 268/2000... Training loss: 2.1569\n",
      "Epoch: 268/2000... Training loss: 2.2621\n",
      "Epoch: 268/2000... Training loss: 2.2457\n",
      "Epoch: 268/2000... Training loss: 2.1632\n",
      "Epoch: 268/2000... Training loss: 2.2132\n",
      "Epoch: 268/2000... Training loss: 2.2243\n",
      "Epoch: 268/2000... Training loss: 1.9706\n",
      "Epoch: 268/2000... Training loss: 2.6228\n",
      "Epoch: 268/2000... Training loss: 2.3267\n",
      "Epoch: 268/2000... Training loss: 2.3923\n",
      "Epoch: 268/2000... Training loss: 2.0745\n",
      "Epoch: 268/2000... Training loss: 2.2878\n",
      "Epoch: 268/2000... Training loss: 2.2178\n",
      "Epoch: 268/2000... Training loss: 2.0644\n",
      "Epoch: 268/2000... Training loss: 2.2406\n",
      "Epoch: 268/2000... Training loss: 2.0833\n",
      "Epoch: 268/2000... Training loss: 2.4989\n",
      "Epoch: 268/2000... Training loss: 1.9342\n",
      "Epoch: 269/2000... Training loss: 2.4235\n",
      "Epoch: 269/2000... Training loss: 2.1226\n",
      "Epoch: 269/2000... Training loss: 1.9979\n",
      "Epoch: 269/2000... Training loss: 2.3760\n",
      "Epoch: 269/2000... Training loss: 1.9819\n",
      "Epoch: 269/2000... Training loss: 2.1239\n",
      "Epoch: 269/2000... Training loss: 2.2560\n",
      "Epoch: 269/2000... Training loss: 2.1041\n",
      "Epoch: 269/2000... Training loss: 2.2437\n",
      "Epoch: 269/2000... Training loss: 2.5025\n",
      "Epoch: 269/2000... Training loss: 2.3729\n",
      "Epoch: 269/2000... Training loss: 2.2781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269/2000... Training loss: 1.9858\n",
      "Epoch: 269/2000... Training loss: 2.0994\n",
      "Epoch: 269/2000... Training loss: 2.3209\n",
      "Epoch: 269/2000... Training loss: 2.2571\n",
      "Epoch: 269/2000... Training loss: 2.4279\n",
      "Epoch: 269/2000... Training loss: 2.2295\n",
      "Epoch: 269/2000... Training loss: 2.0284\n",
      "Epoch: 269/2000... Training loss: 2.0559\n",
      "Epoch: 269/2000... Training loss: 2.2093\n",
      "Epoch: 269/2000... Training loss: 2.4981\n",
      "Epoch: 269/2000... Training loss: 2.0917\n",
      "Epoch: 269/2000... Training loss: 2.2719\n",
      "Epoch: 269/2000... Training loss: 2.1678\n",
      "Epoch: 269/2000... Training loss: 2.2289\n",
      "Epoch: 269/2000... Training loss: 2.1148\n",
      "Epoch: 269/2000... Training loss: 2.5153\n",
      "Epoch: 269/2000... Training loss: 2.4413\n",
      "Epoch: 269/2000... Training loss: 2.3223\n",
      "Epoch: 269/2000... Training loss: 2.2049\n",
      "Epoch: 270/2000... Training loss: 2.0315\n",
      "Epoch: 270/2000... Training loss: 2.0766\n",
      "Epoch: 270/2000... Training loss: 2.2167\n",
      "Epoch: 270/2000... Training loss: 2.2964\n",
      "Epoch: 270/2000... Training loss: 1.9465\n",
      "Epoch: 270/2000... Training loss: 2.2047\n",
      "Epoch: 270/2000... Training loss: 1.9536\n",
      "Epoch: 270/2000... Training loss: 2.1064\n",
      "Epoch: 270/2000... Training loss: 2.1006\n",
      "Epoch: 270/2000... Training loss: 2.4879\n",
      "Epoch: 270/2000... Training loss: 2.3936\n",
      "Epoch: 270/2000... Training loss: 2.0185\n",
      "Epoch: 270/2000... Training loss: 2.3250\n",
      "Epoch: 270/2000... Training loss: 2.2847\n",
      "Epoch: 270/2000... Training loss: 2.2047\n",
      "Epoch: 270/2000... Training loss: 2.0140\n",
      "Epoch: 270/2000... Training loss: 2.1740\n",
      "Epoch: 270/2000... Training loss: 2.0885\n",
      "Epoch: 270/2000... Training loss: 2.2526\n",
      "Epoch: 270/2000... Training loss: 2.3344\n",
      "Epoch: 270/2000... Training loss: 2.1451\n",
      "Epoch: 270/2000... Training loss: 2.2821\n",
      "Epoch: 270/2000... Training loss: 2.4839\n",
      "Epoch: 270/2000... Training loss: 2.1837\n",
      "Epoch: 270/2000... Training loss: 2.2826\n",
      "Epoch: 270/2000... Training loss: 2.3597\n",
      "Epoch: 270/2000... Training loss: 2.3398\n",
      "Epoch: 270/2000... Training loss: 2.1783\n",
      "Epoch: 270/2000... Training loss: 2.4358\n",
      "Epoch: 270/2000... Training loss: 2.1605\n",
      "Epoch: 270/2000... Training loss: 2.2355\n",
      "Epoch: 271/2000... Training loss: 2.2239\n",
      "Epoch: 271/2000... Training loss: 2.0734\n",
      "Epoch: 271/2000... Training loss: 2.1071\n",
      "Epoch: 271/2000... Training loss: 2.3434\n",
      "Epoch: 271/2000... Training loss: 2.1864\n",
      "Epoch: 271/2000... Training loss: 2.0589\n",
      "Epoch: 271/2000... Training loss: 2.2004\n",
      "Epoch: 271/2000... Training loss: 2.3143\n",
      "Epoch: 271/2000... Training loss: 1.9900\n",
      "Epoch: 271/2000... Training loss: 2.4007\n",
      "Epoch: 271/2000... Training loss: 2.2877\n",
      "Epoch: 271/2000... Training loss: 2.0105\n",
      "Epoch: 271/2000... Training loss: 1.9633\n",
      "Epoch: 271/2000... Training loss: 2.2362\n",
      "Epoch: 271/2000... Training loss: 2.2590\n",
      "Epoch: 271/2000... Training loss: 2.1483\n",
      "Epoch: 271/2000... Training loss: 2.2943\n",
      "Epoch: 271/2000... Training loss: 2.2666\n",
      "Epoch: 271/2000... Training loss: 2.1126\n",
      "Epoch: 271/2000... Training loss: 2.2664\n",
      "Epoch: 271/2000... Training loss: 2.3370\n",
      "Epoch: 271/2000... Training loss: 2.0938\n",
      "Epoch: 271/2000... Training loss: 2.2501\n",
      "Epoch: 271/2000... Training loss: 2.4018\n",
      "Epoch: 271/2000... Training loss: 2.1781\n",
      "Epoch: 271/2000... Training loss: 1.9653\n",
      "Epoch: 271/2000... Training loss: 2.5225\n",
      "Epoch: 271/2000... Training loss: 2.1744\n",
      "Epoch: 271/2000... Training loss: 2.2930\n",
      "Epoch: 271/2000... Training loss: 2.4814\n",
      "Epoch: 271/2000... Training loss: 1.9565\n",
      "Epoch: 272/2000... Training loss: 2.1282\n",
      "Epoch: 272/2000... Training loss: 2.2784\n",
      "Epoch: 272/2000... Training loss: 2.3120\n",
      "Epoch: 272/2000... Training loss: 2.2259\n",
      "Epoch: 272/2000... Training loss: 2.2505\n",
      "Epoch: 272/2000... Training loss: 2.4471\n",
      "Epoch: 272/2000... Training loss: 2.2338\n",
      "Epoch: 272/2000... Training loss: 2.3047\n",
      "Epoch: 272/2000... Training loss: 2.2500\n",
      "Epoch: 272/2000... Training loss: 2.3312\n",
      "Epoch: 272/2000... Training loss: 2.3146\n",
      "Epoch: 272/2000... Training loss: 2.0813\n",
      "Epoch: 272/2000... Training loss: 2.1440\n",
      "Epoch: 272/2000... Training loss: 2.3028\n",
      "Epoch: 272/2000... Training loss: 2.1873\n",
      "Epoch: 272/2000... Training loss: 2.3192\n",
      "Epoch: 272/2000... Training loss: 2.0140\n",
      "Epoch: 272/2000... Training loss: 2.0542\n",
      "Epoch: 272/2000... Training loss: 2.3731\n",
      "Epoch: 272/2000... Training loss: 2.2502\n",
      "Epoch: 272/2000... Training loss: 2.3516\n",
      "Epoch: 272/2000... Training loss: 2.2968\n",
      "Epoch: 272/2000... Training loss: 2.1635\n",
      "Epoch: 272/2000... Training loss: 2.0710\n",
      "Epoch: 272/2000... Training loss: 2.0896\n",
      "Epoch: 272/2000... Training loss: 2.1641\n",
      "Epoch: 272/2000... Training loss: 2.4398\n",
      "Epoch: 272/2000... Training loss: 2.2703\n",
      "Epoch: 272/2000... Training loss: 2.0919\n",
      "Epoch: 272/2000... Training loss: 2.0344\n",
      "Epoch: 272/2000... Training loss: 2.3551\n",
      "Epoch: 273/2000... Training loss: 2.1251\n",
      "Epoch: 273/2000... Training loss: 2.2497\n",
      "Epoch: 273/2000... Training loss: 2.1930\n",
      "Epoch: 273/2000... Training loss: 2.2174\n",
      "Epoch: 273/2000... Training loss: 2.0672\n",
      "Epoch: 273/2000... Training loss: 2.0893\n",
      "Epoch: 273/2000... Training loss: 1.9602\n",
      "Epoch: 273/2000... Training loss: 2.1846\n",
      "Epoch: 273/2000... Training loss: 2.0870\n",
      "Epoch: 273/2000... Training loss: 2.2113\n",
      "Epoch: 273/2000... Training loss: 2.0343\n",
      "Epoch: 273/2000... Training loss: 2.2539\n",
      "Epoch: 273/2000... Training loss: 2.0624\n",
      "Epoch: 273/2000... Training loss: 2.2774\n",
      "Epoch: 273/2000... Training loss: 2.1458\n",
      "Epoch: 273/2000... Training loss: 2.0473\n",
      "Epoch: 273/2000... Training loss: 2.0891\n",
      "Epoch: 273/2000... Training loss: 1.9801\n",
      "Epoch: 273/2000... Training loss: 2.0953\n",
      "Epoch: 273/2000... Training loss: 2.1754\n",
      "Epoch: 273/2000... Training loss: 2.2033\n",
      "Epoch: 273/2000... Training loss: 2.1433\n",
      "Epoch: 273/2000... Training loss: 2.4194\n",
      "Epoch: 273/2000... Training loss: 2.2702\n",
      "Epoch: 273/2000... Training loss: 2.3502\n",
      "Epoch: 273/2000... Training loss: 1.9261\n",
      "Epoch: 273/2000... Training loss: 2.0576\n",
      "Epoch: 273/2000... Training loss: 1.9981\n",
      "Epoch: 273/2000... Training loss: 2.3288\n",
      "Epoch: 273/2000... Training loss: 2.2263\n",
      "Epoch: 273/2000... Training loss: 2.5528\n",
      "Epoch: 274/2000... Training loss: 2.0993\n",
      "Epoch: 274/2000... Training loss: 2.1368\n",
      "Epoch: 274/2000... Training loss: 2.0895\n",
      "Epoch: 274/2000... Training loss: 2.0290\n",
      "Epoch: 274/2000... Training loss: 2.1785\n",
      "Epoch: 274/2000... Training loss: 1.9932\n",
      "Epoch: 274/2000... Training loss: 1.9745\n",
      "Epoch: 274/2000... Training loss: 2.1342\n",
      "Epoch: 274/2000... Training loss: 2.1745\n",
      "Epoch: 274/2000... Training loss: 2.3297\n",
      "Epoch: 274/2000... Training loss: 2.3379\n",
      "Epoch: 274/2000... Training loss: 2.1059\n",
      "Epoch: 274/2000... Training loss: 2.2198\n",
      "Epoch: 274/2000... Training loss: 2.3872\n",
      "Epoch: 274/2000... Training loss: 2.0891\n",
      "Epoch: 274/2000... Training loss: 2.1411\n",
      "Epoch: 274/2000... Training loss: 2.1866\n",
      "Epoch: 274/2000... Training loss: 2.1492\n",
      "Epoch: 274/2000... Training loss: 2.2893\n",
      "Epoch: 274/2000... Training loss: 2.0119\n",
      "Epoch: 274/2000... Training loss: 2.1535\n",
      "Epoch: 274/2000... Training loss: 2.2124\n",
      "Epoch: 274/2000... Training loss: 2.1732\n",
      "Epoch: 274/2000... Training loss: 2.0856\n",
      "Epoch: 274/2000... Training loss: 2.3079\n",
      "Epoch: 274/2000... Training loss: 2.2485\n",
      "Epoch: 274/2000... Training loss: 1.9384\n",
      "Epoch: 274/2000... Training loss: 2.0792\n",
      "Epoch: 274/2000... Training loss: 1.9292\n",
      "Epoch: 274/2000... Training loss: 2.0992\n",
      "Epoch: 274/2000... Training loss: 2.0905\n",
      "Epoch: 275/2000... Training loss: 2.1758\n",
      "Epoch: 275/2000... Training loss: 2.0222\n",
      "Epoch: 275/2000... Training loss: 1.9749\n",
      "Epoch: 275/2000... Training loss: 1.9653\n",
      "Epoch: 275/2000... Training loss: 2.1178\n",
      "Epoch: 275/2000... Training loss: 2.1365\n",
      "Epoch: 275/2000... Training loss: 2.1256\n",
      "Epoch: 275/2000... Training loss: 2.3480\n",
      "Epoch: 275/2000... Training loss: 2.2699\n",
      "Epoch: 275/2000... Training loss: 1.7955\n",
      "Epoch: 275/2000... Training loss: 2.0225\n",
      "Epoch: 275/2000... Training loss: 2.1089\n",
      "Epoch: 275/2000... Training loss: 2.0174\n",
      "Epoch: 275/2000... Training loss: 2.2220\n",
      "Epoch: 275/2000... Training loss: 2.2706\n",
      "Epoch: 275/2000... Training loss: 2.1003\n",
      "Epoch: 275/2000... Training loss: 2.0532\n",
      "Epoch: 275/2000... Training loss: 1.8558\n",
      "Epoch: 275/2000... Training loss: 2.1646\n",
      "Epoch: 275/2000... Training loss: 2.0376\n",
      "Epoch: 275/2000... Training loss: 2.2736\n",
      "Epoch: 275/2000... Training loss: 1.9574\n",
      "Epoch: 275/2000... Training loss: 2.2691\n",
      "Epoch: 275/2000... Training loss: 2.1988\n",
      "Epoch: 275/2000... Training loss: 2.4023\n",
      "Epoch: 275/2000... Training loss: 2.5411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275/2000... Training loss: 2.0888\n",
      "Epoch: 275/2000... Training loss: 2.5577\n",
      "Epoch: 275/2000... Training loss: 2.2202\n",
      "Epoch: 275/2000... Training loss: 2.0510\n",
      "Epoch: 275/2000... Training loss: 2.2720\n",
      "Epoch: 276/2000... Training loss: 2.2578\n",
      "Epoch: 276/2000... Training loss: 2.0710\n",
      "Epoch: 276/2000... Training loss: 2.0873\n",
      "Epoch: 276/2000... Training loss: 2.0200\n",
      "Epoch: 276/2000... Training loss: 2.4144\n",
      "Epoch: 276/2000... Training loss: 2.1691\n",
      "Epoch: 276/2000... Training loss: 2.2087\n",
      "Epoch: 276/2000... Training loss: 1.9678\n",
      "Epoch: 276/2000... Training loss: 2.2945\n",
      "Epoch: 276/2000... Training loss: 2.1524\n",
      "Epoch: 276/2000... Training loss: 2.1423\n",
      "Epoch: 276/2000... Training loss: 2.4890\n",
      "Epoch: 276/2000... Training loss: 2.1780\n",
      "Epoch: 276/2000... Training loss: 2.0688\n",
      "Epoch: 276/2000... Training loss: 2.4433\n",
      "Epoch: 276/2000... Training loss: 1.9421\n",
      "Epoch: 276/2000... Training loss: 2.3608\n",
      "Epoch: 276/2000... Training loss: 2.1169\n",
      "Epoch: 276/2000... Training loss: 2.4008\n",
      "Epoch: 276/2000... Training loss: 2.1573\n",
      "Epoch: 276/2000... Training loss: 2.3733\n",
      "Epoch: 276/2000... Training loss: 1.8401\n",
      "Epoch: 276/2000... Training loss: 2.5127\n",
      "Epoch: 276/2000... Training loss: 2.2032\n",
      "Epoch: 276/2000... Training loss: 2.2568\n",
      "Epoch: 276/2000... Training loss: 2.1488\n",
      "Epoch: 276/2000... Training loss: 2.3595\n",
      "Epoch: 276/2000... Training loss: 2.1262\n",
      "Epoch: 276/2000... Training loss: 2.2105\n",
      "Epoch: 276/2000... Training loss: 2.0576\n",
      "Epoch: 276/2000... Training loss: 2.0071\n",
      "Epoch: 277/2000... Training loss: 2.1063\n",
      "Epoch: 277/2000... Training loss: 2.1700\n",
      "Epoch: 277/2000... Training loss: 2.1218\n",
      "Epoch: 277/2000... Training loss: 1.8481\n",
      "Epoch: 277/2000... Training loss: 2.0799\n",
      "Epoch: 277/2000... Training loss: 1.9212\n",
      "Epoch: 277/2000... Training loss: 2.3131\n",
      "Epoch: 277/2000... Training loss: 2.1561\n",
      "Epoch: 277/2000... Training loss: 2.1551\n",
      "Epoch: 277/2000... Training loss: 2.3841\n",
      "Epoch: 277/2000... Training loss: 2.1567\n",
      "Epoch: 277/2000... Training loss: 2.0066\n",
      "Epoch: 277/2000... Training loss: 2.1783\n",
      "Epoch: 277/2000... Training loss: 2.0969\n",
      "Epoch: 277/2000... Training loss: 2.5374\n",
      "Epoch: 277/2000... Training loss: 2.3788\n",
      "Epoch: 277/2000... Training loss: 2.3968\n",
      "Epoch: 277/2000... Training loss: 2.2274\n",
      "Epoch: 277/2000... Training loss: 2.1485\n",
      "Epoch: 277/2000... Training loss: 2.2504\n",
      "Epoch: 277/2000... Training loss: 2.0784\n",
      "Epoch: 277/2000... Training loss: 2.3022\n",
      "Epoch: 277/2000... Training loss: 2.2800\n",
      "Epoch: 277/2000... Training loss: 2.2142\n",
      "Epoch: 277/2000... Training loss: 2.1346\n",
      "Epoch: 277/2000... Training loss: 2.1094\n",
      "Epoch: 277/2000... Training loss: 2.0280\n",
      "Epoch: 277/2000... Training loss: 2.2023\n",
      "Epoch: 277/2000... Training loss: 2.0686\n",
      "Epoch: 277/2000... Training loss: 2.0955\n",
      "Epoch: 277/2000... Training loss: 2.2444\n",
      "Epoch: 278/2000... Training loss: 2.3211\n",
      "Epoch: 278/2000... Training loss: 1.9731\n",
      "Epoch: 278/2000... Training loss: 2.3089\n",
      "Epoch: 278/2000... Training loss: 2.2942\n",
      "Epoch: 278/2000... Training loss: 2.0129\n",
      "Epoch: 278/2000... Training loss: 2.1449\n",
      "Epoch: 278/2000... Training loss: 2.0236\n",
      "Epoch: 278/2000... Training loss: 2.2924\n",
      "Epoch: 278/2000... Training loss: 2.2699\n",
      "Epoch: 278/2000... Training loss: 2.2916\n",
      "Epoch: 278/2000... Training loss: 2.3059\n",
      "Epoch: 278/2000... Training loss: 2.2905\n",
      "Epoch: 278/2000... Training loss: 1.9196\n",
      "Epoch: 278/2000... Training loss: 2.2207\n",
      "Epoch: 278/2000... Training loss: 2.0822\n",
      "Epoch: 278/2000... Training loss: 2.2931\n",
      "Epoch: 278/2000... Training loss: 2.0658\n",
      "Epoch: 278/2000... Training loss: 2.3760\n",
      "Epoch: 278/2000... Training loss: 1.9541\n",
      "Epoch: 278/2000... Training loss: 2.0400\n",
      "Epoch: 278/2000... Training loss: 2.2865\n",
      "Epoch: 278/2000... Training loss: 2.2754\n",
      "Epoch: 278/2000... Training loss: 2.2430\n",
      "Epoch: 278/2000... Training loss: 2.2427\n",
      "Epoch: 278/2000... Training loss: 2.2481\n",
      "Epoch: 278/2000... Training loss: 2.1047\n",
      "Epoch: 278/2000... Training loss: 2.1897\n",
      "Epoch: 278/2000... Training loss: 2.2538\n",
      "Epoch: 278/2000... Training loss: 2.0940\n",
      "Epoch: 278/2000... Training loss: 2.0950\n",
      "Epoch: 278/2000... Training loss: 2.1918\n",
      "Epoch: 279/2000... Training loss: 1.9011\n",
      "Epoch: 279/2000... Training loss: 2.1628\n",
      "Epoch: 279/2000... Training loss: 1.9120\n",
      "Epoch: 279/2000... Training loss: 2.1217\n",
      "Epoch: 279/2000... Training loss: 2.2754\n",
      "Epoch: 279/2000... Training loss: 2.1196\n",
      "Epoch: 279/2000... Training loss: 2.2798\n",
      "Epoch: 279/2000... Training loss: 2.3114\n",
      "Epoch: 279/2000... Training loss: 1.8720\n",
      "Epoch: 279/2000... Training loss: 2.1800\n",
      "Epoch: 279/2000... Training loss: 2.2271\n",
      "Epoch: 279/2000... Training loss: 2.3425\n",
      "Epoch: 279/2000... Training loss: 2.0763\n",
      "Epoch: 279/2000... Training loss: 2.2029\n",
      "Epoch: 279/2000... Training loss: 2.2739\n",
      "Epoch: 279/2000... Training loss: 2.1327\n",
      "Epoch: 279/2000... Training loss: 2.4945\n",
      "Epoch: 279/2000... Training loss: 2.0841\n",
      "Epoch: 279/2000... Training loss: 2.7187\n",
      "Epoch: 279/2000... Training loss: 2.4111\n",
      "Epoch: 279/2000... Training loss: 2.2299\n",
      "Epoch: 279/2000... Training loss: 1.9869\n",
      "Epoch: 279/2000... Training loss: 2.2641\n",
      "Epoch: 279/2000... Training loss: 2.3575\n",
      "Epoch: 279/2000... Training loss: 2.2409\n",
      "Epoch: 279/2000... Training loss: 2.2646\n",
      "Epoch: 279/2000... Training loss: 1.9857\n",
      "Epoch: 279/2000... Training loss: 2.2838\n",
      "Epoch: 279/2000... Training loss: 2.2965\n",
      "Epoch: 279/2000... Training loss: 2.3336\n",
      "Epoch: 279/2000... Training loss: 2.2299\n",
      "Epoch: 280/2000... Training loss: 2.2063\n",
      "Epoch: 280/2000... Training loss: 2.0185\n",
      "Epoch: 280/2000... Training loss: 2.1246\n",
      "Epoch: 280/2000... Training loss: 2.0374\n",
      "Epoch: 280/2000... Training loss: 2.0241\n",
      "Epoch: 280/2000... Training loss: 2.1225\n",
      "Epoch: 280/2000... Training loss: 2.1479\n",
      "Epoch: 280/2000... Training loss: 2.1010\n",
      "Epoch: 280/2000... Training loss: 2.2090\n",
      "Epoch: 280/2000... Training loss: 2.1209\n",
      "Epoch: 280/2000... Training loss: 2.3081\n",
      "Epoch: 280/2000... Training loss: 2.3538\n",
      "Epoch: 280/2000... Training loss: 2.0907\n",
      "Epoch: 280/2000... Training loss: 2.3204\n",
      "Epoch: 280/2000... Training loss: 2.3694\n",
      "Epoch: 280/2000... Training loss: 2.1964\n",
      "Epoch: 280/2000... Training loss: 2.0039\n",
      "Epoch: 280/2000... Training loss: 2.2927\n",
      "Epoch: 280/2000... Training loss: 2.0815\n",
      "Epoch: 280/2000... Training loss: 2.2299\n",
      "Epoch: 280/2000... Training loss: 2.3617\n",
      "Epoch: 280/2000... Training loss: 1.9943\n",
      "Epoch: 280/2000... Training loss: 2.4934\n",
      "Epoch: 280/2000... Training loss: 2.1484\n",
      "Epoch: 280/2000... Training loss: 2.1930\n",
      "Epoch: 280/2000... Training loss: 2.2964\n",
      "Epoch: 280/2000... Training loss: 2.4859\n",
      "Epoch: 280/2000... Training loss: 2.1692\n",
      "Epoch: 280/2000... Training loss: 2.2062\n",
      "Epoch: 280/2000... Training loss: 2.1424\n",
      "Epoch: 280/2000... Training loss: 2.0221\n",
      "Epoch: 281/2000... Training loss: 2.0256\n",
      "Epoch: 281/2000... Training loss: 1.9987\n",
      "Epoch: 281/2000... Training loss: 2.1912\n",
      "Epoch: 281/2000... Training loss: 2.1018\n",
      "Epoch: 281/2000... Training loss: 2.0627\n",
      "Epoch: 281/2000... Training loss: 2.0508\n",
      "Epoch: 281/2000... Training loss: 2.1226\n",
      "Epoch: 281/2000... Training loss: 2.2456\n",
      "Epoch: 281/2000... Training loss: 1.8735\n",
      "Epoch: 281/2000... Training loss: 2.2170\n",
      "Epoch: 281/2000... Training loss: 2.0460\n",
      "Epoch: 281/2000... Training loss: 2.1841\n",
      "Epoch: 281/2000... Training loss: 2.1796\n",
      "Epoch: 281/2000... Training loss: 2.2751\n",
      "Epoch: 281/2000... Training loss: 2.0428\n",
      "Epoch: 281/2000... Training loss: 2.1764\n",
      "Epoch: 281/2000... Training loss: 2.0321\n",
      "Epoch: 281/2000... Training loss: 2.2583\n",
      "Epoch: 281/2000... Training loss: 1.8434\n",
      "Epoch: 281/2000... Training loss: 2.3818\n",
      "Epoch: 281/2000... Training loss: 2.2591\n",
      "Epoch: 281/2000... Training loss: 2.1843\n",
      "Epoch: 281/2000... Training loss: 2.0170\n",
      "Epoch: 281/2000... Training loss: 2.0647\n",
      "Epoch: 281/2000... Training loss: 2.2136\n",
      "Epoch: 281/2000... Training loss: 2.0864\n",
      "Epoch: 281/2000... Training loss: 2.2222\n",
      "Epoch: 281/2000... Training loss: 2.1011\n",
      "Epoch: 281/2000... Training loss: 2.0206\n",
      "Epoch: 281/2000... Training loss: 2.2109\n",
      "Epoch: 281/2000... Training loss: 2.1125\n",
      "Epoch: 282/2000... Training loss: 2.1339\n",
      "Epoch: 282/2000... Training loss: 2.0031\n",
      "Epoch: 282/2000... Training loss: 2.3163\n",
      "Epoch: 282/2000... Training loss: 2.1053\n",
      "Epoch: 282/2000... Training loss: 2.2830\n",
      "Epoch: 282/2000... Training loss: 2.3296\n",
      "Epoch: 282/2000... Training loss: 2.0679\n",
      "Epoch: 282/2000... Training loss: 2.1309\n",
      "Epoch: 282/2000... Training loss: 2.1778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 282/2000... Training loss: 2.1889\n",
      "Epoch: 282/2000... Training loss: 2.2524\n",
      "Epoch: 282/2000... Training loss: 1.9326\n",
      "Epoch: 282/2000... Training loss: 2.4736\n",
      "Epoch: 282/2000... Training loss: 2.1239\n",
      "Epoch: 282/2000... Training loss: 2.3171\n",
      "Epoch: 282/2000... Training loss: 2.2944\n",
      "Epoch: 282/2000... Training loss: 2.1146\n",
      "Epoch: 282/2000... Training loss: 2.2771\n",
      "Epoch: 282/2000... Training loss: 2.4294\n",
      "Epoch: 282/2000... Training loss: 2.1361\n",
      "Epoch: 282/2000... Training loss: 2.2580\n",
      "Epoch: 282/2000... Training loss: 2.3895\n",
      "Epoch: 282/2000... Training loss: 2.1679\n",
      "Epoch: 282/2000... Training loss: 2.2462\n",
      "Epoch: 282/2000... Training loss: 2.1902\n",
      "Epoch: 282/2000... Training loss: 2.2550\n",
      "Epoch: 282/2000... Training loss: 2.2535\n",
      "Epoch: 282/2000... Training loss: 2.1411\n",
      "Epoch: 282/2000... Training loss: 2.3580\n",
      "Epoch: 282/2000... Training loss: 2.0796\n",
      "Epoch: 282/2000... Training loss: 2.1728\n",
      "Epoch: 283/2000... Training loss: 2.1438\n",
      "Epoch: 283/2000... Training loss: 2.0544\n",
      "Epoch: 283/2000... Training loss: 2.4039\n",
      "Epoch: 283/2000... Training loss: 1.9673\n",
      "Epoch: 283/2000... Training loss: 2.2721\n",
      "Epoch: 283/2000... Training loss: 2.2191\n",
      "Epoch: 283/2000... Training loss: 1.9932\n",
      "Epoch: 283/2000... Training loss: 2.0548\n",
      "Epoch: 283/2000... Training loss: 2.0590\n",
      "Epoch: 283/2000... Training loss: 2.2166\n",
      "Epoch: 283/2000... Training loss: 1.6791\n",
      "Epoch: 283/2000... Training loss: 1.9349\n",
      "Epoch: 283/2000... Training loss: 2.1013\n",
      "Epoch: 283/2000... Training loss: 2.0041\n",
      "Epoch: 283/2000... Training loss: 2.3692\n",
      "Epoch: 283/2000... Training loss: 2.3296\n",
      "Epoch: 283/2000... Training loss: 2.2851\n",
      "Epoch: 283/2000... Training loss: 1.9719\n",
      "Epoch: 283/2000... Training loss: 1.9803\n",
      "Epoch: 283/2000... Training loss: 1.9505\n",
      "Epoch: 283/2000... Training loss: 2.0661\n",
      "Epoch: 283/2000... Training loss: 2.0337\n",
      "Epoch: 283/2000... Training loss: 2.1412\n",
      "Epoch: 283/2000... Training loss: 2.1610\n",
      "Epoch: 283/2000... Training loss: 1.9981\n",
      "Epoch: 283/2000... Training loss: 1.8332\n",
      "Epoch: 283/2000... Training loss: 2.1794\n",
      "Epoch: 283/2000... Training loss: 1.9861\n",
      "Epoch: 283/2000... Training loss: 2.2435\n",
      "Epoch: 283/2000... Training loss: 1.9737\n",
      "Epoch: 283/2000... Training loss: 2.4563\n",
      "Epoch: 284/2000... Training loss: 2.0231\n",
      "Epoch: 284/2000... Training loss: 2.0114\n",
      "Epoch: 284/2000... Training loss: 1.8917\n",
      "Epoch: 284/2000... Training loss: 2.0291\n",
      "Epoch: 284/2000... Training loss: 2.2101\n",
      "Epoch: 284/2000... Training loss: 2.1609\n",
      "Epoch: 284/2000... Training loss: 2.2610\n",
      "Epoch: 284/2000... Training loss: 1.9277\n",
      "Epoch: 284/2000... Training loss: 2.1798\n",
      "Epoch: 284/2000... Training loss: 2.1571\n",
      "Epoch: 284/2000... Training loss: 2.2129\n",
      "Epoch: 284/2000... Training loss: 1.8868\n",
      "Epoch: 284/2000... Training loss: 2.1336\n",
      "Epoch: 284/2000... Training loss: 2.1881\n",
      "Epoch: 284/2000... Training loss: 2.1240\n",
      "Epoch: 284/2000... Training loss: 1.8453\n",
      "Epoch: 284/2000... Training loss: 2.2497\n",
      "Epoch: 284/2000... Training loss: 2.0502\n",
      "Epoch: 284/2000... Training loss: 2.0593\n",
      "Epoch: 284/2000... Training loss: 2.0087\n",
      "Epoch: 284/2000... Training loss: 2.1541\n",
      "Epoch: 284/2000... Training loss: 2.0793\n",
      "Epoch: 284/2000... Training loss: 2.3029\n",
      "Epoch: 284/2000... Training loss: 1.9317\n",
      "Epoch: 284/2000... Training loss: 2.0591\n",
      "Epoch: 284/2000... Training loss: 1.9737\n",
      "Epoch: 284/2000... Training loss: 2.0794\n",
      "Epoch: 284/2000... Training loss: 2.0577\n",
      "Epoch: 284/2000... Training loss: 2.4313\n",
      "Epoch: 284/2000... Training loss: 2.2903\n",
      "Epoch: 284/2000... Training loss: 2.1458\n",
      "Epoch: 285/2000... Training loss: 2.0139\n",
      "Epoch: 285/2000... Training loss: 1.9943\n",
      "Epoch: 285/2000... Training loss: 2.0435\n",
      "Epoch: 285/2000... Training loss: 1.9867\n",
      "Epoch: 285/2000... Training loss: 1.8419\n",
      "Epoch: 285/2000... Training loss: 2.3048\n",
      "Epoch: 285/2000... Training loss: 2.1567\n",
      "Epoch: 285/2000... Training loss: 1.9320\n",
      "Epoch: 285/2000... Training loss: 2.0654\n",
      "Epoch: 285/2000... Training loss: 2.0665\n",
      "Epoch: 285/2000... Training loss: 2.4434\n",
      "Epoch: 285/2000... Training loss: 2.0413\n",
      "Epoch: 285/2000... Training loss: 2.1544\n",
      "Epoch: 285/2000... Training loss: 2.3742\n",
      "Epoch: 285/2000... Training loss: 1.9830\n",
      "Epoch: 285/2000... Training loss: 2.2121\n",
      "Epoch: 285/2000... Training loss: 2.1087\n",
      "Epoch: 285/2000... Training loss: 2.2011\n",
      "Epoch: 285/2000... Training loss: 2.0784\n",
      "Epoch: 285/2000... Training loss: 1.9598\n",
      "Epoch: 285/2000... Training loss: 2.3090\n",
      "Epoch: 285/2000... Training loss: 2.0712\n",
      "Epoch: 285/2000... Training loss: 2.0720\n",
      "Epoch: 285/2000... Training loss: 2.1315\n",
      "Epoch: 285/2000... Training loss: 2.2723\n",
      "Epoch: 285/2000... Training loss: 1.9062\n",
      "Epoch: 285/2000... Training loss: 2.0003\n",
      "Epoch: 285/2000... Training loss: 2.3023\n",
      "Epoch: 285/2000... Training loss: 1.8411\n",
      "Epoch: 285/2000... Training loss: 1.9392\n",
      "Epoch: 285/2000... Training loss: 2.1471\n",
      "Epoch: 286/2000... Training loss: 2.1700\n",
      "Epoch: 286/2000... Training loss: 2.2475\n",
      "Epoch: 286/2000... Training loss: 2.1492\n",
      "Epoch: 286/2000... Training loss: 2.0981\n",
      "Epoch: 286/2000... Training loss: 2.3777\n",
      "Epoch: 286/2000... Training loss: 2.2824\n",
      "Epoch: 286/2000... Training loss: 2.0813\n",
      "Epoch: 286/2000... Training loss: 2.1055\n",
      "Epoch: 286/2000... Training loss: 1.9132\n",
      "Epoch: 286/2000... Training loss: 2.2965\n",
      "Epoch: 286/2000... Training loss: 2.2426\n",
      "Epoch: 286/2000... Training loss: 2.0623\n",
      "Epoch: 286/2000... Training loss: 2.2506\n",
      "Epoch: 286/2000... Training loss: 2.2630\n",
      "Epoch: 286/2000... Training loss: 2.0684\n",
      "Epoch: 286/2000... Training loss: 1.9760\n",
      "Epoch: 286/2000... Training loss: 2.3791\n",
      "Epoch: 286/2000... Training loss: 2.1340\n",
      "Epoch: 286/2000... Training loss: 1.8950\n",
      "Epoch: 286/2000... Training loss: 2.1887\n",
      "Epoch: 286/2000... Training loss: 2.0666\n",
      "Epoch: 286/2000... Training loss: 2.1247\n",
      "Epoch: 286/2000... Training loss: 2.1912\n",
      "Epoch: 286/2000... Training loss: 2.2640\n",
      "Epoch: 286/2000... Training loss: 2.1234\n",
      "Epoch: 286/2000... Training loss: 2.1108\n",
      "Epoch: 286/2000... Training loss: 2.1716\n",
      "Epoch: 286/2000... Training loss: 1.9085\n",
      "Epoch: 286/2000... Training loss: 2.1440\n",
      "Epoch: 286/2000... Training loss: 2.2348\n",
      "Epoch: 286/2000... Training loss: 1.9720\n",
      "Epoch: 287/2000... Training loss: 1.9266\n",
      "Epoch: 287/2000... Training loss: 2.0353\n",
      "Epoch: 287/2000... Training loss: 2.1365\n",
      "Epoch: 287/2000... Training loss: 1.9265\n",
      "Epoch: 287/2000... Training loss: 1.8293\n",
      "Epoch: 287/2000... Training loss: 2.0762\n",
      "Epoch: 287/2000... Training loss: 1.9641\n",
      "Epoch: 287/2000... Training loss: 2.1503\n",
      "Epoch: 287/2000... Training loss: 1.9816\n",
      "Epoch: 287/2000... Training loss: 2.1742\n",
      "Epoch: 287/2000... Training loss: 2.0949\n",
      "Epoch: 287/2000... Training loss: 1.9011\n",
      "Epoch: 287/2000... Training loss: 1.9343\n",
      "Epoch: 287/2000... Training loss: 1.9737\n",
      "Epoch: 287/2000... Training loss: 2.3532\n",
      "Epoch: 287/2000... Training loss: 2.1214\n",
      "Epoch: 287/2000... Training loss: 2.1681\n",
      "Epoch: 287/2000... Training loss: 1.7835\n",
      "Epoch: 287/2000... Training loss: 2.5580\n",
      "Epoch: 287/2000... Training loss: 2.2291\n",
      "Epoch: 287/2000... Training loss: 2.1628\n",
      "Epoch: 287/2000... Training loss: 2.2129\n",
      "Epoch: 287/2000... Training loss: 1.9496\n",
      "Epoch: 287/2000... Training loss: 2.2118\n",
      "Epoch: 287/2000... Training loss: 2.1870\n",
      "Epoch: 287/2000... Training loss: 2.0447\n",
      "Epoch: 287/2000... Training loss: 2.1226\n",
      "Epoch: 287/2000... Training loss: 2.0775\n",
      "Epoch: 287/2000... Training loss: 2.1735\n",
      "Epoch: 287/2000... Training loss: 1.9204\n",
      "Epoch: 287/2000... Training loss: 2.3297\n",
      "Epoch: 288/2000... Training loss: 1.9508\n",
      "Epoch: 288/2000... Training loss: 2.0980\n",
      "Epoch: 288/2000... Training loss: 1.9768\n",
      "Epoch: 288/2000... Training loss: 2.1701\n",
      "Epoch: 288/2000... Training loss: 2.0301\n",
      "Epoch: 288/2000... Training loss: 1.9731\n",
      "Epoch: 288/2000... Training loss: 1.9953\n",
      "Epoch: 288/2000... Training loss: 1.9452\n",
      "Epoch: 288/2000... Training loss: 1.9285\n",
      "Epoch: 288/2000... Training loss: 2.3453\n",
      "Epoch: 288/2000... Training loss: 2.2938\n",
      "Epoch: 288/2000... Training loss: 2.2644\n",
      "Epoch: 288/2000... Training loss: 2.0807\n",
      "Epoch: 288/2000... Training loss: 2.1046\n",
      "Epoch: 288/2000... Training loss: 2.1685\n",
      "Epoch: 288/2000... Training loss: 1.8656\n",
      "Epoch: 288/2000... Training loss: 2.3313\n",
      "Epoch: 288/2000... Training loss: 2.0648\n",
      "Epoch: 288/2000... Training loss: 2.3487\n",
      "Epoch: 288/2000... Training loss: 2.2042\n",
      "Epoch: 288/2000... Training loss: 2.1518\n",
      "Epoch: 288/2000... Training loss: 2.3333\n",
      "Epoch: 288/2000... Training loss: 2.0811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 288/2000... Training loss: 1.9241\n",
      "Epoch: 288/2000... Training loss: 2.1294\n",
      "Epoch: 288/2000... Training loss: 2.0982\n",
      "Epoch: 288/2000... Training loss: 2.0993\n",
      "Epoch: 288/2000... Training loss: 2.0552\n",
      "Epoch: 288/2000... Training loss: 2.0604\n",
      "Epoch: 288/2000... Training loss: 2.0079\n",
      "Epoch: 288/2000... Training loss: 2.1253\n",
      "Epoch: 289/2000... Training loss: 1.9102\n",
      "Epoch: 289/2000... Training loss: 2.1733\n",
      "Epoch: 289/2000... Training loss: 2.1418\n",
      "Epoch: 289/2000... Training loss: 1.9192\n",
      "Epoch: 289/2000... Training loss: 2.3096\n",
      "Epoch: 289/2000... Training loss: 2.1709\n",
      "Epoch: 289/2000... Training loss: 2.0274\n",
      "Epoch: 289/2000... Training loss: 2.0117\n",
      "Epoch: 289/2000... Training loss: 2.1047\n",
      "Epoch: 289/2000... Training loss: 1.9979\n",
      "Epoch: 289/2000... Training loss: 2.1916\n",
      "Epoch: 289/2000... Training loss: 2.0101\n",
      "Epoch: 289/2000... Training loss: 1.9479\n",
      "Epoch: 289/2000... Training loss: 1.9175\n",
      "Epoch: 289/2000... Training loss: 2.2660\n",
      "Epoch: 289/2000... Training loss: 1.8977\n",
      "Epoch: 289/2000... Training loss: 2.2335\n",
      "Epoch: 289/2000... Training loss: 2.0690\n",
      "Epoch: 289/2000... Training loss: 2.3333\n",
      "Epoch: 289/2000... Training loss: 2.2596\n",
      "Epoch: 289/2000... Training loss: 1.9150\n",
      "Epoch: 289/2000... Training loss: 1.9700\n",
      "Epoch: 289/2000... Training loss: 2.1195\n",
      "Epoch: 289/2000... Training loss: 2.0464\n",
      "Epoch: 289/2000... Training loss: 2.2574\n",
      "Epoch: 289/2000... Training loss: 1.9760\n",
      "Epoch: 289/2000... Training loss: 2.2490\n",
      "Epoch: 289/2000... Training loss: 1.9828\n",
      "Epoch: 289/2000... Training loss: 1.8230\n",
      "Epoch: 289/2000... Training loss: 2.1481\n",
      "Epoch: 289/2000... Training loss: 2.0978\n",
      "Epoch: 290/2000... Training loss: 2.0675\n",
      "Epoch: 290/2000... Training loss: 2.3308\n",
      "Epoch: 290/2000... Training loss: 2.1786\n",
      "Epoch: 290/2000... Training loss: 2.1192\n",
      "Epoch: 290/2000... Training loss: 1.9952\n",
      "Epoch: 290/2000... Training loss: 1.9336\n",
      "Epoch: 290/2000... Training loss: 1.9882\n",
      "Epoch: 290/2000... Training loss: 2.1467\n",
      "Epoch: 290/2000... Training loss: 2.1369\n",
      "Epoch: 290/2000... Training loss: 2.2789\n",
      "Epoch: 290/2000... Training loss: 2.1254\n",
      "Epoch: 290/2000... Training loss: 2.3524\n",
      "Epoch: 290/2000... Training loss: 2.0433\n",
      "Epoch: 290/2000... Training loss: 2.0447\n",
      "Epoch: 290/2000... Training loss: 2.1559\n",
      "Epoch: 290/2000... Training loss: 2.1100\n",
      "Epoch: 290/2000... Training loss: 1.7216\n",
      "Epoch: 290/2000... Training loss: 1.9706\n",
      "Epoch: 290/2000... Training loss: 2.0092\n",
      "Epoch: 290/2000... Training loss: 2.0546\n",
      "Epoch: 290/2000... Training loss: 2.1241\n",
      "Epoch: 290/2000... Training loss: 1.9771\n",
      "Epoch: 290/2000... Training loss: 2.0459\n",
      "Epoch: 290/2000... Training loss: 1.9209\n",
      "Epoch: 290/2000... Training loss: 2.1578\n",
      "Epoch: 290/2000... Training loss: 2.1520\n",
      "Epoch: 290/2000... Training loss: 2.1386\n",
      "Epoch: 290/2000... Training loss: 2.2129\n",
      "Epoch: 290/2000... Training loss: 2.1192\n",
      "Epoch: 290/2000... Training loss: 1.9698\n",
      "Epoch: 290/2000... Training loss: 2.1191\n",
      "Epoch: 291/2000... Training loss: 2.1004\n",
      "Epoch: 291/2000... Training loss: 1.9631\n",
      "Epoch: 291/2000... Training loss: 2.3821\n",
      "Epoch: 291/2000... Training loss: 2.0933\n",
      "Epoch: 291/2000... Training loss: 2.0461\n",
      "Epoch: 291/2000... Training loss: 2.1339\n",
      "Epoch: 291/2000... Training loss: 1.9461\n",
      "Epoch: 291/2000... Training loss: 2.1344\n",
      "Epoch: 291/2000... Training loss: 1.9615\n",
      "Epoch: 291/2000... Training loss: 1.9800\n",
      "Epoch: 291/2000... Training loss: 2.0073\n",
      "Epoch: 291/2000... Training loss: 2.0743\n",
      "Epoch: 291/2000... Training loss: 2.2696\n",
      "Epoch: 291/2000... Training loss: 2.2456\n",
      "Epoch: 291/2000... Training loss: 2.2137\n",
      "Epoch: 291/2000... Training loss: 1.9511\n",
      "Epoch: 291/2000... Training loss: 2.0543\n",
      "Epoch: 291/2000... Training loss: 1.9023\n",
      "Epoch: 291/2000... Training loss: 1.9953\n",
      "Epoch: 291/2000... Training loss: 2.2894\n",
      "Epoch: 291/2000... Training loss: 2.0731\n",
      "Epoch: 291/2000... Training loss: 2.1377\n",
      "Epoch: 291/2000... Training loss: 2.0360\n",
      "Epoch: 291/2000... Training loss: 2.1718\n",
      "Epoch: 291/2000... Training loss: 2.1012\n",
      "Epoch: 291/2000... Training loss: 1.9447\n",
      "Epoch: 291/2000... Training loss: 1.9376\n",
      "Epoch: 291/2000... Training loss: 2.0686\n",
      "Epoch: 291/2000... Training loss: 1.9778\n",
      "Epoch: 291/2000... Training loss: 2.1666\n",
      "Epoch: 291/2000... Training loss: 2.1702\n",
      "Epoch: 292/2000... Training loss: 2.1338\n",
      "Epoch: 292/2000... Training loss: 2.2284\n",
      "Epoch: 292/2000... Training loss: 2.1706\n",
      "Epoch: 292/2000... Training loss: 2.0477\n",
      "Epoch: 292/2000... Training loss: 1.8675\n",
      "Epoch: 292/2000... Training loss: 2.0155\n",
      "Epoch: 292/2000... Training loss: 2.0593\n",
      "Epoch: 292/2000... Training loss: 2.2423\n",
      "Epoch: 292/2000... Training loss: 2.0239\n",
      "Epoch: 292/2000... Training loss: 2.2520\n",
      "Epoch: 292/2000... Training loss: 2.0067\n",
      "Epoch: 292/2000... Training loss: 1.9816\n",
      "Epoch: 292/2000... Training loss: 1.9509\n",
      "Epoch: 292/2000... Training loss: 2.0270\n",
      "Epoch: 292/2000... Training loss: 1.9907\n",
      "Epoch: 292/2000... Training loss: 2.2027\n",
      "Epoch: 292/2000... Training loss: 2.0742\n",
      "Epoch: 292/2000... Training loss: 2.0657\n",
      "Epoch: 292/2000... Training loss: 2.1941\n",
      "Epoch: 292/2000... Training loss: 1.9279\n",
      "Epoch: 292/2000... Training loss: 2.0224\n",
      "Epoch: 292/2000... Training loss: 1.8542\n",
      "Epoch: 292/2000... Training loss: 2.0644\n",
      "Epoch: 292/2000... Training loss: 1.8847\n",
      "Epoch: 292/2000... Training loss: 2.1313\n",
      "Epoch: 292/2000... Training loss: 2.0796\n",
      "Epoch: 292/2000... Training loss: 2.0549\n",
      "Epoch: 292/2000... Training loss: 2.1231\n",
      "Epoch: 292/2000... Training loss: 1.9643\n",
      "Epoch: 292/2000... Training loss: 2.2828\n",
      "Epoch: 292/2000... Training loss: 2.2182\n",
      "Epoch: 293/2000... Training loss: 2.1447\n",
      "Epoch: 293/2000... Training loss: 2.0213\n",
      "Epoch: 293/2000... Training loss: 2.1753\n",
      "Epoch: 293/2000... Training loss: 1.9998\n",
      "Epoch: 293/2000... Training loss: 1.9336\n",
      "Epoch: 293/2000... Training loss: 2.1461\n",
      "Epoch: 293/2000... Training loss: 1.9041\n",
      "Epoch: 293/2000... Training loss: 2.1800\n",
      "Epoch: 293/2000... Training loss: 1.9178\n",
      "Epoch: 293/2000... Training loss: 2.0768\n",
      "Epoch: 293/2000... Training loss: 2.1311\n",
      "Epoch: 293/2000... Training loss: 2.1985\n",
      "Epoch: 293/2000... Training loss: 1.9046\n",
      "Epoch: 293/2000... Training loss: 1.9447\n",
      "Epoch: 293/2000... Training loss: 1.9795\n",
      "Epoch: 293/2000... Training loss: 2.1184\n",
      "Epoch: 293/2000... Training loss: 2.1543\n",
      "Epoch: 293/2000... Training loss: 1.9777\n",
      "Epoch: 293/2000... Training loss: 2.1382\n",
      "Epoch: 293/2000... Training loss: 2.2309\n",
      "Epoch: 293/2000... Training loss: 2.0116\n",
      "Epoch: 293/2000... Training loss: 2.0204\n",
      "Epoch: 293/2000... Training loss: 2.1286\n",
      "Epoch: 293/2000... Training loss: 2.0483\n",
      "Epoch: 293/2000... Training loss: 2.0980\n",
      "Epoch: 293/2000... Training loss: 2.0053\n",
      "Epoch: 293/2000... Training loss: 2.0239\n",
      "Epoch: 293/2000... Training loss: 2.0154\n",
      "Epoch: 293/2000... Training loss: 1.9650\n",
      "Epoch: 293/2000... Training loss: 2.4273\n",
      "Epoch: 293/2000... Training loss: 2.1520\n",
      "Epoch: 294/2000... Training loss: 1.9105\n",
      "Epoch: 294/2000... Training loss: 2.0490\n",
      "Epoch: 294/2000... Training loss: 1.9278\n",
      "Epoch: 294/2000... Training loss: 1.7247\n",
      "Epoch: 294/2000... Training loss: 2.0859\n",
      "Epoch: 294/2000... Training loss: 1.9627\n",
      "Epoch: 294/2000... Training loss: 1.8202\n",
      "Epoch: 294/2000... Training loss: 1.7658\n",
      "Epoch: 294/2000... Training loss: 2.0926\n",
      "Epoch: 294/2000... Training loss: 2.1951\n",
      "Epoch: 294/2000... Training loss: 2.1369\n",
      "Epoch: 294/2000... Training loss: 1.8685\n",
      "Epoch: 294/2000... Training loss: 1.9772\n",
      "Epoch: 294/2000... Training loss: 2.3628\n",
      "Epoch: 294/2000... Training loss: 2.0197\n",
      "Epoch: 294/2000... Training loss: 2.2151\n",
      "Epoch: 294/2000... Training loss: 1.9769\n",
      "Epoch: 294/2000... Training loss: 2.0792\n",
      "Epoch: 294/2000... Training loss: 1.9070\n",
      "Epoch: 294/2000... Training loss: 2.2329\n",
      "Epoch: 294/2000... Training loss: 2.0304\n",
      "Epoch: 294/2000... Training loss: 2.0693\n",
      "Epoch: 294/2000... Training loss: 2.1147\n",
      "Epoch: 294/2000... Training loss: 2.1642\n",
      "Epoch: 294/2000... Training loss: 2.0367\n",
      "Epoch: 294/2000... Training loss: 2.0546\n",
      "Epoch: 294/2000... Training loss: 1.8948\n",
      "Epoch: 294/2000... Training loss: 2.2676\n",
      "Epoch: 294/2000... Training loss: 2.0429\n",
      "Epoch: 294/2000... Training loss: 2.1076\n",
      "Epoch: 294/2000... Training loss: 2.1685\n",
      "Epoch: 295/2000... Training loss: 2.3003\n",
      "Epoch: 295/2000... Training loss: 1.9148\n",
      "Epoch: 295/2000... Training loss: 1.9084\n",
      "Epoch: 295/2000... Training loss: 1.9895\n",
      "Epoch: 295/2000... Training loss: 2.1893\n",
      "Epoch: 295/2000... Training loss: 2.0984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 295/2000... Training loss: 2.2799\n",
      "Epoch: 295/2000... Training loss: 2.0132\n",
      "Epoch: 295/2000... Training loss: 1.8648\n",
      "Epoch: 295/2000... Training loss: 2.0192\n",
      "Epoch: 295/2000... Training loss: 2.0384\n",
      "Epoch: 295/2000... Training loss: 2.0438\n",
      "Epoch: 295/2000... Training loss: 2.0122\n",
      "Epoch: 295/2000... Training loss: 1.9843\n",
      "Epoch: 295/2000... Training loss: 2.1608\n",
      "Epoch: 295/2000... Training loss: 1.9918\n",
      "Epoch: 295/2000... Training loss: 2.0415\n",
      "Epoch: 295/2000... Training loss: 2.1439\n",
      "Epoch: 295/2000... Training loss: 2.1344\n",
      "Epoch: 295/2000... Training loss: 1.9875\n",
      "Epoch: 295/2000... Training loss: 2.1460\n",
      "Epoch: 295/2000... Training loss: 1.9334\n",
      "Epoch: 295/2000... Training loss: 2.1269\n",
      "Epoch: 295/2000... Training loss: 2.1494\n",
      "Epoch: 295/2000... Training loss: 2.3099\n",
      "Epoch: 295/2000... Training loss: 2.0149\n",
      "Epoch: 295/2000... Training loss: 1.9620\n",
      "Epoch: 295/2000... Training loss: 1.9942\n",
      "Epoch: 295/2000... Training loss: 2.0607\n",
      "Epoch: 295/2000... Training loss: 1.9611\n",
      "Epoch: 295/2000... Training loss: 2.1957\n",
      "Epoch: 296/2000... Training loss: 2.0008\n",
      "Epoch: 296/2000... Training loss: 2.0831\n",
      "Epoch: 296/2000... Training loss: 2.0039\n",
      "Epoch: 296/2000... Training loss: 1.7533\n",
      "Epoch: 296/2000... Training loss: 1.8765\n",
      "Epoch: 296/2000... Training loss: 2.1770\n",
      "Epoch: 296/2000... Training loss: 2.1468\n",
      "Epoch: 296/2000... Training loss: 2.0905\n",
      "Epoch: 296/2000... Training loss: 1.9940\n",
      "Epoch: 296/2000... Training loss: 2.0443\n",
      "Epoch: 296/2000... Training loss: 1.9749\n",
      "Epoch: 296/2000... Training loss: 2.3972\n",
      "Epoch: 296/2000... Training loss: 2.0379\n",
      "Epoch: 296/2000... Training loss: 2.1870\n",
      "Epoch: 296/2000... Training loss: 2.1704\n",
      "Epoch: 296/2000... Training loss: 2.0843\n",
      "Epoch: 296/2000... Training loss: 1.9990\n",
      "Epoch: 296/2000... Training loss: 2.1366\n",
      "Epoch: 296/2000... Training loss: 1.9557\n",
      "Epoch: 296/2000... Training loss: 1.9713\n",
      "Epoch: 296/2000... Training loss: 2.3352\n",
      "Epoch: 296/2000... Training loss: 2.0360\n",
      "Epoch: 296/2000... Training loss: 2.2110\n",
      "Epoch: 296/2000... Training loss: 2.0299\n",
      "Epoch: 296/2000... Training loss: 1.9812\n",
      "Epoch: 296/2000... Training loss: 1.8722\n",
      "Epoch: 296/2000... Training loss: 2.1534\n",
      "Epoch: 296/2000... Training loss: 2.1769\n",
      "Epoch: 296/2000... Training loss: 1.8189\n",
      "Epoch: 296/2000... Training loss: 1.8786\n",
      "Epoch: 296/2000... Training loss: 2.1066\n",
      "Epoch: 297/2000... Training loss: 1.7443\n",
      "Epoch: 297/2000... Training loss: 2.2320\n",
      "Epoch: 297/2000... Training loss: 2.0254\n",
      "Epoch: 297/2000... Training loss: 2.0793\n",
      "Epoch: 297/2000... Training loss: 2.0159\n",
      "Epoch: 297/2000... Training loss: 2.2227\n",
      "Epoch: 297/2000... Training loss: 1.7261\n",
      "Epoch: 297/2000... Training loss: 1.7656\n",
      "Epoch: 297/2000... Training loss: 1.9371\n",
      "Epoch: 297/2000... Training loss: 2.1928\n",
      "Epoch: 297/2000... Training loss: 2.0777\n",
      "Epoch: 297/2000... Training loss: 2.2486\n",
      "Epoch: 297/2000... Training loss: 2.0914\n",
      "Epoch: 297/2000... Training loss: 1.9450\n",
      "Epoch: 297/2000... Training loss: 2.2745\n",
      "Epoch: 297/2000... Training loss: 2.3639\n",
      "Epoch: 297/2000... Training loss: 1.8196\n",
      "Epoch: 297/2000... Training loss: 1.8328\n",
      "Epoch: 297/2000... Training loss: 1.9595\n",
      "Epoch: 297/2000... Training loss: 2.2174\n",
      "Epoch: 297/2000... Training loss: 2.2279\n",
      "Epoch: 297/2000... Training loss: 1.9881\n",
      "Epoch: 297/2000... Training loss: 2.2247\n",
      "Epoch: 297/2000... Training loss: 2.4135\n",
      "Epoch: 297/2000... Training loss: 1.9444\n",
      "Epoch: 297/2000... Training loss: 1.9430\n",
      "Epoch: 297/2000... Training loss: 2.1647\n",
      "Epoch: 297/2000... Training loss: 1.7686\n",
      "Epoch: 297/2000... Training loss: 2.0742\n",
      "Epoch: 297/2000... Training loss: 2.0078\n",
      "Epoch: 297/2000... Training loss: 1.9545\n",
      "Epoch: 298/2000... Training loss: 1.9059\n",
      "Epoch: 298/2000... Training loss: 1.9101\n",
      "Epoch: 298/2000... Training loss: 2.1263\n",
      "Epoch: 298/2000... Training loss: 1.9245\n",
      "Epoch: 298/2000... Training loss: 1.9130\n",
      "Epoch: 298/2000... Training loss: 2.0164\n",
      "Epoch: 298/2000... Training loss: 2.2802\n",
      "Epoch: 298/2000... Training loss: 2.0746\n",
      "Epoch: 298/2000... Training loss: 1.6373\n",
      "Epoch: 298/2000... Training loss: 2.1170\n",
      "Epoch: 298/2000... Training loss: 2.1429\n",
      "Epoch: 298/2000... Training loss: 1.8274\n",
      "Epoch: 298/2000... Training loss: 2.0252\n",
      "Epoch: 298/2000... Training loss: 1.8633\n",
      "Epoch: 298/2000... Training loss: 1.9121\n",
      "Epoch: 298/2000... Training loss: 2.0360\n",
      "Epoch: 298/2000... Training loss: 2.1289\n",
      "Epoch: 298/2000... Training loss: 1.8730\n",
      "Epoch: 298/2000... Training loss: 1.7469\n",
      "Epoch: 298/2000... Training loss: 1.9714\n",
      "Epoch: 298/2000... Training loss: 1.9405\n",
      "Epoch: 298/2000... Training loss: 1.9693\n",
      "Epoch: 298/2000... Training loss: 2.0512\n",
      "Epoch: 298/2000... Training loss: 1.8931\n",
      "Epoch: 298/2000... Training loss: 2.0933\n",
      "Epoch: 298/2000... Training loss: 1.8204\n",
      "Epoch: 298/2000... Training loss: 2.0843\n",
      "Epoch: 298/2000... Training loss: 1.9946\n",
      "Epoch: 298/2000... Training loss: 2.0889\n",
      "Epoch: 298/2000... Training loss: 2.0966\n",
      "Epoch: 298/2000... Training loss: 1.6940\n",
      "Epoch: 299/2000... Training loss: 1.8904\n",
      "Epoch: 299/2000... Training loss: 2.1033\n",
      "Epoch: 299/2000... Training loss: 2.0074\n",
      "Epoch: 299/2000... Training loss: 1.6865\n",
      "Epoch: 299/2000... Training loss: 2.1311\n",
      "Epoch: 299/2000... Training loss: 2.1125\n",
      "Epoch: 299/2000... Training loss: 1.9386\n",
      "Epoch: 299/2000... Training loss: 2.3706\n",
      "Epoch: 299/2000... Training loss: 2.2106\n",
      "Epoch: 299/2000... Training loss: 2.1316\n",
      "Epoch: 299/2000... Training loss: 2.0845\n",
      "Epoch: 299/2000... Training loss: 1.7760\n",
      "Epoch: 299/2000... Training loss: 2.0198\n",
      "Epoch: 299/2000... Training loss: 2.0800\n",
      "Epoch: 299/2000... Training loss: 2.1881\n",
      "Epoch: 299/2000... Training loss: 1.9825\n",
      "Epoch: 299/2000... Training loss: 2.2204\n",
      "Epoch: 299/2000... Training loss: 2.0062\n",
      "Epoch: 299/2000... Training loss: 2.1458\n",
      "Epoch: 299/2000... Training loss: 2.0973\n",
      "Epoch: 299/2000... Training loss: 1.8264\n",
      "Epoch: 299/2000... Training loss: 2.0379\n",
      "Epoch: 299/2000... Training loss: 2.1348\n",
      "Epoch: 299/2000... Training loss: 2.0906\n",
      "Epoch: 299/2000... Training loss: 2.0334\n",
      "Epoch: 299/2000... Training loss: 2.1458\n",
      "Epoch: 299/2000... Training loss: 2.0229\n",
      "Epoch: 299/2000... Training loss: 2.1991\n",
      "Epoch: 299/2000... Training loss: 1.9834\n",
      "Epoch: 299/2000... Training loss: 1.9482\n",
      "Epoch: 299/2000... Training loss: 2.3119\n",
      "Epoch: 300/2000... Training loss: 2.0588\n",
      "Epoch: 300/2000... Training loss: 1.9307\n",
      "Epoch: 300/2000... Training loss: 1.9107\n",
      "Epoch: 300/2000... Training loss: 2.2243\n",
      "Epoch: 300/2000... Training loss: 1.8947\n",
      "Epoch: 300/2000... Training loss: 1.8692\n",
      "Epoch: 300/2000... Training loss: 1.9794\n",
      "Epoch: 300/2000... Training loss: 2.2053\n",
      "Epoch: 300/2000... Training loss: 2.2677\n",
      "Epoch: 300/2000... Training loss: 2.0809\n",
      "Epoch: 300/2000... Training loss: 1.8275\n",
      "Epoch: 300/2000... Training loss: 1.8261\n",
      "Epoch: 300/2000... Training loss: 2.1118\n",
      "Epoch: 300/2000... Training loss: 2.0112\n",
      "Epoch: 300/2000... Training loss: 2.3040\n",
      "Epoch: 300/2000... Training loss: 1.9607\n",
      "Epoch: 300/2000... Training loss: 2.0627\n",
      "Epoch: 300/2000... Training loss: 1.9318\n",
      "Epoch: 300/2000... Training loss: 2.1123\n",
      "Epoch: 300/2000... Training loss: 2.1663\n",
      "Epoch: 300/2000... Training loss: 1.9917\n",
      "Epoch: 300/2000... Training loss: 1.8578\n",
      "Epoch: 300/2000... Training loss: 2.0353\n",
      "Epoch: 300/2000... Training loss: 1.8705\n",
      "Epoch: 300/2000... Training loss: 1.9852\n",
      "Epoch: 300/2000... Training loss: 1.8143\n",
      "Epoch: 300/2000... Training loss: 2.0934\n",
      "Epoch: 300/2000... Training loss: 2.0168\n",
      "Epoch: 300/2000... Training loss: 2.2491\n",
      "Epoch: 300/2000... Training loss: 2.0292\n",
      "Epoch: 300/2000... Training loss: 2.0079\n",
      "Epoch: 301/2000... Training loss: 2.0230\n",
      "Epoch: 301/2000... Training loss: 1.9112\n",
      "Epoch: 301/2000... Training loss: 2.2736\n",
      "Epoch: 301/2000... Training loss: 2.0361\n",
      "Epoch: 301/2000... Training loss: 1.8728\n",
      "Epoch: 301/2000... Training loss: 2.3894\n",
      "Epoch: 301/2000... Training loss: 2.0490\n",
      "Epoch: 301/2000... Training loss: 2.0532\n",
      "Epoch: 301/2000... Training loss: 1.8511\n",
      "Epoch: 301/2000... Training loss: 1.8491\n",
      "Epoch: 301/2000... Training loss: 1.9561\n",
      "Epoch: 301/2000... Training loss: 1.7410\n",
      "Epoch: 301/2000... Training loss: 1.7686\n",
      "Epoch: 301/2000... Training loss: 2.1196\n",
      "Epoch: 301/2000... Training loss: 2.2799\n",
      "Epoch: 301/2000... Training loss: 1.9654\n",
      "Epoch: 301/2000... Training loss: 1.8512\n",
      "Epoch: 301/2000... Training loss: 2.0525\n",
      "Epoch: 301/2000... Training loss: 1.9069\n",
      "Epoch: 301/2000... Training loss: 2.0002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301/2000... Training loss: 1.9022\n",
      "Epoch: 301/2000... Training loss: 2.0766\n",
      "Epoch: 301/2000... Training loss: 2.1710\n",
      "Epoch: 301/2000... Training loss: 2.1231\n",
      "Epoch: 301/2000... Training loss: 1.9292\n",
      "Epoch: 301/2000... Training loss: 1.6422\n",
      "Epoch: 301/2000... Training loss: 1.8824\n",
      "Epoch: 301/2000... Training loss: 1.8590\n",
      "Epoch: 301/2000... Training loss: 1.7529\n",
      "Epoch: 301/2000... Training loss: 1.9111\n",
      "Epoch: 301/2000... Training loss: 2.0212\n",
      "Epoch: 302/2000... Training loss: 1.9572\n",
      "Epoch: 302/2000... Training loss: 2.0060\n",
      "Epoch: 302/2000... Training loss: 1.8892\n",
      "Epoch: 302/2000... Training loss: 1.9800\n",
      "Epoch: 302/2000... Training loss: 1.9355\n",
      "Epoch: 302/2000... Training loss: 1.8607\n",
      "Epoch: 302/2000... Training loss: 2.1362\n",
      "Epoch: 302/2000... Training loss: 1.9249\n",
      "Epoch: 302/2000... Training loss: 2.0588\n",
      "Epoch: 302/2000... Training loss: 2.1503\n",
      "Epoch: 302/2000... Training loss: 2.1392\n",
      "Epoch: 302/2000... Training loss: 1.9883\n",
      "Epoch: 302/2000... Training loss: 2.1283\n",
      "Epoch: 302/2000... Training loss: 1.9757\n",
      "Epoch: 302/2000... Training loss: 1.9153\n",
      "Epoch: 302/2000... Training loss: 2.1536\n",
      "Epoch: 302/2000... Training loss: 2.0819\n",
      "Epoch: 302/2000... Training loss: 1.8305\n",
      "Epoch: 302/2000... Training loss: 1.7910\n",
      "Epoch: 302/2000... Training loss: 2.0217\n",
      "Epoch: 302/2000... Training loss: 2.0929\n",
      "Epoch: 302/2000... Training loss: 1.9171\n",
      "Epoch: 302/2000... Training loss: 1.9860\n",
      "Epoch: 302/2000... Training loss: 1.9806\n",
      "Epoch: 302/2000... Training loss: 2.1629\n",
      "Epoch: 302/2000... Training loss: 1.9932\n",
      "Epoch: 302/2000... Training loss: 1.9665\n",
      "Epoch: 302/2000... Training loss: 1.7745\n",
      "Epoch: 302/2000... Training loss: 2.0839\n",
      "Epoch: 302/2000... Training loss: 1.8090\n",
      "Epoch: 302/2000... Training loss: 2.0510\n",
      "Epoch: 303/2000... Training loss: 1.8285\n",
      "Epoch: 303/2000... Training loss: 2.0214\n",
      "Epoch: 303/2000... Training loss: 1.8576\n",
      "Epoch: 303/2000... Training loss: 2.0879\n",
      "Epoch: 303/2000... Training loss: 2.3392\n",
      "Epoch: 303/2000... Training loss: 1.8967\n",
      "Epoch: 303/2000... Training loss: 2.0785\n",
      "Epoch: 303/2000... Training loss: 1.8740\n",
      "Epoch: 303/2000... Training loss: 1.8469\n",
      "Epoch: 303/2000... Training loss: 1.9609\n",
      "Epoch: 303/2000... Training loss: 2.0621\n",
      "Epoch: 303/2000... Training loss: 2.0506\n",
      "Epoch: 303/2000... Training loss: 2.0042\n",
      "Epoch: 303/2000... Training loss: 2.0023\n",
      "Epoch: 303/2000... Training loss: 1.8695\n",
      "Epoch: 303/2000... Training loss: 1.7959\n",
      "Epoch: 303/2000... Training loss: 1.9267\n",
      "Epoch: 303/2000... Training loss: 1.7589\n",
      "Epoch: 303/2000... Training loss: 2.1689\n",
      "Epoch: 303/2000... Training loss: 2.1720\n",
      "Epoch: 303/2000... Training loss: 1.9269\n",
      "Epoch: 303/2000... Training loss: 2.1671\n",
      "Epoch: 303/2000... Training loss: 2.0410\n",
      "Epoch: 303/2000... Training loss: 2.0343\n",
      "Epoch: 303/2000... Training loss: 1.9231\n",
      "Epoch: 303/2000... Training loss: 1.6395\n",
      "Epoch: 303/2000... Training loss: 2.0587\n",
      "Epoch: 303/2000... Training loss: 1.7749\n",
      "Epoch: 303/2000... Training loss: 1.9755\n",
      "Epoch: 303/2000... Training loss: 2.2085\n",
      "Epoch: 303/2000... Training loss: 1.9586\n",
      "Epoch: 304/2000... Training loss: 1.6058\n",
      "Epoch: 304/2000... Training loss: 1.8719\n",
      "Epoch: 304/2000... Training loss: 1.7521\n",
      "Epoch: 304/2000... Training loss: 1.8228\n",
      "Epoch: 304/2000... Training loss: 2.1406\n",
      "Epoch: 304/2000... Training loss: 1.9028\n",
      "Epoch: 304/2000... Training loss: 1.9983\n",
      "Epoch: 304/2000... Training loss: 2.2076\n",
      "Epoch: 304/2000... Training loss: 1.9319\n",
      "Epoch: 304/2000... Training loss: 2.2648\n",
      "Epoch: 304/2000... Training loss: 1.9361\n",
      "Epoch: 304/2000... Training loss: 2.0390\n",
      "Epoch: 304/2000... Training loss: 2.0294\n",
      "Epoch: 304/2000... Training loss: 2.2718\n",
      "Epoch: 304/2000... Training loss: 2.0365\n",
      "Epoch: 304/2000... Training loss: 2.0291\n",
      "Epoch: 304/2000... Training loss: 1.8828\n",
      "Epoch: 304/2000... Training loss: 1.7252\n",
      "Epoch: 304/2000... Training loss: 2.0128\n",
      "Epoch: 304/2000... Training loss: 1.9363\n",
      "Epoch: 304/2000... Training loss: 1.8554\n",
      "Epoch: 304/2000... Training loss: 1.9374\n",
      "Epoch: 304/2000... Training loss: 2.0484\n",
      "Epoch: 304/2000... Training loss: 2.0742\n",
      "Epoch: 304/2000... Training loss: 1.8958\n",
      "Epoch: 304/2000... Training loss: 2.0288\n",
      "Epoch: 304/2000... Training loss: 1.8038\n",
      "Epoch: 304/2000... Training loss: 1.8879\n",
      "Epoch: 304/2000... Training loss: 1.8656\n",
      "Epoch: 304/2000... Training loss: 2.0936\n",
      "Epoch: 304/2000... Training loss: 1.8073\n",
      "Epoch: 305/2000... Training loss: 1.9641\n",
      "Epoch: 305/2000... Training loss: 1.8812\n",
      "Epoch: 305/2000... Training loss: 2.0054\n",
      "Epoch: 305/2000... Training loss: 1.9354\n",
      "Epoch: 305/2000... Training loss: 2.0549\n",
      "Epoch: 305/2000... Training loss: 1.9281\n",
      "Epoch: 305/2000... Training loss: 1.9716\n",
      "Epoch: 305/2000... Training loss: 1.8562\n",
      "Epoch: 305/2000... Training loss: 1.9176\n",
      "Epoch: 305/2000... Training loss: 2.0961\n",
      "Epoch: 305/2000... Training loss: 2.0706\n",
      "Epoch: 305/2000... Training loss: 2.0393\n",
      "Epoch: 305/2000... Training loss: 1.8685\n",
      "Epoch: 305/2000... Training loss: 1.8914\n",
      "Epoch: 305/2000... Training loss: 1.7164\n",
      "Epoch: 305/2000... Training loss: 1.9704\n",
      "Epoch: 305/2000... Training loss: 1.9716\n",
      "Epoch: 305/2000... Training loss: 2.0347\n",
      "Epoch: 305/2000... Training loss: 2.1448\n",
      "Epoch: 305/2000... Training loss: 1.8584\n",
      "Epoch: 305/2000... Training loss: 2.2299\n",
      "Epoch: 305/2000... Training loss: 1.8188\n",
      "Epoch: 305/2000... Training loss: 2.1236\n",
      "Epoch: 305/2000... Training loss: 2.1307\n",
      "Epoch: 305/2000... Training loss: 1.9831\n",
      "Epoch: 305/2000... Training loss: 1.9902\n",
      "Epoch: 305/2000... Training loss: 2.0793\n",
      "Epoch: 305/2000... Training loss: 1.7962\n",
      "Epoch: 305/2000... Training loss: 1.9779\n",
      "Epoch: 305/2000... Training loss: 1.8669\n",
      "Epoch: 305/2000... Training loss: 2.1183\n",
      "Epoch: 306/2000... Training loss: 1.8285\n",
      "Epoch: 306/2000... Training loss: 2.1797\n",
      "Epoch: 306/2000... Training loss: 1.9723\n",
      "Epoch: 306/2000... Training loss: 2.0113\n",
      "Epoch: 306/2000... Training loss: 2.1860\n",
      "Epoch: 306/2000... Training loss: 2.1461\n",
      "Epoch: 306/2000... Training loss: 1.9412\n",
      "Epoch: 306/2000... Training loss: 2.0885\n",
      "Epoch: 306/2000... Training loss: 2.0131\n",
      "Epoch: 306/2000... Training loss: 1.8936\n",
      "Epoch: 306/2000... Training loss: 1.9554\n",
      "Epoch: 306/2000... Training loss: 1.9913\n",
      "Epoch: 306/2000... Training loss: 1.9468\n",
      "Epoch: 306/2000... Training loss: 2.0375\n",
      "Epoch: 306/2000... Training loss: 2.0461\n",
      "Epoch: 306/2000... Training loss: 2.0681\n",
      "Epoch: 306/2000... Training loss: 2.1797\n",
      "Epoch: 306/2000... Training loss: 1.9449\n",
      "Epoch: 306/2000... Training loss: 1.7757\n",
      "Epoch: 306/2000... Training loss: 1.8277\n",
      "Epoch: 306/2000... Training loss: 1.8052\n",
      "Epoch: 306/2000... Training loss: 1.8828\n",
      "Epoch: 306/2000... Training loss: 1.9489\n",
      "Epoch: 306/2000... Training loss: 2.1839\n",
      "Epoch: 306/2000... Training loss: 1.8345\n",
      "Epoch: 306/2000... Training loss: 1.7928\n",
      "Epoch: 306/2000... Training loss: 1.7849\n",
      "Epoch: 306/2000... Training loss: 1.8321\n",
      "Epoch: 306/2000... Training loss: 2.0785\n",
      "Epoch: 306/2000... Training loss: 1.9905\n",
      "Epoch: 306/2000... Training loss: 2.0309\n",
      "Epoch: 307/2000... Training loss: 1.8831\n",
      "Epoch: 307/2000... Training loss: 1.8966\n",
      "Epoch: 307/2000... Training loss: 2.0086\n",
      "Epoch: 307/2000... Training loss: 1.7976\n",
      "Epoch: 307/2000... Training loss: 1.8394\n",
      "Epoch: 307/2000... Training loss: 2.0116\n",
      "Epoch: 307/2000... Training loss: 1.9739\n",
      "Epoch: 307/2000... Training loss: 1.8296\n",
      "Epoch: 307/2000... Training loss: 1.7786\n",
      "Epoch: 307/2000... Training loss: 1.9562\n",
      "Epoch: 307/2000... Training loss: 2.0832\n",
      "Epoch: 307/2000... Training loss: 2.0830\n",
      "Epoch: 307/2000... Training loss: 2.0737\n",
      "Epoch: 307/2000... Training loss: 2.2900\n",
      "Epoch: 307/2000... Training loss: 1.7745\n",
      "Epoch: 307/2000... Training loss: 1.8949\n",
      "Epoch: 307/2000... Training loss: 2.1528\n",
      "Epoch: 307/2000... Training loss: 1.9391\n",
      "Epoch: 307/2000... Training loss: 2.0956\n",
      "Epoch: 307/2000... Training loss: 2.1684\n",
      "Epoch: 307/2000... Training loss: 1.7472\n",
      "Epoch: 307/2000... Training loss: 2.0524\n",
      "Epoch: 307/2000... Training loss: 1.9118\n",
      "Epoch: 307/2000... Training loss: 1.9211\n",
      "Epoch: 307/2000... Training loss: 1.9440\n",
      "Epoch: 307/2000... Training loss: 1.8952\n",
      "Epoch: 307/2000... Training loss: 2.1693\n",
      "Epoch: 307/2000... Training loss: 1.9880\n",
      "Epoch: 307/2000... Training loss: 1.8831\n",
      "Epoch: 307/2000... Training loss: 1.9396\n",
      "Epoch: 307/2000... Training loss: 1.9315\n",
      "Epoch: 308/2000... Training loss: 1.9317\n",
      "Epoch: 308/2000... Training loss: 1.8255\n",
      "Epoch: 308/2000... Training loss: 2.0141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 308/2000... Training loss: 1.7115\n",
      "Epoch: 308/2000... Training loss: 2.1399\n",
      "Epoch: 308/2000... Training loss: 1.8886\n",
      "Epoch: 308/2000... Training loss: 1.9443\n",
      "Epoch: 308/2000... Training loss: 1.9436\n",
      "Epoch: 308/2000... Training loss: 1.9645\n",
      "Epoch: 308/2000... Training loss: 1.8527\n",
      "Epoch: 308/2000... Training loss: 2.1714\n",
      "Epoch: 308/2000... Training loss: 2.0804\n",
      "Epoch: 308/2000... Training loss: 1.8908\n",
      "Epoch: 308/2000... Training loss: 2.2923\n",
      "Epoch: 308/2000... Training loss: 2.0830\n",
      "Epoch: 308/2000... Training loss: 1.9474\n",
      "Epoch: 308/2000... Training loss: 1.8536\n",
      "Epoch: 308/2000... Training loss: 1.7720\n",
      "Epoch: 308/2000... Training loss: 2.0805\n",
      "Epoch: 308/2000... Training loss: 2.0128\n",
      "Epoch: 308/2000... Training loss: 1.8678\n",
      "Epoch: 308/2000... Training loss: 1.8267\n",
      "Epoch: 308/2000... Training loss: 1.7857\n",
      "Epoch: 308/2000... Training loss: 2.0137\n",
      "Epoch: 308/2000... Training loss: 1.9430\n",
      "Epoch: 308/2000... Training loss: 1.8394\n",
      "Epoch: 308/2000... Training loss: 2.0881\n",
      "Epoch: 308/2000... Training loss: 2.0427\n",
      "Epoch: 308/2000... Training loss: 1.7311\n",
      "Epoch: 308/2000... Training loss: 1.9112\n",
      "Epoch: 308/2000... Training loss: 2.0532\n",
      "Epoch: 309/2000... Training loss: 1.8131\n",
      "Epoch: 309/2000... Training loss: 2.1561\n",
      "Epoch: 309/2000... Training loss: 1.6767\n",
      "Epoch: 309/2000... Training loss: 1.9827\n",
      "Epoch: 309/2000... Training loss: 2.1481\n",
      "Epoch: 309/2000... Training loss: 1.9796\n",
      "Epoch: 309/2000... Training loss: 2.1442\n",
      "Epoch: 309/2000... Training loss: 2.1434\n",
      "Epoch: 309/2000... Training loss: 1.9557\n",
      "Epoch: 309/2000... Training loss: 2.2539\n",
      "Epoch: 309/2000... Training loss: 1.8302\n",
      "Epoch: 309/2000... Training loss: 1.9028\n",
      "Epoch: 309/2000... Training loss: 2.2654\n",
      "Epoch: 309/2000... Training loss: 1.7796\n",
      "Epoch: 309/2000... Training loss: 1.7057\n",
      "Epoch: 309/2000... Training loss: 1.8202\n",
      "Epoch: 309/2000... Training loss: 2.1050\n",
      "Epoch: 309/2000... Training loss: 1.9332\n",
      "Epoch: 309/2000... Training loss: 1.9421\n",
      "Epoch: 309/2000... Training loss: 2.0492\n",
      "Epoch: 309/2000... Training loss: 1.7911\n",
      "Epoch: 309/2000... Training loss: 1.8099\n",
      "Epoch: 309/2000... Training loss: 1.9796\n",
      "Epoch: 309/2000... Training loss: 2.1116\n",
      "Epoch: 309/2000... Training loss: 1.9952\n",
      "Epoch: 309/2000... Training loss: 2.0489\n",
      "Epoch: 309/2000... Training loss: 1.7496\n",
      "Epoch: 309/2000... Training loss: 2.1342\n",
      "Epoch: 309/2000... Training loss: 2.0910\n",
      "Epoch: 309/2000... Training loss: 1.9657\n",
      "Epoch: 309/2000... Training loss: 1.9633\n",
      "Epoch: 310/2000... Training loss: 1.7728\n",
      "Epoch: 310/2000... Training loss: 1.6514\n",
      "Epoch: 310/2000... Training loss: 1.8380\n",
      "Epoch: 310/2000... Training loss: 1.8021\n",
      "Epoch: 310/2000... Training loss: 2.0085\n",
      "Epoch: 310/2000... Training loss: 1.8188\n",
      "Epoch: 310/2000... Training loss: 1.9388\n",
      "Epoch: 310/2000... Training loss: 1.8076\n",
      "Epoch: 310/2000... Training loss: 1.8532\n",
      "Epoch: 310/2000... Training loss: 2.1124\n",
      "Epoch: 310/2000... Training loss: 1.6769\n",
      "Epoch: 310/2000... Training loss: 2.0382\n",
      "Epoch: 310/2000... Training loss: 1.6226\n",
      "Epoch: 310/2000... Training loss: 2.0159\n",
      "Epoch: 310/2000... Training loss: 1.8917\n",
      "Epoch: 310/2000... Training loss: 2.1396\n",
      "Epoch: 310/2000... Training loss: 2.1252\n",
      "Epoch: 310/2000... Training loss: 2.2672\n",
      "Epoch: 310/2000... Training loss: 1.9475\n",
      "Epoch: 310/2000... Training loss: 1.9078\n",
      "Epoch: 310/2000... Training loss: 2.0536\n",
      "Epoch: 310/2000... Training loss: 1.9811\n",
      "Epoch: 310/2000... Training loss: 1.9300\n",
      "Epoch: 310/2000... Training loss: 2.1376\n",
      "Epoch: 310/2000... Training loss: 1.9542\n",
      "Epoch: 310/2000... Training loss: 2.0378\n",
      "Epoch: 310/2000... Training loss: 2.0730\n",
      "Epoch: 310/2000... Training loss: 1.9491\n",
      "Epoch: 310/2000... Training loss: 1.8110\n",
      "Epoch: 310/2000... Training loss: 1.9726\n",
      "Epoch: 310/2000... Training loss: 2.1154\n",
      "Epoch: 311/2000... Training loss: 1.8886\n",
      "Epoch: 311/2000... Training loss: 1.9375\n",
      "Epoch: 311/2000... Training loss: 2.0325\n",
      "Epoch: 311/2000... Training loss: 1.9694\n",
      "Epoch: 311/2000... Training loss: 2.0688\n",
      "Epoch: 311/2000... Training loss: 1.8832\n",
      "Epoch: 311/2000... Training loss: 2.1873\n",
      "Epoch: 311/2000... Training loss: 1.8559\n",
      "Epoch: 311/2000... Training loss: 2.0221\n",
      "Epoch: 311/2000... Training loss: 1.9256\n",
      "Epoch: 311/2000... Training loss: 2.0523\n",
      "Epoch: 311/2000... Training loss: 1.8629\n",
      "Epoch: 311/2000... Training loss: 1.9929\n",
      "Epoch: 311/2000... Training loss: 2.2529\n",
      "Epoch: 311/2000... Training loss: 2.1268\n",
      "Epoch: 311/2000... Training loss: 1.6770\n",
      "Epoch: 311/2000... Training loss: 1.8473\n",
      "Epoch: 311/2000... Training loss: 1.8705\n",
      "Epoch: 311/2000... Training loss: 1.7734\n",
      "Epoch: 311/2000... Training loss: 1.9640\n",
      "Epoch: 311/2000... Training loss: 1.8575\n",
      "Epoch: 311/2000... Training loss: 1.9261\n",
      "Epoch: 311/2000... Training loss: 2.0740\n",
      "Epoch: 311/2000... Training loss: 1.9290\n",
      "Epoch: 311/2000... Training loss: 1.8759\n",
      "Epoch: 311/2000... Training loss: 1.8244\n",
      "Epoch: 311/2000... Training loss: 2.1329\n",
      "Epoch: 311/2000... Training loss: 2.0431\n",
      "Epoch: 311/2000... Training loss: 2.0066\n",
      "Epoch: 311/2000... Training loss: 2.0648\n",
      "Epoch: 311/2000... Training loss: 1.7985\n",
      "Epoch: 312/2000... Training loss: 2.1976\n",
      "Epoch: 312/2000... Training loss: 2.0049\n",
      "Epoch: 312/2000... Training loss: 1.6495\n",
      "Epoch: 312/2000... Training loss: 1.7503\n",
      "Epoch: 312/2000... Training loss: 1.9075\n",
      "Epoch: 312/2000... Training loss: 2.0322\n",
      "Epoch: 312/2000... Training loss: 1.8685\n",
      "Epoch: 312/2000... Training loss: 1.8245\n",
      "Epoch: 312/2000... Training loss: 2.0221\n",
      "Epoch: 312/2000... Training loss: 1.9592\n",
      "Epoch: 312/2000... Training loss: 2.1281\n",
      "Epoch: 312/2000... Training loss: 1.8922\n",
      "Epoch: 312/2000... Training loss: 1.9972\n",
      "Epoch: 312/2000... Training loss: 1.7094\n",
      "Epoch: 312/2000... Training loss: 1.9273\n",
      "Epoch: 312/2000... Training loss: 1.9968\n",
      "Epoch: 312/2000... Training loss: 2.0535\n",
      "Epoch: 312/2000... Training loss: 1.9506\n",
      "Epoch: 312/2000... Training loss: 2.0248\n",
      "Epoch: 312/2000... Training loss: 1.7839\n",
      "Epoch: 312/2000... Training loss: 2.1213\n",
      "Epoch: 312/2000... Training loss: 1.7824\n",
      "Epoch: 312/2000... Training loss: 2.1980\n",
      "Epoch: 312/2000... Training loss: 2.0016\n",
      "Epoch: 312/2000... Training loss: 1.7242\n",
      "Epoch: 312/2000... Training loss: 1.5462\n",
      "Epoch: 312/2000... Training loss: 1.8337\n",
      "Epoch: 312/2000... Training loss: 1.8537\n",
      "Epoch: 312/2000... Training loss: 1.9959\n",
      "Epoch: 312/2000... Training loss: 1.8695\n",
      "Epoch: 312/2000... Training loss: 1.9724\n",
      "Epoch: 313/2000... Training loss: 1.8406\n",
      "Epoch: 313/2000... Training loss: 1.9898\n",
      "Epoch: 313/2000... Training loss: 1.7790\n",
      "Epoch: 313/2000... Training loss: 1.8725\n",
      "Epoch: 313/2000... Training loss: 2.0322\n",
      "Epoch: 313/2000... Training loss: 1.9594\n",
      "Epoch: 313/2000... Training loss: 2.1252\n",
      "Epoch: 313/2000... Training loss: 1.9158\n",
      "Epoch: 313/2000... Training loss: 2.0865\n",
      "Epoch: 313/2000... Training loss: 1.9416\n",
      "Epoch: 313/2000... Training loss: 2.1552\n",
      "Epoch: 313/2000... Training loss: 1.8883\n",
      "Epoch: 313/2000... Training loss: 2.1203\n",
      "Epoch: 313/2000... Training loss: 2.0309\n",
      "Epoch: 313/2000... Training loss: 1.9770\n",
      "Epoch: 313/2000... Training loss: 1.7438\n",
      "Epoch: 313/2000... Training loss: 2.1982\n",
      "Epoch: 313/2000... Training loss: 1.7208\n",
      "Epoch: 313/2000... Training loss: 1.7933\n",
      "Epoch: 313/2000... Training loss: 2.0354\n",
      "Epoch: 313/2000... Training loss: 1.7595\n",
      "Epoch: 313/2000... Training loss: 1.8710\n",
      "Epoch: 313/2000... Training loss: 2.2654\n",
      "Epoch: 313/2000... Training loss: 2.0465\n",
      "Epoch: 313/2000... Training loss: 1.9011\n",
      "Epoch: 313/2000... Training loss: 1.8663\n",
      "Epoch: 313/2000... Training loss: 2.0142\n",
      "Epoch: 313/2000... Training loss: 2.2346\n",
      "Epoch: 313/2000... Training loss: 1.5007\n",
      "Epoch: 313/2000... Training loss: 1.9896\n",
      "Epoch: 313/2000... Training loss: 1.8090\n",
      "Epoch: 314/2000... Training loss: 1.8684\n",
      "Epoch: 314/2000... Training loss: 2.0706\n",
      "Epoch: 314/2000... Training loss: 1.8198\n",
      "Epoch: 314/2000... Training loss: 2.1467\n",
      "Epoch: 314/2000... Training loss: 2.0074\n",
      "Epoch: 314/2000... Training loss: 2.0254\n",
      "Epoch: 314/2000... Training loss: 1.8621\n",
      "Epoch: 314/2000... Training loss: 1.9808\n",
      "Epoch: 314/2000... Training loss: 1.7735\n",
      "Epoch: 314/2000... Training loss: 2.0175\n",
      "Epoch: 314/2000... Training loss: 1.8985\n",
      "Epoch: 314/2000... Training loss: 2.0456\n",
      "Epoch: 314/2000... Training loss: 2.1332\n",
      "Epoch: 314/2000... Training loss: 1.9177\n",
      "Epoch: 314/2000... Training loss: 1.6427\n",
      "Epoch: 314/2000... Training loss: 1.7676\n",
      "Epoch: 314/2000... Training loss: 1.8082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 314/2000... Training loss: 2.0637\n",
      "Epoch: 314/2000... Training loss: 1.7820\n",
      "Epoch: 314/2000... Training loss: 1.9660\n",
      "Epoch: 314/2000... Training loss: 1.7638\n",
      "Epoch: 314/2000... Training loss: 1.8297\n",
      "Epoch: 314/2000... Training loss: 1.9238\n",
      "Epoch: 314/2000... Training loss: 1.7342\n",
      "Epoch: 314/2000... Training loss: 1.9493\n",
      "Epoch: 314/2000... Training loss: 1.7261\n",
      "Epoch: 314/2000... Training loss: 1.8630\n",
      "Epoch: 314/2000... Training loss: 1.6815\n",
      "Epoch: 314/2000... Training loss: 2.1428\n",
      "Epoch: 314/2000... Training loss: 1.9411\n",
      "Epoch: 314/2000... Training loss: 1.7909\n",
      "Epoch: 315/2000... Training loss: 1.7559\n",
      "Epoch: 315/2000... Training loss: 1.8481\n",
      "Epoch: 315/2000... Training loss: 1.8101\n",
      "Epoch: 315/2000... Training loss: 1.8868\n",
      "Epoch: 315/2000... Training loss: 1.7561\n",
      "Epoch: 315/2000... Training loss: 1.7314\n",
      "Epoch: 315/2000... Training loss: 1.7604\n",
      "Epoch: 315/2000... Training loss: 1.9375\n",
      "Epoch: 315/2000... Training loss: 1.9418\n",
      "Epoch: 315/2000... Training loss: 1.9946\n",
      "Epoch: 315/2000... Training loss: 1.8418\n",
      "Epoch: 315/2000... Training loss: 1.7153\n",
      "Epoch: 315/2000... Training loss: 2.1403\n",
      "Epoch: 315/2000... Training loss: 1.9249\n",
      "Epoch: 315/2000... Training loss: 1.9771\n",
      "Epoch: 315/2000... Training loss: 2.1350\n",
      "Epoch: 315/2000... Training loss: 2.0190\n",
      "Epoch: 315/2000... Training loss: 1.7644\n",
      "Epoch: 315/2000... Training loss: 1.8305\n",
      "Epoch: 315/2000... Training loss: 1.8073\n",
      "Epoch: 315/2000... Training loss: 1.7685\n",
      "Epoch: 315/2000... Training loss: 1.6818\n",
      "Epoch: 315/2000... Training loss: 1.9448\n",
      "Epoch: 315/2000... Training loss: 1.9330\n",
      "Epoch: 315/2000... Training loss: 1.8777\n",
      "Epoch: 315/2000... Training loss: 1.6347\n",
      "Epoch: 315/2000... Training loss: 2.1013\n",
      "Epoch: 315/2000... Training loss: 1.7510\n",
      "Epoch: 315/2000... Training loss: 1.9473\n",
      "Epoch: 315/2000... Training loss: 1.9442\n",
      "Epoch: 315/2000... Training loss: 1.8156\n",
      "Epoch: 316/2000... Training loss: 2.0618\n",
      "Epoch: 316/2000... Training loss: 1.7773\n",
      "Epoch: 316/2000... Training loss: 1.9699\n",
      "Epoch: 316/2000... Training loss: 2.0046\n",
      "Epoch: 316/2000... Training loss: 2.0021\n",
      "Epoch: 316/2000... Training loss: 2.0425\n",
      "Epoch: 316/2000... Training loss: 1.7670\n",
      "Epoch: 316/2000... Training loss: 1.9564\n",
      "Epoch: 316/2000... Training loss: 1.7777\n",
      "Epoch: 316/2000... Training loss: 2.0390\n",
      "Epoch: 316/2000... Training loss: 2.0394\n",
      "Epoch: 316/2000... Training loss: 1.7039\n",
      "Epoch: 316/2000... Training loss: 1.8231\n",
      "Epoch: 316/2000... Training loss: 1.8202\n",
      "Epoch: 316/2000... Training loss: 1.9474\n",
      "Epoch: 316/2000... Training loss: 1.8391\n",
      "Epoch: 316/2000... Training loss: 2.0588\n",
      "Epoch: 316/2000... Training loss: 1.8170\n",
      "Epoch: 316/2000... Training loss: 1.8405\n",
      "Epoch: 316/2000... Training loss: 2.1347\n",
      "Epoch: 316/2000... Training loss: 2.0340\n",
      "Epoch: 316/2000... Training loss: 1.9397\n",
      "Epoch: 316/2000... Training loss: 1.9257\n",
      "Epoch: 316/2000... Training loss: 1.6916\n",
      "Epoch: 316/2000... Training loss: 1.8033\n",
      "Epoch: 316/2000... Training loss: 1.6671\n",
      "Epoch: 316/2000... Training loss: 1.8730\n",
      "Epoch: 316/2000... Training loss: 2.2305\n",
      "Epoch: 316/2000... Training loss: 1.7807\n",
      "Epoch: 316/2000... Training loss: 2.0299\n",
      "Epoch: 316/2000... Training loss: 2.0122\n",
      "Epoch: 317/2000... Training loss: 1.9769\n",
      "Epoch: 317/2000... Training loss: 1.8817\n",
      "Epoch: 317/2000... Training loss: 1.9642\n",
      "Epoch: 317/2000... Training loss: 2.1835\n",
      "Epoch: 317/2000... Training loss: 1.8817\n",
      "Epoch: 317/2000... Training loss: 2.3619\n",
      "Epoch: 317/2000... Training loss: 1.8502\n",
      "Epoch: 317/2000... Training loss: 1.8931\n",
      "Epoch: 317/2000... Training loss: 2.2618\n",
      "Epoch: 317/2000... Training loss: 2.0715\n",
      "Epoch: 317/2000... Training loss: 1.7468\n",
      "Epoch: 317/2000... Training loss: 1.9860\n",
      "Epoch: 317/2000... Training loss: 1.7850\n",
      "Epoch: 317/2000... Training loss: 1.9836\n",
      "Epoch: 317/2000... Training loss: 2.0648\n",
      "Epoch: 317/2000... Training loss: 1.8293\n",
      "Epoch: 317/2000... Training loss: 2.0860\n",
      "Epoch: 317/2000... Training loss: 1.8576\n",
      "Epoch: 317/2000... Training loss: 1.8573\n",
      "Epoch: 317/2000... Training loss: 1.7902\n",
      "Epoch: 317/2000... Training loss: 1.8244\n",
      "Epoch: 317/2000... Training loss: 2.1474\n",
      "Epoch: 317/2000... Training loss: 2.0776\n",
      "Epoch: 317/2000... Training loss: 1.9056\n",
      "Epoch: 317/2000... Training loss: 1.6790\n",
      "Epoch: 317/2000... Training loss: 1.6804\n",
      "Epoch: 317/2000... Training loss: 1.7938\n",
      "Epoch: 317/2000... Training loss: 2.0769\n",
      "Epoch: 317/2000... Training loss: 2.0173\n",
      "Epoch: 317/2000... Training loss: 2.1262\n",
      "Epoch: 317/2000... Training loss: 1.9748\n",
      "Epoch: 318/2000... Training loss: 2.0553\n",
      "Epoch: 318/2000... Training loss: 1.9000\n",
      "Epoch: 318/2000... Training loss: 1.8424\n",
      "Epoch: 318/2000... Training loss: 1.7227\n",
      "Epoch: 318/2000... Training loss: 2.1153\n",
      "Epoch: 318/2000... Training loss: 1.6968\n",
      "Epoch: 318/2000... Training loss: 2.1852\n",
      "Epoch: 318/2000... Training loss: 1.8743\n",
      "Epoch: 318/2000... Training loss: 1.8902\n",
      "Epoch: 318/2000... Training loss: 1.9117\n",
      "Epoch: 318/2000... Training loss: 1.9185\n",
      "Epoch: 318/2000... Training loss: 2.0191\n",
      "Epoch: 318/2000... Training loss: 2.0635\n",
      "Epoch: 318/2000... Training loss: 1.9487\n",
      "Epoch: 318/2000... Training loss: 1.9562\n",
      "Epoch: 318/2000... Training loss: 1.7637\n",
      "Epoch: 318/2000... Training loss: 1.6464\n",
      "Epoch: 318/2000... Training loss: 1.9943\n",
      "Epoch: 318/2000... Training loss: 1.9236\n",
      "Epoch: 318/2000... Training loss: 1.8725\n",
      "Epoch: 318/2000... Training loss: 1.9903\n",
      "Epoch: 318/2000... Training loss: 2.0290\n",
      "Epoch: 318/2000... Training loss: 2.0108\n",
      "Epoch: 318/2000... Training loss: 1.7490\n",
      "Epoch: 318/2000... Training loss: 2.0598\n",
      "Epoch: 318/2000... Training loss: 1.8592\n",
      "Epoch: 318/2000... Training loss: 1.6126\n",
      "Epoch: 318/2000... Training loss: 1.8960\n",
      "Epoch: 318/2000... Training loss: 2.1066\n",
      "Epoch: 318/2000... Training loss: 1.9300\n",
      "Epoch: 318/2000... Training loss: 1.7879\n",
      "Epoch: 319/2000... Training loss: 1.6907\n",
      "Epoch: 319/2000... Training loss: 1.6534\n",
      "Epoch: 319/2000... Training loss: 1.9270\n",
      "Epoch: 319/2000... Training loss: 1.9389\n",
      "Epoch: 319/2000... Training loss: 1.6417\n",
      "Epoch: 319/2000... Training loss: 2.0699\n",
      "Epoch: 319/2000... Training loss: 1.9041\n",
      "Epoch: 319/2000... Training loss: 1.7882\n",
      "Epoch: 319/2000... Training loss: 1.9684\n",
      "Epoch: 319/2000... Training loss: 1.8664\n",
      "Epoch: 319/2000... Training loss: 1.8777\n",
      "Epoch: 319/2000... Training loss: 1.6772\n",
      "Epoch: 319/2000... Training loss: 1.9828\n",
      "Epoch: 319/2000... Training loss: 1.8074\n",
      "Epoch: 319/2000... Training loss: 2.0938\n",
      "Epoch: 319/2000... Training loss: 1.8094\n",
      "Epoch: 319/2000... Training loss: 1.8098\n",
      "Epoch: 319/2000... Training loss: 2.0580\n",
      "Epoch: 319/2000... Training loss: 1.8115\n",
      "Epoch: 319/2000... Training loss: 1.9573\n",
      "Epoch: 319/2000... Training loss: 1.9025\n",
      "Epoch: 319/2000... Training loss: 2.1126\n",
      "Epoch: 319/2000... Training loss: 2.0409\n",
      "Epoch: 319/2000... Training loss: 1.9749\n",
      "Epoch: 319/2000... Training loss: 1.9636\n",
      "Epoch: 319/2000... Training loss: 1.7561\n",
      "Epoch: 319/2000... Training loss: 1.8055\n",
      "Epoch: 319/2000... Training loss: 1.8805\n",
      "Epoch: 319/2000... Training loss: 1.9918\n",
      "Epoch: 319/2000... Training loss: 2.0022\n",
      "Epoch: 319/2000... Training loss: 2.0200\n",
      "Epoch: 320/2000... Training loss: 1.8687\n",
      "Epoch: 320/2000... Training loss: 2.1314\n",
      "Epoch: 320/2000... Training loss: 1.9371\n",
      "Epoch: 320/2000... Training loss: 1.9190\n",
      "Epoch: 320/2000... Training loss: 1.6851\n",
      "Epoch: 320/2000... Training loss: 1.7537\n",
      "Epoch: 320/2000... Training loss: 1.8016\n",
      "Epoch: 320/2000... Training loss: 2.2431\n",
      "Epoch: 320/2000... Training loss: 1.7589\n",
      "Epoch: 320/2000... Training loss: 1.9783\n",
      "Epoch: 320/2000... Training loss: 1.9418\n",
      "Epoch: 320/2000... Training loss: 1.7043\n",
      "Epoch: 320/2000... Training loss: 1.8140\n",
      "Epoch: 320/2000... Training loss: 1.8917\n",
      "Epoch: 320/2000... Training loss: 1.9812\n",
      "Epoch: 320/2000... Training loss: 2.0518\n",
      "Epoch: 320/2000... Training loss: 1.9019\n",
      "Epoch: 320/2000... Training loss: 1.9742\n",
      "Epoch: 320/2000... Training loss: 1.8628\n",
      "Epoch: 320/2000... Training loss: 1.7555\n",
      "Epoch: 320/2000... Training loss: 1.9973\n",
      "Epoch: 320/2000... Training loss: 1.8051\n",
      "Epoch: 320/2000... Training loss: 2.0179\n",
      "Epoch: 320/2000... Training loss: 1.7769\n",
      "Epoch: 320/2000... Training loss: 2.0274\n",
      "Epoch: 320/2000... Training loss: 1.8804\n",
      "Epoch: 320/2000... Training loss: 1.6282\n",
      "Epoch: 320/2000... Training loss: 2.1264\n",
      "Epoch: 320/2000... Training loss: 1.7848\n",
      "Epoch: 320/2000... Training loss: 1.8858\n",
      "Epoch: 320/2000... Training loss: 1.9304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 321/2000... Training loss: 1.7308\n",
      "Epoch: 321/2000... Training loss: 1.9591\n",
      "Epoch: 321/2000... Training loss: 1.8116\n",
      "Epoch: 321/2000... Training loss: 1.7115\n",
      "Epoch: 321/2000... Training loss: 1.8999\n",
      "Epoch: 321/2000... Training loss: 1.9624\n",
      "Epoch: 321/2000... Training loss: 1.8435\n",
      "Epoch: 321/2000... Training loss: 2.1164\n",
      "Epoch: 321/2000... Training loss: 1.6863\n",
      "Epoch: 321/2000... Training loss: 2.0414\n",
      "Epoch: 321/2000... Training loss: 2.1396\n",
      "Epoch: 321/2000... Training loss: 1.6926\n",
      "Epoch: 321/2000... Training loss: 1.8574\n",
      "Epoch: 321/2000... Training loss: 2.0136\n",
      "Epoch: 321/2000... Training loss: 1.8418\n",
      "Epoch: 321/2000... Training loss: 1.6434\n",
      "Epoch: 321/2000... Training loss: 1.7784\n",
      "Epoch: 321/2000... Training loss: 1.6993\n",
      "Epoch: 321/2000... Training loss: 1.8265\n",
      "Epoch: 321/2000... Training loss: 1.7099\n",
      "Epoch: 321/2000... Training loss: 1.7495\n",
      "Epoch: 321/2000... Training loss: 1.8187\n",
      "Epoch: 321/2000... Training loss: 2.0123\n",
      "Epoch: 321/2000... Training loss: 1.8493\n",
      "Epoch: 321/2000... Training loss: 1.8185\n",
      "Epoch: 321/2000... Training loss: 1.7735\n",
      "Epoch: 321/2000... Training loss: 1.8226\n",
      "Epoch: 321/2000... Training loss: 1.9686\n",
      "Epoch: 321/2000... Training loss: 1.8189\n",
      "Epoch: 321/2000... Training loss: 1.7086\n",
      "Epoch: 321/2000... Training loss: 2.1332\n",
      "Epoch: 322/2000... Training loss: 1.7792\n",
      "Epoch: 322/2000... Training loss: 1.6572\n",
      "Epoch: 322/2000... Training loss: 1.7602\n",
      "Epoch: 322/2000... Training loss: 2.0719\n",
      "Epoch: 322/2000... Training loss: 2.0308\n",
      "Epoch: 322/2000... Training loss: 1.9426\n",
      "Epoch: 322/2000... Training loss: 2.1483\n",
      "Epoch: 322/2000... Training loss: 1.7738\n",
      "Epoch: 322/2000... Training loss: 1.8898\n",
      "Epoch: 322/2000... Training loss: 1.8916\n",
      "Epoch: 322/2000... Training loss: 2.0313\n",
      "Epoch: 322/2000... Training loss: 1.7275\n",
      "Epoch: 322/2000... Training loss: 1.8945\n",
      "Epoch: 322/2000... Training loss: 2.1446\n",
      "Epoch: 322/2000... Training loss: 1.8057\n",
      "Epoch: 322/2000... Training loss: 1.7394\n",
      "Epoch: 322/2000... Training loss: 1.9377\n",
      "Epoch: 322/2000... Training loss: 1.9093\n",
      "Epoch: 322/2000... Training loss: 1.8341\n",
      "Epoch: 322/2000... Training loss: 1.8488\n",
      "Epoch: 322/2000... Training loss: 2.0535\n",
      "Epoch: 322/2000... Training loss: 1.9860\n",
      "Epoch: 322/2000... Training loss: 1.8344\n",
      "Epoch: 322/2000... Training loss: 1.7682\n",
      "Epoch: 322/2000... Training loss: 1.8158\n",
      "Epoch: 322/2000... Training loss: 1.8389\n",
      "Epoch: 322/2000... Training loss: 1.9366\n",
      "Epoch: 322/2000... Training loss: 2.0511\n",
      "Epoch: 322/2000... Training loss: 1.9663\n",
      "Epoch: 322/2000... Training loss: 2.2175\n",
      "Epoch: 322/2000... Training loss: 1.7689\n",
      "Epoch: 323/2000... Training loss: 1.8544\n",
      "Epoch: 323/2000... Training loss: 1.9653\n",
      "Epoch: 323/2000... Training loss: 1.9064\n",
      "Epoch: 323/2000... Training loss: 1.8576\n",
      "Epoch: 323/2000... Training loss: 1.7892\n",
      "Epoch: 323/2000... Training loss: 1.9114\n",
      "Epoch: 323/2000... Training loss: 1.9057\n",
      "Epoch: 323/2000... Training loss: 1.6422\n",
      "Epoch: 323/2000... Training loss: 1.8706\n",
      "Epoch: 323/2000... Training loss: 1.9295\n",
      "Epoch: 323/2000... Training loss: 1.8774\n",
      "Epoch: 323/2000... Training loss: 1.9673\n",
      "Epoch: 323/2000... Training loss: 2.0245\n",
      "Epoch: 323/2000... Training loss: 1.7264\n",
      "Epoch: 323/2000... Training loss: 1.7179\n",
      "Epoch: 323/2000... Training loss: 1.7364\n",
      "Epoch: 323/2000... Training loss: 1.8703\n",
      "Epoch: 323/2000... Training loss: 1.7637\n",
      "Epoch: 323/2000... Training loss: 1.8197\n",
      "Epoch: 323/2000... Training loss: 1.8301\n",
      "Epoch: 323/2000... Training loss: 2.0411\n",
      "Epoch: 323/2000... Training loss: 1.8662\n",
      "Epoch: 323/2000... Training loss: 1.8849\n",
      "Epoch: 323/2000... Training loss: 1.9389\n",
      "Epoch: 323/2000... Training loss: 1.8700\n",
      "Epoch: 323/2000... Training loss: 1.8900\n",
      "Epoch: 323/2000... Training loss: 1.5883\n",
      "Epoch: 323/2000... Training loss: 1.8326\n",
      "Epoch: 323/2000... Training loss: 2.0350\n",
      "Epoch: 323/2000... Training loss: 1.7984\n",
      "Epoch: 323/2000... Training loss: 2.1494\n",
      "Epoch: 324/2000... Training loss: 1.9850\n",
      "Epoch: 324/2000... Training loss: 1.8272\n",
      "Epoch: 324/2000... Training loss: 1.7086\n",
      "Epoch: 324/2000... Training loss: 2.0203\n",
      "Epoch: 324/2000... Training loss: 1.9622\n",
      "Epoch: 324/2000... Training loss: 1.7512\n",
      "Epoch: 324/2000... Training loss: 1.9158\n",
      "Epoch: 324/2000... Training loss: 1.9774\n",
      "Epoch: 324/2000... Training loss: 1.9390\n",
      "Epoch: 324/2000... Training loss: 1.7519\n",
      "Epoch: 324/2000... Training loss: 1.5370\n",
      "Epoch: 324/2000... Training loss: 1.7960\n",
      "Epoch: 324/2000... Training loss: 1.9178\n",
      "Epoch: 324/2000... Training loss: 1.7059\n",
      "Epoch: 324/2000... Training loss: 1.9046\n",
      "Epoch: 324/2000... Training loss: 1.7865\n",
      "Epoch: 324/2000... Training loss: 1.8354\n",
      "Epoch: 324/2000... Training loss: 1.7946\n",
      "Epoch: 324/2000... Training loss: 1.6297\n",
      "Epoch: 324/2000... Training loss: 1.6246\n",
      "Epoch: 324/2000... Training loss: 1.8327\n",
      "Epoch: 324/2000... Training loss: 1.9110\n",
      "Epoch: 324/2000... Training loss: 1.8842\n",
      "Epoch: 324/2000... Training loss: 1.9209\n",
      "Epoch: 324/2000... Training loss: 1.7817\n",
      "Epoch: 324/2000... Training loss: 1.9759\n",
      "Epoch: 324/2000... Training loss: 1.6185\n",
      "Epoch: 324/2000... Training loss: 2.0311\n",
      "Epoch: 324/2000... Training loss: 1.9147\n",
      "Epoch: 324/2000... Training loss: 1.8675\n",
      "Epoch: 324/2000... Training loss: 1.5708\n",
      "Epoch: 325/2000... Training loss: 1.7202\n",
      "Epoch: 325/2000... Training loss: 2.0123\n",
      "Epoch: 325/2000... Training loss: 2.0784\n",
      "Epoch: 325/2000... Training loss: 1.7866\n",
      "Epoch: 325/2000... Training loss: 1.9266\n",
      "Epoch: 325/2000... Training loss: 1.8921\n",
      "Epoch: 325/2000... Training loss: 1.7620\n",
      "Epoch: 325/2000... Training loss: 2.0207\n",
      "Epoch: 325/2000... Training loss: 1.7707\n",
      "Epoch: 325/2000... Training loss: 1.5776\n",
      "Epoch: 325/2000... Training loss: 1.8370\n",
      "Epoch: 325/2000... Training loss: 1.8095\n",
      "Epoch: 325/2000... Training loss: 1.8705\n",
      "Epoch: 325/2000... Training loss: 1.8567\n",
      "Epoch: 325/2000... Training loss: 1.9739\n",
      "Epoch: 325/2000... Training loss: 1.6979\n",
      "Epoch: 325/2000... Training loss: 1.6775\n",
      "Epoch: 325/2000... Training loss: 1.7901\n",
      "Epoch: 325/2000... Training loss: 1.8455\n",
      "Epoch: 325/2000... Training loss: 1.9085\n",
      "Epoch: 325/2000... Training loss: 1.8034\n",
      "Epoch: 325/2000... Training loss: 1.6578\n",
      "Epoch: 325/2000... Training loss: 1.8342\n",
      "Epoch: 325/2000... Training loss: 1.9378\n",
      "Epoch: 325/2000... Training loss: 1.6748\n",
      "Epoch: 325/2000... Training loss: 1.8584\n",
      "Epoch: 325/2000... Training loss: 1.8219\n",
      "Epoch: 325/2000... Training loss: 1.9238\n",
      "Epoch: 325/2000... Training loss: 1.8924\n",
      "Epoch: 325/2000... Training loss: 1.9336\n",
      "Epoch: 325/2000... Training loss: 1.7599\n",
      "Epoch: 326/2000... Training loss: 1.8811\n",
      "Epoch: 326/2000... Training loss: 1.8911\n",
      "Epoch: 326/2000... Training loss: 1.7660\n",
      "Epoch: 326/2000... Training loss: 1.9903\n",
      "Epoch: 326/2000... Training loss: 1.8072\n",
      "Epoch: 326/2000... Training loss: 1.7753\n",
      "Epoch: 326/2000... Training loss: 1.9105\n",
      "Epoch: 326/2000... Training loss: 1.8456\n",
      "Epoch: 326/2000... Training loss: 1.7586\n",
      "Epoch: 326/2000... Training loss: 1.7696\n",
      "Epoch: 326/2000... Training loss: 2.0503\n",
      "Epoch: 326/2000... Training loss: 1.8133\n",
      "Epoch: 326/2000... Training loss: 1.8237\n",
      "Epoch: 326/2000... Training loss: 1.8548\n",
      "Epoch: 326/2000... Training loss: 1.8602\n",
      "Epoch: 326/2000... Training loss: 1.7883\n",
      "Epoch: 326/2000... Training loss: 2.0596\n",
      "Epoch: 326/2000... Training loss: 1.7540\n",
      "Epoch: 326/2000... Training loss: 2.0562\n",
      "Epoch: 326/2000... Training loss: 1.9160\n",
      "Epoch: 326/2000... Training loss: 1.9109\n",
      "Epoch: 326/2000... Training loss: 1.7124\n",
      "Epoch: 326/2000... Training loss: 2.0874\n",
      "Epoch: 326/2000... Training loss: 1.7177\n",
      "Epoch: 326/2000... Training loss: 1.9269\n",
      "Epoch: 326/2000... Training loss: 1.9650\n",
      "Epoch: 326/2000... Training loss: 1.9435\n",
      "Epoch: 326/2000... Training loss: 1.9044\n",
      "Epoch: 326/2000... Training loss: 1.7222\n",
      "Epoch: 326/2000... Training loss: 1.6713\n",
      "Epoch: 326/2000... Training loss: 1.9443\n",
      "Epoch: 327/2000... Training loss: 1.6854\n",
      "Epoch: 327/2000... Training loss: 1.8852\n",
      "Epoch: 327/2000... Training loss: 1.8390\n",
      "Epoch: 327/2000... Training loss: 1.9057\n",
      "Epoch: 327/2000... Training loss: 1.8077\n",
      "Epoch: 327/2000... Training loss: 2.0994\n",
      "Epoch: 327/2000... Training loss: 1.9713\n",
      "Epoch: 327/2000... Training loss: 1.7857\n",
      "Epoch: 327/2000... Training loss: 1.7669\n",
      "Epoch: 327/2000... Training loss: 1.8270\n",
      "Epoch: 327/2000... Training loss: 1.8462\n",
      "Epoch: 327/2000... Training loss: 1.5578\n",
      "Epoch: 327/2000... Training loss: 1.9825\n",
      "Epoch: 327/2000... Training loss: 1.7921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 327/2000... Training loss: 1.9554\n",
      "Epoch: 327/2000... Training loss: 1.9738\n",
      "Epoch: 327/2000... Training loss: 1.9336\n",
      "Epoch: 327/2000... Training loss: 1.6564\n",
      "Epoch: 327/2000... Training loss: 1.9288\n",
      "Epoch: 327/2000... Training loss: 1.8713\n",
      "Epoch: 327/2000... Training loss: 1.6234\n",
      "Epoch: 327/2000... Training loss: 1.8446\n",
      "Epoch: 327/2000... Training loss: 1.8573\n",
      "Epoch: 327/2000... Training loss: 1.8175\n",
      "Epoch: 327/2000... Training loss: 1.7682\n",
      "Epoch: 327/2000... Training loss: 1.8499\n",
      "Epoch: 327/2000... Training loss: 1.5499\n",
      "Epoch: 327/2000... Training loss: 1.5470\n",
      "Epoch: 327/2000... Training loss: 1.7717\n",
      "Epoch: 327/2000... Training loss: 1.8454\n",
      "Epoch: 327/2000... Training loss: 1.7870\n",
      "Epoch: 328/2000... Training loss: 1.7731\n",
      "Epoch: 328/2000... Training loss: 1.9243\n",
      "Epoch: 328/2000... Training loss: 1.7978\n",
      "Epoch: 328/2000... Training loss: 1.9533\n",
      "Epoch: 328/2000... Training loss: 1.9116\n",
      "Epoch: 328/2000... Training loss: 1.7912\n",
      "Epoch: 328/2000... Training loss: 1.9447\n",
      "Epoch: 328/2000... Training loss: 1.8245\n",
      "Epoch: 328/2000... Training loss: 1.8397\n",
      "Epoch: 328/2000... Training loss: 1.8779\n",
      "Epoch: 328/2000... Training loss: 2.0981\n",
      "Epoch: 328/2000... Training loss: 1.9058\n",
      "Epoch: 328/2000... Training loss: 1.8069\n",
      "Epoch: 328/2000... Training loss: 1.9477\n",
      "Epoch: 328/2000... Training loss: 1.8632\n",
      "Epoch: 328/2000... Training loss: 1.6192\n",
      "Epoch: 328/2000... Training loss: 1.9772\n",
      "Epoch: 328/2000... Training loss: 1.6776\n",
      "Epoch: 328/2000... Training loss: 1.7296\n",
      "Epoch: 328/2000... Training loss: 1.9037\n",
      "Epoch: 328/2000... Training loss: 1.8395\n",
      "Epoch: 328/2000... Training loss: 1.8489\n",
      "Epoch: 328/2000... Training loss: 1.8192\n",
      "Epoch: 328/2000... Training loss: 1.7576\n",
      "Epoch: 328/2000... Training loss: 1.9048\n",
      "Epoch: 328/2000... Training loss: 1.6306\n",
      "Epoch: 328/2000... Training loss: 1.9192\n",
      "Epoch: 328/2000... Training loss: 2.0503\n",
      "Epoch: 328/2000... Training loss: 1.6719\n",
      "Epoch: 328/2000... Training loss: 1.8429\n",
      "Epoch: 328/2000... Training loss: 1.8167\n",
      "Epoch: 329/2000... Training loss: 1.7003\n",
      "Epoch: 329/2000... Training loss: 1.7535\n",
      "Epoch: 329/2000... Training loss: 1.9907\n",
      "Epoch: 329/2000... Training loss: 1.8065\n",
      "Epoch: 329/2000... Training loss: 1.7751\n",
      "Epoch: 329/2000... Training loss: 1.7912\n",
      "Epoch: 329/2000... Training loss: 1.7096\n",
      "Epoch: 329/2000... Training loss: 1.6380\n",
      "Epoch: 329/2000... Training loss: 1.5669\n",
      "Epoch: 329/2000... Training loss: 1.5741\n",
      "Epoch: 329/2000... Training loss: 1.8822\n",
      "Epoch: 329/2000... Training loss: 1.7271\n",
      "Epoch: 329/2000... Training loss: 1.8791\n",
      "Epoch: 329/2000... Training loss: 1.7620\n",
      "Epoch: 329/2000... Training loss: 1.7133\n",
      "Epoch: 329/2000... Training loss: 1.7125\n",
      "Epoch: 329/2000... Training loss: 1.6819\n",
      "Epoch: 329/2000... Training loss: 2.0165\n",
      "Epoch: 329/2000... Training loss: 1.5693\n",
      "Epoch: 329/2000... Training loss: 1.8056\n",
      "Epoch: 329/2000... Training loss: 1.7064\n",
      "Epoch: 329/2000... Training loss: 1.9587\n",
      "Epoch: 329/2000... Training loss: 1.9239\n",
      "Epoch: 329/2000... Training loss: 1.8859\n",
      "Epoch: 329/2000... Training loss: 1.7284\n",
      "Epoch: 329/2000... Training loss: 1.7801\n",
      "Epoch: 329/2000... Training loss: 1.8390\n",
      "Epoch: 329/2000... Training loss: 1.8269\n",
      "Epoch: 329/2000... Training loss: 1.5662\n",
      "Epoch: 329/2000... Training loss: 1.8941\n",
      "Epoch: 329/2000... Training loss: 1.8439\n",
      "Epoch: 330/2000... Training loss: 1.4805\n",
      "Epoch: 330/2000... Training loss: 1.8413\n",
      "Epoch: 330/2000... Training loss: 1.9526\n",
      "Epoch: 330/2000... Training loss: 1.8539\n",
      "Epoch: 330/2000... Training loss: 1.9810\n",
      "Epoch: 330/2000... Training loss: 1.9089\n",
      "Epoch: 330/2000... Training loss: 1.7880\n",
      "Epoch: 330/2000... Training loss: 1.6460\n",
      "Epoch: 330/2000... Training loss: 1.7937\n",
      "Epoch: 330/2000... Training loss: 1.7644\n",
      "Epoch: 330/2000... Training loss: 1.9370\n",
      "Epoch: 330/2000... Training loss: 1.7341\n",
      "Epoch: 330/2000... Training loss: 1.9466\n",
      "Epoch: 330/2000... Training loss: 1.7421\n",
      "Epoch: 330/2000... Training loss: 1.7832\n",
      "Epoch: 330/2000... Training loss: 1.8477\n",
      "Epoch: 330/2000... Training loss: 2.1404\n",
      "Epoch: 330/2000... Training loss: 2.0629\n",
      "Epoch: 330/2000... Training loss: 1.7488\n",
      "Epoch: 330/2000... Training loss: 1.7636\n",
      "Epoch: 330/2000... Training loss: 1.8420\n",
      "Epoch: 330/2000... Training loss: 1.6953\n",
      "Epoch: 330/2000... Training loss: 1.9112\n",
      "Epoch: 330/2000... Training loss: 2.0794\n",
      "Epoch: 330/2000... Training loss: 2.0028\n",
      "Epoch: 330/2000... Training loss: 1.5209\n",
      "Epoch: 330/2000... Training loss: 1.7565\n",
      "Epoch: 330/2000... Training loss: 1.9990\n",
      "Epoch: 330/2000... Training loss: 2.0453\n",
      "Epoch: 330/2000... Training loss: 1.9535\n",
      "Epoch: 330/2000... Training loss: 1.8147\n",
      "Epoch: 331/2000... Training loss: 1.9826\n",
      "Epoch: 331/2000... Training loss: 1.9631\n",
      "Epoch: 331/2000... Training loss: 1.5312\n",
      "Epoch: 331/2000... Training loss: 1.6682\n",
      "Epoch: 331/2000... Training loss: 2.1136\n",
      "Epoch: 331/2000... Training loss: 1.8699\n",
      "Epoch: 331/2000... Training loss: 1.7874\n",
      "Epoch: 331/2000... Training loss: 1.8130\n",
      "Epoch: 331/2000... Training loss: 2.0137\n",
      "Epoch: 331/2000... Training loss: 1.5610\n",
      "Epoch: 331/2000... Training loss: 2.2093\n",
      "Epoch: 331/2000... Training loss: 1.9709\n",
      "Epoch: 331/2000... Training loss: 1.6827\n",
      "Epoch: 331/2000... Training loss: 1.9417\n",
      "Epoch: 331/2000... Training loss: 1.8535\n",
      "Epoch: 331/2000... Training loss: 1.7165\n",
      "Epoch: 331/2000... Training loss: 1.8544\n",
      "Epoch: 331/2000... Training loss: 1.7148\n",
      "Epoch: 331/2000... Training loss: 1.6908\n",
      "Epoch: 331/2000... Training loss: 1.7574\n",
      "Epoch: 331/2000... Training loss: 1.7507\n",
      "Epoch: 331/2000... Training loss: 2.0239\n",
      "Epoch: 331/2000... Training loss: 1.8288\n",
      "Epoch: 331/2000... Training loss: 1.8164\n",
      "Epoch: 331/2000... Training loss: 1.7285\n",
      "Epoch: 331/2000... Training loss: 1.9193\n",
      "Epoch: 331/2000... Training loss: 1.8594\n",
      "Epoch: 331/2000... Training loss: 1.5355\n",
      "Epoch: 331/2000... Training loss: 1.9012\n",
      "Epoch: 331/2000... Training loss: 1.8651\n",
      "Epoch: 331/2000... Training loss: 1.8202\n",
      "Epoch: 332/2000... Training loss: 1.7234\n",
      "Epoch: 332/2000... Training loss: 1.6301\n",
      "Epoch: 332/2000... Training loss: 1.9046\n",
      "Epoch: 332/2000... Training loss: 1.7273\n",
      "Epoch: 332/2000... Training loss: 1.8374\n",
      "Epoch: 332/2000... Training loss: 1.8011\n",
      "Epoch: 332/2000... Training loss: 1.4652\n",
      "Epoch: 332/2000... Training loss: 1.7976\n",
      "Epoch: 332/2000... Training loss: 1.8945\n",
      "Epoch: 332/2000... Training loss: 1.8425\n",
      "Epoch: 332/2000... Training loss: 1.3438\n",
      "Epoch: 332/2000... Training loss: 1.7686\n",
      "Epoch: 332/2000... Training loss: 1.9817\n",
      "Epoch: 332/2000... Training loss: 1.9302\n",
      "Epoch: 332/2000... Training loss: 1.9014\n",
      "Epoch: 332/2000... Training loss: 1.8987\n",
      "Epoch: 332/2000... Training loss: 2.0314\n",
      "Epoch: 332/2000... Training loss: 1.9464\n",
      "Epoch: 332/2000... Training loss: 1.9506\n",
      "Epoch: 332/2000... Training loss: 1.7987\n",
      "Epoch: 332/2000... Training loss: 1.8721\n",
      "Epoch: 332/2000... Training loss: 1.5037\n",
      "Epoch: 332/2000... Training loss: 1.8408\n",
      "Epoch: 332/2000... Training loss: 1.8748\n",
      "Epoch: 332/2000... Training loss: 2.0792\n",
      "Epoch: 332/2000... Training loss: 1.6066\n",
      "Epoch: 332/2000... Training loss: 1.4685\n",
      "Epoch: 332/2000... Training loss: 1.8426\n",
      "Epoch: 332/2000... Training loss: 1.6165\n",
      "Epoch: 332/2000... Training loss: 1.8983\n",
      "Epoch: 332/2000... Training loss: 1.8544\n",
      "Epoch: 333/2000... Training loss: 2.0834\n",
      "Epoch: 333/2000... Training loss: 1.7842\n",
      "Epoch: 333/2000... Training loss: 1.8027\n",
      "Epoch: 333/2000... Training loss: 1.8652\n",
      "Epoch: 333/2000... Training loss: 1.8891\n",
      "Epoch: 333/2000... Training loss: 1.6658\n",
      "Epoch: 333/2000... Training loss: 1.5998\n",
      "Epoch: 333/2000... Training loss: 2.0686\n",
      "Epoch: 333/2000... Training loss: 1.7594\n",
      "Epoch: 333/2000... Training loss: 1.9328\n",
      "Epoch: 333/2000... Training loss: 1.5901\n",
      "Epoch: 333/2000... Training loss: 1.7329\n",
      "Epoch: 333/2000... Training loss: 1.8280\n",
      "Epoch: 333/2000... Training loss: 1.9078\n",
      "Epoch: 333/2000... Training loss: 1.8777\n",
      "Epoch: 333/2000... Training loss: 1.6960\n",
      "Epoch: 333/2000... Training loss: 1.9709\n",
      "Epoch: 333/2000... Training loss: 1.6516\n",
      "Epoch: 333/2000... Training loss: 1.9719\n",
      "Epoch: 333/2000... Training loss: 1.8720\n",
      "Epoch: 333/2000... Training loss: 1.8582\n",
      "Epoch: 333/2000... Training loss: 1.6536\n",
      "Epoch: 333/2000... Training loss: 1.7927\n",
      "Epoch: 333/2000... Training loss: 1.7049\n",
      "Epoch: 333/2000... Training loss: 1.7042\n",
      "Epoch: 333/2000... Training loss: 1.7154\n",
      "Epoch: 333/2000... Training loss: 1.7458\n",
      "Epoch: 333/2000... Training loss: 1.8617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 333/2000... Training loss: 2.0508\n",
      "Epoch: 333/2000... Training loss: 2.0060\n",
      "Epoch: 333/2000... Training loss: 1.5713\n",
      "Epoch: 334/2000... Training loss: 1.6829\n",
      "Epoch: 334/2000... Training loss: 1.7358\n",
      "Epoch: 334/2000... Training loss: 1.8631\n",
      "Epoch: 334/2000... Training loss: 1.5768\n",
      "Epoch: 334/2000... Training loss: 1.7423\n",
      "Epoch: 334/2000... Training loss: 1.7974\n",
      "Epoch: 334/2000... Training loss: 1.8252\n",
      "Epoch: 334/2000... Training loss: 1.8171\n",
      "Epoch: 334/2000... Training loss: 1.8872\n",
      "Epoch: 334/2000... Training loss: 1.6400\n",
      "Epoch: 334/2000... Training loss: 1.7353\n",
      "Epoch: 334/2000... Training loss: 1.6620\n",
      "Epoch: 334/2000... Training loss: 1.8556\n",
      "Epoch: 334/2000... Training loss: 1.9326\n",
      "Epoch: 334/2000... Training loss: 1.6466\n",
      "Epoch: 334/2000... Training loss: 1.4466\n",
      "Epoch: 334/2000... Training loss: 1.8856\n",
      "Epoch: 334/2000... Training loss: 1.6801\n",
      "Epoch: 334/2000... Training loss: 1.6050\n",
      "Epoch: 334/2000... Training loss: 1.8230\n",
      "Epoch: 334/2000... Training loss: 1.7764\n",
      "Epoch: 334/2000... Training loss: 1.8014\n",
      "Epoch: 334/2000... Training loss: 1.9852\n",
      "Epoch: 334/2000... Training loss: 1.8968\n",
      "Epoch: 334/2000... Training loss: 1.7416\n",
      "Epoch: 334/2000... Training loss: 1.7245\n",
      "Epoch: 334/2000... Training loss: 1.7908\n",
      "Epoch: 334/2000... Training loss: 1.9552\n",
      "Epoch: 334/2000... Training loss: 1.6798\n",
      "Epoch: 334/2000... Training loss: 1.4373\n",
      "Epoch: 334/2000... Training loss: 1.8195\n",
      "Epoch: 335/2000... Training loss: 1.5314\n",
      "Epoch: 335/2000... Training loss: 1.8013\n",
      "Epoch: 335/2000... Training loss: 1.4444\n",
      "Epoch: 335/2000... Training loss: 2.0278\n",
      "Epoch: 335/2000... Training loss: 1.6423\n",
      "Epoch: 335/2000... Training loss: 2.0285\n",
      "Epoch: 335/2000... Training loss: 1.6228\n",
      "Epoch: 335/2000... Training loss: 1.7558\n",
      "Epoch: 335/2000... Training loss: 2.0211\n",
      "Epoch: 335/2000... Training loss: 1.8773\n",
      "Epoch: 335/2000... Training loss: 1.9607\n",
      "Epoch: 335/2000... Training loss: 1.6727\n",
      "Epoch: 335/2000... Training loss: 1.7478\n",
      "Epoch: 335/2000... Training loss: 1.8475\n",
      "Epoch: 335/2000... Training loss: 1.9045\n",
      "Epoch: 335/2000... Training loss: 1.8112\n",
      "Epoch: 335/2000... Training loss: 1.7529\n",
      "Epoch: 335/2000... Training loss: 1.8654\n",
      "Epoch: 335/2000... Training loss: 2.0190\n",
      "Epoch: 335/2000... Training loss: 1.9263\n",
      "Epoch: 335/2000... Training loss: 1.7215\n",
      "Epoch: 335/2000... Training loss: 1.7699\n",
      "Epoch: 335/2000... Training loss: 1.8424\n",
      "Epoch: 335/2000... Training loss: 1.7081\n",
      "Epoch: 335/2000... Training loss: 2.0478\n",
      "Epoch: 335/2000... Training loss: 1.6938\n",
      "Epoch: 335/2000... Training loss: 1.9322\n",
      "Epoch: 335/2000... Training loss: 1.8569\n",
      "Epoch: 335/2000... Training loss: 1.9924\n",
      "Epoch: 335/2000... Training loss: 1.7015\n",
      "Epoch: 335/2000... Training loss: 1.7884\n",
      "Epoch: 336/2000... Training loss: 1.8076\n",
      "Epoch: 336/2000... Training loss: 1.7389\n",
      "Epoch: 336/2000... Training loss: 1.4954\n",
      "Epoch: 336/2000... Training loss: 1.6724\n",
      "Epoch: 336/2000... Training loss: 1.7754\n",
      "Epoch: 336/2000... Training loss: 1.7241\n",
      "Epoch: 336/2000... Training loss: 1.8460\n",
      "Epoch: 336/2000... Training loss: 1.9841\n",
      "Epoch: 336/2000... Training loss: 1.6636\n",
      "Epoch: 336/2000... Training loss: 1.7875\n",
      "Epoch: 336/2000... Training loss: 1.8122\n",
      "Epoch: 336/2000... Training loss: 1.7934\n",
      "Epoch: 336/2000... Training loss: 1.7769\n",
      "Epoch: 336/2000... Training loss: 1.8503\n",
      "Epoch: 336/2000... Training loss: 1.7753\n",
      "Epoch: 336/2000... Training loss: 1.6148\n",
      "Epoch: 336/2000... Training loss: 1.8083\n",
      "Epoch: 336/2000... Training loss: 1.5482\n",
      "Epoch: 336/2000... Training loss: 1.8683\n",
      "Epoch: 336/2000... Training loss: 1.8974\n",
      "Epoch: 336/2000... Training loss: 1.5564\n",
      "Epoch: 336/2000... Training loss: 1.9080\n",
      "Epoch: 336/2000... Training loss: 1.7504\n",
      "Epoch: 336/2000... Training loss: 1.6671\n",
      "Epoch: 336/2000... Training loss: 1.7348\n",
      "Epoch: 336/2000... Training loss: 1.5194\n",
      "Epoch: 336/2000... Training loss: 1.8924\n",
      "Epoch: 336/2000... Training loss: 2.0229\n",
      "Epoch: 336/2000... Training loss: 1.7923\n",
      "Epoch: 336/2000... Training loss: 1.5563\n",
      "Epoch: 336/2000... Training loss: 1.6808\n",
      "Epoch: 337/2000... Training loss: 1.8221\n",
      "Epoch: 337/2000... Training loss: 1.9177\n",
      "Epoch: 337/2000... Training loss: 1.7724\n",
      "Epoch: 337/2000... Training loss: 1.6954\n",
      "Epoch: 337/2000... Training loss: 1.9724\n",
      "Epoch: 337/2000... Training loss: 1.8413\n",
      "Epoch: 337/2000... Training loss: 1.7473\n",
      "Epoch: 337/2000... Training loss: 1.5691\n",
      "Epoch: 337/2000... Training loss: 1.8859\n",
      "Epoch: 337/2000... Training loss: 1.7704\n",
      "Epoch: 337/2000... Training loss: 1.6469\n",
      "Epoch: 337/2000... Training loss: 1.6273\n",
      "Epoch: 337/2000... Training loss: 1.7859\n",
      "Epoch: 337/2000... Training loss: 1.7846\n",
      "Epoch: 337/2000... Training loss: 1.7625\n",
      "Epoch: 337/2000... Training loss: 1.5836\n",
      "Epoch: 337/2000... Training loss: 1.8542\n",
      "Epoch: 337/2000... Training loss: 1.8024\n",
      "Epoch: 337/2000... Training loss: 1.4463\n",
      "Epoch: 337/2000... Training loss: 1.5619\n",
      "Epoch: 337/2000... Training loss: 1.6439\n",
      "Epoch: 337/2000... Training loss: 1.8505\n",
      "Epoch: 337/2000... Training loss: 2.0351\n",
      "Epoch: 337/2000... Training loss: 1.8735\n",
      "Epoch: 337/2000... Training loss: 1.5720\n",
      "Epoch: 337/2000... Training loss: 1.7368\n",
      "Epoch: 337/2000... Training loss: 2.0060\n",
      "Epoch: 337/2000... Training loss: 1.6877\n",
      "Epoch: 337/2000... Training loss: 1.3197\n",
      "Epoch: 337/2000... Training loss: 1.7241\n",
      "Epoch: 337/2000... Training loss: 1.7908\n",
      "Epoch: 338/2000... Training loss: 1.6241\n",
      "Epoch: 338/2000... Training loss: 1.8213\n",
      "Epoch: 338/2000... Training loss: 1.8673\n",
      "Epoch: 338/2000... Training loss: 1.7826\n",
      "Epoch: 338/2000... Training loss: 1.7409\n",
      "Epoch: 338/2000... Training loss: 1.7420\n",
      "Epoch: 338/2000... Training loss: 1.7149\n",
      "Epoch: 338/2000... Training loss: 1.3853\n",
      "Epoch: 338/2000... Training loss: 1.7336\n",
      "Epoch: 338/2000... Training loss: 1.6475\n",
      "Epoch: 338/2000... Training loss: 1.7955\n",
      "Epoch: 338/2000... Training loss: 1.7353\n",
      "Epoch: 338/2000... Training loss: 1.9863\n",
      "Epoch: 338/2000... Training loss: 1.8213\n",
      "Epoch: 338/2000... Training loss: 1.9181\n",
      "Epoch: 338/2000... Training loss: 1.6779\n",
      "Epoch: 338/2000... Training loss: 1.7300\n",
      "Epoch: 338/2000... Training loss: 1.9643\n",
      "Epoch: 338/2000... Training loss: 1.9058\n",
      "Epoch: 338/2000... Training loss: 1.8756\n",
      "Epoch: 338/2000... Training loss: 1.5765\n",
      "Epoch: 338/2000... Training loss: 1.7279\n",
      "Epoch: 338/2000... Training loss: 1.7908\n",
      "Epoch: 338/2000... Training loss: 1.4539\n",
      "Epoch: 338/2000... Training loss: 1.6016\n",
      "Epoch: 338/2000... Training loss: 1.6077\n",
      "Epoch: 338/2000... Training loss: 1.7969\n",
      "Epoch: 338/2000... Training loss: 1.7761\n",
      "Epoch: 338/2000... Training loss: 1.6839\n",
      "Epoch: 338/2000... Training loss: 1.7681\n",
      "Epoch: 338/2000... Training loss: 1.7625\n",
      "Epoch: 339/2000... Training loss: 1.7208\n",
      "Epoch: 339/2000... Training loss: 1.7614\n",
      "Epoch: 339/2000... Training loss: 1.5665\n",
      "Epoch: 339/2000... Training loss: 1.8230\n",
      "Epoch: 339/2000... Training loss: 1.7369\n",
      "Epoch: 339/2000... Training loss: 2.0488\n",
      "Epoch: 339/2000... Training loss: 1.5750\n",
      "Epoch: 339/2000... Training loss: 1.7347\n",
      "Epoch: 339/2000... Training loss: 1.6578\n",
      "Epoch: 339/2000... Training loss: 1.8669\n",
      "Epoch: 339/2000... Training loss: 1.6523\n",
      "Epoch: 339/2000... Training loss: 1.9510\n",
      "Epoch: 339/2000... Training loss: 1.6359\n",
      "Epoch: 339/2000... Training loss: 1.8853\n",
      "Epoch: 339/2000... Training loss: 1.7651\n",
      "Epoch: 339/2000... Training loss: 1.6082\n",
      "Epoch: 339/2000... Training loss: 1.4842\n",
      "Epoch: 339/2000... Training loss: 1.6676\n",
      "Epoch: 339/2000... Training loss: 1.7290\n",
      "Epoch: 339/2000... Training loss: 1.9429\n",
      "Epoch: 339/2000... Training loss: 1.6471\n",
      "Epoch: 339/2000... Training loss: 1.5722\n",
      "Epoch: 339/2000... Training loss: 1.7116\n",
      "Epoch: 339/2000... Training loss: 1.6470\n",
      "Epoch: 339/2000... Training loss: 1.5691\n",
      "Epoch: 339/2000... Training loss: 1.7414\n",
      "Epoch: 339/2000... Training loss: 1.5323\n",
      "Epoch: 339/2000... Training loss: 1.5321\n",
      "Epoch: 339/2000... Training loss: 1.7807\n",
      "Epoch: 339/2000... Training loss: 1.8100\n",
      "Epoch: 339/2000... Training loss: 1.6089\n",
      "Epoch: 340/2000... Training loss: 1.7321\n",
      "Epoch: 340/2000... Training loss: 1.7744\n",
      "Epoch: 340/2000... Training loss: 1.3162\n",
      "Epoch: 340/2000... Training loss: 1.5577\n",
      "Epoch: 340/2000... Training loss: 1.8194\n",
      "Epoch: 340/2000... Training loss: 1.6310\n",
      "Epoch: 340/2000... Training loss: 1.8624\n",
      "Epoch: 340/2000... Training loss: 1.8524\n",
      "Epoch: 340/2000... Training loss: 1.7841\n",
      "Epoch: 340/2000... Training loss: 1.5667\n",
      "Epoch: 340/2000... Training loss: 1.7424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 340/2000... Training loss: 1.3462\n",
      "Epoch: 340/2000... Training loss: 1.5247\n",
      "Epoch: 340/2000... Training loss: 1.7948\n",
      "Epoch: 340/2000... Training loss: 1.8239\n",
      "Epoch: 340/2000... Training loss: 1.9548\n",
      "Epoch: 340/2000... Training loss: 1.8508\n",
      "Epoch: 340/2000... Training loss: 2.2027\n",
      "Epoch: 340/2000... Training loss: 1.6120\n",
      "Epoch: 340/2000... Training loss: 1.7898\n",
      "Epoch: 340/2000... Training loss: 1.9386\n",
      "Epoch: 340/2000... Training loss: 1.5041\n",
      "Epoch: 340/2000... Training loss: 1.8679\n",
      "Epoch: 340/2000... Training loss: 1.8150\n",
      "Epoch: 340/2000... Training loss: 1.6278\n",
      "Epoch: 340/2000... Training loss: 1.7723\n",
      "Epoch: 340/2000... Training loss: 1.5598\n",
      "Epoch: 340/2000... Training loss: 1.9331\n",
      "Epoch: 340/2000... Training loss: 1.7533\n",
      "Epoch: 340/2000... Training loss: 1.8433\n",
      "Epoch: 340/2000... Training loss: 1.7576\n",
      "Epoch: 341/2000... Training loss: 1.4520\n",
      "Epoch: 341/2000... Training loss: 1.5046\n",
      "Epoch: 341/2000... Training loss: 1.8651\n",
      "Epoch: 341/2000... Training loss: 1.5958\n",
      "Epoch: 341/2000... Training loss: 1.6326\n",
      "Epoch: 341/2000... Training loss: 1.7402\n",
      "Epoch: 341/2000... Training loss: 1.6117\n",
      "Epoch: 341/2000... Training loss: 1.6042\n",
      "Epoch: 341/2000... Training loss: 1.7369\n",
      "Epoch: 341/2000... Training loss: 1.8213\n",
      "Epoch: 341/2000... Training loss: 1.7833\n",
      "Epoch: 341/2000... Training loss: 1.9076\n",
      "Epoch: 341/2000... Training loss: 1.7345\n",
      "Epoch: 341/2000... Training loss: 1.9176\n",
      "Epoch: 341/2000... Training loss: 1.5543\n",
      "Epoch: 341/2000... Training loss: 1.6978\n",
      "Epoch: 341/2000... Training loss: 1.8254\n",
      "Epoch: 341/2000... Training loss: 1.4229\n",
      "Epoch: 341/2000... Training loss: 1.6347\n",
      "Epoch: 341/2000... Training loss: 1.8999\n",
      "Epoch: 341/2000... Training loss: 1.6967\n",
      "Epoch: 341/2000... Training loss: 1.8377\n",
      "Epoch: 341/2000... Training loss: 1.8010\n",
      "Epoch: 341/2000... Training loss: 1.6343\n",
      "Epoch: 341/2000... Training loss: 1.7722\n",
      "Epoch: 341/2000... Training loss: 1.6193\n",
      "Epoch: 341/2000... Training loss: 2.0908\n",
      "Epoch: 341/2000... Training loss: 1.6130\n",
      "Epoch: 341/2000... Training loss: 1.7636\n",
      "Epoch: 341/2000... Training loss: 1.8135\n",
      "Epoch: 341/2000... Training loss: 1.7092\n",
      "Epoch: 342/2000... Training loss: 1.8043\n",
      "Epoch: 342/2000... Training loss: 1.5899\n",
      "Epoch: 342/2000... Training loss: 1.8276\n",
      "Epoch: 342/2000... Training loss: 1.6824\n",
      "Epoch: 342/2000... Training loss: 1.6024\n",
      "Epoch: 342/2000... Training loss: 1.5545\n",
      "Epoch: 342/2000... Training loss: 1.6012\n",
      "Epoch: 342/2000... Training loss: 1.6840\n",
      "Epoch: 342/2000... Training loss: 1.8927\n",
      "Epoch: 342/2000... Training loss: 1.9729\n",
      "Epoch: 342/2000... Training loss: 1.8202\n",
      "Epoch: 342/2000... Training loss: 1.6257\n",
      "Epoch: 342/2000... Training loss: 1.5236\n",
      "Epoch: 342/2000... Training loss: 1.8721\n",
      "Epoch: 342/2000... Training loss: 1.4130\n",
      "Epoch: 342/2000... Training loss: 1.3848\n",
      "Epoch: 342/2000... Training loss: 1.5688\n",
      "Epoch: 342/2000... Training loss: 2.1160\n",
      "Epoch: 342/2000... Training loss: 1.8528\n",
      "Epoch: 342/2000... Training loss: 1.7188\n",
      "Epoch: 342/2000... Training loss: 1.7523\n",
      "Epoch: 342/2000... Training loss: 1.7843\n",
      "Epoch: 342/2000... Training loss: 1.6913\n",
      "Epoch: 342/2000... Training loss: 1.7335\n",
      "Epoch: 342/2000... Training loss: 1.7482\n",
      "Epoch: 342/2000... Training loss: 1.7821\n",
      "Epoch: 342/2000... Training loss: 1.8640\n",
      "Epoch: 342/2000... Training loss: 2.1567\n",
      "Epoch: 342/2000... Training loss: 1.4883\n",
      "Epoch: 342/2000... Training loss: 1.7151\n",
      "Epoch: 342/2000... Training loss: 1.7899\n",
      "Epoch: 343/2000... Training loss: 1.8519\n",
      "Epoch: 343/2000... Training loss: 1.6752\n",
      "Epoch: 343/2000... Training loss: 1.6063\n",
      "Epoch: 343/2000... Training loss: 1.5118\n",
      "Epoch: 343/2000... Training loss: 1.7504\n",
      "Epoch: 343/2000... Training loss: 1.9006\n",
      "Epoch: 343/2000... Training loss: 1.7383\n",
      "Epoch: 343/2000... Training loss: 1.6337\n",
      "Epoch: 343/2000... Training loss: 1.7246\n",
      "Epoch: 343/2000... Training loss: 1.6602\n",
      "Epoch: 343/2000... Training loss: 1.8270\n",
      "Epoch: 343/2000... Training loss: 1.7666\n",
      "Epoch: 343/2000... Training loss: 1.7052\n",
      "Epoch: 343/2000... Training loss: 2.0917\n",
      "Epoch: 343/2000... Training loss: 1.4427\n",
      "Epoch: 343/2000... Training loss: 1.8134\n",
      "Epoch: 343/2000... Training loss: 1.9527\n",
      "Epoch: 343/2000... Training loss: 1.5946\n",
      "Epoch: 343/2000... Training loss: 1.6967\n",
      "Epoch: 343/2000... Training loss: 1.8545\n",
      "Epoch: 343/2000... Training loss: 1.6961\n",
      "Epoch: 343/2000... Training loss: 1.7946\n",
      "Epoch: 343/2000... Training loss: 1.7461\n",
      "Epoch: 343/2000... Training loss: 1.5761\n",
      "Epoch: 343/2000... Training loss: 1.6732\n",
      "Epoch: 343/2000... Training loss: 1.6468\n",
      "Epoch: 343/2000... Training loss: 1.9093\n",
      "Epoch: 343/2000... Training loss: 1.7150\n",
      "Epoch: 343/2000... Training loss: 1.7833\n",
      "Epoch: 343/2000... Training loss: 1.7015\n",
      "Epoch: 343/2000... Training loss: 1.6392\n",
      "Epoch: 344/2000... Training loss: 1.6598\n",
      "Epoch: 344/2000... Training loss: 1.5722\n",
      "Epoch: 344/2000... Training loss: 1.8197\n",
      "Epoch: 344/2000... Training loss: 1.7391\n",
      "Epoch: 344/2000... Training loss: 1.5796\n",
      "Epoch: 344/2000... Training loss: 1.7426\n",
      "Epoch: 344/2000... Training loss: 1.6632\n",
      "Epoch: 344/2000... Training loss: 1.5814\n",
      "Epoch: 344/2000... Training loss: 1.6598\n",
      "Epoch: 344/2000... Training loss: 1.6839\n",
      "Epoch: 344/2000... Training loss: 1.7732\n",
      "Epoch: 344/2000... Training loss: 1.4576\n",
      "Epoch: 344/2000... Training loss: 1.7709\n",
      "Epoch: 344/2000... Training loss: 1.8413\n",
      "Epoch: 344/2000... Training loss: 1.7879\n",
      "Epoch: 344/2000... Training loss: 1.5837\n",
      "Epoch: 344/2000... Training loss: 1.5542\n",
      "Epoch: 344/2000... Training loss: 1.9155\n",
      "Epoch: 344/2000... Training loss: 1.8632\n",
      "Epoch: 344/2000... Training loss: 1.9588\n",
      "Epoch: 344/2000... Training loss: 1.6718\n",
      "Epoch: 344/2000... Training loss: 1.5070\n",
      "Epoch: 344/2000... Training loss: 1.5645\n",
      "Epoch: 344/2000... Training loss: 1.6189\n",
      "Epoch: 344/2000... Training loss: 1.8947\n",
      "Epoch: 344/2000... Training loss: 1.4629\n",
      "Epoch: 344/2000... Training loss: 1.7070\n",
      "Epoch: 344/2000... Training loss: 1.9961\n",
      "Epoch: 344/2000... Training loss: 1.9601\n",
      "Epoch: 344/2000... Training loss: 1.4467\n",
      "Epoch: 344/2000... Training loss: 1.5203\n",
      "Epoch: 345/2000... Training loss: 1.6418\n",
      "Epoch: 345/2000... Training loss: 1.6960\n",
      "Epoch: 345/2000... Training loss: 1.5601\n",
      "Epoch: 345/2000... Training loss: 1.5152\n",
      "Epoch: 345/2000... Training loss: 1.8567\n",
      "Epoch: 345/2000... Training loss: 1.8797\n",
      "Epoch: 345/2000... Training loss: 1.8152\n",
      "Epoch: 345/2000... Training loss: 1.8100\n",
      "Epoch: 345/2000... Training loss: 1.7606\n",
      "Epoch: 345/2000... Training loss: 1.8696\n",
      "Epoch: 345/2000... Training loss: 1.6723\n",
      "Epoch: 345/2000... Training loss: 1.4460\n",
      "Epoch: 345/2000... Training loss: 1.7305\n",
      "Epoch: 345/2000... Training loss: 1.7432\n",
      "Epoch: 345/2000... Training loss: 1.8812\n",
      "Epoch: 345/2000... Training loss: 1.6276\n",
      "Epoch: 345/2000... Training loss: 1.6408\n",
      "Epoch: 345/2000... Training loss: 1.4466\n",
      "Epoch: 345/2000... Training loss: 1.9002\n",
      "Epoch: 345/2000... Training loss: 1.8920\n",
      "Epoch: 345/2000... Training loss: 1.5462\n",
      "Epoch: 345/2000... Training loss: 1.9803\n",
      "Epoch: 345/2000... Training loss: 1.7737\n",
      "Epoch: 345/2000... Training loss: 2.0611\n",
      "Epoch: 345/2000... Training loss: 1.6587\n",
      "Epoch: 345/2000... Training loss: 1.5711\n",
      "Epoch: 345/2000... Training loss: 1.6610\n",
      "Epoch: 345/2000... Training loss: 1.5189\n",
      "Epoch: 345/2000... Training loss: 1.6531\n",
      "Epoch: 345/2000... Training loss: 1.7666\n",
      "Epoch: 345/2000... Training loss: 1.9110\n",
      "Epoch: 346/2000... Training loss: 1.6682\n",
      "Epoch: 346/2000... Training loss: 1.6337\n",
      "Epoch: 346/2000... Training loss: 1.6863\n",
      "Epoch: 346/2000... Training loss: 1.8799\n",
      "Epoch: 346/2000... Training loss: 1.7410\n",
      "Epoch: 346/2000... Training loss: 2.0398\n",
      "Epoch: 346/2000... Training loss: 1.7618\n",
      "Epoch: 346/2000... Training loss: 1.8012\n",
      "Epoch: 346/2000... Training loss: 1.7100\n",
      "Epoch: 346/2000... Training loss: 1.6276\n",
      "Epoch: 346/2000... Training loss: 1.6009\n",
      "Epoch: 346/2000... Training loss: 1.6352\n",
      "Epoch: 346/2000... Training loss: 1.9854\n",
      "Epoch: 346/2000... Training loss: 1.7725\n",
      "Epoch: 346/2000... Training loss: 1.4722\n",
      "Epoch: 346/2000... Training loss: 1.8229\n",
      "Epoch: 346/2000... Training loss: 1.6055\n",
      "Epoch: 346/2000... Training loss: 1.5557\n",
      "Epoch: 346/2000... Training loss: 1.7341\n",
      "Epoch: 346/2000... Training loss: 1.7072\n",
      "Epoch: 346/2000... Training loss: 1.8146\n",
      "Epoch: 346/2000... Training loss: 1.6653\n",
      "Epoch: 346/2000... Training loss: 1.7559\n",
      "Epoch: 346/2000... Training loss: 1.5603\n",
      "Epoch: 346/2000... Training loss: 1.6226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 346/2000... Training loss: 1.6449\n",
      "Epoch: 346/2000... Training loss: 1.8016\n",
      "Epoch: 346/2000... Training loss: 1.5728\n",
      "Epoch: 346/2000... Training loss: 1.7392\n",
      "Epoch: 346/2000... Training loss: 1.7240\n",
      "Epoch: 346/2000... Training loss: 1.8270\n",
      "Epoch: 347/2000... Training loss: 1.8697\n",
      "Epoch: 347/2000... Training loss: 1.6511\n",
      "Epoch: 347/2000... Training loss: 1.7617\n",
      "Epoch: 347/2000... Training loss: 1.6904\n",
      "Epoch: 347/2000... Training loss: 1.4559\n",
      "Epoch: 347/2000... Training loss: 1.9278\n",
      "Epoch: 347/2000... Training loss: 1.6215\n",
      "Epoch: 347/2000... Training loss: 1.6053\n",
      "Epoch: 347/2000... Training loss: 1.4955\n",
      "Epoch: 347/2000... Training loss: 1.8342\n",
      "Epoch: 347/2000... Training loss: 1.5813\n",
      "Epoch: 347/2000... Training loss: 1.6920\n",
      "Epoch: 347/2000... Training loss: 2.0470\n",
      "Epoch: 347/2000... Training loss: 1.8252\n",
      "Epoch: 347/2000... Training loss: 1.7231\n",
      "Epoch: 347/2000... Training loss: 1.8301\n",
      "Epoch: 347/2000... Training loss: 1.8894\n",
      "Epoch: 347/2000... Training loss: 1.9658\n",
      "Epoch: 347/2000... Training loss: 1.6829\n",
      "Epoch: 347/2000... Training loss: 1.5237\n",
      "Epoch: 347/2000... Training loss: 1.8548\n",
      "Epoch: 347/2000... Training loss: 1.7457\n",
      "Epoch: 347/2000... Training loss: 1.7736\n",
      "Epoch: 347/2000... Training loss: 1.8589\n",
      "Epoch: 347/2000... Training loss: 1.6716\n",
      "Epoch: 347/2000... Training loss: 1.6569\n",
      "Epoch: 347/2000... Training loss: 1.4719\n",
      "Epoch: 347/2000... Training loss: 1.8712\n",
      "Epoch: 347/2000... Training loss: 1.4619\n",
      "Epoch: 347/2000... Training loss: 1.4692\n",
      "Epoch: 347/2000... Training loss: 1.7569\n",
      "Epoch: 348/2000... Training loss: 1.3058\n",
      "Epoch: 348/2000... Training loss: 1.6313\n",
      "Epoch: 348/2000... Training loss: 1.5750\n",
      "Epoch: 348/2000... Training loss: 1.5712\n",
      "Epoch: 348/2000... Training loss: 1.8233\n",
      "Epoch: 348/2000... Training loss: 1.6888\n",
      "Epoch: 348/2000... Training loss: 1.7189\n",
      "Epoch: 348/2000... Training loss: 1.8789\n",
      "Epoch: 348/2000... Training loss: 1.5519\n",
      "Epoch: 348/2000... Training loss: 1.6338\n",
      "Epoch: 348/2000... Training loss: 1.8601\n",
      "Epoch: 348/2000... Training loss: 1.5834\n",
      "Epoch: 348/2000... Training loss: 1.6010\n",
      "Epoch: 348/2000... Training loss: 1.9182\n",
      "Epoch: 348/2000... Training loss: 1.5199\n",
      "Epoch: 348/2000... Training loss: 1.6597\n",
      "Epoch: 348/2000... Training loss: 1.6146\n",
      "Epoch: 348/2000... Training loss: 1.6430\n",
      "Epoch: 348/2000... Training loss: 1.9723\n",
      "Epoch: 348/2000... Training loss: 1.7748\n",
      "Epoch: 348/2000... Training loss: 1.6350\n",
      "Epoch: 348/2000... Training loss: 1.5807\n",
      "Epoch: 348/2000... Training loss: 1.7216\n",
      "Epoch: 348/2000... Training loss: 1.7630\n",
      "Epoch: 348/2000... Training loss: 1.7163\n",
      "Epoch: 348/2000... Training loss: 1.5035\n",
      "Epoch: 348/2000... Training loss: 1.7088\n",
      "Epoch: 348/2000... Training loss: 1.5174\n",
      "Epoch: 348/2000... Training loss: 1.6079\n",
      "Epoch: 348/2000... Training loss: 1.8505\n",
      "Epoch: 348/2000... Training loss: 2.1086\n",
      "Epoch: 349/2000... Training loss: 1.5610\n",
      "Epoch: 349/2000... Training loss: 1.8630\n",
      "Epoch: 349/2000... Training loss: 1.7529\n",
      "Epoch: 349/2000... Training loss: 1.7655\n",
      "Epoch: 349/2000... Training loss: 1.7625\n",
      "Epoch: 349/2000... Training loss: 1.6783\n",
      "Epoch: 349/2000... Training loss: 1.9752\n",
      "Epoch: 349/2000... Training loss: 1.7543\n",
      "Epoch: 349/2000... Training loss: 1.6657\n",
      "Epoch: 349/2000... Training loss: 1.6670\n",
      "Epoch: 349/2000... Training loss: 1.7482\n",
      "Epoch: 349/2000... Training loss: 1.7439\n",
      "Epoch: 349/2000... Training loss: 1.9625\n",
      "Epoch: 349/2000... Training loss: 1.8435\n",
      "Epoch: 349/2000... Training loss: 1.5780\n",
      "Epoch: 349/2000... Training loss: 1.7419\n",
      "Epoch: 349/2000... Training loss: 1.9158\n",
      "Epoch: 349/2000... Training loss: 1.4973\n",
      "Epoch: 349/2000... Training loss: 1.6191\n",
      "Epoch: 349/2000... Training loss: 1.8617\n",
      "Epoch: 349/2000... Training loss: 1.8756\n",
      "Epoch: 349/2000... Training loss: 1.6985\n",
      "Epoch: 349/2000... Training loss: 1.7544\n",
      "Epoch: 349/2000... Training loss: 1.5704\n",
      "Epoch: 349/2000... Training loss: 1.7840\n",
      "Epoch: 349/2000... Training loss: 1.6605\n",
      "Epoch: 349/2000... Training loss: 1.6028\n",
      "Epoch: 349/2000... Training loss: 1.9012\n",
      "Epoch: 349/2000... Training loss: 1.5160\n",
      "Epoch: 349/2000... Training loss: 1.4061\n",
      "Epoch: 349/2000... Training loss: 1.7131\n",
      "Epoch: 350/2000... Training loss: 1.6990\n",
      "Epoch: 350/2000... Training loss: 1.5523\n",
      "Epoch: 350/2000... Training loss: 1.6538\n",
      "Epoch: 350/2000... Training loss: 1.6414\n",
      "Epoch: 350/2000... Training loss: 1.6548\n",
      "Epoch: 350/2000... Training loss: 1.5951\n",
      "Epoch: 350/2000... Training loss: 1.8674\n",
      "Epoch: 350/2000... Training loss: 1.6580\n",
      "Epoch: 350/2000... Training loss: 1.6524\n",
      "Epoch: 350/2000... Training loss: 1.5732\n",
      "Epoch: 350/2000... Training loss: 1.8337\n",
      "Epoch: 350/2000... Training loss: 1.8069\n",
      "Epoch: 350/2000... Training loss: 1.8434\n",
      "Epoch: 350/2000... Training loss: 2.0887\n",
      "Epoch: 350/2000... Training loss: 1.4442\n",
      "Epoch: 350/2000... Training loss: 1.4117\n",
      "Epoch: 350/2000... Training loss: 1.4999\n",
      "Epoch: 350/2000... Training loss: 1.9474\n",
      "Epoch: 350/2000... Training loss: 1.8249\n",
      "Epoch: 350/2000... Training loss: 1.7215\n",
      "Epoch: 350/2000... Training loss: 1.7092\n",
      "Epoch: 350/2000... Training loss: 1.8705\n",
      "Epoch: 350/2000... Training loss: 1.4571\n",
      "Epoch: 350/2000... Training loss: 1.6807\n",
      "Epoch: 350/2000... Training loss: 1.4014\n",
      "Epoch: 350/2000... Training loss: 1.6603\n",
      "Epoch: 350/2000... Training loss: 1.7141\n",
      "Epoch: 350/2000... Training loss: 1.7307\n",
      "Epoch: 350/2000... Training loss: 1.7323\n",
      "Epoch: 350/2000... Training loss: 1.7999\n",
      "Epoch: 350/2000... Training loss: 1.4083\n",
      "Epoch: 351/2000... Training loss: 1.5595\n",
      "Epoch: 351/2000... Training loss: 1.6885\n",
      "Epoch: 351/2000... Training loss: 1.6173\n",
      "Epoch: 351/2000... Training loss: 2.0003\n",
      "Epoch: 351/2000... Training loss: 1.8381\n",
      "Epoch: 351/2000... Training loss: 1.7144\n",
      "Epoch: 351/2000... Training loss: 1.9560\n",
      "Epoch: 351/2000... Training loss: 1.6569\n",
      "Epoch: 351/2000... Training loss: 1.6351\n",
      "Epoch: 351/2000... Training loss: 1.5510\n",
      "Epoch: 351/2000... Training loss: 1.6621\n",
      "Epoch: 351/2000... Training loss: 1.7409\n",
      "Epoch: 351/2000... Training loss: 1.5358\n",
      "Epoch: 351/2000... Training loss: 1.7001\n",
      "Epoch: 351/2000... Training loss: 1.6269\n",
      "Epoch: 351/2000... Training loss: 1.9030\n",
      "Epoch: 351/2000... Training loss: 1.9752\n",
      "Epoch: 351/2000... Training loss: 1.7460\n",
      "Epoch: 351/2000... Training loss: 1.9415\n",
      "Epoch: 351/2000... Training loss: 1.6355\n",
      "Epoch: 351/2000... Training loss: 1.6246\n",
      "Epoch: 351/2000... Training loss: 1.8130\n",
      "Epoch: 351/2000... Training loss: 1.6576\n",
      "Epoch: 351/2000... Training loss: 1.8829\n",
      "Epoch: 351/2000... Training loss: 1.6259\n",
      "Epoch: 351/2000... Training loss: 1.7918\n",
      "Epoch: 351/2000... Training loss: 1.6825\n",
      "Epoch: 351/2000... Training loss: 1.8579\n",
      "Epoch: 351/2000... Training loss: 1.6685\n",
      "Epoch: 351/2000... Training loss: 1.8552\n",
      "Epoch: 351/2000... Training loss: 1.4286\n",
      "Epoch: 352/2000... Training loss: 1.6091\n",
      "Epoch: 352/2000... Training loss: 1.8873\n",
      "Epoch: 352/2000... Training loss: 1.6287\n",
      "Epoch: 352/2000... Training loss: 1.7147\n",
      "Epoch: 352/2000... Training loss: 1.3050\n",
      "Epoch: 352/2000... Training loss: 1.5222\n",
      "Epoch: 352/2000... Training loss: 1.8637\n",
      "Epoch: 352/2000... Training loss: 1.5598\n",
      "Epoch: 352/2000... Training loss: 1.9749\n",
      "Epoch: 352/2000... Training loss: 1.8111\n",
      "Epoch: 352/2000... Training loss: 1.8037\n",
      "Epoch: 352/2000... Training loss: 1.6221\n",
      "Epoch: 352/2000... Training loss: 1.5087\n",
      "Epoch: 352/2000... Training loss: 1.9336\n",
      "Epoch: 352/2000... Training loss: 1.7932\n",
      "Epoch: 352/2000... Training loss: 1.7495\n",
      "Epoch: 352/2000... Training loss: 1.7158\n",
      "Epoch: 352/2000... Training loss: 1.7966\n",
      "Epoch: 352/2000... Training loss: 1.6020\n",
      "Epoch: 352/2000... Training loss: 1.8325\n",
      "Epoch: 352/2000... Training loss: 1.7667\n",
      "Epoch: 352/2000... Training loss: 1.8388\n",
      "Epoch: 352/2000... Training loss: 1.5448\n",
      "Epoch: 352/2000... Training loss: 1.9076\n",
      "Epoch: 352/2000... Training loss: 1.6091\n",
      "Epoch: 352/2000... Training loss: 1.6779\n",
      "Epoch: 352/2000... Training loss: 1.5571\n",
      "Epoch: 352/2000... Training loss: 1.5978\n",
      "Epoch: 352/2000... Training loss: 1.6696\n",
      "Epoch: 352/2000... Training loss: 1.7710\n",
      "Epoch: 352/2000... Training loss: 1.7176\n",
      "Epoch: 353/2000... Training loss: 1.5452\n",
      "Epoch: 353/2000... Training loss: 1.8536\n",
      "Epoch: 353/2000... Training loss: 1.7985\n",
      "Epoch: 353/2000... Training loss: 1.6100\n",
      "Epoch: 353/2000... Training loss: 1.5732\n",
      "Epoch: 353/2000... Training loss: 1.6501\n",
      "Epoch: 353/2000... Training loss: 1.6209\n",
      "Epoch: 353/2000... Training loss: 1.8180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 353/2000... Training loss: 1.8017\n",
      "Epoch: 353/2000... Training loss: 1.7946\n",
      "Epoch: 353/2000... Training loss: 1.7458\n",
      "Epoch: 353/2000... Training loss: 1.8587\n",
      "Epoch: 353/2000... Training loss: 1.8691\n",
      "Epoch: 353/2000... Training loss: 1.9815\n",
      "Epoch: 353/2000... Training loss: 1.4926\n",
      "Epoch: 353/2000... Training loss: 1.5862\n",
      "Epoch: 353/2000... Training loss: 1.7077\n",
      "Epoch: 353/2000... Training loss: 1.8604\n",
      "Epoch: 353/2000... Training loss: 1.7176\n",
      "Epoch: 353/2000... Training loss: 1.6935\n",
      "Epoch: 353/2000... Training loss: 1.6230\n",
      "Epoch: 353/2000... Training loss: 1.5696\n",
      "Epoch: 353/2000... Training loss: 1.6109\n",
      "Epoch: 353/2000... Training loss: 1.4584\n",
      "Epoch: 353/2000... Training loss: 1.5895\n",
      "Epoch: 353/2000... Training loss: 1.7172\n",
      "Epoch: 353/2000... Training loss: 1.7010\n",
      "Epoch: 353/2000... Training loss: 1.5498\n",
      "Epoch: 353/2000... Training loss: 1.7103\n",
      "Epoch: 353/2000... Training loss: 1.6863\n",
      "Epoch: 353/2000... Training loss: 1.9256\n",
      "Epoch: 354/2000... Training loss: 1.4925\n",
      "Epoch: 354/2000... Training loss: 1.4281\n",
      "Epoch: 354/2000... Training loss: 1.7709\n",
      "Epoch: 354/2000... Training loss: 1.6419\n",
      "Epoch: 354/2000... Training loss: 1.6989\n",
      "Epoch: 354/2000... Training loss: 1.5674\n",
      "Epoch: 354/2000... Training loss: 1.6922\n",
      "Epoch: 354/2000... Training loss: 1.5457\n",
      "Epoch: 354/2000... Training loss: 1.6671\n",
      "Epoch: 354/2000... Training loss: 1.8629\n",
      "Epoch: 354/2000... Training loss: 1.6986\n",
      "Epoch: 354/2000... Training loss: 1.4531\n",
      "Epoch: 354/2000... Training loss: 1.7522\n",
      "Epoch: 354/2000... Training loss: 1.5036\n",
      "Epoch: 354/2000... Training loss: 1.5827\n",
      "Epoch: 354/2000... Training loss: 1.7125\n",
      "Epoch: 354/2000... Training loss: 1.7075\n",
      "Epoch: 354/2000... Training loss: 1.8729\n",
      "Epoch: 354/2000... Training loss: 1.5695\n",
      "Epoch: 354/2000... Training loss: 2.0582\n",
      "Epoch: 354/2000... Training loss: 1.5224\n",
      "Epoch: 354/2000... Training loss: 1.6827\n",
      "Epoch: 354/2000... Training loss: 1.6837\n",
      "Epoch: 354/2000... Training loss: 1.6797\n",
      "Epoch: 354/2000... Training loss: 1.4299\n",
      "Epoch: 354/2000... Training loss: 1.6866\n",
      "Epoch: 354/2000... Training loss: 1.5832\n",
      "Epoch: 354/2000... Training loss: 1.5691\n",
      "Epoch: 354/2000... Training loss: 1.7668\n",
      "Epoch: 354/2000... Training loss: 1.6581\n",
      "Epoch: 354/2000... Training loss: 1.7789\n",
      "Epoch: 355/2000... Training loss: 1.5698\n",
      "Epoch: 355/2000... Training loss: 1.6263\n",
      "Epoch: 355/2000... Training loss: 1.8029\n",
      "Epoch: 355/2000... Training loss: 1.5452\n",
      "Epoch: 355/2000... Training loss: 1.6109\n",
      "Epoch: 355/2000... Training loss: 1.5711\n",
      "Epoch: 355/2000... Training loss: 1.6811\n",
      "Epoch: 355/2000... Training loss: 1.9214\n",
      "Epoch: 355/2000... Training loss: 1.6589\n",
      "Epoch: 355/2000... Training loss: 1.6127\n",
      "Epoch: 355/2000... Training loss: 1.5987\n",
      "Epoch: 355/2000... Training loss: 1.5988\n",
      "Epoch: 355/2000... Training loss: 1.7069\n",
      "Epoch: 355/2000... Training loss: 1.8525\n",
      "Epoch: 355/2000... Training loss: 1.7383\n",
      "Epoch: 355/2000... Training loss: 1.6960\n",
      "Epoch: 355/2000... Training loss: 1.5178\n",
      "Epoch: 355/2000... Training loss: 1.6563\n",
      "Epoch: 355/2000... Training loss: 1.7485\n",
      "Epoch: 355/2000... Training loss: 1.6667\n",
      "Epoch: 355/2000... Training loss: 1.5933\n",
      "Epoch: 355/2000... Training loss: 1.9259\n",
      "Epoch: 355/2000... Training loss: 1.5846\n",
      "Epoch: 355/2000... Training loss: 1.6975\n",
      "Epoch: 355/2000... Training loss: 1.3986\n",
      "Epoch: 355/2000... Training loss: 1.6148\n",
      "Epoch: 355/2000... Training loss: 1.4916\n",
      "Epoch: 355/2000... Training loss: 1.6306\n",
      "Epoch: 355/2000... Training loss: 1.4716\n",
      "Epoch: 355/2000... Training loss: 1.8446\n",
      "Epoch: 355/2000... Training loss: 1.8971\n",
      "Epoch: 356/2000... Training loss: 1.8325\n",
      "Epoch: 356/2000... Training loss: 1.6519\n",
      "Epoch: 356/2000... Training loss: 1.6332\n",
      "Epoch: 356/2000... Training loss: 1.7394\n",
      "Epoch: 356/2000... Training loss: 1.5435\n",
      "Epoch: 356/2000... Training loss: 1.5059\n",
      "Epoch: 356/2000... Training loss: 1.8857\n",
      "Epoch: 356/2000... Training loss: 1.7044\n",
      "Epoch: 356/2000... Training loss: 1.8535\n",
      "Epoch: 356/2000... Training loss: 1.7493\n",
      "Epoch: 356/2000... Training loss: 1.7747\n",
      "Epoch: 356/2000... Training loss: 1.4750\n",
      "Epoch: 356/2000... Training loss: 1.3470\n",
      "Epoch: 356/2000... Training loss: 1.8302\n",
      "Epoch: 356/2000... Training loss: 1.3196\n",
      "Epoch: 356/2000... Training loss: 1.8812\n",
      "Epoch: 356/2000... Training loss: 1.7039\n",
      "Epoch: 356/2000... Training loss: 1.9549\n",
      "Epoch: 356/2000... Training loss: 1.5642\n",
      "Epoch: 356/2000... Training loss: 1.8427\n",
      "Epoch: 356/2000... Training loss: 1.5937\n",
      "Epoch: 356/2000... Training loss: 1.5144\n",
      "Epoch: 356/2000... Training loss: 1.7043\n",
      "Epoch: 356/2000... Training loss: 1.8100\n",
      "Epoch: 356/2000... Training loss: 1.4986\n",
      "Epoch: 356/2000... Training loss: 1.5569\n",
      "Epoch: 356/2000... Training loss: 1.5488\n",
      "Epoch: 356/2000... Training loss: 1.4372\n",
      "Epoch: 356/2000... Training loss: 1.5634\n",
      "Epoch: 356/2000... Training loss: 1.7409\n",
      "Epoch: 356/2000... Training loss: 1.7613\n",
      "Epoch: 357/2000... Training loss: 1.6546\n",
      "Epoch: 357/2000... Training loss: 1.4290\n",
      "Epoch: 357/2000... Training loss: 1.7196\n",
      "Epoch: 357/2000... Training loss: 1.4804\n",
      "Epoch: 357/2000... Training loss: 1.5258\n",
      "Epoch: 357/2000... Training loss: 1.7150\n",
      "Epoch: 357/2000... Training loss: 1.8914\n",
      "Epoch: 357/2000... Training loss: 2.0732\n",
      "Epoch: 357/2000... Training loss: 1.5563\n",
      "Epoch: 357/2000... Training loss: 1.6082\n",
      "Epoch: 357/2000... Training loss: 1.5917\n",
      "Epoch: 357/2000... Training loss: 1.7353\n",
      "Epoch: 357/2000... Training loss: 1.6756\n",
      "Epoch: 357/2000... Training loss: 1.6654\n",
      "Epoch: 357/2000... Training loss: 1.6988\n",
      "Epoch: 357/2000... Training loss: 1.7764\n",
      "Epoch: 357/2000... Training loss: 1.8259\n",
      "Epoch: 357/2000... Training loss: 1.6011\n",
      "Epoch: 357/2000... Training loss: 1.4891\n",
      "Epoch: 357/2000... Training loss: 1.8519\n",
      "Epoch: 357/2000... Training loss: 1.6698\n",
      "Epoch: 357/2000... Training loss: 1.4625\n",
      "Epoch: 357/2000... Training loss: 1.6318\n",
      "Epoch: 357/2000... Training loss: 1.6075\n",
      "Epoch: 357/2000... Training loss: 1.6215\n",
      "Epoch: 357/2000... Training loss: 1.6979\n",
      "Epoch: 357/2000... Training loss: 1.7973\n",
      "Epoch: 357/2000... Training loss: 1.7435\n",
      "Epoch: 357/2000... Training loss: 1.7100\n",
      "Epoch: 357/2000... Training loss: 1.8586\n",
      "Epoch: 357/2000... Training loss: 1.7531\n",
      "Epoch: 358/2000... Training loss: 1.7818\n",
      "Epoch: 358/2000... Training loss: 1.6838\n",
      "Epoch: 358/2000... Training loss: 1.5974\n",
      "Epoch: 358/2000... Training loss: 1.7569\n",
      "Epoch: 358/2000... Training loss: 1.8498\n",
      "Epoch: 358/2000... Training loss: 1.3211\n",
      "Epoch: 358/2000... Training loss: 1.6701\n",
      "Epoch: 358/2000... Training loss: 1.7143\n",
      "Epoch: 358/2000... Training loss: 1.4824\n",
      "Epoch: 358/2000... Training loss: 1.6449\n",
      "Epoch: 358/2000... Training loss: 1.6448\n",
      "Epoch: 358/2000... Training loss: 1.5741\n",
      "Epoch: 358/2000... Training loss: 1.7550\n",
      "Epoch: 358/2000... Training loss: 1.8899\n",
      "Epoch: 358/2000... Training loss: 1.7990\n",
      "Epoch: 358/2000... Training loss: 1.5378\n",
      "Epoch: 358/2000... Training loss: 1.7650\n",
      "Epoch: 358/2000... Training loss: 1.7354\n",
      "Epoch: 358/2000... Training loss: 1.3346\n",
      "Epoch: 358/2000... Training loss: 1.7812\n",
      "Epoch: 358/2000... Training loss: 1.7021\n",
      "Epoch: 358/2000... Training loss: 1.5296\n",
      "Epoch: 358/2000... Training loss: 1.7221\n",
      "Epoch: 358/2000... Training loss: 1.8214\n",
      "Epoch: 358/2000... Training loss: 1.6119\n",
      "Epoch: 358/2000... Training loss: 1.5324\n",
      "Epoch: 358/2000... Training loss: 1.7743\n",
      "Epoch: 358/2000... Training loss: 1.9290\n",
      "Epoch: 358/2000... Training loss: 1.7002\n",
      "Epoch: 358/2000... Training loss: 1.6770\n",
      "Epoch: 358/2000... Training loss: 1.5462\n",
      "Epoch: 359/2000... Training loss: 1.6415\n",
      "Epoch: 359/2000... Training loss: 1.4935\n",
      "Epoch: 359/2000... Training loss: 1.5101\n",
      "Epoch: 359/2000... Training loss: 1.5202\n",
      "Epoch: 359/2000... Training loss: 1.7775\n",
      "Epoch: 359/2000... Training loss: 1.5982\n",
      "Epoch: 359/2000... Training loss: 1.5789\n",
      "Epoch: 359/2000... Training loss: 1.6972\n",
      "Epoch: 359/2000... Training loss: 1.4906\n",
      "Epoch: 359/2000... Training loss: 1.6880\n",
      "Epoch: 359/2000... Training loss: 1.8858\n",
      "Epoch: 359/2000... Training loss: 1.7596\n",
      "Epoch: 359/2000... Training loss: 1.7650\n",
      "Epoch: 359/2000... Training loss: 1.8315\n",
      "Epoch: 359/2000... Training loss: 1.7613\n",
      "Epoch: 359/2000... Training loss: 1.3356\n",
      "Epoch: 359/2000... Training loss: 1.5630\n",
      "Epoch: 359/2000... Training loss: 1.5509\n",
      "Epoch: 359/2000... Training loss: 1.6416\n",
      "Epoch: 359/2000... Training loss: 1.8127\n",
      "Epoch: 359/2000... Training loss: 1.5136\n",
      "Epoch: 359/2000... Training loss: 1.8359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 359/2000... Training loss: 1.8726\n",
      "Epoch: 359/2000... Training loss: 1.4531\n",
      "Epoch: 359/2000... Training loss: 1.4755\n",
      "Epoch: 359/2000... Training loss: 1.6457\n",
      "Epoch: 359/2000... Training loss: 1.7754\n",
      "Epoch: 359/2000... Training loss: 1.6043\n",
      "Epoch: 359/2000... Training loss: 1.9829\n",
      "Epoch: 359/2000... Training loss: 1.8414\n",
      "Epoch: 359/2000... Training loss: 1.6378\n",
      "Epoch: 360/2000... Training loss: 1.6538\n",
      "Epoch: 360/2000... Training loss: 1.6177\n",
      "Epoch: 360/2000... Training loss: 1.5402\n",
      "Epoch: 360/2000... Training loss: 1.5323\n",
      "Epoch: 360/2000... Training loss: 1.6609\n",
      "Epoch: 360/2000... Training loss: 1.3798\n",
      "Epoch: 360/2000... Training loss: 1.5772\n",
      "Epoch: 360/2000... Training loss: 1.7251\n",
      "Epoch: 360/2000... Training loss: 1.7805\n",
      "Epoch: 360/2000... Training loss: 1.7716\n",
      "Epoch: 360/2000... Training loss: 1.7887\n",
      "Epoch: 360/2000... Training loss: 1.4025\n",
      "Epoch: 360/2000... Training loss: 1.5405\n",
      "Epoch: 360/2000... Training loss: 1.5748\n",
      "Epoch: 360/2000... Training loss: 1.5041\n",
      "Epoch: 360/2000... Training loss: 1.6572\n",
      "Epoch: 360/2000... Training loss: 1.7934\n",
      "Epoch: 360/2000... Training loss: 1.6477\n",
      "Epoch: 360/2000... Training loss: 1.6602\n",
      "Epoch: 360/2000... Training loss: 1.7078\n",
      "Epoch: 360/2000... Training loss: 1.7302\n",
      "Epoch: 360/2000... Training loss: 1.4426\n",
      "Epoch: 360/2000... Training loss: 1.8169\n",
      "Epoch: 360/2000... Training loss: 1.6612\n",
      "Epoch: 360/2000... Training loss: 1.7160\n",
      "Epoch: 360/2000... Training loss: 1.4027\n",
      "Epoch: 360/2000... Training loss: 1.6983\n",
      "Epoch: 360/2000... Training loss: 2.0515\n",
      "Epoch: 360/2000... Training loss: 1.8737\n",
      "Epoch: 360/2000... Training loss: 1.6551\n",
      "Epoch: 360/2000... Training loss: 1.7982\n",
      "Epoch: 361/2000... Training loss: 1.7653\n",
      "Epoch: 361/2000... Training loss: 1.9654\n",
      "Epoch: 361/2000... Training loss: 1.9375\n",
      "Epoch: 361/2000... Training loss: 1.5997\n",
      "Epoch: 361/2000... Training loss: 1.6493\n",
      "Epoch: 361/2000... Training loss: 1.3665\n",
      "Epoch: 361/2000... Training loss: 1.6614\n",
      "Epoch: 361/2000... Training loss: 1.7802\n",
      "Epoch: 361/2000... Training loss: 1.7994\n",
      "Epoch: 361/2000... Training loss: 1.6848\n",
      "Epoch: 361/2000... Training loss: 1.5538\n",
      "Epoch: 361/2000... Training loss: 1.5427\n",
      "Epoch: 361/2000... Training loss: 1.6297\n",
      "Epoch: 361/2000... Training loss: 1.6364\n",
      "Epoch: 361/2000... Training loss: 1.4050\n",
      "Epoch: 361/2000... Training loss: 1.5439\n",
      "Epoch: 361/2000... Training loss: 1.7447\n",
      "Epoch: 361/2000... Training loss: 1.3717\n",
      "Epoch: 361/2000... Training loss: 1.5657\n",
      "Epoch: 361/2000... Training loss: 1.8050\n",
      "Epoch: 361/2000... Training loss: 1.9453\n",
      "Epoch: 361/2000... Training loss: 1.6373\n",
      "Epoch: 361/2000... Training loss: 1.7296\n",
      "Epoch: 361/2000... Training loss: 1.6692\n",
      "Epoch: 361/2000... Training loss: 1.9168\n",
      "Epoch: 361/2000... Training loss: 1.5936\n",
      "Epoch: 361/2000... Training loss: 1.6308\n",
      "Epoch: 361/2000... Training loss: 1.7592\n",
      "Epoch: 361/2000... Training loss: 1.5787\n",
      "Epoch: 361/2000... Training loss: 1.4809\n",
      "Epoch: 361/2000... Training loss: 1.6697\n",
      "Epoch: 362/2000... Training loss: 1.5310\n",
      "Epoch: 362/2000... Training loss: 1.5980\n",
      "Epoch: 362/2000... Training loss: 1.7705\n",
      "Epoch: 362/2000... Training loss: 1.6413\n",
      "Epoch: 362/2000... Training loss: 1.6458\n",
      "Epoch: 362/2000... Training loss: 1.7630\n",
      "Epoch: 362/2000... Training loss: 1.4744\n",
      "Epoch: 362/2000... Training loss: 1.4432\n",
      "Epoch: 362/2000... Training loss: 1.6835\n",
      "Epoch: 362/2000... Training loss: 1.7735\n",
      "Epoch: 362/2000... Training loss: 1.5581\n",
      "Epoch: 362/2000... Training loss: 1.4874\n",
      "Epoch: 362/2000... Training loss: 1.9658\n",
      "Epoch: 362/2000... Training loss: 1.7495\n",
      "Epoch: 362/2000... Training loss: 1.9576\n",
      "Epoch: 362/2000... Training loss: 1.5736\n",
      "Epoch: 362/2000... Training loss: 1.6066\n",
      "Epoch: 362/2000... Training loss: 2.0597\n",
      "Epoch: 362/2000... Training loss: 1.6654\n",
      "Epoch: 362/2000... Training loss: 1.8123\n",
      "Epoch: 362/2000... Training loss: 1.6241\n",
      "Epoch: 362/2000... Training loss: 1.5096\n",
      "Epoch: 362/2000... Training loss: 1.6297\n",
      "Epoch: 362/2000... Training loss: 1.7223\n",
      "Epoch: 362/2000... Training loss: 1.6452\n",
      "Epoch: 362/2000... Training loss: 1.6855\n",
      "Epoch: 362/2000... Training loss: 1.7348\n",
      "Epoch: 362/2000... Training loss: 1.8050\n",
      "Epoch: 362/2000... Training loss: 1.5613\n",
      "Epoch: 362/2000... Training loss: 1.6579\n",
      "Epoch: 362/2000... Training loss: 1.7017\n",
      "Epoch: 363/2000... Training loss: 1.6576\n",
      "Epoch: 363/2000... Training loss: 1.4541\n",
      "Epoch: 363/2000... Training loss: 1.8935\n",
      "Epoch: 363/2000... Training loss: 1.6660\n",
      "Epoch: 363/2000... Training loss: 1.7110\n",
      "Epoch: 363/2000... Training loss: 1.7337\n",
      "Epoch: 363/2000... Training loss: 1.6349\n",
      "Epoch: 363/2000... Training loss: 1.5691\n",
      "Epoch: 363/2000... Training loss: 1.4799\n",
      "Epoch: 363/2000... Training loss: 1.5422\n",
      "Epoch: 363/2000... Training loss: 1.5570\n",
      "Epoch: 363/2000... Training loss: 1.6413\n",
      "Epoch: 363/2000... Training loss: 1.4126\n",
      "Epoch: 363/2000... Training loss: 1.4159\n",
      "Epoch: 363/2000... Training loss: 1.5265\n",
      "Epoch: 363/2000... Training loss: 1.7582\n",
      "Epoch: 363/2000... Training loss: 1.8493\n",
      "Epoch: 363/2000... Training loss: 1.4781\n",
      "Epoch: 363/2000... Training loss: 1.5167\n",
      "Epoch: 363/2000... Training loss: 1.6261\n",
      "Epoch: 363/2000... Training loss: 1.6926\n",
      "Epoch: 363/2000... Training loss: 1.6895\n",
      "Epoch: 363/2000... Training loss: 1.5059\n",
      "Epoch: 363/2000... Training loss: 1.6144\n",
      "Epoch: 363/2000... Training loss: 1.6649\n",
      "Epoch: 363/2000... Training loss: 1.3895\n",
      "Epoch: 363/2000... Training loss: 1.6363\n",
      "Epoch: 363/2000... Training loss: 1.8658\n",
      "Epoch: 363/2000... Training loss: 1.8490\n",
      "Epoch: 363/2000... Training loss: 1.6518\n",
      "Epoch: 363/2000... Training loss: 1.7836\n",
      "Epoch: 364/2000... Training loss: 1.5731\n",
      "Epoch: 364/2000... Training loss: 1.6749\n",
      "Epoch: 364/2000... Training loss: 1.6046\n",
      "Epoch: 364/2000... Training loss: 1.4610\n",
      "Epoch: 364/2000... Training loss: 1.6267\n",
      "Epoch: 364/2000... Training loss: 1.7057\n",
      "Epoch: 364/2000... Training loss: 1.4210\n",
      "Epoch: 364/2000... Training loss: 1.6264\n",
      "Epoch: 364/2000... Training loss: 1.6334\n",
      "Epoch: 364/2000... Training loss: 1.5543\n",
      "Epoch: 364/2000... Training loss: 1.8534\n",
      "Epoch: 364/2000... Training loss: 1.5457\n",
      "Epoch: 364/2000... Training loss: 2.0385\n",
      "Epoch: 364/2000... Training loss: 1.5809\n",
      "Epoch: 364/2000... Training loss: 1.5343\n",
      "Epoch: 364/2000... Training loss: 1.5224\n",
      "Epoch: 364/2000... Training loss: 1.5437\n",
      "Epoch: 364/2000... Training loss: 1.4887\n",
      "Epoch: 364/2000... Training loss: 1.6832\n",
      "Epoch: 364/2000... Training loss: 1.5334\n",
      "Epoch: 364/2000... Training loss: 1.6101\n",
      "Epoch: 364/2000... Training loss: 1.6018\n",
      "Epoch: 364/2000... Training loss: 1.6596\n",
      "Epoch: 364/2000... Training loss: 1.5125\n",
      "Epoch: 364/2000... Training loss: 1.6300\n",
      "Epoch: 364/2000... Training loss: 1.5410\n",
      "Epoch: 364/2000... Training loss: 1.5433\n",
      "Epoch: 364/2000... Training loss: 1.4176\n",
      "Epoch: 364/2000... Training loss: 1.5627\n",
      "Epoch: 364/2000... Training loss: 1.6139\n",
      "Epoch: 364/2000... Training loss: 1.6980\n",
      "Epoch: 365/2000... Training loss: 1.6210\n",
      "Epoch: 365/2000... Training loss: 1.7015\n",
      "Epoch: 365/2000... Training loss: 1.5730\n",
      "Epoch: 365/2000... Training loss: 1.7620\n",
      "Epoch: 365/2000... Training loss: 1.4877\n",
      "Epoch: 365/2000... Training loss: 1.6327\n",
      "Epoch: 365/2000... Training loss: 1.7036\n",
      "Epoch: 365/2000... Training loss: 1.6898\n",
      "Epoch: 365/2000... Training loss: 1.6270\n",
      "Epoch: 365/2000... Training loss: 1.4596\n",
      "Epoch: 365/2000... Training loss: 1.7832\n",
      "Epoch: 365/2000... Training loss: 1.6026\n",
      "Epoch: 365/2000... Training loss: 1.4164\n",
      "Epoch: 365/2000... Training loss: 1.7449\n",
      "Epoch: 365/2000... Training loss: 1.5735\n",
      "Epoch: 365/2000... Training loss: 1.9395\n",
      "Epoch: 365/2000... Training loss: 1.4544\n",
      "Epoch: 365/2000... Training loss: 1.6290\n",
      "Epoch: 365/2000... Training loss: 1.5410\n",
      "Epoch: 365/2000... Training loss: 1.6832\n",
      "Epoch: 365/2000... Training loss: 1.6796\n",
      "Epoch: 365/2000... Training loss: 1.5810\n",
      "Epoch: 365/2000... Training loss: 1.8112\n",
      "Epoch: 365/2000... Training loss: 1.5317\n",
      "Epoch: 365/2000... Training loss: 1.5731\n",
      "Epoch: 365/2000... Training loss: 1.5222\n",
      "Epoch: 365/2000... Training loss: 1.7075\n",
      "Epoch: 365/2000... Training loss: 1.8744\n",
      "Epoch: 365/2000... Training loss: 1.6665\n",
      "Epoch: 365/2000... Training loss: 1.4924\n",
      "Epoch: 365/2000... Training loss: 1.6237\n",
      "Epoch: 366/2000... Training loss: 1.6558\n",
      "Epoch: 366/2000... Training loss: 1.6387\n",
      "Epoch: 366/2000... Training loss: 1.6812\n",
      "Epoch: 366/2000... Training loss: 1.4829\n",
      "Epoch: 366/2000... Training loss: 1.6077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 366/2000... Training loss: 1.4501\n",
      "Epoch: 366/2000... Training loss: 1.5217\n",
      "Epoch: 366/2000... Training loss: 1.6223\n",
      "Epoch: 366/2000... Training loss: 1.6048\n",
      "Epoch: 366/2000... Training loss: 1.4303\n",
      "Epoch: 366/2000... Training loss: 1.5607\n",
      "Epoch: 366/2000... Training loss: 1.7857\n",
      "Epoch: 366/2000... Training loss: 1.7065\n",
      "Epoch: 366/2000... Training loss: 1.6359\n",
      "Epoch: 366/2000... Training loss: 1.3655\n",
      "Epoch: 366/2000... Training loss: 1.6729\n",
      "Epoch: 366/2000... Training loss: 1.4592\n",
      "Epoch: 366/2000... Training loss: 1.7670\n",
      "Epoch: 366/2000... Training loss: 1.3986\n",
      "Epoch: 366/2000... Training loss: 1.6533\n",
      "Epoch: 366/2000... Training loss: 1.6302\n",
      "Epoch: 366/2000... Training loss: 1.6443\n",
      "Epoch: 366/2000... Training loss: 1.7369\n",
      "Epoch: 366/2000... Training loss: 1.4550\n",
      "Epoch: 366/2000... Training loss: 1.5411\n",
      "Epoch: 366/2000... Training loss: 1.6296\n",
      "Epoch: 366/2000... Training loss: 1.8385\n",
      "Epoch: 366/2000... Training loss: 1.4110\n",
      "Epoch: 366/2000... Training loss: 1.4565\n",
      "Epoch: 366/2000... Training loss: 1.6202\n",
      "Epoch: 366/2000... Training loss: 1.6181\n",
      "Epoch: 367/2000... Training loss: 1.4832\n",
      "Epoch: 367/2000... Training loss: 1.5200\n",
      "Epoch: 367/2000... Training loss: 1.4750\n",
      "Epoch: 367/2000... Training loss: 1.5750\n",
      "Epoch: 367/2000... Training loss: 1.3589\n",
      "Epoch: 367/2000... Training loss: 1.5516\n",
      "Epoch: 367/2000... Training loss: 1.5326\n",
      "Epoch: 367/2000... Training loss: 1.4126\n",
      "Epoch: 367/2000... Training loss: 1.8502\n",
      "Epoch: 367/2000... Training loss: 1.5529\n",
      "Epoch: 367/2000... Training loss: 1.8202\n",
      "Epoch: 367/2000... Training loss: 1.5026\n",
      "Epoch: 367/2000... Training loss: 1.5287\n",
      "Epoch: 367/2000... Training loss: 1.7101\n",
      "Epoch: 367/2000... Training loss: 1.6127\n",
      "Epoch: 367/2000... Training loss: 1.6489\n",
      "Epoch: 367/2000... Training loss: 1.6873\n",
      "Epoch: 367/2000... Training loss: 1.6991\n",
      "Epoch: 367/2000... Training loss: 1.6340\n",
      "Epoch: 367/2000... Training loss: 1.6453\n",
      "Epoch: 367/2000... Training loss: 1.4433\n",
      "Epoch: 367/2000... Training loss: 1.4411\n",
      "Epoch: 367/2000... Training loss: 1.7027\n",
      "Epoch: 367/2000... Training loss: 1.6713\n",
      "Epoch: 367/2000... Training loss: 1.7431\n",
      "Epoch: 367/2000... Training loss: 1.4049\n",
      "Epoch: 367/2000... Training loss: 1.4215\n",
      "Epoch: 367/2000... Training loss: 1.8926\n",
      "Epoch: 367/2000... Training loss: 1.5773\n",
      "Epoch: 367/2000... Training loss: 1.5754\n",
      "Epoch: 367/2000... Training loss: 1.6540\n",
      "Epoch: 368/2000... Training loss: 1.4223\n",
      "Epoch: 368/2000... Training loss: 1.3443\n",
      "Epoch: 368/2000... Training loss: 1.4594\n",
      "Epoch: 368/2000... Training loss: 1.6248\n",
      "Epoch: 368/2000... Training loss: 1.4096\n",
      "Epoch: 368/2000... Training loss: 1.4592\n",
      "Epoch: 368/2000... Training loss: 1.7001\n",
      "Epoch: 368/2000... Training loss: 1.7171\n",
      "Epoch: 368/2000... Training loss: 1.3085\n",
      "Epoch: 368/2000... Training loss: 1.8945\n",
      "Epoch: 368/2000... Training loss: 1.8107\n",
      "Epoch: 368/2000... Training loss: 1.4140\n",
      "Epoch: 368/2000... Training loss: 1.6878\n",
      "Epoch: 368/2000... Training loss: 1.7409\n",
      "Epoch: 368/2000... Training loss: 1.6114\n",
      "Epoch: 368/2000... Training loss: 1.4455\n",
      "Epoch: 368/2000... Training loss: 1.9278\n",
      "Epoch: 368/2000... Training loss: 1.5219\n",
      "Epoch: 368/2000... Training loss: 1.6873\n",
      "Epoch: 368/2000... Training loss: 1.9016\n",
      "Epoch: 368/2000... Training loss: 1.4777\n",
      "Epoch: 368/2000... Training loss: 1.4661\n",
      "Epoch: 368/2000... Training loss: 1.6340\n",
      "Epoch: 368/2000... Training loss: 1.5357\n",
      "Epoch: 368/2000... Training loss: 1.5390\n",
      "Epoch: 368/2000... Training loss: 1.6718\n",
      "Epoch: 368/2000... Training loss: 1.6928\n",
      "Epoch: 368/2000... Training loss: 1.7098\n",
      "Epoch: 368/2000... Training loss: 1.5103\n",
      "Epoch: 368/2000... Training loss: 1.7764\n",
      "Epoch: 368/2000... Training loss: 1.5188\n",
      "Epoch: 369/2000... Training loss: 1.3877\n",
      "Epoch: 369/2000... Training loss: 1.4218\n",
      "Epoch: 369/2000... Training loss: 1.6052\n",
      "Epoch: 369/2000... Training loss: 1.5563\n",
      "Epoch: 369/2000... Training loss: 1.4876\n",
      "Epoch: 369/2000... Training loss: 1.5370\n",
      "Epoch: 369/2000... Training loss: 1.8670\n",
      "Epoch: 369/2000... Training loss: 1.6621\n",
      "Epoch: 369/2000... Training loss: 1.7043\n",
      "Epoch: 369/2000... Training loss: 1.5688\n",
      "Epoch: 369/2000... Training loss: 1.6897\n",
      "Epoch: 369/2000... Training loss: 1.5004\n",
      "Epoch: 369/2000... Training loss: 1.4170\n",
      "Epoch: 369/2000... Training loss: 1.9561\n",
      "Epoch: 369/2000... Training loss: 1.5353\n",
      "Epoch: 369/2000... Training loss: 1.6786\n",
      "Epoch: 369/2000... Training loss: 1.4131\n",
      "Epoch: 369/2000... Training loss: 1.3808\n",
      "Epoch: 369/2000... Training loss: 1.6327\n",
      "Epoch: 369/2000... Training loss: 1.7236\n",
      "Epoch: 369/2000... Training loss: 1.4836\n",
      "Epoch: 369/2000... Training loss: 1.4930\n",
      "Epoch: 369/2000... Training loss: 1.7093\n",
      "Epoch: 369/2000... Training loss: 1.6605\n",
      "Epoch: 369/2000... Training loss: 1.7167\n",
      "Epoch: 369/2000... Training loss: 1.6133\n",
      "Epoch: 369/2000... Training loss: 1.5943\n",
      "Epoch: 369/2000... Training loss: 1.8060\n",
      "Epoch: 369/2000... Training loss: 1.6744\n",
      "Epoch: 369/2000... Training loss: 1.4424\n",
      "Epoch: 369/2000... Training loss: 1.7333\n",
      "Epoch: 370/2000... Training loss: 1.6205\n",
      "Epoch: 370/2000... Training loss: 1.6044\n",
      "Epoch: 370/2000... Training loss: 1.4398\n",
      "Epoch: 370/2000... Training loss: 1.6326\n",
      "Epoch: 370/2000... Training loss: 1.6608\n",
      "Epoch: 370/2000... Training loss: 1.4231\n",
      "Epoch: 370/2000... Training loss: 1.4823\n",
      "Epoch: 370/2000... Training loss: 1.7810\n",
      "Epoch: 370/2000... Training loss: 1.6127\n",
      "Epoch: 370/2000... Training loss: 1.3750\n",
      "Epoch: 370/2000... Training loss: 1.5018\n",
      "Epoch: 370/2000... Training loss: 1.5863\n",
      "Epoch: 370/2000... Training loss: 1.7663\n",
      "Epoch: 370/2000... Training loss: 1.6126\n",
      "Epoch: 370/2000... Training loss: 1.3863\n",
      "Epoch: 370/2000... Training loss: 1.4860\n",
      "Epoch: 370/2000... Training loss: 1.5383\n",
      "Epoch: 370/2000... Training loss: 1.6645\n",
      "Epoch: 370/2000... Training loss: 1.4844\n",
      "Epoch: 370/2000... Training loss: 1.9004\n",
      "Epoch: 370/2000... Training loss: 1.6443\n",
      "Epoch: 370/2000... Training loss: 1.4489\n",
      "Epoch: 370/2000... Training loss: 1.7831\n",
      "Epoch: 370/2000... Training loss: 1.1607\n",
      "Epoch: 370/2000... Training loss: 1.7204\n",
      "Epoch: 370/2000... Training loss: 1.4753\n",
      "Epoch: 370/2000... Training loss: 1.7209\n",
      "Epoch: 370/2000... Training loss: 1.8776\n",
      "Epoch: 370/2000... Training loss: 1.3656\n",
      "Epoch: 370/2000... Training loss: 1.7432\n",
      "Epoch: 370/2000... Training loss: 1.5442\n",
      "Epoch: 371/2000... Training loss: 1.5223\n",
      "Epoch: 371/2000... Training loss: 1.7031\n",
      "Epoch: 371/2000... Training loss: 2.0567\n",
      "Epoch: 371/2000... Training loss: 1.5770\n",
      "Epoch: 371/2000... Training loss: 1.4205\n",
      "Epoch: 371/2000... Training loss: 1.7926\n",
      "Epoch: 371/2000... Training loss: 1.3215\n",
      "Epoch: 371/2000... Training loss: 1.6427\n",
      "Epoch: 371/2000... Training loss: 1.5138\n",
      "Epoch: 371/2000... Training loss: 1.7578\n",
      "Epoch: 371/2000... Training loss: 1.3713\n",
      "Epoch: 371/2000... Training loss: 1.7182\n",
      "Epoch: 371/2000... Training loss: 1.5889\n",
      "Epoch: 371/2000... Training loss: 1.3951\n",
      "Epoch: 371/2000... Training loss: 1.3618\n",
      "Epoch: 371/2000... Training loss: 1.7323\n",
      "Epoch: 371/2000... Training loss: 1.6194\n",
      "Epoch: 371/2000... Training loss: 1.6632\n",
      "Epoch: 371/2000... Training loss: 1.5750\n",
      "Epoch: 371/2000... Training loss: 1.6132\n",
      "Epoch: 371/2000... Training loss: 1.6692\n",
      "Epoch: 371/2000... Training loss: 1.3869\n",
      "Epoch: 371/2000... Training loss: 1.3357\n",
      "Epoch: 371/2000... Training loss: 1.4867\n",
      "Epoch: 371/2000... Training loss: 1.7397\n",
      "Epoch: 371/2000... Training loss: 1.6136\n",
      "Epoch: 371/2000... Training loss: 1.5289\n",
      "Epoch: 371/2000... Training loss: 1.6709\n",
      "Epoch: 371/2000... Training loss: 1.6558\n",
      "Epoch: 371/2000... Training loss: 1.5301\n",
      "Epoch: 371/2000... Training loss: 1.3465\n",
      "Epoch: 372/2000... Training loss: 1.3603\n",
      "Epoch: 372/2000... Training loss: 1.2907\n",
      "Epoch: 372/2000... Training loss: 1.5849\n",
      "Epoch: 372/2000... Training loss: 1.7608\n",
      "Epoch: 372/2000... Training loss: 1.6110\n",
      "Epoch: 372/2000... Training loss: 1.7656\n",
      "Epoch: 372/2000... Training loss: 1.7044\n",
      "Epoch: 372/2000... Training loss: 1.5745\n",
      "Epoch: 372/2000... Training loss: 1.4631\n",
      "Epoch: 372/2000... Training loss: 1.5490\n",
      "Epoch: 372/2000... Training loss: 1.6464\n",
      "Epoch: 372/2000... Training loss: 1.4982\n",
      "Epoch: 372/2000... Training loss: 1.5560\n",
      "Epoch: 372/2000... Training loss: 1.7586\n",
      "Epoch: 372/2000... Training loss: 1.5079\n",
      "Epoch: 372/2000... Training loss: 1.5777\n",
      "Epoch: 372/2000... Training loss: 1.8503\n",
      "Epoch: 372/2000... Training loss: 1.4431\n",
      "Epoch: 372/2000... Training loss: 1.5905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 372/2000... Training loss: 1.6591\n",
      "Epoch: 372/2000... Training loss: 1.3854\n",
      "Epoch: 372/2000... Training loss: 1.4056\n",
      "Epoch: 372/2000... Training loss: 1.7559\n",
      "Epoch: 372/2000... Training loss: 1.7664\n",
      "Epoch: 372/2000... Training loss: 1.6540\n",
      "Epoch: 372/2000... Training loss: 1.6293\n",
      "Epoch: 372/2000... Training loss: 1.5400\n",
      "Epoch: 372/2000... Training loss: 1.8014\n",
      "Epoch: 372/2000... Training loss: 1.5791\n",
      "Epoch: 372/2000... Training loss: 1.5502\n",
      "Epoch: 372/2000... Training loss: 1.7582\n",
      "Epoch: 373/2000... Training loss: 1.5593\n",
      "Epoch: 373/2000... Training loss: 1.5016\n",
      "Epoch: 373/2000... Training loss: 1.5940\n",
      "Epoch: 373/2000... Training loss: 1.5870\n",
      "Epoch: 373/2000... Training loss: 1.3585\n",
      "Epoch: 373/2000... Training loss: 1.8460\n",
      "Epoch: 373/2000... Training loss: 1.6472\n",
      "Epoch: 373/2000... Training loss: 1.8968\n",
      "Epoch: 373/2000... Training loss: 1.3568\n",
      "Epoch: 373/2000... Training loss: 1.5991\n",
      "Epoch: 373/2000... Training loss: 1.7506\n",
      "Epoch: 373/2000... Training loss: 1.8696\n",
      "Epoch: 373/2000... Training loss: 1.6676\n",
      "Epoch: 373/2000... Training loss: 1.5130\n",
      "Epoch: 373/2000... Training loss: 1.4261\n",
      "Epoch: 373/2000... Training loss: 1.5233\n",
      "Epoch: 373/2000... Training loss: 1.4689\n",
      "Epoch: 373/2000... Training loss: 1.6002\n",
      "Epoch: 373/2000... Training loss: 1.7137\n",
      "Epoch: 373/2000... Training loss: 1.5681\n",
      "Epoch: 373/2000... Training loss: 1.9771\n",
      "Epoch: 373/2000... Training loss: 1.4484\n",
      "Epoch: 373/2000... Training loss: 1.3967\n",
      "Epoch: 373/2000... Training loss: 1.5095\n",
      "Epoch: 373/2000... Training loss: 1.5446\n",
      "Epoch: 373/2000... Training loss: 1.5273\n",
      "Epoch: 373/2000... Training loss: 1.5459\n",
      "Epoch: 373/2000... Training loss: 1.6437\n",
      "Epoch: 373/2000... Training loss: 1.5830\n",
      "Epoch: 373/2000... Training loss: 1.4442\n",
      "Epoch: 373/2000... Training loss: 1.7242\n",
      "Epoch: 374/2000... Training loss: 1.4458\n",
      "Epoch: 374/2000... Training loss: 1.7814\n",
      "Epoch: 374/2000... Training loss: 1.6310\n",
      "Epoch: 374/2000... Training loss: 1.6494\n",
      "Epoch: 374/2000... Training loss: 1.7405\n",
      "Epoch: 374/2000... Training loss: 1.6218\n",
      "Epoch: 374/2000... Training loss: 1.5278\n",
      "Epoch: 374/2000... Training loss: 1.6717\n",
      "Epoch: 374/2000... Training loss: 1.5972\n",
      "Epoch: 374/2000... Training loss: 1.5360\n",
      "Epoch: 374/2000... Training loss: 1.4624\n",
      "Epoch: 374/2000... Training loss: 1.6214\n",
      "Epoch: 374/2000... Training loss: 1.5651\n",
      "Epoch: 374/2000... Training loss: 1.6685\n",
      "Epoch: 374/2000... Training loss: 1.4744\n",
      "Epoch: 374/2000... Training loss: 1.8383\n",
      "Epoch: 374/2000... Training loss: 1.5340\n",
      "Epoch: 374/2000... Training loss: 1.4930\n",
      "Epoch: 374/2000... Training loss: 1.5092\n",
      "Epoch: 374/2000... Training loss: 1.3723\n",
      "Epoch: 374/2000... Training loss: 1.7627\n",
      "Epoch: 374/2000... Training loss: 1.5964\n",
      "Epoch: 374/2000... Training loss: 1.6823\n",
      "Epoch: 374/2000... Training loss: 1.7044\n",
      "Epoch: 374/2000... Training loss: 1.6680\n",
      "Epoch: 374/2000... Training loss: 1.5734\n",
      "Epoch: 374/2000... Training loss: 1.8623\n",
      "Epoch: 374/2000... Training loss: 1.6588\n",
      "Epoch: 374/2000... Training loss: 1.6643\n",
      "Epoch: 374/2000... Training loss: 1.5403\n",
      "Epoch: 374/2000... Training loss: 1.4685\n",
      "Epoch: 375/2000... Training loss: 1.5345\n",
      "Epoch: 375/2000... Training loss: 1.6700\n",
      "Epoch: 375/2000... Training loss: 1.8460\n",
      "Epoch: 375/2000... Training loss: 1.4673\n",
      "Epoch: 375/2000... Training loss: 1.4835\n",
      "Epoch: 375/2000... Training loss: 1.7213\n",
      "Epoch: 375/2000... Training loss: 1.6955\n",
      "Epoch: 375/2000... Training loss: 1.5257\n",
      "Epoch: 375/2000... Training loss: 1.5716\n",
      "Epoch: 375/2000... Training loss: 1.5088\n",
      "Epoch: 375/2000... Training loss: 1.6392\n",
      "Epoch: 375/2000... Training loss: 1.5600\n",
      "Epoch: 375/2000... Training loss: 1.6788\n",
      "Epoch: 375/2000... Training loss: 1.6542\n",
      "Epoch: 375/2000... Training loss: 1.9111\n",
      "Epoch: 375/2000... Training loss: 1.4740\n",
      "Epoch: 375/2000... Training loss: 1.6230\n",
      "Epoch: 375/2000... Training loss: 1.6843\n",
      "Epoch: 375/2000... Training loss: 1.5105\n",
      "Epoch: 375/2000... Training loss: 1.7947\n",
      "Epoch: 375/2000... Training loss: 1.3719\n",
      "Epoch: 375/2000... Training loss: 1.6198\n",
      "Epoch: 375/2000... Training loss: 1.5655\n",
      "Epoch: 375/2000... Training loss: 1.6570\n",
      "Epoch: 375/2000... Training loss: 1.4523\n",
      "Epoch: 375/2000... Training loss: 1.3460\n",
      "Epoch: 375/2000... Training loss: 1.5469\n",
      "Epoch: 375/2000... Training loss: 1.7584\n",
      "Epoch: 375/2000... Training loss: 1.7692\n",
      "Epoch: 375/2000... Training loss: 1.7312\n",
      "Epoch: 375/2000... Training loss: 1.6771\n",
      "Epoch: 376/2000... Training loss: 1.5468\n",
      "Epoch: 376/2000... Training loss: 1.7178\n",
      "Epoch: 376/2000... Training loss: 1.4910\n",
      "Epoch: 376/2000... Training loss: 1.5341\n",
      "Epoch: 376/2000... Training loss: 1.7539\n",
      "Epoch: 376/2000... Training loss: 1.6489\n",
      "Epoch: 376/2000... Training loss: 1.6941\n",
      "Epoch: 376/2000... Training loss: 1.5042\n",
      "Epoch: 376/2000... Training loss: 1.6712\n",
      "Epoch: 376/2000... Training loss: 1.6724\n",
      "Epoch: 376/2000... Training loss: 1.6294\n",
      "Epoch: 376/2000... Training loss: 1.4100\n",
      "Epoch: 376/2000... Training loss: 1.3622\n",
      "Epoch: 376/2000... Training loss: 1.7223\n",
      "Epoch: 376/2000... Training loss: 1.4512\n",
      "Epoch: 376/2000... Training loss: 1.7641\n",
      "Epoch: 376/2000... Training loss: 1.6583\n",
      "Epoch: 376/2000... Training loss: 1.4005\n",
      "Epoch: 376/2000... Training loss: 1.6286\n",
      "Epoch: 376/2000... Training loss: 1.7372\n",
      "Epoch: 376/2000... Training loss: 1.4589\n",
      "Epoch: 376/2000... Training loss: 1.7631\n",
      "Epoch: 376/2000... Training loss: 1.5983\n",
      "Epoch: 376/2000... Training loss: 1.5219\n",
      "Epoch: 376/2000... Training loss: 1.5512\n",
      "Epoch: 376/2000... Training loss: 1.4589\n",
      "Epoch: 376/2000... Training loss: 1.5555\n",
      "Epoch: 376/2000... Training loss: 1.6789\n",
      "Epoch: 376/2000... Training loss: 1.5518\n",
      "Epoch: 376/2000... Training loss: 1.5888\n",
      "Epoch: 376/2000... Training loss: 1.6280\n",
      "Epoch: 377/2000... Training loss: 1.5939\n",
      "Epoch: 377/2000... Training loss: 1.6537\n",
      "Epoch: 377/2000... Training loss: 1.5758\n",
      "Epoch: 377/2000... Training loss: 1.6210\n",
      "Epoch: 377/2000... Training loss: 1.4601\n",
      "Epoch: 377/2000... Training loss: 1.5622\n",
      "Epoch: 377/2000... Training loss: 1.3828\n",
      "Epoch: 377/2000... Training loss: 1.5502\n",
      "Epoch: 377/2000... Training loss: 1.5545\n",
      "Epoch: 377/2000... Training loss: 1.6535\n",
      "Epoch: 377/2000... Training loss: 1.6912\n",
      "Epoch: 377/2000... Training loss: 1.6061\n",
      "Epoch: 377/2000... Training loss: 1.3458\n",
      "Epoch: 377/2000... Training loss: 1.6836\n",
      "Epoch: 377/2000... Training loss: 1.2715\n",
      "Epoch: 377/2000... Training loss: 1.5671\n",
      "Epoch: 377/2000... Training loss: 1.8034\n",
      "Epoch: 377/2000... Training loss: 1.5987\n",
      "Epoch: 377/2000... Training loss: 1.5252\n",
      "Epoch: 377/2000... Training loss: 1.4985\n",
      "Epoch: 377/2000... Training loss: 1.6994\n",
      "Epoch: 377/2000... Training loss: 1.5309\n",
      "Epoch: 377/2000... Training loss: 1.7145\n",
      "Epoch: 377/2000... Training loss: 1.6495\n",
      "Epoch: 377/2000... Training loss: 1.5498\n",
      "Epoch: 377/2000... Training loss: 1.7639\n",
      "Epoch: 377/2000... Training loss: 1.5559\n",
      "Epoch: 377/2000... Training loss: 1.7995\n",
      "Epoch: 377/2000... Training loss: 1.7778\n",
      "Epoch: 377/2000... Training loss: 1.5152\n",
      "Epoch: 377/2000... Training loss: 1.4257\n",
      "Epoch: 378/2000... Training loss: 1.4991\n",
      "Epoch: 378/2000... Training loss: 1.2839\n",
      "Epoch: 378/2000... Training loss: 1.4639\n",
      "Epoch: 378/2000... Training loss: 1.8831\n",
      "Epoch: 378/2000... Training loss: 1.4561\n",
      "Epoch: 378/2000... Training loss: 1.5843\n",
      "Epoch: 378/2000... Training loss: 1.5415\n",
      "Epoch: 378/2000... Training loss: 1.6150\n",
      "Epoch: 378/2000... Training loss: 1.6492\n",
      "Epoch: 378/2000... Training loss: 1.3360\n",
      "Epoch: 378/2000... Training loss: 1.3231\n",
      "Epoch: 378/2000... Training loss: 1.4703\n",
      "Epoch: 378/2000... Training loss: 1.4973\n",
      "Epoch: 378/2000... Training loss: 1.7048\n",
      "Epoch: 378/2000... Training loss: 1.6934\n",
      "Epoch: 378/2000... Training loss: 1.3760\n",
      "Epoch: 378/2000... Training loss: 1.5220\n",
      "Epoch: 378/2000... Training loss: 1.7118\n",
      "Epoch: 378/2000... Training loss: 1.4795\n",
      "Epoch: 378/2000... Training loss: 1.7715\n",
      "Epoch: 378/2000... Training loss: 1.5558\n",
      "Epoch: 378/2000... Training loss: 1.5247\n",
      "Epoch: 378/2000... Training loss: 1.4271\n",
      "Epoch: 378/2000... Training loss: 1.6149\n",
      "Epoch: 378/2000... Training loss: 1.6439\n",
      "Epoch: 378/2000... Training loss: 1.4680\n",
      "Epoch: 378/2000... Training loss: 1.3431\n",
      "Epoch: 378/2000... Training loss: 1.7315\n",
      "Epoch: 378/2000... Training loss: 1.4911\n",
      "Epoch: 378/2000... Training loss: 1.3072\n",
      "Epoch: 378/2000... Training loss: 1.7109\n",
      "Epoch: 379/2000... Training loss: 1.3955\n",
      "Epoch: 379/2000... Training loss: 1.4271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 379/2000... Training loss: 1.6439\n",
      "Epoch: 379/2000... Training loss: 1.3497\n",
      "Epoch: 379/2000... Training loss: 1.5150\n",
      "Epoch: 379/2000... Training loss: 1.4753\n",
      "Epoch: 379/2000... Training loss: 1.6452\n",
      "Epoch: 379/2000... Training loss: 1.7535\n",
      "Epoch: 379/2000... Training loss: 1.7485\n",
      "Epoch: 379/2000... Training loss: 1.4960\n",
      "Epoch: 379/2000... Training loss: 1.8721\n",
      "Epoch: 379/2000... Training loss: 1.4397\n",
      "Epoch: 379/2000... Training loss: 1.6539\n",
      "Epoch: 379/2000... Training loss: 1.3980\n",
      "Epoch: 379/2000... Training loss: 1.8036\n",
      "Epoch: 379/2000... Training loss: 1.6383\n",
      "Epoch: 379/2000... Training loss: 1.6389\n",
      "Epoch: 379/2000... Training loss: 1.7818\n",
      "Epoch: 379/2000... Training loss: 1.7222\n",
      "Epoch: 379/2000... Training loss: 1.5834\n",
      "Epoch: 379/2000... Training loss: 1.5436\n",
      "Epoch: 379/2000... Training loss: 1.3878\n",
      "Epoch: 379/2000... Training loss: 1.6196\n",
      "Epoch: 379/2000... Training loss: 1.6424\n",
      "Epoch: 379/2000... Training loss: 1.5320\n",
      "Epoch: 379/2000... Training loss: 1.5568\n",
      "Epoch: 379/2000... Training loss: 1.7870\n",
      "Epoch: 379/2000... Training loss: 1.5933\n",
      "Epoch: 379/2000... Training loss: 1.7541\n",
      "Epoch: 379/2000... Training loss: 1.7376\n",
      "Epoch: 379/2000... Training loss: 1.4286\n",
      "Epoch: 380/2000... Training loss: 1.5434\n",
      "Epoch: 380/2000... Training loss: 1.5838\n",
      "Epoch: 380/2000... Training loss: 1.3056\n",
      "Epoch: 380/2000... Training loss: 1.4990\n",
      "Epoch: 380/2000... Training loss: 1.6176\n",
      "Epoch: 380/2000... Training loss: 1.6616\n",
      "Epoch: 380/2000... Training loss: 1.6112\n",
      "Epoch: 380/2000... Training loss: 1.4989\n",
      "Epoch: 380/2000... Training loss: 1.5548\n",
      "Epoch: 380/2000... Training loss: 1.6302\n",
      "Epoch: 380/2000... Training loss: 1.5901\n",
      "Epoch: 380/2000... Training loss: 1.5462\n",
      "Epoch: 380/2000... Training loss: 1.6205\n",
      "Epoch: 380/2000... Training loss: 1.5598\n",
      "Epoch: 380/2000... Training loss: 1.3374\n",
      "Epoch: 380/2000... Training loss: 1.4689\n",
      "Epoch: 380/2000... Training loss: 1.7657\n",
      "Epoch: 380/2000... Training loss: 1.4053\n",
      "Epoch: 380/2000... Training loss: 1.7090\n",
      "Epoch: 380/2000... Training loss: 1.6829\n",
      "Epoch: 380/2000... Training loss: 1.7036\n",
      "Epoch: 380/2000... Training loss: 1.5073\n",
      "Epoch: 380/2000... Training loss: 1.5766\n",
      "Epoch: 380/2000... Training loss: 1.5319\n",
      "Epoch: 380/2000... Training loss: 1.4504\n",
      "Epoch: 380/2000... Training loss: 1.7727\n",
      "Epoch: 380/2000... Training loss: 1.4789\n",
      "Epoch: 380/2000... Training loss: 1.5735\n",
      "Epoch: 380/2000... Training loss: 1.6710\n",
      "Epoch: 380/2000... Training loss: 1.4744\n",
      "Epoch: 380/2000... Training loss: 1.4749\n",
      "Epoch: 381/2000... Training loss: 1.2678\n",
      "Epoch: 381/2000... Training loss: 1.6548\n",
      "Epoch: 381/2000... Training loss: 1.3448\n",
      "Epoch: 381/2000... Training loss: 1.5323\n",
      "Epoch: 381/2000... Training loss: 1.5149\n",
      "Epoch: 381/2000... Training loss: 1.5015\n",
      "Epoch: 381/2000... Training loss: 1.6435\n",
      "Epoch: 381/2000... Training loss: 1.5351\n",
      "Epoch: 381/2000... Training loss: 1.6723\n",
      "Epoch: 381/2000... Training loss: 1.5547\n",
      "Epoch: 381/2000... Training loss: 1.7336\n",
      "Epoch: 381/2000... Training loss: 1.7173\n",
      "Epoch: 381/2000... Training loss: 1.3908\n",
      "Epoch: 381/2000... Training loss: 1.8036\n",
      "Epoch: 381/2000... Training loss: 1.4546\n",
      "Epoch: 381/2000... Training loss: 1.2688\n",
      "Epoch: 381/2000... Training loss: 1.5745\n",
      "Epoch: 381/2000... Training loss: 1.5458\n",
      "Epoch: 381/2000... Training loss: 1.5311\n",
      "Epoch: 381/2000... Training loss: 1.9078\n",
      "Epoch: 381/2000... Training loss: 1.5505\n",
      "Epoch: 381/2000... Training loss: 1.7112\n",
      "Epoch: 381/2000... Training loss: 1.6010\n",
      "Epoch: 381/2000... Training loss: 1.6888\n",
      "Epoch: 381/2000... Training loss: 1.5778\n",
      "Epoch: 381/2000... Training loss: 1.3109\n",
      "Epoch: 381/2000... Training loss: 1.5494\n",
      "Epoch: 381/2000... Training loss: 1.7005\n",
      "Epoch: 381/2000... Training loss: 1.5857\n",
      "Epoch: 381/2000... Training loss: 1.4498\n",
      "Epoch: 381/2000... Training loss: 1.5068\n",
      "Epoch: 382/2000... Training loss: 1.7973\n",
      "Epoch: 382/2000... Training loss: 1.5995\n",
      "Epoch: 382/2000... Training loss: 1.4258\n",
      "Epoch: 382/2000... Training loss: 1.5759\n",
      "Epoch: 382/2000... Training loss: 1.5730\n",
      "Epoch: 382/2000... Training loss: 1.5244\n",
      "Epoch: 382/2000... Training loss: 1.5637\n",
      "Epoch: 382/2000... Training loss: 1.5931\n",
      "Epoch: 382/2000... Training loss: 1.5085\n",
      "Epoch: 382/2000... Training loss: 1.5901\n",
      "Epoch: 382/2000... Training loss: 1.7223\n",
      "Epoch: 382/2000... Training loss: 1.4826\n",
      "Epoch: 382/2000... Training loss: 1.3101\n",
      "Epoch: 382/2000... Training loss: 1.4562\n",
      "Epoch: 382/2000... Training loss: 1.6954\n",
      "Epoch: 382/2000... Training loss: 1.8818\n",
      "Epoch: 382/2000... Training loss: 1.5562\n",
      "Epoch: 382/2000... Training loss: 1.3649\n",
      "Epoch: 382/2000... Training loss: 1.5948\n",
      "Epoch: 382/2000... Training loss: 1.3914\n",
      "Epoch: 382/2000... Training loss: 1.5234\n",
      "Epoch: 382/2000... Training loss: 1.6989\n",
      "Epoch: 382/2000... Training loss: 1.5109\n",
      "Epoch: 382/2000... Training loss: 1.7154\n",
      "Epoch: 382/2000... Training loss: 1.5483\n",
      "Epoch: 382/2000... Training loss: 1.7586\n",
      "Epoch: 382/2000... Training loss: 1.4430\n",
      "Epoch: 382/2000... Training loss: 1.5925\n",
      "Epoch: 382/2000... Training loss: 1.4141\n",
      "Epoch: 382/2000... Training loss: 1.6968\n",
      "Epoch: 382/2000... Training loss: 1.5309\n",
      "Epoch: 383/2000... Training loss: 1.2903\n",
      "Epoch: 383/2000... Training loss: 1.5398\n",
      "Epoch: 383/2000... Training loss: 1.5648\n",
      "Epoch: 383/2000... Training loss: 1.5217\n",
      "Epoch: 383/2000... Training loss: 1.5504\n",
      "Epoch: 383/2000... Training loss: 1.6150\n",
      "Epoch: 383/2000... Training loss: 1.4950\n",
      "Epoch: 383/2000... Training loss: 1.7564\n",
      "Epoch: 383/2000... Training loss: 1.7735\n",
      "Epoch: 383/2000... Training loss: 1.7070\n",
      "Epoch: 383/2000... Training loss: 1.8256\n",
      "Epoch: 383/2000... Training loss: 1.4882\n",
      "Epoch: 383/2000... Training loss: 1.6297\n",
      "Epoch: 383/2000... Training loss: 1.5726\n",
      "Epoch: 383/2000... Training loss: 1.4989\n",
      "Epoch: 383/2000... Training loss: 1.6699\n",
      "Epoch: 383/2000... Training loss: 1.5049\n",
      "Epoch: 383/2000... Training loss: 1.5662\n",
      "Epoch: 383/2000... Training loss: 1.3741\n",
      "Epoch: 383/2000... Training loss: 1.5541\n",
      "Epoch: 383/2000... Training loss: 1.5852\n",
      "Epoch: 383/2000... Training loss: 1.4045\n",
      "Epoch: 383/2000... Training loss: 1.7036\n",
      "Epoch: 383/2000... Training loss: 1.4747\n",
      "Epoch: 383/2000... Training loss: 1.4949\n",
      "Epoch: 383/2000... Training loss: 1.3525\n",
      "Epoch: 383/2000... Training loss: 1.7815\n",
      "Epoch: 383/2000... Training loss: 1.3878\n",
      "Epoch: 383/2000... Training loss: 1.4648\n",
      "Epoch: 383/2000... Training loss: 1.5958\n",
      "Epoch: 383/2000... Training loss: 1.6167\n",
      "Epoch: 384/2000... Training loss: 1.4414\n",
      "Epoch: 384/2000... Training loss: 1.6473\n",
      "Epoch: 384/2000... Training loss: 1.4084\n",
      "Epoch: 384/2000... Training loss: 1.5059\n",
      "Epoch: 384/2000... Training loss: 1.6641\n",
      "Epoch: 384/2000... Training loss: 1.7438\n",
      "Epoch: 384/2000... Training loss: 1.4300\n",
      "Epoch: 384/2000... Training loss: 1.6381\n",
      "Epoch: 384/2000... Training loss: 1.6285\n",
      "Epoch: 384/2000... Training loss: 1.7604\n",
      "Epoch: 384/2000... Training loss: 1.5926\n",
      "Epoch: 384/2000... Training loss: 1.5503\n",
      "Epoch: 384/2000... Training loss: 1.5526\n",
      "Epoch: 384/2000... Training loss: 1.4687\n",
      "Epoch: 384/2000... Training loss: 1.3533\n",
      "Epoch: 384/2000... Training loss: 1.4957\n",
      "Epoch: 384/2000... Training loss: 1.6385\n",
      "Epoch: 384/2000... Training loss: 1.2971\n",
      "Epoch: 384/2000... Training loss: 1.5751\n",
      "Epoch: 384/2000... Training loss: 1.5423\n",
      "Epoch: 384/2000... Training loss: 1.7542\n",
      "Epoch: 384/2000... Training loss: 1.3932\n",
      "Epoch: 384/2000... Training loss: 1.7799\n",
      "Epoch: 384/2000... Training loss: 1.4274\n",
      "Epoch: 384/2000... Training loss: 1.5434\n",
      "Epoch: 384/2000... Training loss: 1.4412\n",
      "Epoch: 384/2000... Training loss: 1.4150\n",
      "Epoch: 384/2000... Training loss: 1.4717\n",
      "Epoch: 384/2000... Training loss: 1.4139\n",
      "Epoch: 384/2000... Training loss: 1.5414\n",
      "Epoch: 384/2000... Training loss: 1.3982\n",
      "Epoch: 385/2000... Training loss: 1.5467\n",
      "Epoch: 385/2000... Training loss: 1.4959\n",
      "Epoch: 385/2000... Training loss: 1.3526\n",
      "Epoch: 385/2000... Training loss: 1.3879\n",
      "Epoch: 385/2000... Training loss: 1.4566\n",
      "Epoch: 385/2000... Training loss: 1.6729\n",
      "Epoch: 385/2000... Training loss: 1.8268\n",
      "Epoch: 385/2000... Training loss: 1.4350\n",
      "Epoch: 385/2000... Training loss: 1.3607\n",
      "Epoch: 385/2000... Training loss: 1.3185\n",
      "Epoch: 385/2000... Training loss: 1.7335\n",
      "Epoch: 385/2000... Training loss: 1.2773\n",
      "Epoch: 385/2000... Training loss: 1.5472\n",
      "Epoch: 385/2000... Training loss: 1.4367\n",
      "Epoch: 385/2000... Training loss: 1.3207\n",
      "Epoch: 385/2000... Training loss: 1.3836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 385/2000... Training loss: 1.8148\n",
      "Epoch: 385/2000... Training loss: 1.9925\n",
      "Epoch: 385/2000... Training loss: 1.2214\n",
      "Epoch: 385/2000... Training loss: 1.4040\n",
      "Epoch: 385/2000... Training loss: 1.7155\n",
      "Epoch: 385/2000... Training loss: 1.6904\n",
      "Epoch: 385/2000... Training loss: 1.6370\n",
      "Epoch: 385/2000... Training loss: 1.6940\n",
      "Epoch: 385/2000... Training loss: 1.4705\n",
      "Epoch: 385/2000... Training loss: 1.3623\n",
      "Epoch: 385/2000... Training loss: 1.4050\n",
      "Epoch: 385/2000... Training loss: 1.6111\n",
      "Epoch: 385/2000... Training loss: 1.7310\n",
      "Epoch: 385/2000... Training loss: 1.7528\n",
      "Epoch: 385/2000... Training loss: 1.5907\n",
      "Epoch: 386/2000... Training loss: 1.5219\n",
      "Epoch: 386/2000... Training loss: 1.4603\n",
      "Epoch: 386/2000... Training loss: 1.7841\n",
      "Epoch: 386/2000... Training loss: 1.4048\n",
      "Epoch: 386/2000... Training loss: 1.5265\n",
      "Epoch: 386/2000... Training loss: 1.5154\n",
      "Epoch: 386/2000... Training loss: 1.3850\n",
      "Epoch: 386/2000... Training loss: 1.8673\n",
      "Epoch: 386/2000... Training loss: 1.4544\n",
      "Epoch: 386/2000... Training loss: 1.5955\n",
      "Epoch: 386/2000... Training loss: 1.7456\n",
      "Epoch: 386/2000... Training loss: 1.4191\n",
      "Epoch: 386/2000... Training loss: 1.5252\n",
      "Epoch: 386/2000... Training loss: 1.5936\n",
      "Epoch: 386/2000... Training loss: 1.6760\n",
      "Epoch: 386/2000... Training loss: 1.3531\n",
      "Epoch: 386/2000... Training loss: 1.6914\n",
      "Epoch: 386/2000... Training loss: 1.5286\n",
      "Epoch: 386/2000... Training loss: 1.3347\n",
      "Epoch: 386/2000... Training loss: 1.4511\n",
      "Epoch: 386/2000... Training loss: 1.3719\n",
      "Epoch: 386/2000... Training loss: 1.3162\n",
      "Epoch: 386/2000... Training loss: 1.5490\n",
      "Epoch: 386/2000... Training loss: 1.4775\n",
      "Epoch: 386/2000... Training loss: 1.4335\n",
      "Epoch: 386/2000... Training loss: 1.4292\n",
      "Epoch: 386/2000... Training loss: 1.6751\n",
      "Epoch: 386/2000... Training loss: 1.5080\n",
      "Epoch: 386/2000... Training loss: 1.7123\n",
      "Epoch: 386/2000... Training loss: 1.4083\n",
      "Epoch: 386/2000... Training loss: 1.6293\n",
      "Epoch: 387/2000... Training loss: 1.3560\n",
      "Epoch: 387/2000... Training loss: 1.7717\n",
      "Epoch: 387/2000... Training loss: 1.4708\n",
      "Epoch: 387/2000... Training loss: 1.3805\n",
      "Epoch: 387/2000... Training loss: 1.6407\n",
      "Epoch: 387/2000... Training loss: 1.3537\n",
      "Epoch: 387/2000... Training loss: 1.4253\n",
      "Epoch: 387/2000... Training loss: 1.6604\n",
      "Epoch: 387/2000... Training loss: 1.5337\n",
      "Epoch: 387/2000... Training loss: 1.3523\n",
      "Epoch: 387/2000... Training loss: 1.5347\n",
      "Epoch: 387/2000... Training loss: 1.4686\n",
      "Epoch: 387/2000... Training loss: 1.2813\n",
      "Epoch: 387/2000... Training loss: 1.3986\n",
      "Epoch: 387/2000... Training loss: 1.6766\n",
      "Epoch: 387/2000... Training loss: 1.3827\n",
      "Epoch: 387/2000... Training loss: 1.7594\n",
      "Epoch: 387/2000... Training loss: 1.4714\n",
      "Epoch: 387/2000... Training loss: 1.7159\n",
      "Epoch: 387/2000... Training loss: 1.6980\n",
      "Epoch: 387/2000... Training loss: 1.4005\n",
      "Epoch: 387/2000... Training loss: 1.4773\n",
      "Epoch: 387/2000... Training loss: 1.3584\n",
      "Epoch: 387/2000... Training loss: 1.4829\n",
      "Epoch: 387/2000... Training loss: 1.7678\n",
      "Epoch: 387/2000... Training loss: 1.3254\n",
      "Epoch: 387/2000... Training loss: 1.4509\n",
      "Epoch: 387/2000... Training loss: 1.4958\n",
      "Epoch: 387/2000... Training loss: 1.6542\n",
      "Epoch: 387/2000... Training loss: 1.5810\n",
      "Epoch: 387/2000... Training loss: 1.6413\n",
      "Epoch: 388/2000... Training loss: 1.2158\n",
      "Epoch: 388/2000... Training loss: 1.2255\n",
      "Epoch: 388/2000... Training loss: 1.5532\n",
      "Epoch: 388/2000... Training loss: 1.3511\n",
      "Epoch: 388/2000... Training loss: 1.4667\n",
      "Epoch: 388/2000... Training loss: 1.5798\n",
      "Epoch: 388/2000... Training loss: 1.3579\n",
      "Epoch: 388/2000... Training loss: 1.4557\n",
      "Epoch: 388/2000... Training loss: 1.3598\n",
      "Epoch: 388/2000... Training loss: 1.3295\n",
      "Epoch: 388/2000... Training loss: 1.5921\n",
      "Epoch: 388/2000... Training loss: 1.3530\n",
      "Epoch: 388/2000... Training loss: 1.4790\n",
      "Epoch: 388/2000... Training loss: 1.4751\n",
      "Epoch: 388/2000... Training loss: 1.5844\n",
      "Epoch: 388/2000... Training loss: 1.3675\n",
      "Epoch: 388/2000... Training loss: 1.6864\n",
      "Epoch: 388/2000... Training loss: 1.4905\n",
      "Epoch: 388/2000... Training loss: 1.5537\n",
      "Epoch: 388/2000... Training loss: 1.5885\n",
      "Epoch: 388/2000... Training loss: 1.5976\n",
      "Epoch: 388/2000... Training loss: 1.2443\n",
      "Epoch: 388/2000... Training loss: 1.5331\n",
      "Epoch: 388/2000... Training loss: 1.6346\n",
      "Epoch: 388/2000... Training loss: 1.6178\n",
      "Epoch: 388/2000... Training loss: 1.3402\n",
      "Epoch: 388/2000... Training loss: 1.3801\n",
      "Epoch: 388/2000... Training loss: 1.5169\n",
      "Epoch: 388/2000... Training loss: 1.4079\n",
      "Epoch: 388/2000... Training loss: 1.5302\n",
      "Epoch: 388/2000... Training loss: 1.4633\n",
      "Epoch: 389/2000... Training loss: 1.2991\n",
      "Epoch: 389/2000... Training loss: 1.5375\n",
      "Epoch: 389/2000... Training loss: 1.5304\n",
      "Epoch: 389/2000... Training loss: 1.4238\n",
      "Epoch: 389/2000... Training loss: 1.3955\n",
      "Epoch: 389/2000... Training loss: 1.4339\n",
      "Epoch: 389/2000... Training loss: 1.5184\n",
      "Epoch: 389/2000... Training loss: 1.5321\n",
      "Epoch: 389/2000... Training loss: 1.6062\n",
      "Epoch: 389/2000... Training loss: 1.6106\n",
      "Epoch: 389/2000... Training loss: 1.5416\n",
      "Epoch: 389/2000... Training loss: 1.5721\n",
      "Epoch: 389/2000... Training loss: 1.4462\n",
      "Epoch: 389/2000... Training loss: 1.5959\n",
      "Epoch: 389/2000... Training loss: 1.7768\n",
      "Epoch: 389/2000... Training loss: 1.4447\n",
      "Epoch: 389/2000... Training loss: 1.5045\n",
      "Epoch: 389/2000... Training loss: 1.6249\n",
      "Epoch: 389/2000... Training loss: 1.6670\n",
      "Epoch: 389/2000... Training loss: 1.5731\n",
      "Epoch: 389/2000... Training loss: 1.3668\n",
      "Epoch: 389/2000... Training loss: 1.3958\n",
      "Epoch: 389/2000... Training loss: 1.5314\n",
      "Epoch: 389/2000... Training loss: 1.5104\n",
      "Epoch: 389/2000... Training loss: 1.4413\n",
      "Epoch: 389/2000... Training loss: 1.4842\n",
      "Epoch: 389/2000... Training loss: 1.5415\n",
      "Epoch: 389/2000... Training loss: 1.5854\n",
      "Epoch: 389/2000... Training loss: 1.4990\n",
      "Epoch: 389/2000... Training loss: 1.4070\n",
      "Epoch: 389/2000... Training loss: 1.3479\n",
      "Epoch: 390/2000... Training loss: 1.4914\n",
      "Epoch: 390/2000... Training loss: 1.6012\n",
      "Epoch: 390/2000... Training loss: 1.2839\n",
      "Epoch: 390/2000... Training loss: 1.5914\n",
      "Epoch: 390/2000... Training loss: 1.5191\n",
      "Epoch: 390/2000... Training loss: 1.3895\n",
      "Epoch: 390/2000... Training loss: 1.5137\n",
      "Epoch: 390/2000... Training loss: 1.6247\n",
      "Epoch: 390/2000... Training loss: 1.5701\n",
      "Epoch: 390/2000... Training loss: 1.7647\n",
      "Epoch: 390/2000... Training loss: 1.6636\n",
      "Epoch: 390/2000... Training loss: 1.4641\n",
      "Epoch: 390/2000... Training loss: 1.6271\n",
      "Epoch: 390/2000... Training loss: 1.6671\n",
      "Epoch: 390/2000... Training loss: 1.7091\n",
      "Epoch: 390/2000... Training loss: 1.5697\n",
      "Epoch: 390/2000... Training loss: 1.6074\n",
      "Epoch: 390/2000... Training loss: 1.5771\n",
      "Epoch: 390/2000... Training loss: 1.5335\n",
      "Epoch: 390/2000... Training loss: 1.5688\n",
      "Epoch: 390/2000... Training loss: 1.3826\n",
      "Epoch: 390/2000... Training loss: 1.6784\n",
      "Epoch: 390/2000... Training loss: 1.6701\n",
      "Epoch: 390/2000... Training loss: 1.5755\n",
      "Epoch: 390/2000... Training loss: 1.4922\n",
      "Epoch: 390/2000... Training loss: 1.4997\n",
      "Epoch: 390/2000... Training loss: 1.3303\n",
      "Epoch: 390/2000... Training loss: 1.3861\n",
      "Epoch: 390/2000... Training loss: 1.4148\n",
      "Epoch: 390/2000... Training loss: 1.4544\n",
      "Epoch: 390/2000... Training loss: 1.5483\n",
      "Epoch: 391/2000... Training loss: 1.3734\n",
      "Epoch: 391/2000... Training loss: 1.6042\n",
      "Epoch: 391/2000... Training loss: 1.4295\n",
      "Epoch: 391/2000... Training loss: 1.8889\n",
      "Epoch: 391/2000... Training loss: 1.4512\n",
      "Epoch: 391/2000... Training loss: 1.4170\n",
      "Epoch: 391/2000... Training loss: 1.7749\n",
      "Epoch: 391/2000... Training loss: 1.4188\n",
      "Epoch: 391/2000... Training loss: 1.5485\n",
      "Epoch: 391/2000... Training loss: 1.3078\n",
      "Epoch: 391/2000... Training loss: 1.4475\n",
      "Epoch: 391/2000... Training loss: 1.2909\n",
      "Epoch: 391/2000... Training loss: 1.7998\n",
      "Epoch: 391/2000... Training loss: 1.6955\n",
      "Epoch: 391/2000... Training loss: 1.5171\n",
      "Epoch: 391/2000... Training loss: 1.4634\n",
      "Epoch: 391/2000... Training loss: 1.7371\n",
      "Epoch: 391/2000... Training loss: 1.4495\n",
      "Epoch: 391/2000... Training loss: 1.5535\n",
      "Epoch: 391/2000... Training loss: 1.7219\n",
      "Epoch: 391/2000... Training loss: 1.5624\n",
      "Epoch: 391/2000... Training loss: 1.5319\n",
      "Epoch: 391/2000... Training loss: 1.6373\n",
      "Epoch: 391/2000... Training loss: 1.7373\n",
      "Epoch: 391/2000... Training loss: 1.4708\n",
      "Epoch: 391/2000... Training loss: 1.1674\n",
      "Epoch: 391/2000... Training loss: 1.4495\n",
      "Epoch: 391/2000... Training loss: 1.4009\n",
      "Epoch: 391/2000... Training loss: 1.6537\n",
      "Epoch: 391/2000... Training loss: 1.5830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 391/2000... Training loss: 1.3013\n",
      "Epoch: 392/2000... Training loss: 1.3508\n",
      "Epoch: 392/2000... Training loss: 1.4817\n",
      "Epoch: 392/2000... Training loss: 1.6745\n",
      "Epoch: 392/2000... Training loss: 1.4354\n",
      "Epoch: 392/2000... Training loss: 1.4932\n",
      "Epoch: 392/2000... Training loss: 1.2948\n",
      "Epoch: 392/2000... Training loss: 1.4185\n",
      "Epoch: 392/2000... Training loss: 1.5879\n",
      "Epoch: 392/2000... Training loss: 1.6490\n",
      "Epoch: 392/2000... Training loss: 1.5101\n",
      "Epoch: 392/2000... Training loss: 1.3896\n",
      "Epoch: 392/2000... Training loss: 1.7328\n",
      "Epoch: 392/2000... Training loss: 1.5433\n",
      "Epoch: 392/2000... Training loss: 1.6020\n",
      "Epoch: 392/2000... Training loss: 1.4290\n",
      "Epoch: 392/2000... Training loss: 1.3186\n",
      "Epoch: 392/2000... Training loss: 1.4586\n",
      "Epoch: 392/2000... Training loss: 1.3600\n",
      "Epoch: 392/2000... Training loss: 1.5352\n",
      "Epoch: 392/2000... Training loss: 1.3927\n",
      "Epoch: 392/2000... Training loss: 1.2558\n",
      "Epoch: 392/2000... Training loss: 1.6007\n",
      "Epoch: 392/2000... Training loss: 1.6975\n",
      "Epoch: 392/2000... Training loss: 1.3602\n",
      "Epoch: 392/2000... Training loss: 1.3669\n",
      "Epoch: 392/2000... Training loss: 1.4417\n",
      "Epoch: 392/2000... Training loss: 1.4412\n",
      "Epoch: 392/2000... Training loss: 1.5779\n",
      "Epoch: 392/2000... Training loss: 1.5173\n",
      "Epoch: 392/2000... Training loss: 1.6989\n",
      "Epoch: 392/2000... Training loss: 1.4664\n",
      "Epoch: 393/2000... Training loss: 1.4898\n",
      "Epoch: 393/2000... Training loss: 1.5008\n",
      "Epoch: 393/2000... Training loss: 1.4431\n",
      "Epoch: 393/2000... Training loss: 1.4823\n",
      "Epoch: 393/2000... Training loss: 1.3525\n",
      "Epoch: 393/2000... Training loss: 1.5518\n",
      "Epoch: 393/2000... Training loss: 1.3584\n",
      "Epoch: 393/2000... Training loss: 1.3840\n",
      "Epoch: 393/2000... Training loss: 1.5490\n",
      "Epoch: 393/2000... Training loss: 1.5487\n",
      "Epoch: 393/2000... Training loss: 1.6104\n",
      "Epoch: 393/2000... Training loss: 1.3624\n",
      "Epoch: 393/2000... Training loss: 1.5593\n",
      "Epoch: 393/2000... Training loss: 1.3366\n",
      "Epoch: 393/2000... Training loss: 1.2937\n",
      "Epoch: 393/2000... Training loss: 1.6120\n",
      "Epoch: 393/2000... Training loss: 1.5338\n",
      "Epoch: 393/2000... Training loss: 1.2796\n",
      "Epoch: 393/2000... Training loss: 1.5706\n",
      "Epoch: 393/2000... Training loss: 1.8051\n",
      "Epoch: 393/2000... Training loss: 1.5133\n",
      "Epoch: 393/2000... Training loss: 1.4281\n",
      "Epoch: 393/2000... Training loss: 1.7909\n",
      "Epoch: 393/2000... Training loss: 1.4805\n",
      "Epoch: 393/2000... Training loss: 1.6996\n",
      "Epoch: 393/2000... Training loss: 1.6548\n",
      "Epoch: 393/2000... Training loss: 1.4322\n",
      "Epoch: 393/2000... Training loss: 1.7619\n",
      "Epoch: 393/2000... Training loss: 1.5427\n",
      "Epoch: 393/2000... Training loss: 1.3340\n",
      "Epoch: 393/2000... Training loss: 1.5722\n",
      "Epoch: 394/2000... Training loss: 1.2034\n",
      "Epoch: 394/2000... Training loss: 1.5176\n",
      "Epoch: 394/2000... Training loss: 1.4680\n",
      "Epoch: 394/2000... Training loss: 1.7487\n",
      "Epoch: 394/2000... Training loss: 1.1692\n",
      "Epoch: 394/2000... Training loss: 1.4202\n",
      "Epoch: 394/2000... Training loss: 1.8348\n",
      "Epoch: 394/2000... Training loss: 1.3973\n",
      "Epoch: 394/2000... Training loss: 1.3779\n",
      "Epoch: 394/2000... Training loss: 1.3569\n",
      "Epoch: 394/2000... Training loss: 1.5818\n",
      "Epoch: 394/2000... Training loss: 1.4503\n",
      "Epoch: 394/2000... Training loss: 1.3490\n",
      "Epoch: 394/2000... Training loss: 1.2390\n",
      "Epoch: 394/2000... Training loss: 1.6626\n",
      "Epoch: 394/2000... Training loss: 1.6337\n",
      "Epoch: 394/2000... Training loss: 1.5771\n",
      "Epoch: 394/2000... Training loss: 1.6943\n",
      "Epoch: 394/2000... Training loss: 1.4670\n",
      "Epoch: 394/2000... Training loss: 1.5403\n",
      "Epoch: 394/2000... Training loss: 1.7406\n",
      "Epoch: 394/2000... Training loss: 1.5903\n",
      "Epoch: 394/2000... Training loss: 1.5412\n",
      "Epoch: 394/2000... Training loss: 1.6359\n",
      "Epoch: 394/2000... Training loss: 1.5353\n",
      "Epoch: 394/2000... Training loss: 1.4111\n",
      "Epoch: 394/2000... Training loss: 1.3302\n",
      "Epoch: 394/2000... Training loss: 1.4791\n",
      "Epoch: 394/2000... Training loss: 1.5797\n",
      "Epoch: 394/2000... Training loss: 1.4115\n",
      "Epoch: 394/2000... Training loss: 1.5034\n",
      "Epoch: 395/2000... Training loss: 1.3726\n",
      "Epoch: 395/2000... Training loss: 1.3902\n",
      "Epoch: 395/2000... Training loss: 1.5383\n",
      "Epoch: 395/2000... Training loss: 1.7370\n",
      "Epoch: 395/2000... Training loss: 1.4672\n",
      "Epoch: 395/2000... Training loss: 1.5498\n",
      "Epoch: 395/2000... Training loss: 1.5206\n",
      "Epoch: 395/2000... Training loss: 1.6903\n",
      "Epoch: 395/2000... Training loss: 1.5645\n",
      "Epoch: 395/2000... Training loss: 1.7235\n",
      "Epoch: 395/2000... Training loss: 1.4888\n",
      "Epoch: 395/2000... Training loss: 1.4902\n",
      "Epoch: 395/2000... Training loss: 1.6360\n",
      "Epoch: 395/2000... Training loss: 1.2390\n",
      "Epoch: 395/2000... Training loss: 1.4468\n",
      "Epoch: 395/2000... Training loss: 1.6653\n",
      "Epoch: 395/2000... Training loss: 1.4574\n",
      "Epoch: 395/2000... Training loss: 1.6110\n",
      "Epoch: 395/2000... Training loss: 1.3456\n",
      "Epoch: 395/2000... Training loss: 1.6362\n",
      "Epoch: 395/2000... Training loss: 1.7108\n",
      "Epoch: 395/2000... Training loss: 1.4461\n",
      "Epoch: 395/2000... Training loss: 1.6115\n",
      "Epoch: 395/2000... Training loss: 1.3244\n",
      "Epoch: 395/2000... Training loss: 1.5172\n",
      "Epoch: 395/2000... Training loss: 1.4815\n",
      "Epoch: 395/2000... Training loss: 1.6365\n",
      "Epoch: 395/2000... Training loss: 1.2535\n",
      "Epoch: 395/2000... Training loss: 1.3980\n",
      "Epoch: 395/2000... Training loss: 1.5548\n",
      "Epoch: 395/2000... Training loss: 1.3495\n",
      "Epoch: 396/2000... Training loss: 1.1686\n",
      "Epoch: 396/2000... Training loss: 1.5842\n",
      "Epoch: 396/2000... Training loss: 1.8029\n",
      "Epoch: 396/2000... Training loss: 1.5556\n",
      "Epoch: 396/2000... Training loss: 1.3811\n",
      "Epoch: 396/2000... Training loss: 1.5385\n",
      "Epoch: 396/2000... Training loss: 1.5188\n",
      "Epoch: 396/2000... Training loss: 1.4240\n",
      "Epoch: 396/2000... Training loss: 1.5810\n",
      "Epoch: 396/2000... Training loss: 1.6826\n",
      "Epoch: 396/2000... Training loss: 1.4233\n",
      "Epoch: 396/2000... Training loss: 1.4358\n",
      "Epoch: 396/2000... Training loss: 1.2388\n",
      "Epoch: 396/2000... Training loss: 1.4770\n",
      "Epoch: 396/2000... Training loss: 1.3364\n",
      "Epoch: 396/2000... Training loss: 1.4625\n",
      "Epoch: 396/2000... Training loss: 1.5114\n",
      "Epoch: 396/2000... Training loss: 1.5270\n",
      "Epoch: 396/2000... Training loss: 1.7296\n",
      "Epoch: 396/2000... Training loss: 1.6800\n",
      "Epoch: 396/2000... Training loss: 1.4842\n",
      "Epoch: 396/2000... Training loss: 1.4648\n",
      "Epoch: 396/2000... Training loss: 1.4039\n",
      "Epoch: 396/2000... Training loss: 1.6659\n",
      "Epoch: 396/2000... Training loss: 1.5304\n",
      "Epoch: 396/2000... Training loss: 1.2922\n",
      "Epoch: 396/2000... Training loss: 1.5243\n",
      "Epoch: 396/2000... Training loss: 1.5425\n",
      "Epoch: 396/2000... Training loss: 1.5563\n",
      "Epoch: 396/2000... Training loss: 1.4074\n",
      "Epoch: 396/2000... Training loss: 1.4222\n",
      "Epoch: 397/2000... Training loss: 1.4157\n",
      "Epoch: 397/2000... Training loss: 1.4548\n",
      "Epoch: 397/2000... Training loss: 1.4486\n",
      "Epoch: 397/2000... Training loss: 1.5586\n",
      "Epoch: 397/2000... Training loss: 1.4242\n",
      "Epoch: 397/2000... Training loss: 1.1781\n",
      "Epoch: 397/2000... Training loss: 1.5961\n",
      "Epoch: 397/2000... Training loss: 1.3666\n",
      "Epoch: 397/2000... Training loss: 1.4896\n",
      "Epoch: 397/2000... Training loss: 1.4616\n",
      "Epoch: 397/2000... Training loss: 1.7413\n",
      "Epoch: 397/2000... Training loss: 1.4243\n",
      "Epoch: 397/2000... Training loss: 1.5971\n",
      "Epoch: 397/2000... Training loss: 1.4907\n",
      "Epoch: 397/2000... Training loss: 1.4932\n",
      "Epoch: 397/2000... Training loss: 1.1892\n",
      "Epoch: 397/2000... Training loss: 1.4573\n",
      "Epoch: 397/2000... Training loss: 1.3715\n",
      "Epoch: 397/2000... Training loss: 1.5866\n",
      "Epoch: 397/2000... Training loss: 1.6323\n",
      "Epoch: 397/2000... Training loss: 1.3163\n",
      "Epoch: 397/2000... Training loss: 1.4942\n",
      "Epoch: 397/2000... Training loss: 1.6102\n",
      "Epoch: 397/2000... Training loss: 1.5879\n",
      "Epoch: 397/2000... Training loss: 1.6019\n",
      "Epoch: 397/2000... Training loss: 1.5364\n",
      "Epoch: 397/2000... Training loss: 1.4193\n",
      "Epoch: 397/2000... Training loss: 1.4779\n",
      "Epoch: 397/2000... Training loss: 1.3373\n",
      "Epoch: 397/2000... Training loss: 1.6991\n",
      "Epoch: 397/2000... Training loss: 1.3138\n",
      "Epoch: 398/2000... Training loss: 1.5011\n",
      "Epoch: 398/2000... Training loss: 1.2191\n",
      "Epoch: 398/2000... Training loss: 1.6484\n",
      "Epoch: 398/2000... Training loss: 1.1927\n",
      "Epoch: 398/2000... Training loss: 1.4387\n",
      "Epoch: 398/2000... Training loss: 1.3771\n",
      "Epoch: 398/2000... Training loss: 1.4937\n",
      "Epoch: 398/2000... Training loss: 1.5030\n",
      "Epoch: 398/2000... Training loss: 1.5275\n",
      "Epoch: 398/2000... Training loss: 1.4212\n",
      "Epoch: 398/2000... Training loss: 1.4543\n",
      "Epoch: 398/2000... Training loss: 1.3843\n",
      "Epoch: 398/2000... Training loss: 1.7601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 398/2000... Training loss: 1.4264\n",
      "Epoch: 398/2000... Training loss: 1.5941\n",
      "Epoch: 398/2000... Training loss: 1.2063\n",
      "Epoch: 398/2000... Training loss: 1.5398\n",
      "Epoch: 398/2000... Training loss: 1.4416\n",
      "Epoch: 398/2000... Training loss: 1.1604\n",
      "Epoch: 398/2000... Training loss: 1.5698\n",
      "Epoch: 398/2000... Training loss: 1.4041\n",
      "Epoch: 398/2000... Training loss: 1.6424\n",
      "Epoch: 398/2000... Training loss: 1.4623\n",
      "Epoch: 398/2000... Training loss: 1.4140\n",
      "Epoch: 398/2000... Training loss: 1.2722\n",
      "Epoch: 398/2000... Training loss: 1.4855\n",
      "Epoch: 398/2000... Training loss: 1.6413\n",
      "Epoch: 398/2000... Training loss: 1.3736\n",
      "Epoch: 398/2000... Training loss: 1.4351\n",
      "Epoch: 398/2000... Training loss: 1.6521\n",
      "Epoch: 398/2000... Training loss: 1.4991\n",
      "Epoch: 399/2000... Training loss: 1.4019\n",
      "Epoch: 399/2000... Training loss: 1.2628\n",
      "Epoch: 399/2000... Training loss: 1.2104\n",
      "Epoch: 399/2000... Training loss: 1.5792\n",
      "Epoch: 399/2000... Training loss: 1.3225\n",
      "Epoch: 399/2000... Training loss: 1.5377\n",
      "Epoch: 399/2000... Training loss: 1.5774\n",
      "Epoch: 399/2000... Training loss: 1.2855\n",
      "Epoch: 399/2000... Training loss: 1.3309\n",
      "Epoch: 399/2000... Training loss: 1.5796\n",
      "Epoch: 399/2000... Training loss: 1.7280\n",
      "Epoch: 399/2000... Training loss: 1.5880\n",
      "Epoch: 399/2000... Training loss: 1.6935\n",
      "Epoch: 399/2000... Training loss: 1.4464\n",
      "Epoch: 399/2000... Training loss: 1.5266\n",
      "Epoch: 399/2000... Training loss: 1.2548\n",
      "Epoch: 399/2000... Training loss: 1.4961\n",
      "Epoch: 399/2000... Training loss: 1.5929\n",
      "Epoch: 399/2000... Training loss: 1.8396\n",
      "Epoch: 399/2000... Training loss: 1.2653\n",
      "Epoch: 399/2000... Training loss: 1.4136\n",
      "Epoch: 399/2000... Training loss: 1.6788\n",
      "Epoch: 399/2000... Training loss: 1.6393\n",
      "Epoch: 399/2000... Training loss: 1.6356\n",
      "Epoch: 399/2000... Training loss: 1.3272\n",
      "Epoch: 399/2000... Training loss: 1.3375\n",
      "Epoch: 399/2000... Training loss: 1.5018\n",
      "Epoch: 399/2000... Training loss: 1.5037\n",
      "Epoch: 399/2000... Training loss: 1.3585\n",
      "Epoch: 399/2000... Training loss: 1.3891\n",
      "Epoch: 399/2000... Training loss: 1.4235\n",
      "Epoch: 400/2000... Training loss: 1.6132\n",
      "Epoch: 400/2000... Training loss: 1.5270\n",
      "Epoch: 400/2000... Training loss: 1.6905\n",
      "Epoch: 400/2000... Training loss: 1.3411\n",
      "Epoch: 400/2000... Training loss: 1.3962\n",
      "Epoch: 400/2000... Training loss: 1.2447\n",
      "Epoch: 400/2000... Training loss: 1.4927\n",
      "Epoch: 400/2000... Training loss: 1.3792\n",
      "Epoch: 400/2000... Training loss: 1.4461\n",
      "Epoch: 400/2000... Training loss: 1.6013\n",
      "Epoch: 400/2000... Training loss: 1.4852\n",
      "Epoch: 400/2000... Training loss: 1.5336\n",
      "Epoch: 400/2000... Training loss: 1.4257\n",
      "Epoch: 400/2000... Training loss: 1.6768\n",
      "Epoch: 400/2000... Training loss: 1.3660\n",
      "Epoch: 400/2000... Training loss: 1.5843\n",
      "Epoch: 400/2000... Training loss: 1.2586\n",
      "Epoch: 400/2000... Training loss: 1.7248\n",
      "Epoch: 400/2000... Training loss: 1.6481\n",
      "Epoch: 400/2000... Training loss: 1.4593\n",
      "Epoch: 400/2000... Training loss: 1.3940\n",
      "Epoch: 400/2000... Training loss: 1.4483\n",
      "Epoch: 400/2000... Training loss: 1.4223\n",
      "Epoch: 400/2000... Training loss: 1.4308\n",
      "Epoch: 400/2000... Training loss: 1.3319\n",
      "Epoch: 400/2000... Training loss: 1.2406\n",
      "Epoch: 400/2000... Training loss: 1.4317\n",
      "Epoch: 400/2000... Training loss: 1.5693\n",
      "Epoch: 400/2000... Training loss: 1.4430\n",
      "Epoch: 400/2000... Training loss: 1.5522\n",
      "Epoch: 400/2000... Training loss: 1.6026\n",
      "Epoch: 401/2000... Training loss: 1.4918\n",
      "Epoch: 401/2000... Training loss: 1.3826\n",
      "Epoch: 401/2000... Training loss: 1.4336\n",
      "Epoch: 401/2000... Training loss: 1.5522\n",
      "Epoch: 401/2000... Training loss: 1.4683\n",
      "Epoch: 401/2000... Training loss: 1.5288\n",
      "Epoch: 401/2000... Training loss: 1.2592\n",
      "Epoch: 401/2000... Training loss: 1.6484\n",
      "Epoch: 401/2000... Training loss: 1.4260\n",
      "Epoch: 401/2000... Training loss: 1.2602\n",
      "Epoch: 401/2000... Training loss: 1.4973\n",
      "Epoch: 401/2000... Training loss: 1.3692\n",
      "Epoch: 401/2000... Training loss: 1.5880\n",
      "Epoch: 401/2000... Training loss: 1.5564\n",
      "Epoch: 401/2000... Training loss: 1.2447\n",
      "Epoch: 401/2000... Training loss: 1.5247\n",
      "Epoch: 401/2000... Training loss: 1.5484\n",
      "Epoch: 401/2000... Training loss: 1.2820\n",
      "Epoch: 401/2000... Training loss: 1.6478\n",
      "Epoch: 401/2000... Training loss: 1.4020\n",
      "Epoch: 401/2000... Training loss: 1.6353\n",
      "Epoch: 401/2000... Training loss: 1.4743\n",
      "Epoch: 401/2000... Training loss: 1.4909\n",
      "Epoch: 401/2000... Training loss: 1.5340\n",
      "Epoch: 401/2000... Training loss: 1.5023\n",
      "Epoch: 401/2000... Training loss: 1.2739\n",
      "Epoch: 401/2000... Training loss: 1.3897\n",
      "Epoch: 401/2000... Training loss: 1.3260\n",
      "Epoch: 401/2000... Training loss: 1.5719\n",
      "Epoch: 401/2000... Training loss: 1.6874\n",
      "Epoch: 401/2000... Training loss: 1.4424\n",
      "Epoch: 402/2000... Training loss: 1.5001\n",
      "Epoch: 402/2000... Training loss: 1.4503\n",
      "Epoch: 402/2000... Training loss: 1.2658\n",
      "Epoch: 402/2000... Training loss: 1.2809\n",
      "Epoch: 402/2000... Training loss: 1.5606\n",
      "Epoch: 402/2000... Training loss: 1.6342\n",
      "Epoch: 402/2000... Training loss: 1.5933\n",
      "Epoch: 402/2000... Training loss: 1.2518\n",
      "Epoch: 402/2000... Training loss: 1.6364\n",
      "Epoch: 402/2000... Training loss: 1.4423\n",
      "Epoch: 402/2000... Training loss: 1.4444\n",
      "Epoch: 402/2000... Training loss: 1.4473\n",
      "Epoch: 402/2000... Training loss: 1.5568\n",
      "Epoch: 402/2000... Training loss: 1.3238\n",
      "Epoch: 402/2000... Training loss: 1.2926\n",
      "Epoch: 402/2000... Training loss: 1.5843\n",
      "Epoch: 402/2000... Training loss: 1.5030\n",
      "Epoch: 402/2000... Training loss: 1.4957\n",
      "Epoch: 402/2000... Training loss: 1.5381\n",
      "Epoch: 402/2000... Training loss: 1.2421\n",
      "Epoch: 402/2000... Training loss: 1.4829\n",
      "Epoch: 402/2000... Training loss: 1.4823\n",
      "Epoch: 402/2000... Training loss: 1.4000\n",
      "Epoch: 402/2000... Training loss: 1.4839\n",
      "Epoch: 402/2000... Training loss: 1.3462\n",
      "Epoch: 402/2000... Training loss: 1.3737\n",
      "Epoch: 402/2000... Training loss: 1.2847\n",
      "Epoch: 402/2000... Training loss: 1.6953\n",
      "Epoch: 402/2000... Training loss: 1.5626\n",
      "Epoch: 402/2000... Training loss: 1.3014\n",
      "Epoch: 402/2000... Training loss: 1.5526\n",
      "Epoch: 403/2000... Training loss: 1.4154\n",
      "Epoch: 403/2000... Training loss: 1.4757\n",
      "Epoch: 403/2000... Training loss: 1.4434\n",
      "Epoch: 403/2000... Training loss: 1.7108\n",
      "Epoch: 403/2000... Training loss: 1.1346\n",
      "Epoch: 403/2000... Training loss: 1.4799\n",
      "Epoch: 403/2000... Training loss: 1.4855\n",
      "Epoch: 403/2000... Training loss: 1.6501\n",
      "Epoch: 403/2000... Training loss: 1.3849\n",
      "Epoch: 403/2000... Training loss: 1.5378\n",
      "Epoch: 403/2000... Training loss: 1.4988\n",
      "Epoch: 403/2000... Training loss: 1.5319\n",
      "Epoch: 403/2000... Training loss: 1.3290\n",
      "Epoch: 403/2000... Training loss: 1.5934\n",
      "Epoch: 403/2000... Training loss: 1.4713\n",
      "Epoch: 403/2000... Training loss: 1.4835\n",
      "Epoch: 403/2000... Training loss: 1.4375\n",
      "Epoch: 403/2000... Training loss: 1.5352\n",
      "Epoch: 403/2000... Training loss: 1.5223\n",
      "Epoch: 403/2000... Training loss: 1.5621\n",
      "Epoch: 403/2000... Training loss: 1.2309\n",
      "Epoch: 403/2000... Training loss: 1.4387\n",
      "Epoch: 403/2000... Training loss: 1.5123\n",
      "Epoch: 403/2000... Training loss: 1.4356\n",
      "Epoch: 403/2000... Training loss: 1.4895\n",
      "Epoch: 403/2000... Training loss: 1.4049\n",
      "Epoch: 403/2000... Training loss: 1.6135\n",
      "Epoch: 403/2000... Training loss: 1.2134\n",
      "Epoch: 403/2000... Training loss: 1.4976\n",
      "Epoch: 403/2000... Training loss: 1.5446\n",
      "Epoch: 403/2000... Training loss: 1.5740\n",
      "Epoch: 404/2000... Training loss: 1.3044\n",
      "Epoch: 404/2000... Training loss: 1.0764\n",
      "Epoch: 404/2000... Training loss: 1.5835\n",
      "Epoch: 404/2000... Training loss: 1.6497\n",
      "Epoch: 404/2000... Training loss: 1.5722\n",
      "Epoch: 404/2000... Training loss: 1.4538\n",
      "Epoch: 404/2000... Training loss: 1.3670\n",
      "Epoch: 404/2000... Training loss: 1.4503\n",
      "Epoch: 404/2000... Training loss: 1.6272\n",
      "Epoch: 404/2000... Training loss: 1.5898\n",
      "Epoch: 404/2000... Training loss: 1.5012\n",
      "Epoch: 404/2000... Training loss: 1.4998\n",
      "Epoch: 404/2000... Training loss: 1.3810\n",
      "Epoch: 404/2000... Training loss: 1.4680\n",
      "Epoch: 404/2000... Training loss: 1.4050\n",
      "Epoch: 404/2000... Training loss: 1.4340\n",
      "Epoch: 404/2000... Training loss: 1.3432\n",
      "Epoch: 404/2000... Training loss: 1.4131\n",
      "Epoch: 404/2000... Training loss: 1.1991\n",
      "Epoch: 404/2000... Training loss: 1.4607\n",
      "Epoch: 404/2000... Training loss: 1.6087\n",
      "Epoch: 404/2000... Training loss: 1.6213\n",
      "Epoch: 404/2000... Training loss: 1.4587\n",
      "Epoch: 404/2000... Training loss: 1.5668\n",
      "Epoch: 404/2000... Training loss: 1.4090\n",
      "Epoch: 404/2000... Training loss: 1.3408\n",
      "Epoch: 404/2000... Training loss: 1.5487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 404/2000... Training loss: 1.5298\n",
      "Epoch: 404/2000... Training loss: 1.4719\n",
      "Epoch: 404/2000... Training loss: 1.6558\n",
      "Epoch: 404/2000... Training loss: 1.4686\n",
      "Epoch: 405/2000... Training loss: 1.4423\n",
      "Epoch: 405/2000... Training loss: 1.7471\n",
      "Epoch: 405/2000... Training loss: 1.4026\n",
      "Epoch: 405/2000... Training loss: 1.2134\n",
      "Epoch: 405/2000... Training loss: 1.3150\n",
      "Epoch: 405/2000... Training loss: 1.6364\n",
      "Epoch: 405/2000... Training loss: 1.6117\n",
      "Epoch: 405/2000... Training loss: 1.3431\n",
      "Epoch: 405/2000... Training loss: 1.3797\n",
      "Epoch: 405/2000... Training loss: 1.4583\n",
      "Epoch: 405/2000... Training loss: 1.5588\n",
      "Epoch: 405/2000... Training loss: 1.3560\n",
      "Epoch: 405/2000... Training loss: 1.5033\n",
      "Epoch: 405/2000... Training loss: 1.4313\n",
      "Epoch: 405/2000... Training loss: 1.4270\n",
      "Epoch: 405/2000... Training loss: 1.3460\n",
      "Epoch: 405/2000... Training loss: 1.1411\n",
      "Epoch: 405/2000... Training loss: 1.2656\n",
      "Epoch: 405/2000... Training loss: 1.5546\n",
      "Epoch: 405/2000... Training loss: 1.5031\n",
      "Epoch: 405/2000... Training loss: 1.4059\n",
      "Epoch: 405/2000... Training loss: 1.3661\n",
      "Epoch: 405/2000... Training loss: 1.6898\n",
      "Epoch: 405/2000... Training loss: 1.6030\n",
      "Epoch: 405/2000... Training loss: 1.5819\n",
      "Epoch: 405/2000... Training loss: 1.4297\n",
      "Epoch: 405/2000... Training loss: 1.5577\n",
      "Epoch: 405/2000... Training loss: 1.4400\n",
      "Epoch: 405/2000... Training loss: 1.4157\n",
      "Epoch: 405/2000... Training loss: 1.4484\n",
      "Epoch: 405/2000... Training loss: 1.7582\n",
      "Epoch: 406/2000... Training loss: 1.5334\n",
      "Epoch: 406/2000... Training loss: 1.4483\n",
      "Epoch: 406/2000... Training loss: 1.3138\n",
      "Epoch: 406/2000... Training loss: 1.2793\n",
      "Epoch: 406/2000... Training loss: 1.5073\n",
      "Epoch: 406/2000... Training loss: 1.6812\n",
      "Epoch: 406/2000... Training loss: 1.3084\n",
      "Epoch: 406/2000... Training loss: 1.6232\n",
      "Epoch: 406/2000... Training loss: 1.7149\n",
      "Epoch: 406/2000... Training loss: 1.7740\n",
      "Epoch: 406/2000... Training loss: 1.6535\n",
      "Epoch: 406/2000... Training loss: 1.4518\n",
      "Epoch: 406/2000... Training loss: 1.3768\n",
      "Epoch: 406/2000... Training loss: 1.4984\n",
      "Epoch: 406/2000... Training loss: 1.4697\n",
      "Epoch: 406/2000... Training loss: 1.4094\n",
      "Epoch: 406/2000... Training loss: 1.6039\n",
      "Epoch: 406/2000... Training loss: 1.3287\n",
      "Epoch: 406/2000... Training loss: 1.4690\n",
      "Epoch: 406/2000... Training loss: 1.6102\n",
      "Epoch: 406/2000... Training loss: 1.3641\n",
      "Epoch: 406/2000... Training loss: 1.5998\n",
      "Epoch: 406/2000... Training loss: 1.4077\n",
      "Epoch: 406/2000... Training loss: 1.4877\n",
      "Epoch: 406/2000... Training loss: 1.3017\n",
      "Epoch: 406/2000... Training loss: 1.3378\n",
      "Epoch: 406/2000... Training loss: 1.3831\n",
      "Epoch: 406/2000... Training loss: 1.5058\n",
      "Epoch: 406/2000... Training loss: 1.4766\n",
      "Epoch: 406/2000... Training loss: 1.5373\n",
      "Epoch: 406/2000... Training loss: 1.5975\n",
      "Epoch: 407/2000... Training loss: 1.3964\n",
      "Epoch: 407/2000... Training loss: 1.6390\n",
      "Epoch: 407/2000... Training loss: 1.2719\n",
      "Epoch: 407/2000... Training loss: 1.3330\n",
      "Epoch: 407/2000... Training loss: 1.3652\n",
      "Epoch: 407/2000... Training loss: 1.6774\n",
      "Epoch: 407/2000... Training loss: 1.4670\n",
      "Epoch: 407/2000... Training loss: 1.4156\n",
      "Epoch: 407/2000... Training loss: 1.5307\n",
      "Epoch: 407/2000... Training loss: 1.5231\n",
      "Epoch: 407/2000... Training loss: 1.4848\n",
      "Epoch: 407/2000... Training loss: 1.4847\n",
      "Epoch: 407/2000... Training loss: 1.2513\n",
      "Epoch: 407/2000... Training loss: 1.6002\n",
      "Epoch: 407/2000... Training loss: 1.4312\n",
      "Epoch: 407/2000... Training loss: 1.2425\n",
      "Epoch: 407/2000... Training loss: 1.6460\n",
      "Epoch: 407/2000... Training loss: 1.4109\n",
      "Epoch: 407/2000... Training loss: 1.5620\n",
      "Epoch: 407/2000... Training loss: 1.2804\n",
      "Epoch: 407/2000... Training loss: 1.1926\n",
      "Epoch: 407/2000... Training loss: 1.5255\n",
      "Epoch: 407/2000... Training loss: 1.3014\n",
      "Epoch: 407/2000... Training loss: 1.4911\n",
      "Epoch: 407/2000... Training loss: 1.6644\n",
      "Epoch: 407/2000... Training loss: 1.3041\n",
      "Epoch: 407/2000... Training loss: 1.3806\n",
      "Epoch: 407/2000... Training loss: 1.3362\n",
      "Epoch: 407/2000... Training loss: 1.4308\n",
      "Epoch: 407/2000... Training loss: 1.2648\n",
      "Epoch: 407/2000... Training loss: 1.5234\n",
      "Epoch: 408/2000... Training loss: 1.3526\n",
      "Epoch: 408/2000... Training loss: 1.3142\n",
      "Epoch: 408/2000... Training loss: 1.6511\n",
      "Epoch: 408/2000... Training loss: 1.3150\n",
      "Epoch: 408/2000... Training loss: 1.4036\n",
      "Epoch: 408/2000... Training loss: 1.5879\n",
      "Epoch: 408/2000... Training loss: 1.4607\n",
      "Epoch: 408/2000... Training loss: 1.5548\n",
      "Epoch: 408/2000... Training loss: 1.3688\n",
      "Epoch: 408/2000... Training loss: 1.3309\n",
      "Epoch: 408/2000... Training loss: 1.6107\n",
      "Epoch: 408/2000... Training loss: 1.2354\n",
      "Epoch: 408/2000... Training loss: 1.5443\n",
      "Epoch: 408/2000... Training loss: 1.3001\n",
      "Epoch: 408/2000... Training loss: 1.4985\n",
      "Epoch: 408/2000... Training loss: 1.3914\n",
      "Epoch: 408/2000... Training loss: 1.3410\n",
      "Epoch: 408/2000... Training loss: 1.3922\n",
      "Epoch: 408/2000... Training loss: 1.4503\n",
      "Epoch: 408/2000... Training loss: 1.2203\n",
      "Epoch: 408/2000... Training loss: 1.2089\n",
      "Epoch: 408/2000... Training loss: 1.4447\n",
      "Epoch: 408/2000... Training loss: 1.3809\n",
      "Epoch: 408/2000... Training loss: 1.5001\n",
      "Epoch: 408/2000... Training loss: 1.3462\n",
      "Epoch: 408/2000... Training loss: 1.2359\n",
      "Epoch: 408/2000... Training loss: 1.5032\n",
      "Epoch: 408/2000... Training loss: 1.2804\n",
      "Epoch: 408/2000... Training loss: 1.6269\n",
      "Epoch: 408/2000... Training loss: 1.3476\n",
      "Epoch: 408/2000... Training loss: 1.3100\n",
      "Epoch: 409/2000... Training loss: 1.5813\n",
      "Epoch: 409/2000... Training loss: 1.2408\n",
      "Epoch: 409/2000... Training loss: 1.6036\n",
      "Epoch: 409/2000... Training loss: 1.2468\n",
      "Epoch: 409/2000... Training loss: 1.4152\n",
      "Epoch: 409/2000... Training loss: 1.0654\n",
      "Epoch: 409/2000... Training loss: 1.3879\n",
      "Epoch: 409/2000... Training loss: 1.2753\n",
      "Epoch: 409/2000... Training loss: 1.4221\n",
      "Epoch: 409/2000... Training loss: 1.3982\n",
      "Epoch: 409/2000... Training loss: 1.3005\n",
      "Epoch: 409/2000... Training loss: 1.5131\n",
      "Epoch: 409/2000... Training loss: 1.5616\n",
      "Epoch: 409/2000... Training loss: 1.4566\n",
      "Epoch: 409/2000... Training loss: 1.3874\n",
      "Epoch: 409/2000... Training loss: 1.4135\n",
      "Epoch: 409/2000... Training loss: 1.3291\n",
      "Epoch: 409/2000... Training loss: 1.4825\n",
      "Epoch: 409/2000... Training loss: 1.6151\n",
      "Epoch: 409/2000... Training loss: 1.2958\n",
      "Epoch: 409/2000... Training loss: 1.5369\n",
      "Epoch: 409/2000... Training loss: 1.5666\n",
      "Epoch: 409/2000... Training loss: 1.3226\n",
      "Epoch: 409/2000... Training loss: 1.3470\n",
      "Epoch: 409/2000... Training loss: 1.3371\n",
      "Epoch: 409/2000... Training loss: 1.6625\n",
      "Epoch: 409/2000... Training loss: 1.3004\n",
      "Epoch: 409/2000... Training loss: 1.6913\n",
      "Epoch: 409/2000... Training loss: 1.4253\n",
      "Epoch: 409/2000... Training loss: 1.3757\n",
      "Epoch: 409/2000... Training loss: 1.3587\n",
      "Epoch: 410/2000... Training loss: 1.2553\n",
      "Epoch: 410/2000... Training loss: 1.4101\n",
      "Epoch: 410/2000... Training loss: 1.2590\n",
      "Epoch: 410/2000... Training loss: 1.4211\n",
      "Epoch: 410/2000... Training loss: 1.2775\n",
      "Epoch: 410/2000... Training loss: 1.4229\n",
      "Epoch: 410/2000... Training loss: 1.5163\n",
      "Epoch: 410/2000... Training loss: 1.1643\n",
      "Epoch: 410/2000... Training loss: 1.2516\n",
      "Epoch: 410/2000... Training loss: 1.4537\n",
      "Epoch: 410/2000... Training loss: 1.7969\n",
      "Epoch: 410/2000... Training loss: 1.5948\n",
      "Epoch: 410/2000... Training loss: 1.5315\n",
      "Epoch: 410/2000... Training loss: 1.3985\n",
      "Epoch: 410/2000... Training loss: 1.3497\n",
      "Epoch: 410/2000... Training loss: 1.4614\n",
      "Epoch: 410/2000... Training loss: 1.3990\n",
      "Epoch: 410/2000... Training loss: 1.3609\n",
      "Epoch: 410/2000... Training loss: 1.5451\n",
      "Epoch: 410/2000... Training loss: 1.5493\n",
      "Epoch: 410/2000... Training loss: 1.4473\n",
      "Epoch: 410/2000... Training loss: 1.2915\n",
      "Epoch: 410/2000... Training loss: 1.3083\n",
      "Epoch: 410/2000... Training loss: 1.2245\n",
      "Epoch: 410/2000... Training loss: 1.2709\n",
      "Epoch: 410/2000... Training loss: 1.4345\n",
      "Epoch: 410/2000... Training loss: 1.6716\n",
      "Epoch: 410/2000... Training loss: 1.3183\n",
      "Epoch: 410/2000... Training loss: 1.5494\n",
      "Epoch: 410/2000... Training loss: 1.1783\n",
      "Epoch: 410/2000... Training loss: 1.6362\n",
      "Epoch: 411/2000... Training loss: 1.2233\n",
      "Epoch: 411/2000... Training loss: 1.4154\n",
      "Epoch: 411/2000... Training loss: 1.3732\n",
      "Epoch: 411/2000... Training loss: 1.2466\n",
      "Epoch: 411/2000... Training loss: 1.5060\n",
      "Epoch: 411/2000... Training loss: 1.2027\n",
      "Epoch: 411/2000... Training loss: 1.7090\n",
      "Epoch: 411/2000... Training loss: 1.5311\n",
      "Epoch: 411/2000... Training loss: 1.5758\n",
      "Epoch: 411/2000... Training loss: 1.4336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 411/2000... Training loss: 1.2138\n",
      "Epoch: 411/2000... Training loss: 1.4425\n",
      "Epoch: 411/2000... Training loss: 1.4829\n",
      "Epoch: 411/2000... Training loss: 1.2270\n",
      "Epoch: 411/2000... Training loss: 1.4571\n",
      "Epoch: 411/2000... Training loss: 1.3632\n",
      "Epoch: 411/2000... Training loss: 1.4447\n",
      "Epoch: 411/2000... Training loss: 1.6796\n",
      "Epoch: 411/2000... Training loss: 1.2777\n",
      "Epoch: 411/2000... Training loss: 1.4819\n",
      "Epoch: 411/2000... Training loss: 1.4325\n",
      "Epoch: 411/2000... Training loss: 1.5536\n",
      "Epoch: 411/2000... Training loss: 1.6747\n",
      "Epoch: 411/2000... Training loss: 1.2383\n",
      "Epoch: 411/2000... Training loss: 1.4597\n",
      "Epoch: 411/2000... Training loss: 1.3387\n",
      "Epoch: 411/2000... Training loss: 1.4011\n",
      "Epoch: 411/2000... Training loss: 1.3403\n",
      "Epoch: 411/2000... Training loss: 1.3076\n",
      "Epoch: 411/2000... Training loss: 1.4426\n",
      "Epoch: 411/2000... Training loss: 1.2820\n",
      "Epoch: 412/2000... Training loss: 1.4924\n",
      "Epoch: 412/2000... Training loss: 1.5278\n",
      "Epoch: 412/2000... Training loss: 1.3382\n",
      "Epoch: 412/2000... Training loss: 1.4290\n",
      "Epoch: 412/2000... Training loss: 1.1156\n",
      "Epoch: 412/2000... Training loss: 1.6504\n",
      "Epoch: 412/2000... Training loss: 1.5530\n",
      "Epoch: 412/2000... Training loss: 1.2716\n",
      "Epoch: 412/2000... Training loss: 1.3712\n",
      "Epoch: 412/2000... Training loss: 1.2656\n",
      "Epoch: 412/2000... Training loss: 1.4433\n",
      "Epoch: 412/2000... Training loss: 1.4235\n",
      "Epoch: 412/2000... Training loss: 1.4959\n",
      "Epoch: 412/2000... Training loss: 1.5753\n",
      "Epoch: 412/2000... Training loss: 1.5629\n",
      "Epoch: 412/2000... Training loss: 1.1991\n",
      "Epoch: 412/2000... Training loss: 1.3577\n",
      "Epoch: 412/2000... Training loss: 1.3316\n",
      "Epoch: 412/2000... Training loss: 1.4512\n",
      "Epoch: 412/2000... Training loss: 1.6590\n",
      "Epoch: 412/2000... Training loss: 1.3585\n",
      "Epoch: 412/2000... Training loss: 1.6953\n",
      "Epoch: 412/2000... Training loss: 1.4081\n",
      "Epoch: 412/2000... Training loss: 1.2609\n",
      "Epoch: 412/2000... Training loss: 1.4994\n",
      "Epoch: 412/2000... Training loss: 1.7025\n",
      "Epoch: 412/2000... Training loss: 1.6649\n",
      "Epoch: 412/2000... Training loss: 1.4541\n",
      "Epoch: 412/2000... Training loss: 1.3899\n",
      "Epoch: 412/2000... Training loss: 1.4679\n",
      "Epoch: 412/2000... Training loss: 1.2352\n",
      "Epoch: 413/2000... Training loss: 1.4980\n",
      "Epoch: 413/2000... Training loss: 1.4570\n",
      "Epoch: 413/2000... Training loss: 1.4557\n",
      "Epoch: 413/2000... Training loss: 1.3258\n",
      "Epoch: 413/2000... Training loss: 1.2875\n",
      "Epoch: 413/2000... Training loss: 1.2900\n",
      "Epoch: 413/2000... Training loss: 1.2265\n",
      "Epoch: 413/2000... Training loss: 1.3507\n",
      "Epoch: 413/2000... Training loss: 1.5583\n",
      "Epoch: 413/2000... Training loss: 1.4236\n",
      "Epoch: 413/2000... Training loss: 1.7127\n",
      "Epoch: 413/2000... Training loss: 1.4371\n",
      "Epoch: 413/2000... Training loss: 1.2812\n",
      "Epoch: 413/2000... Training loss: 1.4501\n",
      "Epoch: 413/2000... Training loss: 1.3635\n",
      "Epoch: 413/2000... Training loss: 1.4027\n",
      "Epoch: 413/2000... Training loss: 1.5311\n",
      "Epoch: 413/2000... Training loss: 1.3322\n",
      "Epoch: 413/2000... Training loss: 1.2352\n",
      "Epoch: 413/2000... Training loss: 1.5633\n",
      "Epoch: 413/2000... Training loss: 1.4007\n",
      "Epoch: 413/2000... Training loss: 1.2711\n",
      "Epoch: 413/2000... Training loss: 1.4890\n",
      "Epoch: 413/2000... Training loss: 1.4455\n",
      "Epoch: 413/2000... Training loss: 1.6572\n",
      "Epoch: 413/2000... Training loss: 1.5835\n",
      "Epoch: 413/2000... Training loss: 1.4643\n",
      "Epoch: 413/2000... Training loss: 1.6286\n",
      "Epoch: 413/2000... Training loss: 1.3957\n",
      "Epoch: 413/2000... Training loss: 1.2385\n",
      "Epoch: 413/2000... Training loss: 1.5472\n",
      "Epoch: 414/2000... Training loss: 1.3815\n",
      "Epoch: 414/2000... Training loss: 1.3550\n",
      "Epoch: 414/2000... Training loss: 1.4640\n",
      "Epoch: 414/2000... Training loss: 1.3578\n",
      "Epoch: 414/2000... Training loss: 1.5891\n",
      "Epoch: 414/2000... Training loss: 1.5912\n",
      "Epoch: 414/2000... Training loss: 1.2914\n",
      "Epoch: 414/2000... Training loss: 1.4322\n",
      "Epoch: 414/2000... Training loss: 1.6648\n",
      "Epoch: 414/2000... Training loss: 1.5432\n",
      "Epoch: 414/2000... Training loss: 1.3668\n",
      "Epoch: 414/2000... Training loss: 1.2987\n",
      "Epoch: 414/2000... Training loss: 1.5363\n",
      "Epoch: 414/2000... Training loss: 1.4275\n",
      "Epoch: 414/2000... Training loss: 1.2894\n",
      "Epoch: 414/2000... Training loss: 1.5025\n",
      "Epoch: 414/2000... Training loss: 1.3101\n",
      "Epoch: 414/2000... Training loss: 1.2987\n",
      "Epoch: 414/2000... Training loss: 1.2842\n",
      "Epoch: 414/2000... Training loss: 1.8060\n",
      "Epoch: 414/2000... Training loss: 1.5574\n",
      "Epoch: 414/2000... Training loss: 1.3274\n",
      "Epoch: 414/2000... Training loss: 1.6166\n",
      "Epoch: 414/2000... Training loss: 1.5536\n",
      "Epoch: 414/2000... Training loss: 1.2101\n",
      "Epoch: 414/2000... Training loss: 1.5774\n",
      "Epoch: 414/2000... Training loss: 1.6149\n",
      "Epoch: 414/2000... Training loss: 1.3738\n",
      "Epoch: 414/2000... Training loss: 1.4422\n",
      "Epoch: 414/2000... Training loss: 1.5954\n",
      "Epoch: 414/2000... Training loss: 1.5029\n",
      "Epoch: 415/2000... Training loss: 1.4346\n",
      "Epoch: 415/2000... Training loss: 1.1957\n",
      "Epoch: 415/2000... Training loss: 1.3243\n",
      "Epoch: 415/2000... Training loss: 1.3991\n",
      "Epoch: 415/2000... Training loss: 1.1481\n",
      "Epoch: 415/2000... Training loss: 1.5591\n",
      "Epoch: 415/2000... Training loss: 1.2625\n",
      "Epoch: 415/2000... Training loss: 1.3078\n",
      "Epoch: 415/2000... Training loss: 1.3119\n",
      "Epoch: 415/2000... Training loss: 1.4441\n",
      "Epoch: 415/2000... Training loss: 1.4836\n",
      "Epoch: 415/2000... Training loss: 1.3204\n",
      "Epoch: 415/2000... Training loss: 1.5042\n",
      "Epoch: 415/2000... Training loss: 1.4024\n",
      "Epoch: 415/2000... Training loss: 1.5779\n",
      "Epoch: 415/2000... Training loss: 1.3642\n",
      "Epoch: 415/2000... Training loss: 1.4964\n",
      "Epoch: 415/2000... Training loss: 1.3506\n",
      "Epoch: 415/2000... Training loss: 1.4138\n",
      "Epoch: 415/2000... Training loss: 1.4196\n",
      "Epoch: 415/2000... Training loss: 1.3154\n",
      "Epoch: 415/2000... Training loss: 1.2510\n",
      "Epoch: 415/2000... Training loss: 1.2796\n",
      "Epoch: 415/2000... Training loss: 1.3582\n",
      "Epoch: 415/2000... Training loss: 1.3856\n",
      "Epoch: 415/2000... Training loss: 1.4736\n",
      "Epoch: 415/2000... Training loss: 1.3729\n",
      "Epoch: 415/2000... Training loss: 1.2480\n",
      "Epoch: 415/2000... Training loss: 1.3680\n",
      "Epoch: 415/2000... Training loss: 1.4045\n",
      "Epoch: 415/2000... Training loss: 1.1152\n",
      "Epoch: 416/2000... Training loss: 1.3156\n",
      "Epoch: 416/2000... Training loss: 1.3260\n",
      "Epoch: 416/2000... Training loss: 1.3517\n",
      "Epoch: 416/2000... Training loss: 1.5539\n",
      "Epoch: 416/2000... Training loss: 1.3227\n",
      "Epoch: 416/2000... Training loss: 1.3287\n",
      "Epoch: 416/2000... Training loss: 1.5526\n",
      "Epoch: 416/2000... Training loss: 1.3157\n",
      "Epoch: 416/2000... Training loss: 1.5054\n",
      "Epoch: 416/2000... Training loss: 1.3196\n",
      "Epoch: 416/2000... Training loss: 1.4934\n",
      "Epoch: 416/2000... Training loss: 1.3186\n",
      "Epoch: 416/2000... Training loss: 1.4608\n",
      "Epoch: 416/2000... Training loss: 1.0947\n",
      "Epoch: 416/2000... Training loss: 1.1171\n",
      "Epoch: 416/2000... Training loss: 1.3662\n",
      "Epoch: 416/2000... Training loss: 1.3815\n",
      "Epoch: 416/2000... Training loss: 1.4498\n",
      "Epoch: 416/2000... Training loss: 1.6000\n",
      "Epoch: 416/2000... Training loss: 1.5987\n",
      "Epoch: 416/2000... Training loss: 1.1360\n",
      "Epoch: 416/2000... Training loss: 1.3604\n",
      "Epoch: 416/2000... Training loss: 1.5316\n",
      "Epoch: 416/2000... Training loss: 1.4168\n",
      "Epoch: 416/2000... Training loss: 1.2373\n",
      "Epoch: 416/2000... Training loss: 1.1305\n",
      "Epoch: 416/2000... Training loss: 1.4633\n",
      "Epoch: 416/2000... Training loss: 1.3746\n",
      "Epoch: 416/2000... Training loss: 1.3150\n",
      "Epoch: 416/2000... Training loss: 1.3731\n",
      "Epoch: 416/2000... Training loss: 1.4257\n",
      "Epoch: 417/2000... Training loss: 1.3258\n",
      "Epoch: 417/2000... Training loss: 1.4332\n",
      "Epoch: 417/2000... Training loss: 1.5511\n",
      "Epoch: 417/2000... Training loss: 1.4693\n",
      "Epoch: 417/2000... Training loss: 1.5387\n",
      "Epoch: 417/2000... Training loss: 1.4095\n",
      "Epoch: 417/2000... Training loss: 1.2733\n",
      "Epoch: 417/2000... Training loss: 1.2507\n",
      "Epoch: 417/2000... Training loss: 1.4279\n",
      "Epoch: 417/2000... Training loss: 1.4309\n",
      "Epoch: 417/2000... Training loss: 1.6243\n",
      "Epoch: 417/2000... Training loss: 1.2624\n",
      "Epoch: 417/2000... Training loss: 1.2223\n",
      "Epoch: 417/2000... Training loss: 1.5912\n",
      "Epoch: 417/2000... Training loss: 1.2670\n",
      "Epoch: 417/2000... Training loss: 1.4735\n",
      "Epoch: 417/2000... Training loss: 1.4172\n",
      "Epoch: 417/2000... Training loss: 1.3561\n",
      "Epoch: 417/2000... Training loss: 1.4622\n",
      "Epoch: 417/2000... Training loss: 1.5452\n",
      "Epoch: 417/2000... Training loss: 1.4855\n",
      "Epoch: 417/2000... Training loss: 1.3543\n",
      "Epoch: 417/2000... Training loss: 1.5636\n",
      "Epoch: 417/2000... Training loss: 1.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 417/2000... Training loss: 1.3932\n",
      "Epoch: 417/2000... Training loss: 1.3982\n",
      "Epoch: 417/2000... Training loss: 1.4869\n",
      "Epoch: 417/2000... Training loss: 1.2029\n",
      "Epoch: 417/2000... Training loss: 1.2875\n",
      "Epoch: 417/2000... Training loss: 1.3390\n",
      "Epoch: 417/2000... Training loss: 1.4176\n",
      "Epoch: 418/2000... Training loss: 1.3518\n",
      "Epoch: 418/2000... Training loss: 1.4418\n",
      "Epoch: 418/2000... Training loss: 1.1814\n",
      "Epoch: 418/2000... Training loss: 1.1080\n",
      "Epoch: 418/2000... Training loss: 1.2657\n",
      "Epoch: 418/2000... Training loss: 1.4171\n",
      "Epoch: 418/2000... Training loss: 1.2772\n",
      "Epoch: 418/2000... Training loss: 1.3009\n",
      "Epoch: 418/2000... Training loss: 1.5286\n",
      "Epoch: 418/2000... Training loss: 1.4381\n",
      "Epoch: 418/2000... Training loss: 1.4004\n",
      "Epoch: 418/2000... Training loss: 1.4831\n",
      "Epoch: 418/2000... Training loss: 1.2580\n",
      "Epoch: 418/2000... Training loss: 1.5126\n",
      "Epoch: 418/2000... Training loss: 1.5951\n",
      "Epoch: 418/2000... Training loss: 1.2815\n",
      "Epoch: 418/2000... Training loss: 1.3862\n",
      "Epoch: 418/2000... Training loss: 1.5361\n",
      "Epoch: 418/2000... Training loss: 1.3828\n",
      "Epoch: 418/2000... Training loss: 1.2296\n",
      "Epoch: 418/2000... Training loss: 1.4339\n",
      "Epoch: 418/2000... Training loss: 1.2759\n",
      "Epoch: 418/2000... Training loss: 1.5782\n",
      "Epoch: 418/2000... Training loss: 1.3737\n",
      "Epoch: 418/2000... Training loss: 1.3125\n",
      "Epoch: 418/2000... Training loss: 1.5316\n",
      "Epoch: 418/2000... Training loss: 1.5232\n",
      "Epoch: 418/2000... Training loss: 1.2255\n",
      "Epoch: 418/2000... Training loss: 1.5166\n",
      "Epoch: 418/2000... Training loss: 1.5586\n",
      "Epoch: 418/2000... Training loss: 1.5319\n",
      "Epoch: 419/2000... Training loss: 1.3559\n",
      "Epoch: 419/2000... Training loss: 1.3706\n",
      "Epoch: 419/2000... Training loss: 1.2833\n",
      "Epoch: 419/2000... Training loss: 1.5517\n",
      "Epoch: 419/2000... Training loss: 1.3375\n",
      "Epoch: 419/2000... Training loss: 1.3971\n",
      "Epoch: 419/2000... Training loss: 1.4174\n",
      "Epoch: 419/2000... Training loss: 1.4224\n",
      "Epoch: 419/2000... Training loss: 1.1278\n",
      "Epoch: 419/2000... Training loss: 1.3475\n",
      "Epoch: 419/2000... Training loss: 1.2431\n",
      "Epoch: 419/2000... Training loss: 1.5307\n",
      "Epoch: 419/2000... Training loss: 1.5859\n",
      "Epoch: 419/2000... Training loss: 1.5144\n",
      "Epoch: 419/2000... Training loss: 1.3117\n",
      "Epoch: 419/2000... Training loss: 1.5328\n",
      "Epoch: 419/2000... Training loss: 1.3629\n",
      "Epoch: 419/2000... Training loss: 1.3920\n",
      "Epoch: 419/2000... Training loss: 1.4036\n",
      "Epoch: 419/2000... Training loss: 1.5638\n",
      "Epoch: 419/2000... Training loss: 1.4654\n",
      "Epoch: 419/2000... Training loss: 1.5192\n",
      "Epoch: 419/2000... Training loss: 1.3219\n",
      "Epoch: 419/2000... Training loss: 1.6314\n",
      "Epoch: 419/2000... Training loss: 1.5563\n",
      "Epoch: 419/2000... Training loss: 1.4351\n",
      "Epoch: 419/2000... Training loss: 1.0964\n",
      "Epoch: 419/2000... Training loss: 1.5707\n",
      "Epoch: 419/2000... Training loss: 1.3432\n",
      "Epoch: 419/2000... Training loss: 1.3672\n",
      "Epoch: 419/2000... Training loss: 1.1966\n",
      "Epoch: 420/2000... Training loss: 1.5259\n",
      "Epoch: 420/2000... Training loss: 1.2965\n",
      "Epoch: 420/2000... Training loss: 1.3232\n",
      "Epoch: 420/2000... Training loss: 1.2068\n",
      "Epoch: 420/2000... Training loss: 1.3123\n",
      "Epoch: 420/2000... Training loss: 1.3450\n",
      "Epoch: 420/2000... Training loss: 1.4863\n",
      "Epoch: 420/2000... Training loss: 1.4331\n",
      "Epoch: 420/2000... Training loss: 1.1706\n",
      "Epoch: 420/2000... Training loss: 1.5166\n",
      "Epoch: 420/2000... Training loss: 1.4379\n",
      "Epoch: 420/2000... Training loss: 1.3925\n",
      "Epoch: 420/2000... Training loss: 1.3994\n",
      "Epoch: 420/2000... Training loss: 1.2086\n",
      "Epoch: 420/2000... Training loss: 1.3378\n",
      "Epoch: 420/2000... Training loss: 1.3298\n",
      "Epoch: 420/2000... Training loss: 1.4994\n",
      "Epoch: 420/2000... Training loss: 1.1816\n",
      "Epoch: 420/2000... Training loss: 1.3805\n",
      "Epoch: 420/2000... Training loss: 1.2605\n",
      "Epoch: 420/2000... Training loss: 1.2719\n",
      "Epoch: 420/2000... Training loss: 1.5275\n",
      "Epoch: 420/2000... Training loss: 1.3992\n",
      "Epoch: 420/2000... Training loss: 1.5244\n",
      "Epoch: 420/2000... Training loss: 1.3783\n",
      "Epoch: 420/2000... Training loss: 1.6724\n",
      "Epoch: 420/2000... Training loss: 1.5701\n",
      "Epoch: 420/2000... Training loss: 1.3794\n",
      "Epoch: 420/2000... Training loss: 1.2951\n",
      "Epoch: 420/2000... Training loss: 1.4461\n",
      "Epoch: 420/2000... Training loss: 1.4675\n",
      "Epoch: 421/2000... Training loss: 1.2788\n",
      "Epoch: 421/2000... Training loss: 1.4196\n",
      "Epoch: 421/2000... Training loss: 1.2096\n",
      "Epoch: 421/2000... Training loss: 1.5493\n",
      "Epoch: 421/2000... Training loss: 1.3834\n",
      "Epoch: 421/2000... Training loss: 1.3733\n",
      "Epoch: 421/2000... Training loss: 1.2368\n",
      "Epoch: 421/2000... Training loss: 1.5186\n",
      "Epoch: 421/2000... Training loss: 1.3505\n",
      "Epoch: 421/2000... Training loss: 1.3397\n",
      "Epoch: 421/2000... Training loss: 1.4095\n",
      "Epoch: 421/2000... Training loss: 1.3144\n",
      "Epoch: 421/2000... Training loss: 1.3532\n",
      "Epoch: 421/2000... Training loss: 1.4951\n",
      "Epoch: 421/2000... Training loss: 1.3149\n",
      "Epoch: 421/2000... Training loss: 1.3585\n",
      "Epoch: 421/2000... Training loss: 1.2992\n",
      "Epoch: 421/2000... Training loss: 1.2752\n",
      "Epoch: 421/2000... Training loss: 1.3269\n",
      "Epoch: 421/2000... Training loss: 1.4700\n",
      "Epoch: 421/2000... Training loss: 1.1177\n",
      "Epoch: 421/2000... Training loss: 1.3407\n",
      "Epoch: 421/2000... Training loss: 1.6216\n",
      "Epoch: 421/2000... Training loss: 1.6477\n",
      "Epoch: 421/2000... Training loss: 1.4313\n",
      "Epoch: 421/2000... Training loss: 1.3434\n",
      "Epoch: 421/2000... Training loss: 1.4263\n",
      "Epoch: 421/2000... Training loss: 1.0923\n",
      "Epoch: 421/2000... Training loss: 1.3747\n",
      "Epoch: 421/2000... Training loss: 1.4468\n",
      "Epoch: 421/2000... Training loss: 1.3284\n",
      "Epoch: 422/2000... Training loss: 1.4206\n",
      "Epoch: 422/2000... Training loss: 1.2964\n",
      "Epoch: 422/2000... Training loss: 1.3448\n",
      "Epoch: 422/2000... Training loss: 1.1753\n",
      "Epoch: 422/2000... Training loss: 1.2511\n",
      "Epoch: 422/2000... Training loss: 1.3371\n",
      "Epoch: 422/2000... Training loss: 1.3531\n",
      "Epoch: 422/2000... Training loss: 1.2982\n",
      "Epoch: 422/2000... Training loss: 1.6646\n",
      "Epoch: 422/2000... Training loss: 1.2992\n",
      "Epoch: 422/2000... Training loss: 1.4616\n",
      "Epoch: 422/2000... Training loss: 1.4147\n",
      "Epoch: 422/2000... Training loss: 1.4150\n",
      "Epoch: 422/2000... Training loss: 1.2870\n",
      "Epoch: 422/2000... Training loss: 1.3653\n",
      "Epoch: 422/2000... Training loss: 1.3226\n",
      "Epoch: 422/2000... Training loss: 1.4388\n",
      "Epoch: 422/2000... Training loss: 1.3577\n",
      "Epoch: 422/2000... Training loss: 1.1146\n",
      "Epoch: 422/2000... Training loss: 1.4505\n",
      "Epoch: 422/2000... Training loss: 1.1740\n",
      "Epoch: 422/2000... Training loss: 1.4877\n",
      "Epoch: 422/2000... Training loss: 1.5577\n",
      "Epoch: 422/2000... Training loss: 1.3837\n",
      "Epoch: 422/2000... Training loss: 1.4098\n",
      "Epoch: 422/2000... Training loss: 1.6792\n",
      "Epoch: 422/2000... Training loss: 1.3893\n",
      "Epoch: 422/2000... Training loss: 1.2352\n",
      "Epoch: 422/2000... Training loss: 1.3193\n",
      "Epoch: 422/2000... Training loss: 1.5393\n",
      "Epoch: 422/2000... Training loss: 1.2444\n",
      "Epoch: 423/2000... Training loss: 1.1257\n",
      "Epoch: 423/2000... Training loss: 1.5706\n",
      "Epoch: 423/2000... Training loss: 1.4722\n",
      "Epoch: 423/2000... Training loss: 1.3958\n",
      "Epoch: 423/2000... Training loss: 1.5316\n",
      "Epoch: 423/2000... Training loss: 1.5027\n",
      "Epoch: 423/2000... Training loss: 1.2194\n",
      "Epoch: 423/2000... Training loss: 1.4876\n",
      "Epoch: 423/2000... Training loss: 1.3561\n",
      "Epoch: 423/2000... Training loss: 1.3840\n",
      "Epoch: 423/2000... Training loss: 1.4522\n",
      "Epoch: 423/2000... Training loss: 1.5472\n",
      "Epoch: 423/2000... Training loss: 1.4530\n",
      "Epoch: 423/2000... Training loss: 1.5162\n",
      "Epoch: 423/2000... Training loss: 1.4413\n",
      "Epoch: 423/2000... Training loss: 1.4023\n",
      "Epoch: 423/2000... Training loss: 1.4945\n",
      "Epoch: 423/2000... Training loss: 1.4047\n",
      "Epoch: 423/2000... Training loss: 1.0951\n",
      "Epoch: 423/2000... Training loss: 1.4432\n",
      "Epoch: 423/2000... Training loss: 1.4526\n",
      "Epoch: 423/2000... Training loss: 1.2624\n",
      "Epoch: 423/2000... Training loss: 1.4237\n",
      "Epoch: 423/2000... Training loss: 1.5588\n",
      "Epoch: 423/2000... Training loss: 1.3064\n",
      "Epoch: 423/2000... Training loss: 1.4645\n",
      "Epoch: 423/2000... Training loss: 1.5660\n",
      "Epoch: 423/2000... Training loss: 1.2714\n",
      "Epoch: 423/2000... Training loss: 1.3640\n",
      "Epoch: 423/2000... Training loss: 1.4300\n",
      "Epoch: 423/2000... Training loss: 1.0175\n",
      "Epoch: 424/2000... Training loss: 1.3929\n",
      "Epoch: 424/2000... Training loss: 1.5274\n",
      "Epoch: 424/2000... Training loss: 1.4160\n",
      "Epoch: 424/2000... Training loss: 1.3125\n",
      "Epoch: 424/2000... Training loss: 1.4346\n",
      "Epoch: 424/2000... Training loss: 1.5487\n",
      "Epoch: 424/2000... Training loss: 1.4102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 424/2000... Training loss: 1.1694\n",
      "Epoch: 424/2000... Training loss: 1.7564\n",
      "Epoch: 424/2000... Training loss: 1.4705\n",
      "Epoch: 424/2000... Training loss: 1.4828\n",
      "Epoch: 424/2000... Training loss: 1.2282\n",
      "Epoch: 424/2000... Training loss: 1.2396\n",
      "Epoch: 424/2000... Training loss: 1.2633\n",
      "Epoch: 424/2000... Training loss: 1.2955\n",
      "Epoch: 424/2000... Training loss: 1.3193\n",
      "Epoch: 424/2000... Training loss: 1.3599\n",
      "Epoch: 424/2000... Training loss: 1.6568\n",
      "Epoch: 424/2000... Training loss: 1.4292\n",
      "Epoch: 424/2000... Training loss: 1.4777\n",
      "Epoch: 424/2000... Training loss: 1.5509\n",
      "Epoch: 424/2000... Training loss: 1.2195\n",
      "Epoch: 424/2000... Training loss: 1.4731\n",
      "Epoch: 424/2000... Training loss: 1.4339\n",
      "Epoch: 424/2000... Training loss: 1.4399\n",
      "Epoch: 424/2000... Training loss: 1.4629\n",
      "Epoch: 424/2000... Training loss: 1.4580\n",
      "Epoch: 424/2000... Training loss: 1.3390\n",
      "Epoch: 424/2000... Training loss: 1.2466\n",
      "Epoch: 424/2000... Training loss: 1.3379\n",
      "Epoch: 424/2000... Training loss: 1.5216\n",
      "Epoch: 425/2000... Training loss: 1.3818\n",
      "Epoch: 425/2000... Training loss: 1.4012\n",
      "Epoch: 425/2000... Training loss: 1.4902\n",
      "Epoch: 425/2000... Training loss: 1.4356\n",
      "Epoch: 425/2000... Training loss: 1.4821\n",
      "Epoch: 425/2000... Training loss: 1.2307\n",
      "Epoch: 425/2000... Training loss: 1.5243\n",
      "Epoch: 425/2000... Training loss: 1.6893\n",
      "Epoch: 425/2000... Training loss: 1.3393\n",
      "Epoch: 425/2000... Training loss: 1.4830\n",
      "Epoch: 425/2000... Training loss: 1.3311\n",
      "Epoch: 425/2000... Training loss: 1.5468\n",
      "Epoch: 425/2000... Training loss: 1.4128\n",
      "Epoch: 425/2000... Training loss: 1.3700\n",
      "Epoch: 425/2000... Training loss: 1.4611\n",
      "Epoch: 425/2000... Training loss: 1.2908\n",
      "Epoch: 425/2000... Training loss: 1.3045\n",
      "Epoch: 425/2000... Training loss: 1.3426\n",
      "Epoch: 425/2000... Training loss: 1.4874\n",
      "Epoch: 425/2000... Training loss: 1.4797\n",
      "Epoch: 425/2000... Training loss: 1.2244\n",
      "Epoch: 425/2000... Training loss: 1.3213\n",
      "Epoch: 425/2000... Training loss: 1.3068\n",
      "Epoch: 425/2000... Training loss: 1.2840\n",
      "Epoch: 425/2000... Training loss: 1.3685\n",
      "Epoch: 425/2000... Training loss: 1.4036\n",
      "Epoch: 425/2000... Training loss: 1.2619\n",
      "Epoch: 425/2000... Training loss: 1.2852\n",
      "Epoch: 425/2000... Training loss: 1.5114\n",
      "Epoch: 425/2000... Training loss: 1.4942\n",
      "Epoch: 425/2000... Training loss: 1.6333\n",
      "Epoch: 426/2000... Training loss: 1.5024\n",
      "Epoch: 426/2000... Training loss: 1.4108\n",
      "Epoch: 426/2000... Training loss: 1.2094\n",
      "Epoch: 426/2000... Training loss: 1.4823\n",
      "Epoch: 426/2000... Training loss: 1.4202\n",
      "Epoch: 426/2000... Training loss: 1.5594\n",
      "Epoch: 426/2000... Training loss: 1.1650\n",
      "Epoch: 426/2000... Training loss: 1.0784\n",
      "Epoch: 426/2000... Training loss: 1.3048\n",
      "Epoch: 426/2000... Training loss: 1.3256\n",
      "Epoch: 426/2000... Training loss: 1.3992\n",
      "Epoch: 426/2000... Training loss: 1.2175\n",
      "Epoch: 426/2000... Training loss: 1.2266\n",
      "Epoch: 426/2000... Training loss: 1.4583\n",
      "Epoch: 426/2000... Training loss: 1.2004\n",
      "Epoch: 426/2000... Training loss: 1.2747\n",
      "Epoch: 426/2000... Training loss: 1.2192\n",
      "Epoch: 426/2000... Training loss: 1.3211\n",
      "Epoch: 426/2000... Training loss: 1.2612\n",
      "Epoch: 426/2000... Training loss: 1.5075\n",
      "Epoch: 426/2000... Training loss: 1.4128\n",
      "Epoch: 426/2000... Training loss: 1.3311\n",
      "Epoch: 426/2000... Training loss: 1.6388\n",
      "Epoch: 426/2000... Training loss: 1.3106\n",
      "Epoch: 426/2000... Training loss: 1.3451\n",
      "Epoch: 426/2000... Training loss: 1.0755\n",
      "Epoch: 426/2000... Training loss: 1.5184\n",
      "Epoch: 426/2000... Training loss: 1.3934\n",
      "Epoch: 426/2000... Training loss: 1.2175\n",
      "Epoch: 426/2000... Training loss: 1.3715\n",
      "Epoch: 426/2000... Training loss: 1.1165\n",
      "Epoch: 427/2000... Training loss: 1.4403\n",
      "Epoch: 427/2000... Training loss: 1.2452\n",
      "Epoch: 427/2000... Training loss: 1.4868\n",
      "Epoch: 427/2000... Training loss: 1.4623\n",
      "Epoch: 427/2000... Training loss: 1.2713\n",
      "Epoch: 427/2000... Training loss: 1.0890\n",
      "Epoch: 427/2000... Training loss: 1.5596\n",
      "Epoch: 427/2000... Training loss: 1.3464\n",
      "Epoch: 427/2000... Training loss: 1.4974\n",
      "Epoch: 427/2000... Training loss: 1.3720\n",
      "Epoch: 427/2000... Training loss: 1.2637\n",
      "Epoch: 427/2000... Training loss: 1.1553\n",
      "Epoch: 427/2000... Training loss: 1.4854\n",
      "Epoch: 427/2000... Training loss: 1.2889\n",
      "Epoch: 427/2000... Training loss: 1.3856\n",
      "Epoch: 427/2000... Training loss: 1.5472\n",
      "Epoch: 427/2000... Training loss: 1.3881\n",
      "Epoch: 427/2000... Training loss: 1.2870\n",
      "Epoch: 427/2000... Training loss: 1.7152\n",
      "Epoch: 427/2000... Training loss: 1.3847\n",
      "Epoch: 427/2000... Training loss: 1.1605\n",
      "Epoch: 427/2000... Training loss: 1.3622\n",
      "Epoch: 427/2000... Training loss: 1.5303\n",
      "Epoch: 427/2000... Training loss: 1.3525\n",
      "Epoch: 427/2000... Training loss: 1.2526\n",
      "Epoch: 427/2000... Training loss: 1.1823\n",
      "Epoch: 427/2000... Training loss: 1.1849\n",
      "Epoch: 427/2000... Training loss: 1.2368\n",
      "Epoch: 427/2000... Training loss: 1.3077\n",
      "Epoch: 427/2000... Training loss: 1.6062\n",
      "Epoch: 427/2000... Training loss: 1.3834\n",
      "Epoch: 428/2000... Training loss: 1.3230\n",
      "Epoch: 428/2000... Training loss: 1.4209\n",
      "Epoch: 428/2000... Training loss: 1.4322\n",
      "Epoch: 428/2000... Training loss: 1.3364\n",
      "Epoch: 428/2000... Training loss: 1.1490\n",
      "Epoch: 428/2000... Training loss: 1.4143\n",
      "Epoch: 428/2000... Training loss: 1.0702\n",
      "Epoch: 428/2000... Training loss: 1.2514\n",
      "Epoch: 428/2000... Training loss: 1.6086\n",
      "Epoch: 428/2000... Training loss: 1.3316\n",
      "Epoch: 428/2000... Training loss: 1.3343\n",
      "Epoch: 428/2000... Training loss: 1.4360\n",
      "Epoch: 428/2000... Training loss: 1.3597\n",
      "Epoch: 428/2000... Training loss: 1.3571\n",
      "Epoch: 428/2000... Training loss: 1.5534\n",
      "Epoch: 428/2000... Training loss: 1.2218\n",
      "Epoch: 428/2000... Training loss: 1.7375\n",
      "Epoch: 428/2000... Training loss: 1.3749\n",
      "Epoch: 428/2000... Training loss: 1.3062\n",
      "Epoch: 428/2000... Training loss: 1.2544\n",
      "Epoch: 428/2000... Training loss: 1.3229\n",
      "Epoch: 428/2000... Training loss: 1.2158\n",
      "Epoch: 428/2000... Training loss: 1.4356\n",
      "Epoch: 428/2000... Training loss: 1.3858\n",
      "Epoch: 428/2000... Training loss: 1.3222\n",
      "Epoch: 428/2000... Training loss: 1.4224\n",
      "Epoch: 428/2000... Training loss: 1.3335\n",
      "Epoch: 428/2000... Training loss: 1.4821\n",
      "Epoch: 428/2000... Training loss: 1.4550\n",
      "Epoch: 428/2000... Training loss: 1.2006\n",
      "Epoch: 428/2000... Training loss: 1.4462\n",
      "Epoch: 429/2000... Training loss: 1.4182\n",
      "Epoch: 429/2000... Training loss: 1.6173\n",
      "Epoch: 429/2000... Training loss: 1.1819\n",
      "Epoch: 429/2000... Training loss: 1.1947\n",
      "Epoch: 429/2000... Training loss: 1.4104\n",
      "Epoch: 429/2000... Training loss: 1.3948\n",
      "Epoch: 429/2000... Training loss: 1.2579\n",
      "Epoch: 429/2000... Training loss: 1.6030\n",
      "Epoch: 429/2000... Training loss: 1.2555\n",
      "Epoch: 429/2000... Training loss: 1.4804\n",
      "Epoch: 429/2000... Training loss: 1.1690\n",
      "Epoch: 429/2000... Training loss: 1.1807\n",
      "Epoch: 429/2000... Training loss: 1.3960\n",
      "Epoch: 429/2000... Training loss: 1.6794\n",
      "Epoch: 429/2000... Training loss: 1.4432\n",
      "Epoch: 429/2000... Training loss: 1.2509\n",
      "Epoch: 429/2000... Training loss: 1.4544\n",
      "Epoch: 429/2000... Training loss: 1.2152\n",
      "Epoch: 429/2000... Training loss: 1.4822\n",
      "Epoch: 429/2000... Training loss: 1.4456\n",
      "Epoch: 429/2000... Training loss: 1.2963\n",
      "Epoch: 429/2000... Training loss: 1.3516\n",
      "Epoch: 429/2000... Training loss: 1.3821\n",
      "Epoch: 429/2000... Training loss: 1.2364\n",
      "Epoch: 429/2000... Training loss: 1.2713\n",
      "Epoch: 429/2000... Training loss: 1.4852\n",
      "Epoch: 429/2000... Training loss: 1.1887\n",
      "Epoch: 429/2000... Training loss: 1.2579\n",
      "Epoch: 429/2000... Training loss: 1.2713\n",
      "Epoch: 429/2000... Training loss: 1.1605\n",
      "Epoch: 429/2000... Training loss: 1.3007\n",
      "Epoch: 430/2000... Training loss: 1.2267\n",
      "Epoch: 430/2000... Training loss: 1.2270\n",
      "Epoch: 430/2000... Training loss: 1.2257\n",
      "Epoch: 430/2000... Training loss: 1.2863\n",
      "Epoch: 430/2000... Training loss: 1.1772\n",
      "Epoch: 430/2000... Training loss: 1.4747\n",
      "Epoch: 430/2000... Training loss: 1.3528\n",
      "Epoch: 430/2000... Training loss: 1.2244\n",
      "Epoch: 430/2000... Training loss: 1.2757\n",
      "Epoch: 430/2000... Training loss: 1.3002\n",
      "Epoch: 430/2000... Training loss: 1.3497\n",
      "Epoch: 430/2000... Training loss: 1.3113\n",
      "Epoch: 430/2000... Training loss: 1.3797\n",
      "Epoch: 430/2000... Training loss: 1.4661\n",
      "Epoch: 430/2000... Training loss: 1.2617\n",
      "Epoch: 430/2000... Training loss: 1.3209\n",
      "Epoch: 430/2000... Training loss: 1.1858\n",
      "Epoch: 430/2000... Training loss: 1.2719\n",
      "Epoch: 430/2000... Training loss: 1.4382\n",
      "Epoch: 430/2000... Training loss: 1.1599\n",
      "Epoch: 430/2000... Training loss: 1.3582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 430/2000... Training loss: 1.3211\n",
      "Epoch: 430/2000... Training loss: 1.3162\n",
      "Epoch: 430/2000... Training loss: 1.3044\n",
      "Epoch: 430/2000... Training loss: 1.0895\n",
      "Epoch: 430/2000... Training loss: 1.2958\n",
      "Epoch: 430/2000... Training loss: 1.3285\n",
      "Epoch: 430/2000... Training loss: 1.3754\n",
      "Epoch: 430/2000... Training loss: 1.2807\n",
      "Epoch: 430/2000... Training loss: 1.3802\n",
      "Epoch: 430/2000... Training loss: 1.4630\n",
      "Epoch: 431/2000... Training loss: 1.2514\n",
      "Epoch: 431/2000... Training loss: 0.9136\n",
      "Epoch: 431/2000... Training loss: 1.6584\n",
      "Epoch: 431/2000... Training loss: 0.9755\n",
      "Epoch: 431/2000... Training loss: 1.4552\n",
      "Epoch: 431/2000... Training loss: 1.2486\n",
      "Epoch: 431/2000... Training loss: 1.3999\n",
      "Epoch: 431/2000... Training loss: 1.3157\n",
      "Epoch: 431/2000... Training loss: 1.4133\n",
      "Epoch: 431/2000... Training loss: 1.3003\n",
      "Epoch: 431/2000... Training loss: 1.2326\n",
      "Epoch: 431/2000... Training loss: 1.3609\n",
      "Epoch: 431/2000... Training loss: 1.2177\n",
      "Epoch: 431/2000... Training loss: 1.3192\n",
      "Epoch: 431/2000... Training loss: 1.2579\n",
      "Epoch: 431/2000... Training loss: 1.3570\n",
      "Epoch: 431/2000... Training loss: 1.2573\n",
      "Epoch: 431/2000... Training loss: 1.1978\n",
      "Epoch: 431/2000... Training loss: 1.4725\n",
      "Epoch: 431/2000... Training loss: 1.3951\n",
      "Epoch: 431/2000... Training loss: 1.3410\n",
      "Epoch: 431/2000... Training loss: 1.3557\n",
      "Epoch: 431/2000... Training loss: 1.2742\n",
      "Epoch: 431/2000... Training loss: 1.4692\n",
      "Epoch: 431/2000... Training loss: 1.4580\n",
      "Epoch: 431/2000... Training loss: 1.2620\n",
      "Epoch: 431/2000... Training loss: 1.3089\n",
      "Epoch: 431/2000... Training loss: 1.2515\n",
      "Epoch: 431/2000... Training loss: 1.1480\n",
      "Epoch: 431/2000... Training loss: 1.4264\n",
      "Epoch: 431/2000... Training loss: 1.6408\n",
      "Epoch: 432/2000... Training loss: 1.1557\n",
      "Epoch: 432/2000... Training loss: 1.2784\n",
      "Epoch: 432/2000... Training loss: 1.3896\n",
      "Epoch: 432/2000... Training loss: 1.4470\n",
      "Epoch: 432/2000... Training loss: 1.3342\n",
      "Epoch: 432/2000... Training loss: 1.5472\n",
      "Epoch: 432/2000... Training loss: 1.4074\n",
      "Epoch: 432/2000... Training loss: 1.5222\n",
      "Epoch: 432/2000... Training loss: 1.2692\n",
      "Epoch: 432/2000... Training loss: 1.5565\n",
      "Epoch: 432/2000... Training loss: 1.1708\n",
      "Epoch: 432/2000... Training loss: 1.3856\n",
      "Epoch: 432/2000... Training loss: 1.3788\n",
      "Epoch: 432/2000... Training loss: 1.5012\n",
      "Epoch: 432/2000... Training loss: 1.4403\n",
      "Epoch: 432/2000... Training loss: 1.4498\n",
      "Epoch: 432/2000... Training loss: 1.7726\n",
      "Epoch: 432/2000... Training loss: 1.5613\n",
      "Epoch: 432/2000... Training loss: 1.3855\n",
      "Epoch: 432/2000... Training loss: 1.3143\n",
      "Epoch: 432/2000... Training loss: 1.6350\n",
      "Epoch: 432/2000... Training loss: 1.4597\n",
      "Epoch: 432/2000... Training loss: 1.3646\n",
      "Epoch: 432/2000... Training loss: 1.6975\n",
      "Epoch: 432/2000... Training loss: 1.4151\n",
      "Epoch: 432/2000... Training loss: 1.6309\n",
      "Epoch: 432/2000... Training loss: 1.2047\n",
      "Epoch: 432/2000... Training loss: 1.5182\n",
      "Epoch: 432/2000... Training loss: 1.4137\n",
      "Epoch: 432/2000... Training loss: 1.3783\n",
      "Epoch: 432/2000... Training loss: 1.2261\n",
      "Epoch: 433/2000... Training loss: 1.3403\n",
      "Epoch: 433/2000... Training loss: 1.5090\n",
      "Epoch: 433/2000... Training loss: 1.4382\n",
      "Epoch: 433/2000... Training loss: 1.4516\n",
      "Epoch: 433/2000... Training loss: 1.2353\n",
      "Epoch: 433/2000... Training loss: 1.3708\n",
      "Epoch: 433/2000... Training loss: 1.2392\n",
      "Epoch: 433/2000... Training loss: 1.4233\n",
      "Epoch: 433/2000... Training loss: 1.2639\n",
      "Epoch: 433/2000... Training loss: 1.6992\n",
      "Epoch: 433/2000... Training loss: 1.3689\n",
      "Epoch: 433/2000... Training loss: 1.3439\n",
      "Epoch: 433/2000... Training loss: 1.3122\n",
      "Epoch: 433/2000... Training loss: 1.4486\n",
      "Epoch: 433/2000... Training loss: 1.6815\n",
      "Epoch: 433/2000... Training loss: 1.4696\n",
      "Epoch: 433/2000... Training loss: 1.3451\n",
      "Epoch: 433/2000... Training loss: 1.0721\n",
      "Epoch: 433/2000... Training loss: 1.4489\n",
      "Epoch: 433/2000... Training loss: 1.2842\n",
      "Epoch: 433/2000... Training loss: 1.1772\n",
      "Epoch: 433/2000... Training loss: 1.3345\n",
      "Epoch: 433/2000... Training loss: 1.3426\n",
      "Epoch: 433/2000... Training loss: 1.3872\n",
      "Epoch: 433/2000... Training loss: 1.2555\n",
      "Epoch: 433/2000... Training loss: 1.3940\n",
      "Epoch: 433/2000... Training loss: 1.3720\n",
      "Epoch: 433/2000... Training loss: 1.3470\n",
      "Epoch: 433/2000... Training loss: 1.1696\n",
      "Epoch: 433/2000... Training loss: 1.3927\n",
      "Epoch: 433/2000... Training loss: 1.6224\n",
      "Epoch: 434/2000... Training loss: 1.3428\n",
      "Epoch: 434/2000... Training loss: 1.2174\n",
      "Epoch: 434/2000... Training loss: 1.4235\n",
      "Epoch: 434/2000... Training loss: 1.3004\n",
      "Epoch: 434/2000... Training loss: 1.2049\n",
      "Epoch: 434/2000... Training loss: 1.3733\n",
      "Epoch: 434/2000... Training loss: 1.2341\n",
      "Epoch: 434/2000... Training loss: 1.1547\n",
      "Epoch: 434/2000... Training loss: 1.3235\n",
      "Epoch: 434/2000... Training loss: 1.3353\n",
      "Epoch: 434/2000... Training loss: 1.5255\n",
      "Epoch: 434/2000... Training loss: 1.4413\n",
      "Epoch: 434/2000... Training loss: 1.1570\n",
      "Epoch: 434/2000... Training loss: 1.2151\n",
      "Epoch: 434/2000... Training loss: 1.3586\n",
      "Epoch: 434/2000... Training loss: 1.3905\n",
      "Epoch: 434/2000... Training loss: 1.3808\n",
      "Epoch: 434/2000... Training loss: 1.2601\n",
      "Epoch: 434/2000... Training loss: 1.1407\n",
      "Epoch: 434/2000... Training loss: 1.3021\n",
      "Epoch: 434/2000... Training loss: 1.3949\n",
      "Epoch: 434/2000... Training loss: 1.3383\n",
      "Epoch: 434/2000... Training loss: 1.2730\n",
      "Epoch: 434/2000... Training loss: 1.3143\n",
      "Epoch: 434/2000... Training loss: 1.4254\n",
      "Epoch: 434/2000... Training loss: 1.2381\n",
      "Epoch: 434/2000... Training loss: 1.3149\n",
      "Epoch: 434/2000... Training loss: 1.5391\n",
      "Epoch: 434/2000... Training loss: 1.0636\n",
      "Epoch: 434/2000... Training loss: 1.2297\n",
      "Epoch: 434/2000... Training loss: 1.1608\n",
      "Epoch: 435/2000... Training loss: 1.3917\n",
      "Epoch: 435/2000... Training loss: 1.4370\n",
      "Epoch: 435/2000... Training loss: 1.2866\n",
      "Epoch: 435/2000... Training loss: 1.3970\n",
      "Epoch: 435/2000... Training loss: 1.1994\n",
      "Epoch: 435/2000... Training loss: 1.1765\n",
      "Epoch: 435/2000... Training loss: 1.1251\n",
      "Epoch: 435/2000... Training loss: 1.4960\n",
      "Epoch: 435/2000... Training loss: 1.3437\n",
      "Epoch: 435/2000... Training loss: 1.4301\n",
      "Epoch: 435/2000... Training loss: 1.1132\n",
      "Epoch: 435/2000... Training loss: 1.1517\n",
      "Epoch: 435/2000... Training loss: 1.4678\n",
      "Epoch: 435/2000... Training loss: 1.3257\n",
      "Epoch: 435/2000... Training loss: 1.2976\n",
      "Epoch: 435/2000... Training loss: 1.2792\n",
      "Epoch: 435/2000... Training loss: 1.3595\n",
      "Epoch: 435/2000... Training loss: 1.3517\n",
      "Epoch: 435/2000... Training loss: 1.1616\n",
      "Epoch: 435/2000... Training loss: 1.2760\n",
      "Epoch: 435/2000... Training loss: 1.2983\n",
      "Epoch: 435/2000... Training loss: 1.1917\n",
      "Epoch: 435/2000... Training loss: 1.2763\n",
      "Epoch: 435/2000... Training loss: 1.4217\n",
      "Epoch: 435/2000... Training loss: 1.2956\n",
      "Epoch: 435/2000... Training loss: 1.1682\n",
      "Epoch: 435/2000... Training loss: 1.3112\n",
      "Epoch: 435/2000... Training loss: 1.2338\n",
      "Epoch: 435/2000... Training loss: 1.5787\n",
      "Epoch: 435/2000... Training loss: 1.4584\n",
      "Epoch: 435/2000... Training loss: 1.2847\n",
      "Epoch: 436/2000... Training loss: 1.2386\n",
      "Epoch: 436/2000... Training loss: 1.1731\n",
      "Epoch: 436/2000... Training loss: 1.2867\n",
      "Epoch: 436/2000... Training loss: 1.2941\n",
      "Epoch: 436/2000... Training loss: 1.2300\n",
      "Epoch: 436/2000... Training loss: 1.4062\n",
      "Epoch: 436/2000... Training loss: 1.0626\n",
      "Epoch: 436/2000... Training loss: 1.2873\n",
      "Epoch: 436/2000... Training loss: 1.4827\n",
      "Epoch: 436/2000... Training loss: 1.4562\n",
      "Epoch: 436/2000... Training loss: 1.4313\n",
      "Epoch: 436/2000... Training loss: 1.1763\n",
      "Epoch: 436/2000... Training loss: 1.4756\n",
      "Epoch: 436/2000... Training loss: 1.0289\n",
      "Epoch: 436/2000... Training loss: 1.3450\n",
      "Epoch: 436/2000... Training loss: 1.2433\n",
      "Epoch: 436/2000... Training loss: 1.4355\n",
      "Epoch: 436/2000... Training loss: 1.6488\n",
      "Epoch: 436/2000... Training loss: 1.4014\n",
      "Epoch: 436/2000... Training loss: 1.3183\n",
      "Epoch: 436/2000... Training loss: 1.3538\n",
      "Epoch: 436/2000... Training loss: 1.5172\n",
      "Epoch: 436/2000... Training loss: 1.3093\n",
      "Epoch: 436/2000... Training loss: 1.3444\n",
      "Epoch: 436/2000... Training loss: 1.4811\n",
      "Epoch: 436/2000... Training loss: 1.2121\n",
      "Epoch: 436/2000... Training loss: 1.5056\n",
      "Epoch: 436/2000... Training loss: 1.4950\n",
      "Epoch: 436/2000... Training loss: 1.4046\n",
      "Epoch: 436/2000... Training loss: 1.2521\n",
      "Epoch: 436/2000... Training loss: 1.3038\n",
      "Epoch: 437/2000... Training loss: 1.5782\n",
      "Epoch: 437/2000... Training loss: 1.5845\n",
      "Epoch: 437/2000... Training loss: 1.4879\n",
      "Epoch: 437/2000... Training loss: 1.3402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 437/2000... Training loss: 1.1278\n",
      "Epoch: 437/2000... Training loss: 1.2396\n",
      "Epoch: 437/2000... Training loss: 1.1421\n",
      "Epoch: 437/2000... Training loss: 1.4454\n",
      "Epoch: 437/2000... Training loss: 1.0656\n",
      "Epoch: 437/2000... Training loss: 1.4186\n",
      "Epoch: 437/2000... Training loss: 1.3158\n",
      "Epoch: 437/2000... Training loss: 1.3220\n",
      "Epoch: 437/2000... Training loss: 1.3133\n",
      "Epoch: 437/2000... Training loss: 1.3785\n",
      "Epoch: 437/2000... Training loss: 1.1621\n",
      "Epoch: 437/2000... Training loss: 1.3564\n",
      "Epoch: 437/2000... Training loss: 1.3347\n",
      "Epoch: 437/2000... Training loss: 1.5609\n",
      "Epoch: 437/2000... Training loss: 1.1016\n",
      "Epoch: 437/2000... Training loss: 1.0889\n",
      "Epoch: 437/2000... Training loss: 1.6206\n",
      "Epoch: 437/2000... Training loss: 1.2327\n",
      "Epoch: 437/2000... Training loss: 1.3646\n",
      "Epoch: 437/2000... Training loss: 1.4526\n",
      "Epoch: 437/2000... Training loss: 1.1067\n",
      "Epoch: 437/2000... Training loss: 1.5455\n",
      "Epoch: 437/2000... Training loss: 1.3669\n",
      "Epoch: 437/2000... Training loss: 1.2302\n",
      "Epoch: 437/2000... Training loss: 1.3221\n",
      "Epoch: 437/2000... Training loss: 1.4686\n",
      "Epoch: 437/2000... Training loss: 1.3897\n",
      "Epoch: 438/2000... Training loss: 1.4012\n",
      "Epoch: 438/2000... Training loss: 1.2777\n",
      "Epoch: 438/2000... Training loss: 1.4739\n",
      "Epoch: 438/2000... Training loss: 1.4445\n",
      "Epoch: 438/2000... Training loss: 1.2768\n",
      "Epoch: 438/2000... Training loss: 1.2948\n",
      "Epoch: 438/2000... Training loss: 1.2338\n",
      "Epoch: 438/2000... Training loss: 1.5068\n",
      "Epoch: 438/2000... Training loss: 1.4459\n",
      "Epoch: 438/2000... Training loss: 1.3736\n",
      "Epoch: 438/2000... Training loss: 1.2802\n",
      "Epoch: 438/2000... Training loss: 1.1784\n",
      "Epoch: 438/2000... Training loss: 1.2009\n",
      "Epoch: 438/2000... Training loss: 1.3338\n",
      "Epoch: 438/2000... Training loss: 1.3130\n",
      "Epoch: 438/2000... Training loss: 1.3204\n",
      "Epoch: 438/2000... Training loss: 1.4758\n",
      "Epoch: 438/2000... Training loss: 1.4992\n",
      "Epoch: 438/2000... Training loss: 1.4860\n",
      "Epoch: 438/2000... Training loss: 1.3979\n",
      "Epoch: 438/2000... Training loss: 1.1271\n",
      "Epoch: 438/2000... Training loss: 1.0634\n",
      "Epoch: 438/2000... Training loss: 1.5100\n",
      "Epoch: 438/2000... Training loss: 1.0745\n",
      "Epoch: 438/2000... Training loss: 1.6099\n",
      "Epoch: 438/2000... Training loss: 1.4230\n",
      "Epoch: 438/2000... Training loss: 1.0253\n",
      "Epoch: 438/2000... Training loss: 1.1663\n",
      "Epoch: 438/2000... Training loss: 1.2486\n",
      "Epoch: 438/2000... Training loss: 1.1786\n",
      "Epoch: 438/2000... Training loss: 1.4338\n",
      "Epoch: 439/2000... Training loss: 1.3428\n",
      "Epoch: 439/2000... Training loss: 1.3292\n",
      "Epoch: 439/2000... Training loss: 1.1626\n",
      "Epoch: 439/2000... Training loss: 1.3366\n",
      "Epoch: 439/2000... Training loss: 1.4476\n",
      "Epoch: 439/2000... Training loss: 1.3402\n",
      "Epoch: 439/2000... Training loss: 1.2212\n",
      "Epoch: 439/2000... Training loss: 1.2321\n",
      "Epoch: 439/2000... Training loss: 1.3207\n",
      "Epoch: 439/2000... Training loss: 1.2588\n",
      "Epoch: 439/2000... Training loss: 1.2113\n",
      "Epoch: 439/2000... Training loss: 1.1807\n",
      "Epoch: 439/2000... Training loss: 1.4923\n",
      "Epoch: 439/2000... Training loss: 1.4307\n",
      "Epoch: 439/2000... Training loss: 1.0991\n",
      "Epoch: 439/2000... Training loss: 1.2483\n",
      "Epoch: 439/2000... Training loss: 1.3433\n",
      "Epoch: 439/2000... Training loss: 1.4307\n",
      "Epoch: 439/2000... Training loss: 1.3172\n",
      "Epoch: 439/2000... Training loss: 1.3773\n",
      "Epoch: 439/2000... Training loss: 1.5290\n",
      "Epoch: 439/2000... Training loss: 1.3255\n",
      "Epoch: 439/2000... Training loss: 1.3858\n",
      "Epoch: 439/2000... Training loss: 1.3028\n",
      "Epoch: 439/2000... Training loss: 1.4224\n",
      "Epoch: 439/2000... Training loss: 1.4629\n",
      "Epoch: 439/2000... Training loss: 1.3519\n",
      "Epoch: 439/2000... Training loss: 1.4976\n",
      "Epoch: 439/2000... Training loss: 1.4631\n",
      "Epoch: 439/2000... Training loss: 1.2085\n",
      "Epoch: 439/2000... Training loss: 1.4811\n",
      "Epoch: 440/2000... Training loss: 0.9812\n",
      "Epoch: 440/2000... Training loss: 1.1980\n",
      "Epoch: 440/2000... Training loss: 1.4475\n",
      "Epoch: 440/2000... Training loss: 1.1505\n",
      "Epoch: 440/2000... Training loss: 1.4606\n",
      "Epoch: 440/2000... Training loss: 1.4852\n",
      "Epoch: 440/2000... Training loss: 1.2867\n",
      "Epoch: 440/2000... Training loss: 1.3508\n",
      "Epoch: 440/2000... Training loss: 1.3094\n",
      "Epoch: 440/2000... Training loss: 1.5863\n",
      "Epoch: 440/2000... Training loss: 1.4498\n",
      "Epoch: 440/2000... Training loss: 1.6737\n",
      "Epoch: 440/2000... Training loss: 1.1899\n",
      "Epoch: 440/2000... Training loss: 1.0639\n",
      "Epoch: 440/2000... Training loss: 1.0865\n",
      "Epoch: 440/2000... Training loss: 1.3124\n",
      "Epoch: 440/2000... Training loss: 1.2506\n",
      "Epoch: 440/2000... Training loss: 1.5266\n",
      "Epoch: 440/2000... Training loss: 1.4332\n",
      "Epoch: 440/2000... Training loss: 1.3389\n",
      "Epoch: 440/2000... Training loss: 1.3022\n",
      "Epoch: 440/2000... Training loss: 1.2863\n",
      "Epoch: 440/2000... Training loss: 1.2767\n",
      "Epoch: 440/2000... Training loss: 1.3418\n",
      "Epoch: 440/2000... Training loss: 1.3279\n",
      "Epoch: 440/2000... Training loss: 1.1571\n",
      "Epoch: 440/2000... Training loss: 1.1909\n",
      "Epoch: 440/2000... Training loss: 1.2596\n",
      "Epoch: 440/2000... Training loss: 1.4174\n",
      "Epoch: 440/2000... Training loss: 1.3261\n",
      "Epoch: 440/2000... Training loss: 1.4380\n",
      "Epoch: 441/2000... Training loss: 1.2187\n",
      "Epoch: 441/2000... Training loss: 1.3057\n",
      "Epoch: 441/2000... Training loss: 1.3331\n",
      "Epoch: 441/2000... Training loss: 1.3480\n",
      "Epoch: 441/2000... Training loss: 1.4349\n",
      "Epoch: 441/2000... Training loss: 1.3802\n",
      "Epoch: 441/2000... Training loss: 1.2199\n",
      "Epoch: 441/2000... Training loss: 1.1346\n",
      "Epoch: 441/2000... Training loss: 1.3742\n",
      "Epoch: 441/2000... Training loss: 1.2616\n",
      "Epoch: 441/2000... Training loss: 1.4237\n",
      "Epoch: 441/2000... Training loss: 1.1859\n",
      "Epoch: 441/2000... Training loss: 1.4588\n",
      "Epoch: 441/2000... Training loss: 1.2439\n",
      "Epoch: 441/2000... Training loss: 1.4331\n",
      "Epoch: 441/2000... Training loss: 1.2189\n",
      "Epoch: 441/2000... Training loss: 1.3671\n",
      "Epoch: 441/2000... Training loss: 1.0254\n",
      "Epoch: 441/2000... Training loss: 1.2048\n",
      "Epoch: 441/2000... Training loss: 1.2374\n",
      "Epoch: 441/2000... Training loss: 1.2474\n",
      "Epoch: 441/2000... Training loss: 1.2982\n",
      "Epoch: 441/2000... Training loss: 1.5500\n",
      "Epoch: 441/2000... Training loss: 1.2963\n",
      "Epoch: 441/2000... Training loss: 1.3691\n",
      "Epoch: 441/2000... Training loss: 1.3278\n",
      "Epoch: 441/2000... Training loss: 1.2176\n",
      "Epoch: 441/2000... Training loss: 1.4790\n",
      "Epoch: 441/2000... Training loss: 1.2944\n",
      "Epoch: 441/2000... Training loss: 1.1789\n",
      "Epoch: 441/2000... Training loss: 1.5397\n",
      "Epoch: 442/2000... Training loss: 1.4759\n",
      "Epoch: 442/2000... Training loss: 1.3951\n",
      "Epoch: 442/2000... Training loss: 1.2447\n",
      "Epoch: 442/2000... Training loss: 1.4312\n",
      "Epoch: 442/2000... Training loss: 1.2920\n",
      "Epoch: 442/2000... Training loss: 1.1669\n",
      "Epoch: 442/2000... Training loss: 1.2617\n",
      "Epoch: 442/2000... Training loss: 1.0374\n",
      "Epoch: 442/2000... Training loss: 1.0696\n",
      "Epoch: 442/2000... Training loss: 1.3717\n",
      "Epoch: 442/2000... Training loss: 1.3479\n",
      "Epoch: 442/2000... Training loss: 1.3833\n",
      "Epoch: 442/2000... Training loss: 1.3379\n",
      "Epoch: 442/2000... Training loss: 1.3009\n",
      "Epoch: 442/2000... Training loss: 1.5405\n",
      "Epoch: 442/2000... Training loss: 1.1241\n",
      "Epoch: 442/2000... Training loss: 1.4548\n",
      "Epoch: 442/2000... Training loss: 1.3643\n",
      "Epoch: 442/2000... Training loss: 1.3583\n",
      "Epoch: 442/2000... Training loss: 1.3143\n",
      "Epoch: 442/2000... Training loss: 1.2891\n",
      "Epoch: 442/2000... Training loss: 1.5416\n",
      "Epoch: 442/2000... Training loss: 1.3415\n",
      "Epoch: 442/2000... Training loss: 1.3511\n",
      "Epoch: 442/2000... Training loss: 1.4241\n",
      "Epoch: 442/2000... Training loss: 1.6072\n",
      "Epoch: 442/2000... Training loss: 1.3782\n",
      "Epoch: 442/2000... Training loss: 1.1592\n",
      "Epoch: 442/2000... Training loss: 1.2918\n",
      "Epoch: 442/2000... Training loss: 1.4300\n",
      "Epoch: 442/2000... Training loss: 1.1651\n",
      "Epoch: 443/2000... Training loss: 1.2928\n",
      "Epoch: 443/2000... Training loss: 1.2043\n",
      "Epoch: 443/2000... Training loss: 1.3751\n",
      "Epoch: 443/2000... Training loss: 1.1816\n",
      "Epoch: 443/2000... Training loss: 1.2957\n",
      "Epoch: 443/2000... Training loss: 1.3158\n",
      "Epoch: 443/2000... Training loss: 1.4132\n",
      "Epoch: 443/2000... Training loss: 1.1762\n",
      "Epoch: 443/2000... Training loss: 1.2803\n",
      "Epoch: 443/2000... Training loss: 1.4317\n",
      "Epoch: 443/2000... Training loss: 1.3127\n",
      "Epoch: 443/2000... Training loss: 1.2595\n",
      "Epoch: 443/2000... Training loss: 1.1445\n",
      "Epoch: 443/2000... Training loss: 1.1874\n",
      "Epoch: 443/2000... Training loss: 1.3569\n",
      "Epoch: 443/2000... Training loss: 1.2029\n",
      "Epoch: 443/2000... Training loss: 1.4423\n",
      "Epoch: 443/2000... Training loss: 1.2306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 443/2000... Training loss: 1.2859\n",
      "Epoch: 443/2000... Training loss: 1.4220\n",
      "Epoch: 443/2000... Training loss: 1.2572\n",
      "Epoch: 443/2000... Training loss: 1.2742\n",
      "Epoch: 443/2000... Training loss: 1.3347\n",
      "Epoch: 443/2000... Training loss: 1.2145\n",
      "Epoch: 443/2000... Training loss: 1.1944\n",
      "Epoch: 443/2000... Training loss: 1.6727\n",
      "Epoch: 443/2000... Training loss: 1.1974\n",
      "Epoch: 443/2000... Training loss: 1.3704\n",
      "Epoch: 443/2000... Training loss: 1.2185\n",
      "Epoch: 443/2000... Training loss: 1.2512\n",
      "Epoch: 443/2000... Training loss: 1.1282\n",
      "Epoch: 444/2000... Training loss: 1.2633\n",
      "Epoch: 444/2000... Training loss: 1.4310\n",
      "Epoch: 444/2000... Training loss: 1.4315\n",
      "Epoch: 444/2000... Training loss: 1.3590\n",
      "Epoch: 444/2000... Training loss: 1.3890\n",
      "Epoch: 444/2000... Training loss: 1.4438\n",
      "Epoch: 444/2000... Training loss: 1.4035\n",
      "Epoch: 444/2000... Training loss: 1.3844\n",
      "Epoch: 444/2000... Training loss: 1.3842\n",
      "Epoch: 444/2000... Training loss: 1.5300\n",
      "Epoch: 444/2000... Training loss: 1.2066\n",
      "Epoch: 444/2000... Training loss: 1.1678\n",
      "Epoch: 444/2000... Training loss: 1.1219\n",
      "Epoch: 444/2000... Training loss: 1.2422\n",
      "Epoch: 444/2000... Training loss: 1.4389\n",
      "Epoch: 444/2000... Training loss: 1.1880\n",
      "Epoch: 444/2000... Training loss: 1.1611\n",
      "Epoch: 444/2000... Training loss: 1.2634\n",
      "Epoch: 444/2000... Training loss: 1.2758\n",
      "Epoch: 444/2000... Training loss: 1.1807\n",
      "Epoch: 444/2000... Training loss: 1.3607\n",
      "Epoch: 444/2000... Training loss: 1.1939\n",
      "Epoch: 444/2000... Training loss: 1.3831\n",
      "Epoch: 444/2000... Training loss: 1.3103\n",
      "Epoch: 444/2000... Training loss: 1.1824\n",
      "Epoch: 444/2000... Training loss: 1.3970\n",
      "Epoch: 444/2000... Training loss: 1.2378\n",
      "Epoch: 444/2000... Training loss: 1.3054\n",
      "Epoch: 444/2000... Training loss: 1.2345\n",
      "Epoch: 444/2000... Training loss: 1.3747\n",
      "Epoch: 444/2000... Training loss: 1.2477\n",
      "Epoch: 445/2000... Training loss: 1.3529\n",
      "Epoch: 445/2000... Training loss: 1.1887\n",
      "Epoch: 445/2000... Training loss: 1.2007\n",
      "Epoch: 445/2000... Training loss: 1.4420\n",
      "Epoch: 445/2000... Training loss: 1.3163\n",
      "Epoch: 445/2000... Training loss: 1.2722\n",
      "Epoch: 445/2000... Training loss: 1.2843\n",
      "Epoch: 445/2000... Training loss: 1.3275\n",
      "Epoch: 445/2000... Training loss: 1.4420\n",
      "Epoch: 445/2000... Training loss: 1.4226\n",
      "Epoch: 445/2000... Training loss: 1.3930\n",
      "Epoch: 445/2000... Training loss: 1.2124\n",
      "Epoch: 445/2000... Training loss: 1.2177\n",
      "Epoch: 445/2000... Training loss: 1.2453\n",
      "Epoch: 445/2000... Training loss: 1.4372\n",
      "Epoch: 445/2000... Training loss: 1.3425\n",
      "Epoch: 445/2000... Training loss: 1.1745\n",
      "Epoch: 445/2000... Training loss: 1.0908\n",
      "Epoch: 445/2000... Training loss: 1.1263\n",
      "Epoch: 445/2000... Training loss: 1.3333\n",
      "Epoch: 445/2000... Training loss: 1.4131\n",
      "Epoch: 445/2000... Training loss: 0.9203\n",
      "Epoch: 445/2000... Training loss: 1.4581\n",
      "Epoch: 445/2000... Training loss: 1.2071\n",
      "Epoch: 445/2000... Training loss: 1.1177\n",
      "Epoch: 445/2000... Training loss: 1.3601\n",
      "Epoch: 445/2000... Training loss: 1.2878\n",
      "Epoch: 445/2000... Training loss: 1.2470\n",
      "Epoch: 445/2000... Training loss: 1.3545\n",
      "Epoch: 445/2000... Training loss: 1.3198\n",
      "Epoch: 445/2000... Training loss: 1.2939\n",
      "Epoch: 446/2000... Training loss: 1.2814\n",
      "Epoch: 446/2000... Training loss: 1.1770\n",
      "Epoch: 446/2000... Training loss: 1.4019\n",
      "Epoch: 446/2000... Training loss: 1.2832\n",
      "Epoch: 446/2000... Training loss: 1.4277\n",
      "Epoch: 446/2000... Training loss: 1.1692\n",
      "Epoch: 446/2000... Training loss: 1.1654\n",
      "Epoch: 446/2000... Training loss: 0.9151\n",
      "Epoch: 446/2000... Training loss: 1.4603\n",
      "Epoch: 446/2000... Training loss: 1.3782\n",
      "Epoch: 446/2000... Training loss: 1.0903\n",
      "Epoch: 446/2000... Training loss: 1.2290\n",
      "Epoch: 446/2000... Training loss: 1.3738\n",
      "Epoch: 446/2000... Training loss: 1.4524\n",
      "Epoch: 446/2000... Training loss: 1.1752\n",
      "Epoch: 446/2000... Training loss: 1.2589\n",
      "Epoch: 446/2000... Training loss: 1.2674\n",
      "Epoch: 446/2000... Training loss: 1.1432\n",
      "Epoch: 446/2000... Training loss: 1.2143\n",
      "Epoch: 446/2000... Training loss: 1.6893\n",
      "Epoch: 446/2000... Training loss: 1.1445\n",
      "Epoch: 446/2000... Training loss: 0.9947\n",
      "Epoch: 446/2000... Training loss: 0.9987\n",
      "Epoch: 446/2000... Training loss: 1.5187\n",
      "Epoch: 446/2000... Training loss: 1.1395\n",
      "Epoch: 446/2000... Training loss: 1.2957\n",
      "Epoch: 446/2000... Training loss: 1.2452\n",
      "Epoch: 446/2000... Training loss: 1.4210\n",
      "Epoch: 446/2000... Training loss: 1.2883\n",
      "Epoch: 446/2000... Training loss: 1.1583\n",
      "Epoch: 446/2000... Training loss: 1.2982\n",
      "Epoch: 447/2000... Training loss: 1.1304\n",
      "Epoch: 447/2000... Training loss: 1.5486\n",
      "Epoch: 447/2000... Training loss: 1.1332\n",
      "Epoch: 447/2000... Training loss: 1.3937\n",
      "Epoch: 447/2000... Training loss: 1.1022\n",
      "Epoch: 447/2000... Training loss: 1.2023\n",
      "Epoch: 447/2000... Training loss: 1.2143\n",
      "Epoch: 447/2000... Training loss: 1.1856\n",
      "Epoch: 447/2000... Training loss: 1.0633\n",
      "Epoch: 447/2000... Training loss: 1.4078\n",
      "Epoch: 447/2000... Training loss: 1.3767\n",
      "Epoch: 447/2000... Training loss: 1.3919\n",
      "Epoch: 447/2000... Training loss: 1.2263\n",
      "Epoch: 447/2000... Training loss: 1.0809\n",
      "Epoch: 447/2000... Training loss: 1.1594\n",
      "Epoch: 447/2000... Training loss: 1.1760\n",
      "Epoch: 447/2000... Training loss: 1.5050\n",
      "Epoch: 447/2000... Training loss: 1.3324\n",
      "Epoch: 447/2000... Training loss: 1.3153\n",
      "Epoch: 447/2000... Training loss: 1.4317\n",
      "Epoch: 447/2000... Training loss: 1.1918\n",
      "Epoch: 447/2000... Training loss: 0.9645\n",
      "Epoch: 447/2000... Training loss: 1.3315\n",
      "Epoch: 447/2000... Training loss: 1.0399\n",
      "Epoch: 447/2000... Training loss: 1.2922\n",
      "Epoch: 447/2000... Training loss: 1.3154\n",
      "Epoch: 447/2000... Training loss: 1.2534\n",
      "Epoch: 447/2000... Training loss: 1.2643\n",
      "Epoch: 447/2000... Training loss: 1.2624\n",
      "Epoch: 447/2000... Training loss: 1.3288\n",
      "Epoch: 447/2000... Training loss: 1.2920\n",
      "Epoch: 448/2000... Training loss: 1.2406\n",
      "Epoch: 448/2000... Training loss: 1.2400\n",
      "Epoch: 448/2000... Training loss: 1.1992\n",
      "Epoch: 448/2000... Training loss: 1.2228\n",
      "Epoch: 448/2000... Training loss: 1.2608\n",
      "Epoch: 448/2000... Training loss: 1.2453\n",
      "Epoch: 448/2000... Training loss: 1.2271\n",
      "Epoch: 448/2000... Training loss: 1.1938\n",
      "Epoch: 448/2000... Training loss: 0.9691\n",
      "Epoch: 448/2000... Training loss: 1.1718\n",
      "Epoch: 448/2000... Training loss: 1.3046\n",
      "Epoch: 448/2000... Training loss: 1.1240\n",
      "Epoch: 448/2000... Training loss: 1.3214\n",
      "Epoch: 448/2000... Training loss: 1.1370\n",
      "Epoch: 448/2000... Training loss: 1.2680\n",
      "Epoch: 448/2000... Training loss: 1.1794\n",
      "Epoch: 448/2000... Training loss: 1.2690\n",
      "Epoch: 448/2000... Training loss: 1.0876\n",
      "Epoch: 448/2000... Training loss: 1.0628\n",
      "Epoch: 448/2000... Training loss: 1.2226\n",
      "Epoch: 448/2000... Training loss: 1.1576\n",
      "Epoch: 448/2000... Training loss: 1.1885\n",
      "Epoch: 448/2000... Training loss: 1.2036\n",
      "Epoch: 448/2000... Training loss: 1.4681\n",
      "Epoch: 448/2000... Training loss: 1.5078\n",
      "Epoch: 448/2000... Training loss: 1.2748\n",
      "Epoch: 448/2000... Training loss: 1.2054\n",
      "Epoch: 448/2000... Training loss: 1.1458\n",
      "Epoch: 448/2000... Training loss: 1.2778\n",
      "Epoch: 448/2000... Training loss: 1.0678\n",
      "Epoch: 448/2000... Training loss: 1.5346\n",
      "Epoch: 449/2000... Training loss: 1.1702\n",
      "Epoch: 449/2000... Training loss: 1.0833\n",
      "Epoch: 449/2000... Training loss: 1.4519\n",
      "Epoch: 449/2000... Training loss: 1.1704\n",
      "Epoch: 449/2000... Training loss: 1.3179\n",
      "Epoch: 449/2000... Training loss: 1.2831\n",
      "Epoch: 449/2000... Training loss: 1.3178\n",
      "Epoch: 449/2000... Training loss: 1.3522\n",
      "Epoch: 449/2000... Training loss: 1.3755\n",
      "Epoch: 449/2000... Training loss: 1.1732\n",
      "Epoch: 449/2000... Training loss: 1.2981\n",
      "Epoch: 449/2000... Training loss: 1.2225\n",
      "Epoch: 449/2000... Training loss: 1.2197\n",
      "Epoch: 449/2000... Training loss: 1.3300\n",
      "Epoch: 449/2000... Training loss: 1.2737\n",
      "Epoch: 449/2000... Training loss: 1.3149\n",
      "Epoch: 449/2000... Training loss: 1.3520\n",
      "Epoch: 449/2000... Training loss: 1.2733\n",
      "Epoch: 449/2000... Training loss: 1.2030\n",
      "Epoch: 449/2000... Training loss: 1.2004\n",
      "Epoch: 449/2000... Training loss: 1.5548\n",
      "Epoch: 449/2000... Training loss: 1.3061\n",
      "Epoch: 449/2000... Training loss: 1.2811\n",
      "Epoch: 449/2000... Training loss: 1.3419\n",
      "Epoch: 449/2000... Training loss: 1.1382\n",
      "Epoch: 449/2000... Training loss: 1.3804\n",
      "Epoch: 449/2000... Training loss: 1.0890\n",
      "Epoch: 449/2000... Training loss: 1.4041\n",
      "Epoch: 449/2000... Training loss: 1.0838\n",
      "Epoch: 449/2000... Training loss: 1.5422\n",
      "Epoch: 449/2000... Training loss: 1.1694\n",
      "Epoch: 450/2000... Training loss: 1.5791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 450/2000... Training loss: 1.3291\n",
      "Epoch: 450/2000... Training loss: 1.2755\n",
      "Epoch: 450/2000... Training loss: 1.3828\n",
      "Epoch: 450/2000... Training loss: 1.2539\n",
      "Epoch: 450/2000... Training loss: 1.4836\n",
      "Epoch: 450/2000... Training loss: 1.2299\n",
      "Epoch: 450/2000... Training loss: 1.1727\n",
      "Epoch: 450/2000... Training loss: 1.3705\n",
      "Epoch: 450/2000... Training loss: 1.4725\n",
      "Epoch: 450/2000... Training loss: 1.1372\n",
      "Epoch: 450/2000... Training loss: 1.2150\n",
      "Epoch: 450/2000... Training loss: 1.4432\n",
      "Epoch: 450/2000... Training loss: 1.4865\n",
      "Epoch: 450/2000... Training loss: 1.1269\n",
      "Epoch: 450/2000... Training loss: 1.0682\n",
      "Epoch: 450/2000... Training loss: 1.2149\n",
      "Epoch: 450/2000... Training loss: 1.1776\n",
      "Epoch: 450/2000... Training loss: 1.3234\n",
      "Epoch: 450/2000... Training loss: 1.3029\n",
      "Epoch: 450/2000... Training loss: 1.1352\n",
      "Epoch: 450/2000... Training loss: 1.1246\n",
      "Epoch: 450/2000... Training loss: 1.1575\n",
      "Epoch: 450/2000... Training loss: 1.3586\n",
      "Epoch: 450/2000... Training loss: 1.1776\n",
      "Epoch: 450/2000... Training loss: 1.3213\n",
      "Epoch: 450/2000... Training loss: 1.4064\n",
      "Epoch: 450/2000... Training loss: 1.1901\n",
      "Epoch: 450/2000... Training loss: 1.2818\n",
      "Epoch: 450/2000... Training loss: 1.2652\n",
      "Epoch: 450/2000... Training loss: 1.1510\n",
      "Epoch: 451/2000... Training loss: 1.4083\n",
      "Epoch: 451/2000... Training loss: 1.0119\n",
      "Epoch: 451/2000... Training loss: 1.3128\n",
      "Epoch: 451/2000... Training loss: 1.1824\n",
      "Epoch: 451/2000... Training loss: 1.0339\n",
      "Epoch: 451/2000... Training loss: 1.3077\n",
      "Epoch: 451/2000... Training loss: 1.2344\n",
      "Epoch: 451/2000... Training loss: 1.1410\n",
      "Epoch: 451/2000... Training loss: 1.1680\n",
      "Epoch: 451/2000... Training loss: 1.2238\n",
      "Epoch: 451/2000... Training loss: 1.3367\n",
      "Epoch: 451/2000... Training loss: 1.0199\n",
      "Epoch: 451/2000... Training loss: 1.3754\n",
      "Epoch: 451/2000... Training loss: 1.4289\n",
      "Epoch: 451/2000... Training loss: 1.0829\n",
      "Epoch: 451/2000... Training loss: 1.2311\n",
      "Epoch: 451/2000... Training loss: 1.5833\n",
      "Epoch: 451/2000... Training loss: 1.0492\n",
      "Epoch: 451/2000... Training loss: 1.2951\n",
      "Epoch: 451/2000... Training loss: 1.1519\n",
      "Epoch: 451/2000... Training loss: 1.1621\n",
      "Epoch: 451/2000... Training loss: 1.2876\n",
      "Epoch: 451/2000... Training loss: 1.3219\n",
      "Epoch: 451/2000... Training loss: 1.1571\n",
      "Epoch: 451/2000... Training loss: 1.0595\n",
      "Epoch: 451/2000... Training loss: 1.1695\n",
      "Epoch: 451/2000... Training loss: 1.1047\n",
      "Epoch: 451/2000... Training loss: 1.2534\n",
      "Epoch: 451/2000... Training loss: 1.1770\n",
      "Epoch: 451/2000... Training loss: 1.1953\n",
      "Epoch: 451/2000... Training loss: 1.1321\n",
      "Epoch: 452/2000... Training loss: 1.0732\n",
      "Epoch: 452/2000... Training loss: 1.4262\n",
      "Epoch: 452/2000... Training loss: 1.3472\n",
      "Epoch: 452/2000... Training loss: 1.3286\n",
      "Epoch: 452/2000... Training loss: 1.0468\n",
      "Epoch: 452/2000... Training loss: 1.3123\n",
      "Epoch: 452/2000... Training loss: 1.2989\n",
      "Epoch: 452/2000... Training loss: 1.2092\n",
      "Epoch: 452/2000... Training loss: 1.0958\n",
      "Epoch: 452/2000... Training loss: 1.1276\n",
      "Epoch: 452/2000... Training loss: 1.5119\n",
      "Epoch: 452/2000... Training loss: 1.4061\n",
      "Epoch: 452/2000... Training loss: 1.4037\n",
      "Epoch: 452/2000... Training loss: 1.1995\n",
      "Epoch: 452/2000... Training loss: 1.1412\n",
      "Epoch: 452/2000... Training loss: 1.2520\n",
      "Epoch: 452/2000... Training loss: 1.3054\n",
      "Epoch: 452/2000... Training loss: 1.1474\n",
      "Epoch: 452/2000... Training loss: 1.1348\n",
      "Epoch: 452/2000... Training loss: 1.2430\n",
      "Epoch: 452/2000... Training loss: 1.2186\n",
      "Epoch: 452/2000... Training loss: 1.2000\n",
      "Epoch: 452/2000... Training loss: 1.2467\n",
      "Epoch: 452/2000... Training loss: 1.5470\n",
      "Epoch: 452/2000... Training loss: 1.2857\n",
      "Epoch: 452/2000... Training loss: 1.1425\n",
      "Epoch: 452/2000... Training loss: 1.4338\n",
      "Epoch: 452/2000... Training loss: 1.1547\n",
      "Epoch: 452/2000... Training loss: 1.4304\n",
      "Epoch: 452/2000... Training loss: 1.1674\n",
      "Epoch: 452/2000... Training loss: 1.3642\n",
      "Epoch: 453/2000... Training loss: 1.2094\n",
      "Epoch: 453/2000... Training loss: 1.2880\n",
      "Epoch: 453/2000... Training loss: 1.1451\n",
      "Epoch: 453/2000... Training loss: 1.1617\n",
      "Epoch: 453/2000... Training loss: 1.2060\n",
      "Epoch: 453/2000... Training loss: 1.2507\n",
      "Epoch: 453/2000... Training loss: 1.1461\n",
      "Epoch: 453/2000... Training loss: 1.1834\n",
      "Epoch: 453/2000... Training loss: 1.5017\n",
      "Epoch: 453/2000... Training loss: 1.0652\n",
      "Epoch: 453/2000... Training loss: 1.1949\n",
      "Epoch: 453/2000... Training loss: 1.3814\n",
      "Epoch: 453/2000... Training loss: 1.1409\n",
      "Epoch: 453/2000... Training loss: 1.1130\n",
      "Epoch: 453/2000... Training loss: 1.2939\n",
      "Epoch: 453/2000... Training loss: 1.3471\n",
      "Epoch: 453/2000... Training loss: 1.3165\n",
      "Epoch: 453/2000... Training loss: 1.4121\n",
      "Epoch: 453/2000... Training loss: 1.4586\n",
      "Epoch: 453/2000... Training loss: 1.2380\n",
      "Epoch: 453/2000... Training loss: 1.2413\n",
      "Epoch: 453/2000... Training loss: 1.4218\n",
      "Epoch: 453/2000... Training loss: 1.5452\n",
      "Epoch: 453/2000... Training loss: 1.4044\n",
      "Epoch: 453/2000... Training loss: 1.2196\n",
      "Epoch: 453/2000... Training loss: 1.3641\n",
      "Epoch: 453/2000... Training loss: 1.0788\n",
      "Epoch: 453/2000... Training loss: 1.1321\n",
      "Epoch: 453/2000... Training loss: 1.0999\n",
      "Epoch: 453/2000... Training loss: 1.2563\n",
      "Epoch: 453/2000... Training loss: 1.1932\n",
      "Epoch: 454/2000... Training loss: 1.2564\n",
      "Epoch: 454/2000... Training loss: 1.3207\n",
      "Epoch: 454/2000... Training loss: 1.2636\n",
      "Epoch: 454/2000... Training loss: 1.3234\n",
      "Epoch: 454/2000... Training loss: 1.0590\n",
      "Epoch: 454/2000... Training loss: 1.4171\n",
      "Epoch: 454/2000... Training loss: 1.0539\n",
      "Epoch: 454/2000... Training loss: 1.2615\n",
      "Epoch: 454/2000... Training loss: 1.3346\n",
      "Epoch: 454/2000... Training loss: 1.2121\n",
      "Epoch: 454/2000... Training loss: 1.1102\n",
      "Epoch: 454/2000... Training loss: 1.1888\n",
      "Epoch: 454/2000... Training loss: 1.2354\n",
      "Epoch: 454/2000... Training loss: 1.2571\n",
      "Epoch: 454/2000... Training loss: 1.2404\n",
      "Epoch: 454/2000... Training loss: 1.0848\n",
      "Epoch: 454/2000... Training loss: 1.2090\n",
      "Epoch: 454/2000... Training loss: 1.0828\n",
      "Epoch: 454/2000... Training loss: 1.4963\n",
      "Epoch: 454/2000... Training loss: 1.1702\n",
      "Epoch: 454/2000... Training loss: 1.1448\n",
      "Epoch: 454/2000... Training loss: 1.2194\n",
      "Epoch: 454/2000... Training loss: 1.4947\n",
      "Epoch: 454/2000... Training loss: 1.3680\n",
      "Epoch: 454/2000... Training loss: 1.1745\n",
      "Epoch: 454/2000... Training loss: 1.2595\n",
      "Epoch: 454/2000... Training loss: 1.4072\n",
      "Epoch: 454/2000... Training loss: 1.1973\n",
      "Epoch: 454/2000... Training loss: 1.3473\n",
      "Epoch: 454/2000... Training loss: 1.2358\n",
      "Epoch: 454/2000... Training loss: 1.3355\n",
      "Epoch: 455/2000... Training loss: 1.2693\n",
      "Epoch: 455/2000... Training loss: 1.4131\n",
      "Epoch: 455/2000... Training loss: 1.0416\n",
      "Epoch: 455/2000... Training loss: 1.1640\n",
      "Epoch: 455/2000... Training loss: 1.2136\n",
      "Epoch: 455/2000... Training loss: 1.4108\n",
      "Epoch: 455/2000... Training loss: 1.2531\n",
      "Epoch: 455/2000... Training loss: 0.9306\n",
      "Epoch: 455/2000... Training loss: 0.9619\n",
      "Epoch: 455/2000... Training loss: 1.1631\n",
      "Epoch: 455/2000... Training loss: 1.1855\n",
      "Epoch: 455/2000... Training loss: 1.1792\n",
      "Epoch: 455/2000... Training loss: 1.0961\n",
      "Epoch: 455/2000... Training loss: 1.4777\n",
      "Epoch: 455/2000... Training loss: 1.0672\n",
      "Epoch: 455/2000... Training loss: 1.1494\n",
      "Epoch: 455/2000... Training loss: 1.4557\n",
      "Epoch: 455/2000... Training loss: 1.2018\n",
      "Epoch: 455/2000... Training loss: 1.3023\n",
      "Epoch: 455/2000... Training loss: 1.2022\n",
      "Epoch: 455/2000... Training loss: 1.0546\n",
      "Epoch: 455/2000... Training loss: 1.2979\n",
      "Epoch: 455/2000... Training loss: 1.3066\n",
      "Epoch: 455/2000... Training loss: 1.5904\n",
      "Epoch: 455/2000... Training loss: 1.4386\n",
      "Epoch: 455/2000... Training loss: 1.2045\n",
      "Epoch: 455/2000... Training loss: 1.2270\n",
      "Epoch: 455/2000... Training loss: 1.0533\n",
      "Epoch: 455/2000... Training loss: 1.4347\n",
      "Epoch: 455/2000... Training loss: 1.0988\n",
      "Epoch: 455/2000... Training loss: 1.4665\n",
      "Epoch: 456/2000... Training loss: 1.1730\n",
      "Epoch: 456/2000... Training loss: 1.6346\n",
      "Epoch: 456/2000... Training loss: 1.3392\n",
      "Epoch: 456/2000... Training loss: 1.1992\n",
      "Epoch: 456/2000... Training loss: 1.4668\n",
      "Epoch: 456/2000... Training loss: 1.2090\n",
      "Epoch: 456/2000... Training loss: 1.4935\n",
      "Epoch: 456/2000... Training loss: 1.2443\n",
      "Epoch: 456/2000... Training loss: 1.5174\n",
      "Epoch: 456/2000... Training loss: 1.1693\n",
      "Epoch: 456/2000... Training loss: 1.4630\n",
      "Epoch: 456/2000... Training loss: 1.1638\n",
      "Epoch: 456/2000... Training loss: 1.2110\n",
      "Epoch: 456/2000... Training loss: 1.2384\n",
      "Epoch: 456/2000... Training loss: 1.3933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 456/2000... Training loss: 1.2148\n",
      "Epoch: 456/2000... Training loss: 1.2428\n",
      "Epoch: 456/2000... Training loss: 1.3730\n",
      "Epoch: 456/2000... Training loss: 1.0970\n",
      "Epoch: 456/2000... Training loss: 1.4115\n",
      "Epoch: 456/2000... Training loss: 1.1877\n",
      "Epoch: 456/2000... Training loss: 1.4049\n",
      "Epoch: 456/2000... Training loss: 1.0681\n",
      "Epoch: 456/2000... Training loss: 1.1712\n",
      "Epoch: 456/2000... Training loss: 1.1536\n",
      "Epoch: 456/2000... Training loss: 1.2236\n",
      "Epoch: 456/2000... Training loss: 1.2695\n",
      "Epoch: 456/2000... Training loss: 1.3248\n",
      "Epoch: 456/2000... Training loss: 1.3683\n",
      "Epoch: 456/2000... Training loss: 1.4216\n",
      "Epoch: 456/2000... Training loss: 1.1760\n",
      "Epoch: 457/2000... Training loss: 1.3457\n",
      "Epoch: 457/2000... Training loss: 1.1365\n",
      "Epoch: 457/2000... Training loss: 1.2000\n",
      "Epoch: 457/2000... Training loss: 1.2767\n",
      "Epoch: 457/2000... Training loss: 1.2170\n",
      "Epoch: 457/2000... Training loss: 1.1089\n",
      "Epoch: 457/2000... Training loss: 1.3579\n",
      "Epoch: 457/2000... Training loss: 1.1414\n",
      "Epoch: 457/2000... Training loss: 1.0996\n",
      "Epoch: 457/2000... Training loss: 1.2105\n",
      "Epoch: 457/2000... Training loss: 1.4456\n",
      "Epoch: 457/2000... Training loss: 0.9670\n",
      "Epoch: 457/2000... Training loss: 1.1567\n",
      "Epoch: 457/2000... Training loss: 1.0980\n",
      "Epoch: 457/2000... Training loss: 1.4036\n",
      "Epoch: 457/2000... Training loss: 1.1819\n",
      "Epoch: 457/2000... Training loss: 1.5197\n",
      "Epoch: 457/2000... Training loss: 1.2904\n",
      "Epoch: 457/2000... Training loss: 1.2309\n",
      "Epoch: 457/2000... Training loss: 1.2069\n",
      "Epoch: 457/2000... Training loss: 1.1321\n",
      "Epoch: 457/2000... Training loss: 1.2978\n",
      "Epoch: 457/2000... Training loss: 1.0173\n",
      "Epoch: 457/2000... Training loss: 1.3556\n",
      "Epoch: 457/2000... Training loss: 1.4211\n",
      "Epoch: 457/2000... Training loss: 1.1789\n",
      "Epoch: 457/2000... Training loss: 1.1729\n",
      "Epoch: 457/2000... Training loss: 1.2549\n",
      "Epoch: 457/2000... Training loss: 1.2057\n",
      "Epoch: 457/2000... Training loss: 1.0260\n",
      "Epoch: 457/2000... Training loss: 1.2803\n",
      "Epoch: 458/2000... Training loss: 1.1354\n",
      "Epoch: 458/2000... Training loss: 1.4184\n",
      "Epoch: 458/2000... Training loss: 1.3582\n",
      "Epoch: 458/2000... Training loss: 1.2741\n",
      "Epoch: 458/2000... Training loss: 1.3493\n",
      "Epoch: 458/2000... Training loss: 1.2837\n",
      "Epoch: 458/2000... Training loss: 1.0983\n",
      "Epoch: 458/2000... Training loss: 1.5330\n",
      "Epoch: 458/2000... Training loss: 1.1941\n",
      "Epoch: 458/2000... Training loss: 1.0950\n",
      "Epoch: 458/2000... Training loss: 1.2700\n",
      "Epoch: 458/2000... Training loss: 1.1079\n",
      "Epoch: 458/2000... Training loss: 1.1741\n",
      "Epoch: 458/2000... Training loss: 1.1688\n",
      "Epoch: 458/2000... Training loss: 1.0307\n",
      "Epoch: 458/2000... Training loss: 1.2895\n",
      "Epoch: 458/2000... Training loss: 1.1685\n",
      "Epoch: 458/2000... Training loss: 1.3583\n",
      "Epoch: 458/2000... Training loss: 1.1110\n",
      "Epoch: 458/2000... Training loss: 1.2089\n",
      "Epoch: 458/2000... Training loss: 1.2737\n",
      "Epoch: 458/2000... Training loss: 1.3649\n",
      "Epoch: 458/2000... Training loss: 1.4849\n",
      "Epoch: 458/2000... Training loss: 1.3291\n",
      "Epoch: 458/2000... Training loss: 1.1409\n",
      "Epoch: 458/2000... Training loss: 1.1833\n",
      "Epoch: 458/2000... Training loss: 0.9119\n",
      "Epoch: 458/2000... Training loss: 0.9353\n",
      "Epoch: 458/2000... Training loss: 1.1030\n",
      "Epoch: 458/2000... Training loss: 1.3082\n",
      "Epoch: 458/2000... Training loss: 1.2997\n",
      "Epoch: 459/2000... Training loss: 1.3172\n",
      "Epoch: 459/2000... Training loss: 1.1015\n",
      "Epoch: 459/2000... Training loss: 1.3878\n",
      "Epoch: 459/2000... Training loss: 1.2777\n",
      "Epoch: 459/2000... Training loss: 1.2384\n",
      "Epoch: 459/2000... Training loss: 1.1453\n",
      "Epoch: 459/2000... Training loss: 1.1746\n",
      "Epoch: 459/2000... Training loss: 1.1878\n",
      "Epoch: 459/2000... Training loss: 1.1725\n",
      "Epoch: 459/2000... Training loss: 1.2836\n",
      "Epoch: 459/2000... Training loss: 1.4806\n",
      "Epoch: 459/2000... Training loss: 1.0363\n",
      "Epoch: 459/2000... Training loss: 1.2224\n",
      "Epoch: 459/2000... Training loss: 1.3871\n",
      "Epoch: 459/2000... Training loss: 1.0716\n",
      "Epoch: 459/2000... Training loss: 0.9682\n",
      "Epoch: 459/2000... Training loss: 1.2857\n",
      "Epoch: 459/2000... Training loss: 1.0838\n",
      "Epoch: 459/2000... Training loss: 1.0754\n",
      "Epoch: 459/2000... Training loss: 1.2433\n",
      "Epoch: 459/2000... Training loss: 1.1178\n",
      "Epoch: 459/2000... Training loss: 1.1939\n",
      "Epoch: 459/2000... Training loss: 1.3916\n",
      "Epoch: 459/2000... Training loss: 1.1167\n",
      "Epoch: 459/2000... Training loss: 1.0873\n",
      "Epoch: 459/2000... Training loss: 1.4588\n",
      "Epoch: 459/2000... Training loss: 1.1889\n",
      "Epoch: 459/2000... Training loss: 0.9516\n",
      "Epoch: 459/2000... Training loss: 1.1886\n",
      "Epoch: 459/2000... Training loss: 1.3246\n",
      "Epoch: 459/2000... Training loss: 1.0447\n",
      "Epoch: 460/2000... Training loss: 1.2075\n",
      "Epoch: 460/2000... Training loss: 1.5963\n",
      "Epoch: 460/2000... Training loss: 1.3135\n",
      "Epoch: 460/2000... Training loss: 0.8457\n",
      "Epoch: 460/2000... Training loss: 1.4853\n",
      "Epoch: 460/2000... Training loss: 1.2928\n",
      "Epoch: 460/2000... Training loss: 1.3961\n",
      "Epoch: 460/2000... Training loss: 1.3438\n",
      "Epoch: 460/2000... Training loss: 1.0767\n",
      "Epoch: 460/2000... Training loss: 1.2592\n",
      "Epoch: 460/2000... Training loss: 1.2964\n",
      "Epoch: 460/2000... Training loss: 1.3020\n",
      "Epoch: 460/2000... Training loss: 1.1605\n",
      "Epoch: 460/2000... Training loss: 1.3163\n",
      "Epoch: 460/2000... Training loss: 1.0032\n",
      "Epoch: 460/2000... Training loss: 1.1801\n",
      "Epoch: 460/2000... Training loss: 1.0324\n",
      "Epoch: 460/2000... Training loss: 1.3263\n",
      "Epoch: 460/2000... Training loss: 1.1688\n",
      "Epoch: 460/2000... Training loss: 1.1405\n",
      "Epoch: 460/2000... Training loss: 1.2130\n",
      "Epoch: 460/2000... Training loss: 1.2070\n",
      "Epoch: 460/2000... Training loss: 1.2215\n",
      "Epoch: 460/2000... Training loss: 1.2532\n",
      "Epoch: 460/2000... Training loss: 1.3539\n",
      "Epoch: 460/2000... Training loss: 1.0453\n",
      "Epoch: 460/2000... Training loss: 1.0429\n",
      "Epoch: 460/2000... Training loss: 1.2645\n",
      "Epoch: 460/2000... Training loss: 1.3874\n",
      "Epoch: 460/2000... Training loss: 1.1590\n",
      "Epoch: 460/2000... Training loss: 1.4427\n",
      "Epoch: 461/2000... Training loss: 1.4165\n",
      "Epoch: 461/2000... Training loss: 1.4181\n",
      "Epoch: 461/2000... Training loss: 1.1694\n",
      "Epoch: 461/2000... Training loss: 1.1515\n",
      "Epoch: 461/2000... Training loss: 1.0859\n",
      "Epoch: 461/2000... Training loss: 1.1656\n",
      "Epoch: 461/2000... Training loss: 1.2630\n",
      "Epoch: 461/2000... Training loss: 1.4103\n",
      "Epoch: 461/2000... Training loss: 1.1203\n",
      "Epoch: 461/2000... Training loss: 1.1641\n",
      "Epoch: 461/2000... Training loss: 1.2029\n",
      "Epoch: 461/2000... Training loss: 1.1248\n",
      "Epoch: 461/2000... Training loss: 0.9911\n",
      "Epoch: 461/2000... Training loss: 1.2265\n",
      "Epoch: 461/2000... Training loss: 1.2021\n",
      "Epoch: 461/2000... Training loss: 0.9783\n",
      "Epoch: 461/2000... Training loss: 1.3212\n",
      "Epoch: 461/2000... Training loss: 0.9881\n",
      "Epoch: 461/2000... Training loss: 1.3516\n",
      "Epoch: 461/2000... Training loss: 0.9668\n",
      "Epoch: 461/2000... Training loss: 1.1249\n",
      "Epoch: 461/2000... Training loss: 1.1588\n",
      "Epoch: 461/2000... Training loss: 1.3651\n",
      "Epoch: 461/2000... Training loss: 1.4814\n",
      "Epoch: 461/2000... Training loss: 1.5116\n",
      "Epoch: 461/2000... Training loss: 1.2777\n",
      "Epoch: 461/2000... Training loss: 1.2897\n",
      "Epoch: 461/2000... Training loss: 1.2650\n",
      "Epoch: 461/2000... Training loss: 1.3867\n",
      "Epoch: 461/2000... Training loss: 1.0749\n",
      "Epoch: 461/2000... Training loss: 1.0756\n",
      "Epoch: 462/2000... Training loss: 1.3341\n",
      "Epoch: 462/2000... Training loss: 1.1177\n",
      "Epoch: 462/2000... Training loss: 1.2209\n",
      "Epoch: 462/2000... Training loss: 1.1789\n",
      "Epoch: 462/2000... Training loss: 1.2851\n",
      "Epoch: 462/2000... Training loss: 1.3580\n",
      "Epoch: 462/2000... Training loss: 1.1694\n",
      "Epoch: 462/2000... Training loss: 0.8704\n",
      "Epoch: 462/2000... Training loss: 1.1428\n",
      "Epoch: 462/2000... Training loss: 1.0140\n",
      "Epoch: 462/2000... Training loss: 1.2104\n",
      "Epoch: 462/2000... Training loss: 1.2708\n",
      "Epoch: 462/2000... Training loss: 1.4981\n",
      "Epoch: 462/2000... Training loss: 1.1902\n",
      "Epoch: 462/2000... Training loss: 1.0727\n",
      "Epoch: 462/2000... Training loss: 1.3275\n",
      "Epoch: 462/2000... Training loss: 1.2050\n",
      "Epoch: 462/2000... Training loss: 1.1325\n",
      "Epoch: 462/2000... Training loss: 1.4463\n",
      "Epoch: 462/2000... Training loss: 1.5197\n",
      "Epoch: 462/2000... Training loss: 1.2661\n",
      "Epoch: 462/2000... Training loss: 1.4499\n",
      "Epoch: 462/2000... Training loss: 1.2177\n",
      "Epoch: 462/2000... Training loss: 1.2497\n",
      "Epoch: 462/2000... Training loss: 1.0158\n",
      "Epoch: 462/2000... Training loss: 1.2890\n",
      "Epoch: 462/2000... Training loss: 1.3746\n",
      "Epoch: 462/2000... Training loss: 1.2323\n",
      "Epoch: 462/2000... Training loss: 1.1341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 462/2000... Training loss: 1.2319\n",
      "Epoch: 462/2000... Training loss: 1.4613\n",
      "Epoch: 463/2000... Training loss: 1.3421\n",
      "Epoch: 463/2000... Training loss: 1.2391\n",
      "Epoch: 463/2000... Training loss: 1.4457\n",
      "Epoch: 463/2000... Training loss: 0.9596\n",
      "Epoch: 463/2000... Training loss: 1.0739\n",
      "Epoch: 463/2000... Training loss: 1.2637\n",
      "Epoch: 463/2000... Training loss: 1.0398\n",
      "Epoch: 463/2000... Training loss: 1.1330\n",
      "Epoch: 463/2000... Training loss: 1.2037\n",
      "Epoch: 463/2000... Training loss: 1.1588\n",
      "Epoch: 463/2000... Training loss: 1.3528\n",
      "Epoch: 463/2000... Training loss: 1.1411\n",
      "Epoch: 463/2000... Training loss: 0.9532\n",
      "Epoch: 463/2000... Training loss: 1.1941\n",
      "Epoch: 463/2000... Training loss: 1.2225\n",
      "Epoch: 463/2000... Training loss: 1.1077\n",
      "Epoch: 463/2000... Training loss: 1.3352\n",
      "Epoch: 463/2000... Training loss: 1.0238\n",
      "Epoch: 463/2000... Training loss: 0.9909\n",
      "Epoch: 463/2000... Training loss: 1.3669\n",
      "Epoch: 463/2000... Training loss: 1.0466\n",
      "Epoch: 463/2000... Training loss: 1.2758\n",
      "Epoch: 463/2000... Training loss: 1.0700\n",
      "Epoch: 463/2000... Training loss: 1.4321\n",
      "Epoch: 463/2000... Training loss: 1.1846\n",
      "Epoch: 463/2000... Training loss: 1.0204\n",
      "Epoch: 463/2000... Training loss: 1.1635\n",
      "Epoch: 463/2000... Training loss: 1.2658\n",
      "Epoch: 463/2000... Training loss: 1.2570\n",
      "Epoch: 463/2000... Training loss: 1.2633\n",
      "Epoch: 463/2000... Training loss: 1.0436\n",
      "Epoch: 464/2000... Training loss: 1.1687\n",
      "Epoch: 464/2000... Training loss: 1.2496\n",
      "Epoch: 464/2000... Training loss: 1.0222\n",
      "Epoch: 464/2000... Training loss: 1.3682\n",
      "Epoch: 464/2000... Training loss: 1.2997\n",
      "Epoch: 464/2000... Training loss: 1.1383\n",
      "Epoch: 464/2000... Training loss: 1.3790\n",
      "Epoch: 464/2000... Training loss: 0.9763\n",
      "Epoch: 464/2000... Training loss: 1.2672\n",
      "Epoch: 464/2000... Training loss: 1.3035\n",
      "Epoch: 464/2000... Training loss: 1.2461\n",
      "Epoch: 464/2000... Training loss: 1.3087\n",
      "Epoch: 464/2000... Training loss: 1.1252\n",
      "Epoch: 464/2000... Training loss: 1.1176\n",
      "Epoch: 464/2000... Training loss: 1.0808\n",
      "Epoch: 464/2000... Training loss: 1.2165\n",
      "Epoch: 464/2000... Training loss: 1.5692\n",
      "Epoch: 464/2000... Training loss: 1.1148\n",
      "Epoch: 464/2000... Training loss: 1.0764\n",
      "Epoch: 464/2000... Training loss: 1.1934\n",
      "Epoch: 464/2000... Training loss: 1.3020\n",
      "Epoch: 464/2000... Training loss: 1.2787\n",
      "Epoch: 464/2000... Training loss: 1.1394\n",
      "Epoch: 464/2000... Training loss: 1.2446\n",
      "Epoch: 464/2000... Training loss: 1.1865\n",
      "Epoch: 464/2000... Training loss: 1.0446\n",
      "Epoch: 464/2000... Training loss: 1.0999\n",
      "Epoch: 464/2000... Training loss: 1.2374\n",
      "Epoch: 464/2000... Training loss: 1.3623\n",
      "Epoch: 464/2000... Training loss: 1.4830\n",
      "Epoch: 464/2000... Training loss: 1.2604\n",
      "Epoch: 465/2000... Training loss: 1.0566\n",
      "Epoch: 465/2000... Training loss: 1.0762\n",
      "Epoch: 465/2000... Training loss: 1.2191\n",
      "Epoch: 465/2000... Training loss: 0.9611\n",
      "Epoch: 465/2000... Training loss: 1.2777\n",
      "Epoch: 465/2000... Training loss: 1.3965\n",
      "Epoch: 465/2000... Training loss: 1.3375\n",
      "Epoch: 465/2000... Training loss: 1.0961\n",
      "Epoch: 465/2000... Training loss: 1.3251\n",
      "Epoch: 465/2000... Training loss: 1.3245\n",
      "Epoch: 465/2000... Training loss: 0.9827\n",
      "Epoch: 465/2000... Training loss: 1.4464\n",
      "Epoch: 465/2000... Training loss: 1.3898\n",
      "Epoch: 465/2000... Training loss: 1.2873\n",
      "Epoch: 465/2000... Training loss: 1.0048\n",
      "Epoch: 465/2000... Training loss: 1.2986\n",
      "Epoch: 465/2000... Training loss: 1.1092\n",
      "Epoch: 465/2000... Training loss: 1.0835\n",
      "Epoch: 465/2000... Training loss: 1.4022\n",
      "Epoch: 465/2000... Training loss: 1.2123\n",
      "Epoch: 465/2000... Training loss: 1.0660\n",
      "Epoch: 465/2000... Training loss: 1.1365\n",
      "Epoch: 465/2000... Training loss: 1.2140\n",
      "Epoch: 465/2000... Training loss: 1.3509\n",
      "Epoch: 465/2000... Training loss: 1.4614\n",
      "Epoch: 465/2000... Training loss: 1.2739\n",
      "Epoch: 465/2000... Training loss: 1.1855\n",
      "Epoch: 465/2000... Training loss: 1.0442\n",
      "Epoch: 465/2000... Training loss: 1.2253\n",
      "Epoch: 465/2000... Training loss: 1.3848\n",
      "Epoch: 465/2000... Training loss: 1.1774\n",
      "Epoch: 466/2000... Training loss: 1.2744\n",
      "Epoch: 466/2000... Training loss: 1.4401\n",
      "Epoch: 466/2000... Training loss: 1.3932\n",
      "Epoch: 466/2000... Training loss: 1.2411\n",
      "Epoch: 466/2000... Training loss: 1.0516\n",
      "Epoch: 466/2000... Training loss: 1.2435\n",
      "Epoch: 466/2000... Training loss: 1.1431\n",
      "Epoch: 466/2000... Training loss: 1.0886\n",
      "Epoch: 466/2000... Training loss: 1.3684\n",
      "Epoch: 466/2000... Training loss: 1.1064\n",
      "Epoch: 466/2000... Training loss: 1.3640\n",
      "Epoch: 466/2000... Training loss: 1.0614\n",
      "Epoch: 466/2000... Training loss: 1.2311\n",
      "Epoch: 466/2000... Training loss: 1.3669\n",
      "Epoch: 466/2000... Training loss: 1.0644\n",
      "Epoch: 466/2000... Training loss: 1.0846\n",
      "Epoch: 466/2000... Training loss: 1.2645\n",
      "Epoch: 466/2000... Training loss: 1.1971\n",
      "Epoch: 466/2000... Training loss: 1.2231\n",
      "Epoch: 466/2000... Training loss: 1.3439\n",
      "Epoch: 466/2000... Training loss: 1.0329\n",
      "Epoch: 466/2000... Training loss: 1.1454\n",
      "Epoch: 466/2000... Training loss: 1.2103\n",
      "Epoch: 466/2000... Training loss: 1.4059\n",
      "Epoch: 466/2000... Training loss: 1.3438\n",
      "Epoch: 466/2000... Training loss: 1.2030\n",
      "Epoch: 466/2000... Training loss: 1.1338\n",
      "Epoch: 466/2000... Training loss: 1.2879\n",
      "Epoch: 466/2000... Training loss: 1.3011\n",
      "Epoch: 466/2000... Training loss: 1.0786\n",
      "Epoch: 466/2000... Training loss: 1.1151\n",
      "Epoch: 467/2000... Training loss: 1.0954\n",
      "Epoch: 467/2000... Training loss: 1.2672\n",
      "Epoch: 467/2000... Training loss: 1.1045\n",
      "Epoch: 467/2000... Training loss: 1.1893\n",
      "Epoch: 467/2000... Training loss: 1.2915\n",
      "Epoch: 467/2000... Training loss: 1.2667\n",
      "Epoch: 467/2000... Training loss: 1.2371\n",
      "Epoch: 467/2000... Training loss: 1.0457\n",
      "Epoch: 467/2000... Training loss: 1.0239\n",
      "Epoch: 467/2000... Training loss: 1.3552\n",
      "Epoch: 467/2000... Training loss: 1.3138\n",
      "Epoch: 467/2000... Training loss: 1.1246\n",
      "Epoch: 467/2000... Training loss: 1.4542\n",
      "Epoch: 467/2000... Training loss: 1.1971\n",
      "Epoch: 467/2000... Training loss: 1.2787\n",
      "Epoch: 467/2000... Training loss: 1.3593\n",
      "Epoch: 467/2000... Training loss: 1.2672\n",
      "Epoch: 467/2000... Training loss: 1.1250\n",
      "Epoch: 467/2000... Training loss: 1.1331\n",
      "Epoch: 467/2000... Training loss: 1.1765\n",
      "Epoch: 467/2000... Training loss: 1.4444\n",
      "Epoch: 467/2000... Training loss: 1.0152\n",
      "Epoch: 467/2000... Training loss: 1.2375\n",
      "Epoch: 467/2000... Training loss: 1.1395\n",
      "Epoch: 467/2000... Training loss: 1.1722\n",
      "Epoch: 467/2000... Training loss: 1.3227\n",
      "Epoch: 467/2000... Training loss: 1.2239\n",
      "Epoch: 467/2000... Training loss: 1.1189\n",
      "Epoch: 467/2000... Training loss: 1.1900\n",
      "Epoch: 467/2000... Training loss: 1.2267\n",
      "Epoch: 467/2000... Training loss: 1.2478\n",
      "Epoch: 468/2000... Training loss: 1.3654\n",
      "Epoch: 468/2000... Training loss: 1.2975\n",
      "Epoch: 468/2000... Training loss: 1.1890\n",
      "Epoch: 468/2000... Training loss: 1.2390\n",
      "Epoch: 468/2000... Training loss: 1.3404\n",
      "Epoch: 468/2000... Training loss: 1.1843\n",
      "Epoch: 468/2000... Training loss: 1.1914\n",
      "Epoch: 468/2000... Training loss: 1.1828\n",
      "Epoch: 468/2000... Training loss: 1.3360\n",
      "Epoch: 468/2000... Training loss: 1.0900\n",
      "Epoch: 468/2000... Training loss: 0.9973\n",
      "Epoch: 468/2000... Training loss: 1.2953\n",
      "Epoch: 468/2000... Training loss: 1.1188\n",
      "Epoch: 468/2000... Training loss: 1.3913\n",
      "Epoch: 468/2000... Training loss: 0.9741\n",
      "Epoch: 468/2000... Training loss: 1.0886\n",
      "Epoch: 468/2000... Training loss: 1.2282\n",
      "Epoch: 468/2000... Training loss: 0.8345\n",
      "Epoch: 468/2000... Training loss: 1.2096\n",
      "Epoch: 468/2000... Training loss: 1.2689\n",
      "Epoch: 468/2000... Training loss: 1.4284\n",
      "Epoch: 468/2000... Training loss: 1.3595\n",
      "Epoch: 468/2000... Training loss: 1.0232\n",
      "Epoch: 468/2000... Training loss: 1.3426\n",
      "Epoch: 468/2000... Training loss: 1.1854\n",
      "Epoch: 468/2000... Training loss: 0.9114\n",
      "Epoch: 468/2000... Training loss: 1.0301\n",
      "Epoch: 468/2000... Training loss: 1.0184\n",
      "Epoch: 468/2000... Training loss: 1.2314\n",
      "Epoch: 468/2000... Training loss: 1.2612\n",
      "Epoch: 468/2000... Training loss: 1.0937\n",
      "Epoch: 469/2000... Training loss: 1.1870\n",
      "Epoch: 469/2000... Training loss: 1.2763\n",
      "Epoch: 469/2000... Training loss: 1.1881\n",
      "Epoch: 469/2000... Training loss: 1.0964\n",
      "Epoch: 469/2000... Training loss: 1.2116\n",
      "Epoch: 469/2000... Training loss: 1.1137\n",
      "Epoch: 469/2000... Training loss: 1.0376\n",
      "Epoch: 469/2000... Training loss: 1.3574\n",
      "Epoch: 469/2000... Training loss: 1.0106\n",
      "Epoch: 469/2000... Training loss: 1.2452\n",
      "Epoch: 469/2000... Training loss: 1.1221\n",
      "Epoch: 469/2000... Training loss: 1.2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 469/2000... Training loss: 0.9940\n",
      "Epoch: 469/2000... Training loss: 1.3260\n",
      "Epoch: 469/2000... Training loss: 1.2610\n",
      "Epoch: 469/2000... Training loss: 1.3251\n",
      "Epoch: 469/2000... Training loss: 0.9822\n",
      "Epoch: 469/2000... Training loss: 1.0381\n",
      "Epoch: 469/2000... Training loss: 0.9467\n",
      "Epoch: 469/2000... Training loss: 1.3539\n",
      "Epoch: 469/2000... Training loss: 1.0763\n",
      "Epoch: 469/2000... Training loss: 1.1560\n",
      "Epoch: 469/2000... Training loss: 1.2873\n",
      "Epoch: 469/2000... Training loss: 1.0810\n",
      "Epoch: 469/2000... Training loss: 1.2479\n",
      "Epoch: 469/2000... Training loss: 0.9438\n",
      "Epoch: 469/2000... Training loss: 0.9497\n",
      "Epoch: 469/2000... Training loss: 1.1710\n",
      "Epoch: 469/2000... Training loss: 1.1490\n",
      "Epoch: 469/2000... Training loss: 1.1049\n",
      "Epoch: 469/2000... Training loss: 1.2437\n",
      "Epoch: 470/2000... Training loss: 1.2322\n",
      "Epoch: 470/2000... Training loss: 1.4138\n",
      "Epoch: 470/2000... Training loss: 1.1733\n",
      "Epoch: 470/2000... Training loss: 0.9990\n",
      "Epoch: 470/2000... Training loss: 1.3896\n",
      "Epoch: 470/2000... Training loss: 1.1367\n",
      "Epoch: 470/2000... Training loss: 1.0983\n",
      "Epoch: 470/2000... Training loss: 1.0186\n",
      "Epoch: 470/2000... Training loss: 1.2104\n",
      "Epoch: 470/2000... Training loss: 1.2065\n",
      "Epoch: 470/2000... Training loss: 1.2426\n",
      "Epoch: 470/2000... Training loss: 1.1591\n",
      "Epoch: 470/2000... Training loss: 1.1202\n",
      "Epoch: 470/2000... Training loss: 1.2606\n",
      "Epoch: 470/2000... Training loss: 1.1533\n",
      "Epoch: 470/2000... Training loss: 0.9411\n",
      "Epoch: 470/2000... Training loss: 1.0982\n",
      "Epoch: 470/2000... Training loss: 1.0234\n",
      "Epoch: 470/2000... Training loss: 1.3583\n",
      "Epoch: 470/2000... Training loss: 1.2024\n",
      "Epoch: 470/2000... Training loss: 1.3587\n",
      "Epoch: 470/2000... Training loss: 0.9839\n",
      "Epoch: 470/2000... Training loss: 1.1546\n",
      "Epoch: 470/2000... Training loss: 1.0633\n",
      "Epoch: 470/2000... Training loss: 1.1732\n",
      "Epoch: 470/2000... Training loss: 1.2000\n",
      "Epoch: 470/2000... Training loss: 1.3779\n",
      "Epoch: 470/2000... Training loss: 0.9863\n",
      "Epoch: 470/2000... Training loss: 0.9925\n",
      "Epoch: 470/2000... Training loss: 1.3340\n",
      "Epoch: 470/2000... Training loss: 1.1177\n",
      "Epoch: 471/2000... Training loss: 1.2151\n",
      "Epoch: 471/2000... Training loss: 1.1997\n",
      "Epoch: 471/2000... Training loss: 1.2931\n",
      "Epoch: 471/2000... Training loss: 1.0731\n",
      "Epoch: 471/2000... Training loss: 1.0664\n",
      "Epoch: 471/2000... Training loss: 0.9846\n",
      "Epoch: 471/2000... Training loss: 1.1587\n",
      "Epoch: 471/2000... Training loss: 0.9485\n",
      "Epoch: 471/2000... Training loss: 1.2861\n",
      "Epoch: 471/2000... Training loss: 1.3760\n",
      "Epoch: 471/2000... Training loss: 0.9262\n",
      "Epoch: 471/2000... Training loss: 1.1625\n",
      "Epoch: 471/2000... Training loss: 1.1352\n",
      "Epoch: 471/2000... Training loss: 1.1816\n",
      "Epoch: 471/2000... Training loss: 1.0178\n",
      "Epoch: 471/2000... Training loss: 1.1718\n",
      "Epoch: 471/2000... Training loss: 1.3105\n",
      "Epoch: 471/2000... Training loss: 1.1797\n",
      "Epoch: 471/2000... Training loss: 1.1823\n",
      "Epoch: 471/2000... Training loss: 0.9501\n",
      "Epoch: 471/2000... Training loss: 1.0064\n",
      "Epoch: 471/2000... Training loss: 0.9997\n",
      "Epoch: 471/2000... Training loss: 1.2744\n",
      "Epoch: 471/2000... Training loss: 1.1742\n",
      "Epoch: 471/2000... Training loss: 1.2012\n",
      "Epoch: 471/2000... Training loss: 1.0685\n",
      "Epoch: 471/2000... Training loss: 1.0420\n",
      "Epoch: 471/2000... Training loss: 0.9065\n",
      "Epoch: 471/2000... Training loss: 1.1522\n",
      "Epoch: 471/2000... Training loss: 1.2442\n",
      "Epoch: 471/2000... Training loss: 0.9921\n",
      "Epoch: 472/2000... Training loss: 0.9920\n",
      "Epoch: 472/2000... Training loss: 1.0675\n",
      "Epoch: 472/2000... Training loss: 1.1019\n",
      "Epoch: 472/2000... Training loss: 1.1786\n",
      "Epoch: 472/2000... Training loss: 1.0413\n",
      "Epoch: 472/2000... Training loss: 1.2812\n",
      "Epoch: 472/2000... Training loss: 1.0082\n",
      "Epoch: 472/2000... Training loss: 1.3330\n",
      "Epoch: 472/2000... Training loss: 1.1095\n",
      "Epoch: 472/2000... Training loss: 1.0122\n",
      "Epoch: 472/2000... Training loss: 1.2555\n",
      "Epoch: 472/2000... Training loss: 1.2036\n",
      "Epoch: 472/2000... Training loss: 1.1505\n",
      "Epoch: 472/2000... Training loss: 0.9730\n",
      "Epoch: 472/2000... Training loss: 0.8987\n",
      "Epoch: 472/2000... Training loss: 1.2196\n",
      "Epoch: 472/2000... Training loss: 1.2238\n",
      "Epoch: 472/2000... Training loss: 1.1384\n",
      "Epoch: 472/2000... Training loss: 1.0494\n",
      "Epoch: 472/2000... Training loss: 1.0827\n",
      "Epoch: 472/2000... Training loss: 0.8718\n",
      "Epoch: 472/2000... Training loss: 1.1831\n",
      "Epoch: 472/2000... Training loss: 1.4184\n",
      "Epoch: 472/2000... Training loss: 1.2616\n",
      "Epoch: 472/2000... Training loss: 1.0456\n",
      "Epoch: 472/2000... Training loss: 1.2013\n",
      "Epoch: 472/2000... Training loss: 0.9133\n",
      "Epoch: 472/2000... Training loss: 1.2361\n",
      "Epoch: 472/2000... Training loss: 0.9583\n",
      "Epoch: 472/2000... Training loss: 1.2025\n",
      "Epoch: 472/2000... Training loss: 1.1639\n",
      "Epoch: 473/2000... Training loss: 0.9600\n",
      "Epoch: 473/2000... Training loss: 1.1507\n",
      "Epoch: 473/2000... Training loss: 1.1484\n",
      "Epoch: 473/2000... Training loss: 1.2020\n",
      "Epoch: 473/2000... Training loss: 1.1218\n",
      "Epoch: 473/2000... Training loss: 1.2178\n",
      "Epoch: 473/2000... Training loss: 1.0497\n",
      "Epoch: 473/2000... Training loss: 1.1149\n",
      "Epoch: 473/2000... Training loss: 1.0844\n",
      "Epoch: 473/2000... Training loss: 1.2621\n",
      "Epoch: 473/2000... Training loss: 1.0643\n",
      "Epoch: 473/2000... Training loss: 1.2766\n",
      "Epoch: 473/2000... Training loss: 1.1632\n",
      "Epoch: 473/2000... Training loss: 1.2690\n",
      "Epoch: 473/2000... Training loss: 1.1037\n",
      "Epoch: 473/2000... Training loss: 1.3899\n",
      "Epoch: 473/2000... Training loss: 1.1410\n",
      "Epoch: 473/2000... Training loss: 1.0831\n",
      "Epoch: 473/2000... Training loss: 1.3180\n",
      "Epoch: 473/2000... Training loss: 1.4139\n",
      "Epoch: 473/2000... Training loss: 0.9731\n",
      "Epoch: 473/2000... Training loss: 1.2530\n",
      "Epoch: 473/2000... Training loss: 1.1559\n",
      "Epoch: 473/2000... Training loss: 1.2184\n",
      "Epoch: 473/2000... Training loss: 1.1192\n",
      "Epoch: 473/2000... Training loss: 1.0383\n",
      "Epoch: 473/2000... Training loss: 0.9653\n",
      "Epoch: 473/2000... Training loss: 1.1620\n",
      "Epoch: 473/2000... Training loss: 1.2472\n",
      "Epoch: 473/2000... Training loss: 1.1701\n",
      "Epoch: 473/2000... Training loss: 1.0889\n",
      "Epoch: 474/2000... Training loss: 1.1894\n",
      "Epoch: 474/2000... Training loss: 1.1791\n",
      "Epoch: 474/2000... Training loss: 1.2249\n",
      "Epoch: 474/2000... Training loss: 1.0339\n",
      "Epoch: 474/2000... Training loss: 1.3351\n",
      "Epoch: 474/2000... Training loss: 0.9995\n",
      "Epoch: 474/2000... Training loss: 1.1669\n",
      "Epoch: 474/2000... Training loss: 1.1475\n",
      "Epoch: 474/2000... Training loss: 1.1419\n",
      "Epoch: 474/2000... Training loss: 1.0444\n",
      "Epoch: 474/2000... Training loss: 1.1749\n",
      "Epoch: 474/2000... Training loss: 1.0523\n",
      "Epoch: 474/2000... Training loss: 1.1209\n",
      "Epoch: 474/2000... Training loss: 1.3242\n",
      "Epoch: 474/2000... Training loss: 1.3118\n",
      "Epoch: 474/2000... Training loss: 1.2967\n",
      "Epoch: 474/2000... Training loss: 1.0331\n",
      "Epoch: 474/2000... Training loss: 1.2574\n",
      "Epoch: 474/2000... Training loss: 1.2452\n",
      "Epoch: 474/2000... Training loss: 1.3932\n",
      "Epoch: 474/2000... Training loss: 1.0898\n",
      "Epoch: 474/2000... Training loss: 1.1033\n",
      "Epoch: 474/2000... Training loss: 0.9967\n",
      "Epoch: 474/2000... Training loss: 1.2109\n",
      "Epoch: 474/2000... Training loss: 1.2562\n",
      "Epoch: 474/2000... Training loss: 1.1991\n",
      "Epoch: 474/2000... Training loss: 1.1691\n",
      "Epoch: 474/2000... Training loss: 1.3332\n",
      "Epoch: 474/2000... Training loss: 1.2267\n",
      "Epoch: 474/2000... Training loss: 1.1396\n",
      "Epoch: 474/2000... Training loss: 1.1847\n",
      "Epoch: 475/2000... Training loss: 1.2701\n",
      "Epoch: 475/2000... Training loss: 0.9487\n",
      "Epoch: 475/2000... Training loss: 1.1853\n",
      "Epoch: 475/2000... Training loss: 0.9417\n",
      "Epoch: 475/2000... Training loss: 1.0248\n",
      "Epoch: 475/2000... Training loss: 1.0980\n",
      "Epoch: 475/2000... Training loss: 1.0302\n",
      "Epoch: 475/2000... Training loss: 1.4306\n",
      "Epoch: 475/2000... Training loss: 1.5435\n",
      "Epoch: 475/2000... Training loss: 1.2270\n",
      "Epoch: 475/2000... Training loss: 1.4569\n",
      "Epoch: 475/2000... Training loss: 1.0121\n",
      "Epoch: 475/2000... Training loss: 1.0636\n",
      "Epoch: 475/2000... Training loss: 1.1172\n",
      "Epoch: 475/2000... Training loss: 1.1817\n",
      "Epoch: 475/2000... Training loss: 1.0817\n",
      "Epoch: 475/2000... Training loss: 1.3064\n",
      "Epoch: 475/2000... Training loss: 0.9884\n",
      "Epoch: 475/2000... Training loss: 0.9084\n",
      "Epoch: 475/2000... Training loss: 1.3690\n",
      "Epoch: 475/2000... Training loss: 1.1748\n",
      "Epoch: 475/2000... Training loss: 0.9130\n",
      "Epoch: 475/2000... Training loss: 1.2968\n",
      "Epoch: 475/2000... Training loss: 1.2899\n",
      "Epoch: 475/2000... Training loss: 0.9384\n",
      "Epoch: 475/2000... Training loss: 1.0545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 475/2000... Training loss: 1.0151\n",
      "Epoch: 475/2000... Training loss: 1.2085\n",
      "Epoch: 475/2000... Training loss: 1.2529\n",
      "Epoch: 475/2000... Training loss: 1.3265\n",
      "Epoch: 475/2000... Training loss: 0.9230\n",
      "Epoch: 476/2000... Training loss: 0.8546\n",
      "Epoch: 476/2000... Training loss: 1.1393\n",
      "Epoch: 476/2000... Training loss: 1.1015\n",
      "Epoch: 476/2000... Training loss: 1.2398\n",
      "Epoch: 476/2000... Training loss: 1.2286\n",
      "Epoch: 476/2000... Training loss: 1.2214\n",
      "Epoch: 476/2000... Training loss: 1.2212\n",
      "Epoch: 476/2000... Training loss: 1.1795\n",
      "Epoch: 476/2000... Training loss: 0.9862\n",
      "Epoch: 476/2000... Training loss: 1.1755\n",
      "Epoch: 476/2000... Training loss: 1.1882\n",
      "Epoch: 476/2000... Training loss: 1.2334\n",
      "Epoch: 476/2000... Training loss: 1.1180\n",
      "Epoch: 476/2000... Training loss: 1.1088\n",
      "Epoch: 476/2000... Training loss: 1.0184\n",
      "Epoch: 476/2000... Training loss: 1.2202\n",
      "Epoch: 476/2000... Training loss: 1.0911\n",
      "Epoch: 476/2000... Training loss: 0.9071\n",
      "Epoch: 476/2000... Training loss: 0.9226\n",
      "Epoch: 476/2000... Training loss: 1.0823\n",
      "Epoch: 476/2000... Training loss: 1.2306\n",
      "Epoch: 476/2000... Training loss: 0.9148\n",
      "Epoch: 476/2000... Training loss: 1.1042\n",
      "Epoch: 476/2000... Training loss: 1.1354\n",
      "Epoch: 476/2000... Training loss: 1.3393\n",
      "Epoch: 476/2000... Training loss: 1.1413\n",
      "Epoch: 476/2000... Training loss: 1.1966\n",
      "Epoch: 476/2000... Training loss: 1.1999\n",
      "Epoch: 476/2000... Training loss: 1.2711\n",
      "Epoch: 476/2000... Training loss: 1.1283\n",
      "Epoch: 476/2000... Training loss: 1.1203\n",
      "Epoch: 477/2000... Training loss: 1.0891\n",
      "Epoch: 477/2000... Training loss: 1.2341\n",
      "Epoch: 477/2000... Training loss: 1.0268\n",
      "Epoch: 477/2000... Training loss: 1.1649\n",
      "Epoch: 477/2000... Training loss: 1.2823\n",
      "Epoch: 477/2000... Training loss: 1.3593\n",
      "Epoch: 477/2000... Training loss: 1.0298\n",
      "Epoch: 477/2000... Training loss: 1.0893\n",
      "Epoch: 477/2000... Training loss: 1.0997\n",
      "Epoch: 477/2000... Training loss: 1.2420\n",
      "Epoch: 477/2000... Training loss: 1.1933\n",
      "Epoch: 477/2000... Training loss: 1.0513\n",
      "Epoch: 477/2000... Training loss: 1.0188\n",
      "Epoch: 477/2000... Training loss: 1.2966\n",
      "Epoch: 477/2000... Training loss: 1.1506\n",
      "Epoch: 477/2000... Training loss: 1.1928\n",
      "Epoch: 477/2000... Training loss: 1.1234\n",
      "Epoch: 477/2000... Training loss: 1.1274\n",
      "Epoch: 477/2000... Training loss: 1.1885\n",
      "Epoch: 477/2000... Training loss: 1.1121\n",
      "Epoch: 477/2000... Training loss: 1.0986\n",
      "Epoch: 477/2000... Training loss: 1.0295\n",
      "Epoch: 477/2000... Training loss: 0.9897\n",
      "Epoch: 477/2000... Training loss: 1.1057\n",
      "Epoch: 477/2000... Training loss: 1.1717\n",
      "Epoch: 477/2000... Training loss: 1.3883\n",
      "Epoch: 477/2000... Training loss: 1.0712\n",
      "Epoch: 477/2000... Training loss: 1.2479\n",
      "Epoch: 477/2000... Training loss: 1.0790\n",
      "Epoch: 477/2000... Training loss: 1.1416\n",
      "Epoch: 477/2000... Training loss: 0.8725\n",
      "Epoch: 478/2000... Training loss: 1.1993\n",
      "Epoch: 478/2000... Training loss: 1.3530\n",
      "Epoch: 478/2000... Training loss: 1.5048\n",
      "Epoch: 478/2000... Training loss: 1.1884\n",
      "Epoch: 478/2000... Training loss: 1.0709\n",
      "Epoch: 478/2000... Training loss: 1.4137\n",
      "Epoch: 478/2000... Training loss: 1.1434\n",
      "Epoch: 478/2000... Training loss: 1.2761\n",
      "Epoch: 478/2000... Training loss: 1.0786\n",
      "Epoch: 478/2000... Training loss: 1.0189\n",
      "Epoch: 478/2000... Training loss: 1.1124\n",
      "Epoch: 478/2000... Training loss: 1.2953\n",
      "Epoch: 478/2000... Training loss: 1.1549\n",
      "Epoch: 478/2000... Training loss: 1.1128\n",
      "Epoch: 478/2000... Training loss: 1.2541\n",
      "Epoch: 478/2000... Training loss: 1.3321\n",
      "Epoch: 478/2000... Training loss: 1.0862\n",
      "Epoch: 478/2000... Training loss: 0.8882\n",
      "Epoch: 478/2000... Training loss: 1.0381\n",
      "Epoch: 478/2000... Training loss: 1.3269\n",
      "Epoch: 478/2000... Training loss: 1.3121\n",
      "Epoch: 478/2000... Training loss: 1.0639\n",
      "Epoch: 478/2000... Training loss: 1.5399\n",
      "Epoch: 478/2000... Training loss: 1.1257\n",
      "Epoch: 478/2000... Training loss: 1.0254\n",
      "Epoch: 478/2000... Training loss: 1.1075\n",
      "Epoch: 478/2000... Training loss: 1.0362\n",
      "Epoch: 478/2000... Training loss: 0.8844\n",
      "Epoch: 478/2000... Training loss: 1.2416\n",
      "Epoch: 478/2000... Training loss: 1.3280\n",
      "Epoch: 478/2000... Training loss: 1.0154\n",
      "Epoch: 479/2000... Training loss: 1.2935\n",
      "Epoch: 479/2000... Training loss: 1.2549\n",
      "Epoch: 479/2000... Training loss: 1.0380\n",
      "Epoch: 479/2000... Training loss: 1.1898\n",
      "Epoch: 479/2000... Training loss: 0.9984\n",
      "Epoch: 479/2000... Training loss: 1.4627\n",
      "Epoch: 479/2000... Training loss: 1.0339\n",
      "Epoch: 479/2000... Training loss: 1.1105\n",
      "Epoch: 479/2000... Training loss: 1.1649\n",
      "Epoch: 479/2000... Training loss: 1.1562\n",
      "Epoch: 479/2000... Training loss: 1.0226\n",
      "Epoch: 479/2000... Training loss: 1.0976\n",
      "Epoch: 479/2000... Training loss: 1.2448\n",
      "Epoch: 479/2000... Training loss: 1.1676\n",
      "Epoch: 479/2000... Training loss: 1.0737\n",
      "Epoch: 479/2000... Training loss: 1.0151\n",
      "Epoch: 479/2000... Training loss: 1.0745\n",
      "Epoch: 479/2000... Training loss: 1.1820\n",
      "Epoch: 479/2000... Training loss: 1.1922\n",
      "Epoch: 479/2000... Training loss: 1.1735\n",
      "Epoch: 479/2000... Training loss: 0.8071\n",
      "Epoch: 479/2000... Training loss: 1.0992\n",
      "Epoch: 479/2000... Training loss: 1.2386\n",
      "Epoch: 479/2000... Training loss: 1.0819\n",
      "Epoch: 479/2000... Training loss: 1.1262\n",
      "Epoch: 479/2000... Training loss: 0.9630\n",
      "Epoch: 479/2000... Training loss: 1.1678\n",
      "Epoch: 479/2000... Training loss: 0.9345\n",
      "Epoch: 479/2000... Training loss: 1.1698\n",
      "Epoch: 479/2000... Training loss: 1.1720\n",
      "Epoch: 479/2000... Training loss: 1.1160\n",
      "Epoch: 480/2000... Training loss: 1.2128\n",
      "Epoch: 480/2000... Training loss: 1.1589\n",
      "Epoch: 480/2000... Training loss: 1.1660\n",
      "Epoch: 480/2000... Training loss: 0.9929\n",
      "Epoch: 480/2000... Training loss: 1.5521\n",
      "Epoch: 480/2000... Training loss: 1.3153\n",
      "Epoch: 480/2000... Training loss: 1.3649\n",
      "Epoch: 480/2000... Training loss: 1.1028\n",
      "Epoch: 480/2000... Training loss: 1.1084\n",
      "Epoch: 480/2000... Training loss: 1.1016\n",
      "Epoch: 480/2000... Training loss: 1.0006\n",
      "Epoch: 480/2000... Training loss: 1.1263\n",
      "Epoch: 480/2000... Training loss: 1.3050\n",
      "Epoch: 480/2000... Training loss: 0.8984\n",
      "Epoch: 480/2000... Training loss: 0.9788\n",
      "Epoch: 480/2000... Training loss: 1.1050\n",
      "Epoch: 480/2000... Training loss: 1.0209\n",
      "Epoch: 480/2000... Training loss: 1.0647\n",
      "Epoch: 480/2000... Training loss: 0.8640\n",
      "Epoch: 480/2000... Training loss: 1.3680\n",
      "Epoch: 480/2000... Training loss: 1.0074\n",
      "Epoch: 480/2000... Training loss: 1.3655\n",
      "Epoch: 480/2000... Training loss: 1.1104\n",
      "Epoch: 480/2000... Training loss: 1.2948\n",
      "Epoch: 480/2000... Training loss: 1.1126\n",
      "Epoch: 480/2000... Training loss: 1.2928\n",
      "Epoch: 480/2000... Training loss: 1.4110\n",
      "Epoch: 480/2000... Training loss: 1.2752\n",
      "Epoch: 480/2000... Training loss: 1.0860\n",
      "Epoch: 480/2000... Training loss: 1.1225\n",
      "Epoch: 480/2000... Training loss: 1.0739\n",
      "Epoch: 481/2000... Training loss: 1.2167\n",
      "Epoch: 481/2000... Training loss: 1.1271\n",
      "Epoch: 481/2000... Training loss: 1.0209\n",
      "Epoch: 481/2000... Training loss: 1.1465\n",
      "Epoch: 481/2000... Training loss: 1.2744\n",
      "Epoch: 481/2000... Training loss: 1.2990\n",
      "Epoch: 481/2000... Training loss: 1.0949\n",
      "Epoch: 481/2000... Training loss: 1.2551\n",
      "Epoch: 481/2000... Training loss: 1.4427\n",
      "Epoch: 481/2000... Training loss: 1.3213\n",
      "Epoch: 481/2000... Training loss: 1.2376\n",
      "Epoch: 481/2000... Training loss: 1.0170\n",
      "Epoch: 481/2000... Training loss: 1.3578\n",
      "Epoch: 481/2000... Training loss: 0.9886\n",
      "Epoch: 481/2000... Training loss: 1.0856\n",
      "Epoch: 481/2000... Training loss: 1.3504\n",
      "Epoch: 481/2000... Training loss: 0.9574\n",
      "Epoch: 481/2000... Training loss: 1.1155\n",
      "Epoch: 481/2000... Training loss: 1.0510\n",
      "Epoch: 481/2000... Training loss: 1.0656\n",
      "Epoch: 481/2000... Training loss: 1.1733\n",
      "Epoch: 481/2000... Training loss: 1.0981\n",
      "Epoch: 481/2000... Training loss: 1.3490\n",
      "Epoch: 481/2000... Training loss: 1.0260\n",
      "Epoch: 481/2000... Training loss: 1.3210\n",
      "Epoch: 481/2000... Training loss: 1.1004\n",
      "Epoch: 481/2000... Training loss: 1.4765\n",
      "Epoch: 481/2000... Training loss: 1.1318\n",
      "Epoch: 481/2000... Training loss: 1.1879\n",
      "Epoch: 481/2000... Training loss: 1.3675\n",
      "Epoch: 481/2000... Training loss: 1.1441\n",
      "Epoch: 482/2000... Training loss: 1.0135\n",
      "Epoch: 482/2000... Training loss: 1.0050\n",
      "Epoch: 482/2000... Training loss: 1.4537\n",
      "Epoch: 482/2000... Training loss: 1.2337\n",
      "Epoch: 482/2000... Training loss: 1.2153\n",
      "Epoch: 482/2000... Training loss: 0.9729\n",
      "Epoch: 482/2000... Training loss: 1.0356\n",
      "Epoch: 482/2000... Training loss: 1.1539\n",
      "Epoch: 482/2000... Training loss: 1.1148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 482/2000... Training loss: 1.0051\n",
      "Epoch: 482/2000... Training loss: 0.9937\n",
      "Epoch: 482/2000... Training loss: 1.1616\n",
      "Epoch: 482/2000... Training loss: 1.0968\n",
      "Epoch: 482/2000... Training loss: 1.0790\n",
      "Epoch: 482/2000... Training loss: 1.0825\n",
      "Epoch: 482/2000... Training loss: 1.0819\n",
      "Epoch: 482/2000... Training loss: 0.9769\n",
      "Epoch: 482/2000... Training loss: 1.0439\n",
      "Epoch: 482/2000... Training loss: 0.9574\n",
      "Epoch: 482/2000... Training loss: 0.9258\n",
      "Epoch: 482/2000... Training loss: 1.0992\n",
      "Epoch: 482/2000... Training loss: 1.1038\n",
      "Epoch: 482/2000... Training loss: 1.1334\n",
      "Epoch: 482/2000... Training loss: 1.0668\n",
      "Epoch: 482/2000... Training loss: 1.4935\n",
      "Epoch: 482/2000... Training loss: 1.1002\n",
      "Epoch: 482/2000... Training loss: 1.2248\n",
      "Epoch: 482/2000... Training loss: 1.2271\n",
      "Epoch: 482/2000... Training loss: 1.0774\n",
      "Epoch: 482/2000... Training loss: 1.1782\n",
      "Epoch: 482/2000... Training loss: 0.9908\n",
      "Epoch: 483/2000... Training loss: 1.0900\n",
      "Epoch: 483/2000... Training loss: 1.3618\n",
      "Epoch: 483/2000... Training loss: 0.9979\n",
      "Epoch: 483/2000... Training loss: 1.1194\n",
      "Epoch: 483/2000... Training loss: 0.9782\n",
      "Epoch: 483/2000... Training loss: 0.9200\n",
      "Epoch: 483/2000... Training loss: 1.1622\n",
      "Epoch: 483/2000... Training loss: 0.9534\n",
      "Epoch: 483/2000... Training loss: 1.0227\n",
      "Epoch: 483/2000... Training loss: 1.0665\n",
      "Epoch: 483/2000... Training loss: 1.0824\n",
      "Epoch: 483/2000... Training loss: 1.1936\n",
      "Epoch: 483/2000... Training loss: 1.0623\n",
      "Epoch: 483/2000... Training loss: 1.3619\n",
      "Epoch: 483/2000... Training loss: 1.0155\n",
      "Epoch: 483/2000... Training loss: 1.2458\n",
      "Epoch: 483/2000... Training loss: 1.1812\n",
      "Epoch: 483/2000... Training loss: 1.1995\n",
      "Epoch: 483/2000... Training loss: 1.1440\n",
      "Epoch: 483/2000... Training loss: 1.2717\n",
      "Epoch: 483/2000... Training loss: 1.2072\n",
      "Epoch: 483/2000... Training loss: 0.9232\n",
      "Epoch: 483/2000... Training loss: 1.0326\n",
      "Epoch: 483/2000... Training loss: 0.9034\n",
      "Epoch: 483/2000... Training loss: 0.9160\n",
      "Epoch: 483/2000... Training loss: 1.1322\n",
      "Epoch: 483/2000... Training loss: 1.1209\n",
      "Epoch: 483/2000... Training loss: 0.9046\n",
      "Epoch: 483/2000... Training loss: 1.0721\n",
      "Epoch: 483/2000... Training loss: 1.0613\n",
      "Epoch: 483/2000... Training loss: 0.9116\n",
      "Epoch: 484/2000... Training loss: 1.1027\n",
      "Epoch: 484/2000... Training loss: 1.2449\n",
      "Epoch: 484/2000... Training loss: 1.0185\n",
      "Epoch: 484/2000... Training loss: 1.0379\n",
      "Epoch: 484/2000... Training loss: 1.2122\n",
      "Epoch: 484/2000... Training loss: 1.0191\n",
      "Epoch: 484/2000... Training loss: 0.9537\n",
      "Epoch: 484/2000... Training loss: 1.0033\n",
      "Epoch: 484/2000... Training loss: 1.3535\n",
      "Epoch: 484/2000... Training loss: 1.0825\n",
      "Epoch: 484/2000... Training loss: 1.2471\n",
      "Epoch: 484/2000... Training loss: 1.1588\n",
      "Epoch: 484/2000... Training loss: 1.1396\n",
      "Epoch: 484/2000... Training loss: 0.9653\n",
      "Epoch: 484/2000... Training loss: 1.1111\n",
      "Epoch: 484/2000... Training loss: 1.0236\n",
      "Epoch: 484/2000... Training loss: 1.0677\n",
      "Epoch: 484/2000... Training loss: 0.9175\n",
      "Epoch: 484/2000... Training loss: 1.1167\n",
      "Epoch: 484/2000... Training loss: 1.1592\n",
      "Epoch: 484/2000... Training loss: 1.1141\n",
      "Epoch: 484/2000... Training loss: 1.3300\n",
      "Epoch: 484/2000... Training loss: 1.1765\n",
      "Epoch: 484/2000... Training loss: 1.1567\n",
      "Epoch: 484/2000... Training loss: 1.1018\n",
      "Epoch: 484/2000... Training loss: 0.9547\n",
      "Epoch: 484/2000... Training loss: 1.0406\n",
      "Epoch: 484/2000... Training loss: 1.3549\n",
      "Epoch: 484/2000... Training loss: 0.8310\n",
      "Epoch: 484/2000... Training loss: 0.9022\n",
      "Epoch: 484/2000... Training loss: 1.5648\n",
      "Epoch: 485/2000... Training loss: 1.4113\n",
      "Epoch: 485/2000... Training loss: 1.0152\n",
      "Epoch: 485/2000... Training loss: 1.3378\n",
      "Epoch: 485/2000... Training loss: 0.9839\n",
      "Epoch: 485/2000... Training loss: 1.3032\n",
      "Epoch: 485/2000... Training loss: 1.2323\n",
      "Epoch: 485/2000... Training loss: 1.0791\n",
      "Epoch: 485/2000... Training loss: 1.1615\n",
      "Epoch: 485/2000... Training loss: 1.0532\n",
      "Epoch: 485/2000... Training loss: 1.1271\n",
      "Epoch: 485/2000... Training loss: 1.2446\n",
      "Epoch: 485/2000... Training loss: 0.9416\n",
      "Epoch: 485/2000... Training loss: 1.1583\n",
      "Epoch: 485/2000... Training loss: 0.9921\n",
      "Epoch: 485/2000... Training loss: 1.2458\n",
      "Epoch: 485/2000... Training loss: 1.0506\n",
      "Epoch: 485/2000... Training loss: 1.0124\n",
      "Epoch: 485/2000... Training loss: 0.9258\n",
      "Epoch: 485/2000... Training loss: 1.0417\n",
      "Epoch: 485/2000... Training loss: 1.1395\n",
      "Epoch: 485/2000... Training loss: 1.0084\n",
      "Epoch: 485/2000... Training loss: 1.0618\n",
      "Epoch: 485/2000... Training loss: 1.2513\n",
      "Epoch: 485/2000... Training loss: 1.2262\n",
      "Epoch: 485/2000... Training loss: 1.2443\n",
      "Epoch: 485/2000... Training loss: 1.0744\n",
      "Epoch: 485/2000... Training loss: 1.0894\n",
      "Epoch: 485/2000... Training loss: 1.1944\n",
      "Epoch: 485/2000... Training loss: 0.9069\n",
      "Epoch: 485/2000... Training loss: 1.2290\n",
      "Epoch: 485/2000... Training loss: 1.3090\n",
      "Epoch: 486/2000... Training loss: 1.3453\n",
      "Epoch: 486/2000... Training loss: 1.1236\n",
      "Epoch: 486/2000... Training loss: 1.0266\n",
      "Epoch: 486/2000... Training loss: 1.0433\n",
      "Epoch: 486/2000... Training loss: 1.1003\n",
      "Epoch: 486/2000... Training loss: 0.7797\n",
      "Epoch: 486/2000... Training loss: 1.1385\n",
      "Epoch: 486/2000... Training loss: 0.9304\n",
      "Epoch: 486/2000... Training loss: 1.0704\n",
      "Epoch: 486/2000... Training loss: 1.3768\n",
      "Epoch: 486/2000... Training loss: 1.3165\n",
      "Epoch: 486/2000... Training loss: 1.1432\n",
      "Epoch: 486/2000... Training loss: 0.9532\n",
      "Epoch: 486/2000... Training loss: 1.2801\n",
      "Epoch: 486/2000... Training loss: 1.0523\n",
      "Epoch: 486/2000... Training loss: 1.0657\n",
      "Epoch: 486/2000... Training loss: 1.1682\n",
      "Epoch: 486/2000... Training loss: 1.0373\n",
      "Epoch: 486/2000... Training loss: 0.9991\n",
      "Epoch: 486/2000... Training loss: 1.1506\n",
      "Epoch: 486/2000... Training loss: 1.4642\n",
      "Epoch: 486/2000... Training loss: 1.2098\n",
      "Epoch: 486/2000... Training loss: 1.2001\n",
      "Epoch: 486/2000... Training loss: 1.1737\n",
      "Epoch: 486/2000... Training loss: 1.0265\n",
      "Epoch: 486/2000... Training loss: 1.1306\n",
      "Epoch: 486/2000... Training loss: 0.9353\n",
      "Epoch: 486/2000... Training loss: 1.1250\n",
      "Epoch: 486/2000... Training loss: 1.0162\n",
      "Epoch: 486/2000... Training loss: 1.0840\n",
      "Epoch: 486/2000... Training loss: 1.0154\n",
      "Epoch: 487/2000... Training loss: 1.1588\n",
      "Epoch: 487/2000... Training loss: 1.4627\n",
      "Epoch: 487/2000... Training loss: 0.9708\n",
      "Epoch: 487/2000... Training loss: 1.1862\n",
      "Epoch: 487/2000... Training loss: 1.1498\n",
      "Epoch: 487/2000... Training loss: 1.0877\n",
      "Epoch: 487/2000... Training loss: 1.1581\n",
      "Epoch: 487/2000... Training loss: 1.0657\n",
      "Epoch: 487/2000... Training loss: 1.3152\n",
      "Epoch: 487/2000... Training loss: 1.1320\n",
      "Epoch: 487/2000... Training loss: 1.4647\n",
      "Epoch: 487/2000... Training loss: 1.0571\n",
      "Epoch: 487/2000... Training loss: 0.9745\n",
      "Epoch: 487/2000... Training loss: 1.1861\n",
      "Epoch: 487/2000... Training loss: 1.0145\n",
      "Epoch: 487/2000... Training loss: 1.0119\n",
      "Epoch: 487/2000... Training loss: 1.0224\n",
      "Epoch: 487/2000... Training loss: 1.2254\n",
      "Epoch: 487/2000... Training loss: 1.2062\n",
      "Epoch: 487/2000... Training loss: 1.0473\n",
      "Epoch: 487/2000... Training loss: 1.1065\n",
      "Epoch: 487/2000... Training loss: 1.1493\n",
      "Epoch: 487/2000... Training loss: 1.2222\n",
      "Epoch: 487/2000... Training loss: 1.1915\n",
      "Epoch: 487/2000... Training loss: 1.2648\n",
      "Epoch: 487/2000... Training loss: 1.0999\n",
      "Epoch: 487/2000... Training loss: 1.0892\n",
      "Epoch: 487/2000... Training loss: 1.2295\n",
      "Epoch: 487/2000... Training loss: 1.0846\n",
      "Epoch: 487/2000... Training loss: 1.1652\n",
      "Epoch: 487/2000... Training loss: 1.0150\n",
      "Epoch: 488/2000... Training loss: 0.9632\n",
      "Epoch: 488/2000... Training loss: 1.0613\n",
      "Epoch: 488/2000... Training loss: 0.9719\n",
      "Epoch: 488/2000... Training loss: 0.9238\n",
      "Epoch: 488/2000... Training loss: 1.0300\n",
      "Epoch: 488/2000... Training loss: 1.1210\n",
      "Epoch: 488/2000... Training loss: 1.1468\n",
      "Epoch: 488/2000... Training loss: 1.1431\n",
      "Epoch: 488/2000... Training loss: 1.1527\n",
      "Epoch: 488/2000... Training loss: 1.1726\n",
      "Epoch: 488/2000... Training loss: 0.9445\n",
      "Epoch: 488/2000... Training loss: 1.0602\n",
      "Epoch: 488/2000... Training loss: 0.9882\n",
      "Epoch: 488/2000... Training loss: 1.2954\n",
      "Epoch: 488/2000... Training loss: 1.1335\n",
      "Epoch: 488/2000... Training loss: 0.9486\n",
      "Epoch: 488/2000... Training loss: 1.2437\n",
      "Epoch: 488/2000... Training loss: 1.0149\n",
      "Epoch: 488/2000... Training loss: 1.0161\n",
      "Epoch: 488/2000... Training loss: 1.0754\n",
      "Epoch: 488/2000... Training loss: 1.1455\n",
      "Epoch: 488/2000... Training loss: 0.8839\n",
      "Epoch: 488/2000... Training loss: 1.1882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 488/2000... Training loss: 1.1564\n",
      "Epoch: 488/2000... Training loss: 1.2536\n",
      "Epoch: 488/2000... Training loss: 1.1099\n",
      "Epoch: 488/2000... Training loss: 1.0306\n",
      "Epoch: 488/2000... Training loss: 1.1311\n",
      "Epoch: 488/2000... Training loss: 0.9470\n",
      "Epoch: 488/2000... Training loss: 1.0656\n",
      "Epoch: 488/2000... Training loss: 0.9865\n",
      "Epoch: 489/2000... Training loss: 1.0331\n",
      "Epoch: 489/2000... Training loss: 1.1802\n",
      "Epoch: 489/2000... Training loss: 1.1026\n",
      "Epoch: 489/2000... Training loss: 1.1808\n",
      "Epoch: 489/2000... Training loss: 1.1707\n",
      "Epoch: 489/2000... Training loss: 1.1221\n",
      "Epoch: 489/2000... Training loss: 1.2098\n",
      "Epoch: 489/2000... Training loss: 1.3017\n",
      "Epoch: 489/2000... Training loss: 1.0266\n",
      "Epoch: 489/2000... Training loss: 1.2887\n",
      "Epoch: 489/2000... Training loss: 1.1428\n",
      "Epoch: 489/2000... Training loss: 1.5374\n",
      "Epoch: 489/2000... Training loss: 1.1527\n",
      "Epoch: 489/2000... Training loss: 1.1336\n",
      "Epoch: 489/2000... Training loss: 1.1397\n",
      "Epoch: 489/2000... Training loss: 1.1990\n",
      "Epoch: 489/2000... Training loss: 1.2476\n",
      "Epoch: 489/2000... Training loss: 1.0782\n",
      "Epoch: 489/2000... Training loss: 1.1501\n",
      "Epoch: 489/2000... Training loss: 1.0892\n",
      "Epoch: 489/2000... Training loss: 0.9275\n",
      "Epoch: 489/2000... Training loss: 1.2193\n",
      "Epoch: 489/2000... Training loss: 1.0745\n",
      "Epoch: 489/2000... Training loss: 0.8343\n",
      "Epoch: 489/2000... Training loss: 1.0475\n",
      "Epoch: 489/2000... Training loss: 1.2600\n",
      "Epoch: 489/2000... Training loss: 1.0972\n",
      "Epoch: 489/2000... Training loss: 1.3495\n",
      "Epoch: 489/2000... Training loss: 1.2391\n",
      "Epoch: 489/2000... Training loss: 1.1645\n",
      "Epoch: 489/2000... Training loss: 1.0371\n",
      "Epoch: 490/2000... Training loss: 1.2550\n",
      "Epoch: 490/2000... Training loss: 1.3893\n",
      "Epoch: 490/2000... Training loss: 1.3512\n",
      "Epoch: 490/2000... Training loss: 1.2514\n",
      "Epoch: 490/2000... Training loss: 1.3049\n",
      "Epoch: 490/2000... Training loss: 1.3663\n",
      "Epoch: 490/2000... Training loss: 1.0832\n",
      "Epoch: 490/2000... Training loss: 1.1352\n",
      "Epoch: 490/2000... Training loss: 0.8650\n",
      "Epoch: 490/2000... Training loss: 1.1899\n",
      "Epoch: 490/2000... Training loss: 1.1740\n",
      "Epoch: 490/2000... Training loss: 1.1225\n",
      "Epoch: 490/2000... Training loss: 1.1846\n",
      "Epoch: 490/2000... Training loss: 1.0275\n",
      "Epoch: 490/2000... Training loss: 0.9278\n",
      "Epoch: 490/2000... Training loss: 1.2963\n",
      "Epoch: 490/2000... Training loss: 1.1948\n",
      "Epoch: 490/2000... Training loss: 0.8296\n",
      "Epoch: 490/2000... Training loss: 1.2587\n",
      "Epoch: 490/2000... Training loss: 1.2349\n",
      "Epoch: 490/2000... Training loss: 1.1027\n",
      "Epoch: 490/2000... Training loss: 1.0866\n",
      "Epoch: 490/2000... Training loss: 1.0298\n",
      "Epoch: 490/2000... Training loss: 1.1331\n",
      "Epoch: 490/2000... Training loss: 1.0379\n",
      "Epoch: 490/2000... Training loss: 1.3142\n",
      "Epoch: 490/2000... Training loss: 0.8907\n",
      "Epoch: 490/2000... Training loss: 0.9935\n",
      "Epoch: 490/2000... Training loss: 1.0467\n",
      "Epoch: 490/2000... Training loss: 1.3149\n",
      "Epoch: 490/2000... Training loss: 1.4284\n",
      "Epoch: 491/2000... Training loss: 1.1173\n",
      "Epoch: 491/2000... Training loss: 0.9991\n",
      "Epoch: 491/2000... Training loss: 0.8695\n",
      "Epoch: 491/2000... Training loss: 1.2645\n",
      "Epoch: 491/2000... Training loss: 1.0112\n",
      "Epoch: 491/2000... Training loss: 1.1199\n",
      "Epoch: 491/2000... Training loss: 1.1421\n",
      "Epoch: 491/2000... Training loss: 0.9928\n",
      "Epoch: 491/2000... Training loss: 1.1955\n",
      "Epoch: 491/2000... Training loss: 0.9324\n",
      "Epoch: 491/2000... Training loss: 0.9966\n",
      "Epoch: 491/2000... Training loss: 1.2486\n",
      "Epoch: 491/2000... Training loss: 1.1986\n",
      "Epoch: 491/2000... Training loss: 1.0619\n",
      "Epoch: 491/2000... Training loss: 1.2768\n",
      "Epoch: 491/2000... Training loss: 0.9432\n",
      "Epoch: 491/2000... Training loss: 1.1378\n",
      "Epoch: 491/2000... Training loss: 1.3477\n",
      "Epoch: 491/2000... Training loss: 0.8942\n",
      "Epoch: 491/2000... Training loss: 1.1025\n",
      "Epoch: 491/2000... Training loss: 1.0642\n",
      "Epoch: 491/2000... Training loss: 0.9536\n",
      "Epoch: 491/2000... Training loss: 1.1743\n",
      "Epoch: 491/2000... Training loss: 1.0392\n",
      "Epoch: 491/2000... Training loss: 1.0984\n",
      "Epoch: 491/2000... Training loss: 0.8890\n",
      "Epoch: 491/2000... Training loss: 1.4693\n",
      "Epoch: 491/2000... Training loss: 1.0721\n",
      "Epoch: 491/2000... Training loss: 0.9217\n",
      "Epoch: 491/2000... Training loss: 1.0961\n",
      "Epoch: 491/2000... Training loss: 1.0175\n",
      "Epoch: 492/2000... Training loss: 1.0353\n",
      "Epoch: 492/2000... Training loss: 1.1750\n",
      "Epoch: 492/2000... Training loss: 1.2369\n",
      "Epoch: 492/2000... Training loss: 1.3244\n",
      "Epoch: 492/2000... Training loss: 1.1691\n",
      "Epoch: 492/2000... Training loss: 1.1730\n",
      "Epoch: 492/2000... Training loss: 1.0687\n",
      "Epoch: 492/2000... Training loss: 1.1860\n",
      "Epoch: 492/2000... Training loss: 1.3551\n",
      "Epoch: 492/2000... Training loss: 1.0178\n",
      "Epoch: 492/2000... Training loss: 1.3056\n",
      "Epoch: 492/2000... Training loss: 1.0867\n",
      "Epoch: 492/2000... Training loss: 1.1672\n",
      "Epoch: 492/2000... Training loss: 1.3421\n",
      "Epoch: 492/2000... Training loss: 1.0139\n",
      "Epoch: 492/2000... Training loss: 1.2536\n",
      "Epoch: 492/2000... Training loss: 1.1171\n",
      "Epoch: 492/2000... Training loss: 0.9392\n",
      "Epoch: 492/2000... Training loss: 1.1152\n",
      "Epoch: 492/2000... Training loss: 0.9473\n",
      "Epoch: 492/2000... Training loss: 0.9914\n",
      "Epoch: 492/2000... Training loss: 1.0721\n",
      "Epoch: 492/2000... Training loss: 1.2591\n",
      "Epoch: 492/2000... Training loss: 1.1731\n",
      "Epoch: 492/2000... Training loss: 0.9892\n",
      "Epoch: 492/2000... Training loss: 1.2557\n",
      "Epoch: 492/2000... Training loss: 1.1950\n",
      "Epoch: 492/2000... Training loss: 1.2677\n",
      "Epoch: 492/2000... Training loss: 1.0399\n",
      "Epoch: 492/2000... Training loss: 1.0682\n",
      "Epoch: 492/2000... Training loss: 1.1366\n",
      "Epoch: 493/2000... Training loss: 1.0185\n",
      "Epoch: 493/2000... Training loss: 0.9973\n",
      "Epoch: 493/2000... Training loss: 1.0273\n",
      "Epoch: 493/2000... Training loss: 1.1558\n",
      "Epoch: 493/2000... Training loss: 0.9788\n",
      "Epoch: 493/2000... Training loss: 1.4041\n",
      "Epoch: 493/2000... Training loss: 1.1895\n",
      "Epoch: 493/2000... Training loss: 1.0645\n",
      "Epoch: 493/2000... Training loss: 1.1208\n",
      "Epoch: 493/2000... Training loss: 1.2795\n",
      "Epoch: 493/2000... Training loss: 1.3054\n",
      "Epoch: 493/2000... Training loss: 1.1437\n",
      "Epoch: 493/2000... Training loss: 1.2412\n",
      "Epoch: 493/2000... Training loss: 1.1990\n",
      "Epoch: 493/2000... Training loss: 1.2210\n",
      "Epoch: 493/2000... Training loss: 1.1561\n",
      "Epoch: 493/2000... Training loss: 1.0272\n",
      "Epoch: 493/2000... Training loss: 0.9655\n",
      "Epoch: 493/2000... Training loss: 1.2077\n",
      "Epoch: 493/2000... Training loss: 1.1250\n",
      "Epoch: 493/2000... Training loss: 0.9790\n",
      "Epoch: 493/2000... Training loss: 1.0314\n",
      "Epoch: 493/2000... Training loss: 0.9281\n",
      "Epoch: 493/2000... Training loss: 1.1803\n",
      "Epoch: 493/2000... Training loss: 1.2537\n",
      "Epoch: 493/2000... Training loss: 1.1460\n",
      "Epoch: 493/2000... Training loss: 1.1775\n",
      "Epoch: 493/2000... Training loss: 1.2686\n",
      "Epoch: 493/2000... Training loss: 0.8784\n",
      "Epoch: 493/2000... Training loss: 1.0349\n",
      "Epoch: 493/2000... Training loss: 1.0752\n",
      "Epoch: 494/2000... Training loss: 1.2068\n",
      "Epoch: 494/2000... Training loss: 1.2918\n",
      "Epoch: 494/2000... Training loss: 0.7702\n",
      "Epoch: 494/2000... Training loss: 1.0240\n",
      "Epoch: 494/2000... Training loss: 1.1581\n",
      "Epoch: 494/2000... Training loss: 1.0213\n",
      "Epoch: 494/2000... Training loss: 1.0484\n",
      "Epoch: 494/2000... Training loss: 1.1669\n",
      "Epoch: 494/2000... Training loss: 1.1098\n",
      "Epoch: 494/2000... Training loss: 1.1550\n",
      "Epoch: 494/2000... Training loss: 1.1869\n",
      "Epoch: 494/2000... Training loss: 1.2123\n",
      "Epoch: 494/2000... Training loss: 1.3673\n",
      "Epoch: 494/2000... Training loss: 1.1482\n",
      "Epoch: 494/2000... Training loss: 1.0353\n",
      "Epoch: 494/2000... Training loss: 1.0678\n",
      "Epoch: 494/2000... Training loss: 1.1708\n",
      "Epoch: 494/2000... Training loss: 1.2594\n",
      "Epoch: 494/2000... Training loss: 0.9982\n",
      "Epoch: 494/2000... Training loss: 1.0940\n",
      "Epoch: 494/2000... Training loss: 0.9400\n",
      "Epoch: 494/2000... Training loss: 1.1129\n",
      "Epoch: 494/2000... Training loss: 1.0945\n",
      "Epoch: 494/2000... Training loss: 1.1890\n",
      "Epoch: 494/2000... Training loss: 1.3724\n",
      "Epoch: 494/2000... Training loss: 1.0696\n",
      "Epoch: 494/2000... Training loss: 1.0222\n",
      "Epoch: 494/2000... Training loss: 1.0021\n",
      "Epoch: 494/2000... Training loss: 1.1808\n",
      "Epoch: 494/2000... Training loss: 0.9586\n",
      "Epoch: 494/2000... Training loss: 0.9631\n",
      "Epoch: 495/2000... Training loss: 0.8749\n",
      "Epoch: 495/2000... Training loss: 0.9452\n",
      "Epoch: 495/2000... Training loss: 1.2567\n",
      "Epoch: 495/2000... Training loss: 1.0123\n",
      "Epoch: 495/2000... Training loss: 0.8193\n",
      "Epoch: 495/2000... Training loss: 1.1511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 495/2000... Training loss: 1.2476\n",
      "Epoch: 495/2000... Training loss: 1.2622\n",
      "Epoch: 495/2000... Training loss: 0.9729\n",
      "Epoch: 495/2000... Training loss: 1.0732\n",
      "Epoch: 495/2000... Training loss: 0.8876\n",
      "Epoch: 495/2000... Training loss: 0.8920\n",
      "Epoch: 495/2000... Training loss: 1.3122\n",
      "Epoch: 495/2000... Training loss: 1.1597\n",
      "Epoch: 495/2000... Training loss: 1.0520\n",
      "Epoch: 495/2000... Training loss: 1.2473\n",
      "Epoch: 495/2000... Training loss: 1.0266\n",
      "Epoch: 495/2000... Training loss: 1.1567\n",
      "Epoch: 495/2000... Training loss: 0.9138\n",
      "Epoch: 495/2000... Training loss: 0.9629\n",
      "Epoch: 495/2000... Training loss: 1.1736\n",
      "Epoch: 495/2000... Training loss: 1.2349\n",
      "Epoch: 495/2000... Training loss: 1.1935\n",
      "Epoch: 495/2000... Training loss: 1.1149\n",
      "Epoch: 495/2000... Training loss: 1.0981\n",
      "Epoch: 495/2000... Training loss: 1.1469\n",
      "Epoch: 495/2000... Training loss: 1.1002\n",
      "Epoch: 495/2000... Training loss: 1.0682\n",
      "Epoch: 495/2000... Training loss: 1.0003\n",
      "Epoch: 495/2000... Training loss: 1.0709\n",
      "Epoch: 495/2000... Training loss: 1.1695\n",
      "Epoch: 496/2000... Training loss: 1.2472\n",
      "Epoch: 496/2000... Training loss: 1.2532\n",
      "Epoch: 496/2000... Training loss: 1.0898\n",
      "Epoch: 496/2000... Training loss: 0.9956\n",
      "Epoch: 496/2000... Training loss: 0.9827\n",
      "Epoch: 496/2000... Training loss: 0.8646\n",
      "Epoch: 496/2000... Training loss: 1.0965\n",
      "Epoch: 496/2000... Training loss: 1.0586\n",
      "Epoch: 496/2000... Training loss: 0.8350\n",
      "Epoch: 496/2000... Training loss: 0.9208\n",
      "Epoch: 496/2000... Training loss: 1.0306\n",
      "Epoch: 496/2000... Training loss: 1.0612\n",
      "Epoch: 496/2000... Training loss: 0.9913\n",
      "Epoch: 496/2000... Training loss: 0.9479\n",
      "Epoch: 496/2000... Training loss: 0.9878\n",
      "Epoch: 496/2000... Training loss: 1.1695\n",
      "Epoch: 496/2000... Training loss: 1.3265\n",
      "Epoch: 496/2000... Training loss: 1.2869\n",
      "Epoch: 496/2000... Training loss: 1.1449\n",
      "Epoch: 496/2000... Training loss: 1.1411\n",
      "Epoch: 496/2000... Training loss: 1.1206\n",
      "Epoch: 496/2000... Training loss: 0.9171\n",
      "Epoch: 496/2000... Training loss: 0.8251\n",
      "Epoch: 496/2000... Training loss: 1.1762\n",
      "Epoch: 496/2000... Training loss: 1.1284\n",
      "Epoch: 496/2000... Training loss: 1.0662\n",
      "Epoch: 496/2000... Training loss: 1.1225\n",
      "Epoch: 496/2000... Training loss: 0.9442\n",
      "Epoch: 496/2000... Training loss: 1.0031\n",
      "Epoch: 496/2000... Training loss: 1.1035\n",
      "Epoch: 496/2000... Training loss: 1.1134\n",
      "Epoch: 497/2000... Training loss: 1.0511\n",
      "Epoch: 497/2000... Training loss: 1.0440\n",
      "Epoch: 497/2000... Training loss: 0.9278\n",
      "Epoch: 497/2000... Training loss: 1.0806\n",
      "Epoch: 497/2000... Training loss: 1.1107\n",
      "Epoch: 497/2000... Training loss: 0.9210\n",
      "Epoch: 497/2000... Training loss: 0.8601\n",
      "Epoch: 497/2000... Training loss: 1.0581\n",
      "Epoch: 497/2000... Training loss: 1.2944\n",
      "Epoch: 497/2000... Training loss: 1.1017\n",
      "Epoch: 497/2000... Training loss: 1.0909\n",
      "Epoch: 497/2000... Training loss: 0.9633\n",
      "Epoch: 497/2000... Training loss: 0.9414\n",
      "Epoch: 497/2000... Training loss: 1.1811\n",
      "Epoch: 497/2000... Training loss: 1.0297\n",
      "Epoch: 497/2000... Training loss: 1.1983\n",
      "Epoch: 497/2000... Training loss: 1.0509\n",
      "Epoch: 497/2000... Training loss: 0.8932\n",
      "Epoch: 497/2000... Training loss: 0.9773\n",
      "Epoch: 497/2000... Training loss: 1.1562\n",
      "Epoch: 497/2000... Training loss: 1.0624\n",
      "Epoch: 497/2000... Training loss: 1.0465\n",
      "Epoch: 497/2000... Training loss: 1.3033\n",
      "Epoch: 497/2000... Training loss: 1.0035\n",
      "Epoch: 497/2000... Training loss: 1.2432\n",
      "Epoch: 497/2000... Training loss: 1.0518\n",
      "Epoch: 497/2000... Training loss: 1.1755\n",
      "Epoch: 497/2000... Training loss: 1.1801\n",
      "Epoch: 497/2000... Training loss: 0.9464\n",
      "Epoch: 497/2000... Training loss: 1.1041\n",
      "Epoch: 497/2000... Training loss: 0.9682\n",
      "Epoch: 498/2000... Training loss: 1.1072\n",
      "Epoch: 498/2000... Training loss: 0.9861\n",
      "Epoch: 498/2000... Training loss: 1.0270\n",
      "Epoch: 498/2000... Training loss: 1.1115\n",
      "Epoch: 498/2000... Training loss: 0.9991\n",
      "Epoch: 498/2000... Training loss: 1.1015\n",
      "Epoch: 498/2000... Training loss: 1.1796\n",
      "Epoch: 498/2000... Training loss: 1.1478\n",
      "Epoch: 498/2000... Training loss: 1.0141\n",
      "Epoch: 498/2000... Training loss: 1.0866\n",
      "Epoch: 498/2000... Training loss: 1.0748\n",
      "Epoch: 498/2000... Training loss: 1.2445\n",
      "Epoch: 498/2000... Training loss: 0.9212\n",
      "Epoch: 498/2000... Training loss: 0.9825\n",
      "Epoch: 498/2000... Training loss: 0.9565\n",
      "Epoch: 498/2000... Training loss: 1.0269\n",
      "Epoch: 498/2000... Training loss: 1.0694\n",
      "Epoch: 498/2000... Training loss: 1.0973\n",
      "Epoch: 498/2000... Training loss: 0.9384\n",
      "Epoch: 498/2000... Training loss: 1.0718\n",
      "Epoch: 498/2000... Training loss: 1.3585\n",
      "Epoch: 498/2000... Training loss: 0.9683\n",
      "Epoch: 498/2000... Training loss: 1.1763\n",
      "Epoch: 498/2000... Training loss: 1.3414\n",
      "Epoch: 498/2000... Training loss: 1.0686\n",
      "Epoch: 498/2000... Training loss: 0.9100\n",
      "Epoch: 498/2000... Training loss: 1.3324\n",
      "Epoch: 498/2000... Training loss: 1.0249\n",
      "Epoch: 498/2000... Training loss: 0.9699\n",
      "Epoch: 498/2000... Training loss: 1.0676\n",
      "Epoch: 498/2000... Training loss: 1.4083\n",
      "Epoch: 499/2000... Training loss: 1.0734\n",
      "Epoch: 499/2000... Training loss: 1.2162\n",
      "Epoch: 499/2000... Training loss: 1.1462\n",
      "Epoch: 499/2000... Training loss: 0.9031\n",
      "Epoch: 499/2000... Training loss: 0.8659\n",
      "Epoch: 499/2000... Training loss: 0.8675\n",
      "Epoch: 499/2000... Training loss: 1.1789\n",
      "Epoch: 499/2000... Training loss: 1.1449\n",
      "Epoch: 499/2000... Training loss: 0.8790\n",
      "Epoch: 499/2000... Training loss: 1.0404\n",
      "Epoch: 499/2000... Training loss: 0.9488\n",
      "Epoch: 499/2000... Training loss: 0.8869\n",
      "Epoch: 499/2000... Training loss: 1.1034\n",
      "Epoch: 499/2000... Training loss: 1.2283\n",
      "Epoch: 499/2000... Training loss: 1.4483\n",
      "Epoch: 499/2000... Training loss: 0.9433\n",
      "Epoch: 499/2000... Training loss: 0.9012\n",
      "Epoch: 499/2000... Training loss: 1.2121\n",
      "Epoch: 499/2000... Training loss: 1.1789\n",
      "Epoch: 499/2000... Training loss: 1.3117\n",
      "Epoch: 499/2000... Training loss: 1.1360\n",
      "Epoch: 499/2000... Training loss: 1.1422\n",
      "Epoch: 499/2000... Training loss: 1.0256\n",
      "Epoch: 499/2000... Training loss: 1.0339\n",
      "Epoch: 499/2000... Training loss: 1.1068\n",
      "Epoch: 499/2000... Training loss: 1.0377\n",
      "Epoch: 499/2000... Training loss: 0.8686\n",
      "Epoch: 499/2000... Training loss: 1.1687\n",
      "Epoch: 499/2000... Training loss: 0.9735\n",
      "Epoch: 499/2000... Training loss: 1.1586\n",
      "Epoch: 499/2000... Training loss: 1.1089\n",
      "Epoch: 500/2000... Training loss: 1.0119\n",
      "Epoch: 500/2000... Training loss: 1.1611\n",
      "Epoch: 500/2000... Training loss: 0.9359\n",
      "Epoch: 500/2000... Training loss: 1.1704\n",
      "Epoch: 500/2000... Training loss: 1.0626\n",
      "Epoch: 500/2000... Training loss: 1.0889\n",
      "Epoch: 500/2000... Training loss: 1.0969\n",
      "Epoch: 500/2000... Training loss: 1.1951\n",
      "Epoch: 500/2000... Training loss: 0.9607\n",
      "Epoch: 500/2000... Training loss: 1.1049\n",
      "Epoch: 500/2000... Training loss: 1.1592\n",
      "Epoch: 500/2000... Training loss: 0.8719\n",
      "Epoch: 500/2000... Training loss: 1.0729\n",
      "Epoch: 500/2000... Training loss: 1.0859\n",
      "Epoch: 500/2000... Training loss: 1.1032\n",
      "Epoch: 500/2000... Training loss: 1.2820\n",
      "Epoch: 500/2000... Training loss: 1.0090\n",
      "Epoch: 500/2000... Training loss: 1.1188\n",
      "Epoch: 500/2000... Training loss: 0.9125\n",
      "Epoch: 500/2000... Training loss: 1.2159\n",
      "Epoch: 500/2000... Training loss: 0.9851\n",
      "Epoch: 500/2000... Training loss: 0.9907\n",
      "Epoch: 500/2000... Training loss: 0.9789\n",
      "Epoch: 500/2000... Training loss: 1.1345\n",
      "Epoch: 500/2000... Training loss: 1.0555\n",
      "Epoch: 500/2000... Training loss: 0.9143\n",
      "Epoch: 500/2000... Training loss: 1.0254\n",
      "Epoch: 500/2000... Training loss: 1.2880\n",
      "Epoch: 500/2000... Training loss: 1.2825\n",
      "Epoch: 500/2000... Training loss: 1.4094\n",
      "Epoch: 500/2000... Training loss: 1.0846\n",
      "Epoch: 501/2000... Training loss: 1.0937\n",
      "Epoch: 501/2000... Training loss: 0.9092\n",
      "Epoch: 501/2000... Training loss: 1.3347\n",
      "Epoch: 501/2000... Training loss: 1.0461\n",
      "Epoch: 501/2000... Training loss: 0.8070\n",
      "Epoch: 501/2000... Training loss: 1.1468\n",
      "Epoch: 501/2000... Training loss: 1.2228\n",
      "Epoch: 501/2000... Training loss: 1.1435\n",
      "Epoch: 501/2000... Training loss: 1.1356\n",
      "Epoch: 501/2000... Training loss: 0.9912\n",
      "Epoch: 501/2000... Training loss: 1.1146\n",
      "Epoch: 501/2000... Training loss: 1.1108\n",
      "Epoch: 501/2000... Training loss: 0.9111\n",
      "Epoch: 501/2000... Training loss: 0.9224\n",
      "Epoch: 501/2000... Training loss: 0.9840\n",
      "Epoch: 501/2000... Training loss: 0.9275\n",
      "Epoch: 501/2000... Training loss: 0.9613\n",
      "Epoch: 501/2000... Training loss: 0.8926\n",
      "Epoch: 501/2000... Training loss: 1.0154\n",
      "Epoch: 501/2000... Training loss: 1.1021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 501/2000... Training loss: 1.0849\n",
      "Epoch: 501/2000... Training loss: 1.2438\n",
      "Epoch: 501/2000... Training loss: 1.1776\n",
      "Epoch: 501/2000... Training loss: 1.1119\n",
      "Epoch: 501/2000... Training loss: 1.0404\n",
      "Epoch: 501/2000... Training loss: 0.9853\n",
      "Epoch: 501/2000... Training loss: 1.3525\n",
      "Epoch: 501/2000... Training loss: 1.2203\n",
      "Epoch: 501/2000... Training loss: 0.9448\n",
      "Epoch: 501/2000... Training loss: 0.9497\n",
      "Epoch: 501/2000... Training loss: 1.2864\n",
      "Epoch: 502/2000... Training loss: 1.1747\n",
      "Epoch: 502/2000... Training loss: 1.3203\n",
      "Epoch: 502/2000... Training loss: 0.9305\n",
      "Epoch: 502/2000... Training loss: 0.9047\n",
      "Epoch: 502/2000... Training loss: 1.2153\n",
      "Epoch: 502/2000... Training loss: 0.9409\n",
      "Epoch: 502/2000... Training loss: 1.2196\n",
      "Epoch: 502/2000... Training loss: 1.1180\n",
      "Epoch: 502/2000... Training loss: 1.2027\n",
      "Epoch: 502/2000... Training loss: 1.0748\n",
      "Epoch: 502/2000... Training loss: 1.0772\n",
      "Epoch: 502/2000... Training loss: 0.8838\n",
      "Epoch: 502/2000... Training loss: 1.2015\n",
      "Epoch: 502/2000... Training loss: 1.2126\n",
      "Epoch: 502/2000... Training loss: 1.2048\n",
      "Epoch: 502/2000... Training loss: 0.8972\n",
      "Epoch: 502/2000... Training loss: 1.0833\n",
      "Epoch: 502/2000... Training loss: 0.9968\n",
      "Epoch: 502/2000... Training loss: 1.2138\n",
      "Epoch: 502/2000... Training loss: 1.0339\n",
      "Epoch: 502/2000... Training loss: 0.9463\n",
      "Epoch: 502/2000... Training loss: 1.1633\n",
      "Epoch: 502/2000... Training loss: 1.1301\n",
      "Epoch: 502/2000... Training loss: 1.0750\n",
      "Epoch: 502/2000... Training loss: 1.0330\n",
      "Epoch: 502/2000... Training loss: 1.1607\n",
      "Epoch: 502/2000... Training loss: 1.0767\n",
      "Epoch: 502/2000... Training loss: 1.1111\n",
      "Epoch: 502/2000... Training loss: 1.1685\n",
      "Epoch: 502/2000... Training loss: 1.1132\n",
      "Epoch: 502/2000... Training loss: 1.1165\n",
      "Epoch: 503/2000... Training loss: 1.1237\n",
      "Epoch: 503/2000... Training loss: 1.0882\n",
      "Epoch: 503/2000... Training loss: 1.0606\n",
      "Epoch: 503/2000... Training loss: 1.0508\n",
      "Epoch: 503/2000... Training loss: 1.0965\n",
      "Epoch: 503/2000... Training loss: 0.8603\n",
      "Epoch: 503/2000... Training loss: 1.1184\n",
      "Epoch: 503/2000... Training loss: 1.2462\n",
      "Epoch: 503/2000... Training loss: 1.1602\n",
      "Epoch: 503/2000... Training loss: 1.0241\n",
      "Epoch: 503/2000... Training loss: 1.0260\n",
      "Epoch: 503/2000... Training loss: 1.2254\n",
      "Epoch: 503/2000... Training loss: 1.1419\n",
      "Epoch: 503/2000... Training loss: 0.9478\n",
      "Epoch: 503/2000... Training loss: 1.2520\n",
      "Epoch: 503/2000... Training loss: 0.8578\n",
      "Epoch: 503/2000... Training loss: 1.0525\n",
      "Epoch: 503/2000... Training loss: 1.0029\n",
      "Epoch: 503/2000... Training loss: 1.0247\n",
      "Epoch: 503/2000... Training loss: 0.9990\n",
      "Epoch: 503/2000... Training loss: 0.7618\n",
      "Epoch: 503/2000... Training loss: 0.9611\n",
      "Epoch: 503/2000... Training loss: 1.2858\n",
      "Epoch: 503/2000... Training loss: 0.9768\n",
      "Epoch: 503/2000... Training loss: 1.0423\n",
      "Epoch: 503/2000... Training loss: 1.2407\n",
      "Epoch: 503/2000... Training loss: 1.2568\n",
      "Epoch: 503/2000... Training loss: 1.1029\n",
      "Epoch: 503/2000... Training loss: 1.0380\n",
      "Epoch: 503/2000... Training loss: 0.9603\n",
      "Epoch: 503/2000... Training loss: 1.0829\n",
      "Epoch: 504/2000... Training loss: 1.1213\n",
      "Epoch: 504/2000... Training loss: 1.1074\n",
      "Epoch: 504/2000... Training loss: 0.9292\n",
      "Epoch: 504/2000... Training loss: 1.2460\n",
      "Epoch: 504/2000... Training loss: 0.8769\n",
      "Epoch: 504/2000... Training loss: 0.9649\n",
      "Epoch: 504/2000... Training loss: 1.1253\n",
      "Epoch: 504/2000... Training loss: 1.1599\n",
      "Epoch: 504/2000... Training loss: 1.1312\n",
      "Epoch: 504/2000... Training loss: 0.9671\n",
      "Epoch: 504/2000... Training loss: 0.9675\n",
      "Epoch: 504/2000... Training loss: 0.9418\n",
      "Epoch: 504/2000... Training loss: 1.2050\n",
      "Epoch: 504/2000... Training loss: 1.1226\n",
      "Epoch: 504/2000... Training loss: 1.1682\n",
      "Epoch: 504/2000... Training loss: 1.2811\n",
      "Epoch: 504/2000... Training loss: 1.1786\n",
      "Epoch: 504/2000... Training loss: 1.1934\n",
      "Epoch: 504/2000... Training loss: 1.0396\n",
      "Epoch: 504/2000... Training loss: 1.1770\n",
      "Epoch: 504/2000... Training loss: 1.0831\n",
      "Epoch: 504/2000... Training loss: 0.9024\n",
      "Epoch: 504/2000... Training loss: 0.7633\n",
      "Epoch: 504/2000... Training loss: 1.1440\n",
      "Epoch: 504/2000... Training loss: 1.0445\n",
      "Epoch: 504/2000... Training loss: 1.0976\n",
      "Epoch: 504/2000... Training loss: 1.1380\n",
      "Epoch: 504/2000... Training loss: 0.8922\n",
      "Epoch: 504/2000... Training loss: 1.2093\n",
      "Epoch: 504/2000... Training loss: 1.2272\n",
      "Epoch: 504/2000... Training loss: 1.0765\n",
      "Epoch: 505/2000... Training loss: 1.2283\n",
      "Epoch: 505/2000... Training loss: 1.1942\n",
      "Epoch: 505/2000... Training loss: 1.0001\n",
      "Epoch: 505/2000... Training loss: 0.9451\n",
      "Epoch: 505/2000... Training loss: 1.1611\n",
      "Epoch: 505/2000... Training loss: 1.0920\n",
      "Epoch: 505/2000... Training loss: 1.1744\n",
      "Epoch: 505/2000... Training loss: 0.7850\n",
      "Epoch: 505/2000... Training loss: 1.3728\n",
      "Epoch: 505/2000... Training loss: 0.9180\n",
      "Epoch: 505/2000... Training loss: 0.9544\n",
      "Epoch: 505/2000... Training loss: 0.8663\n",
      "Epoch: 505/2000... Training loss: 0.9514\n",
      "Epoch: 505/2000... Training loss: 1.2850\n",
      "Epoch: 505/2000... Training loss: 0.9909\n",
      "Epoch: 505/2000... Training loss: 1.0862\n",
      "Epoch: 505/2000... Training loss: 1.0455\n",
      "Epoch: 505/2000... Training loss: 0.8796\n",
      "Epoch: 505/2000... Training loss: 0.9853\n",
      "Epoch: 505/2000... Training loss: 0.8794\n",
      "Epoch: 505/2000... Training loss: 1.2511\n",
      "Epoch: 505/2000... Training loss: 0.9739\n",
      "Epoch: 505/2000... Training loss: 0.9720\n",
      "Epoch: 505/2000... Training loss: 0.9443\n",
      "Epoch: 505/2000... Training loss: 0.9479\n",
      "Epoch: 505/2000... Training loss: 1.1153\n",
      "Epoch: 505/2000... Training loss: 0.9130\n",
      "Epoch: 505/2000... Training loss: 1.0210\n",
      "Epoch: 505/2000... Training loss: 1.2011\n",
      "Epoch: 505/2000... Training loss: 1.1595\n",
      "Epoch: 505/2000... Training loss: 0.9685\n",
      "Epoch: 506/2000... Training loss: 1.0513\n",
      "Epoch: 506/2000... Training loss: 1.2328\n",
      "Epoch: 506/2000... Training loss: 1.0028\n",
      "Epoch: 506/2000... Training loss: 0.8840\n",
      "Epoch: 506/2000... Training loss: 1.0870\n",
      "Epoch: 506/2000... Training loss: 1.1773\n",
      "Epoch: 506/2000... Training loss: 1.0132\n",
      "Epoch: 506/2000... Training loss: 1.0990\n",
      "Epoch: 506/2000... Training loss: 1.0509\n",
      "Epoch: 506/2000... Training loss: 0.8312\n",
      "Epoch: 506/2000... Training loss: 1.0291\n",
      "Epoch: 506/2000... Training loss: 1.2146\n",
      "Epoch: 506/2000... Training loss: 1.1118\n",
      "Epoch: 506/2000... Training loss: 1.1014\n",
      "Epoch: 506/2000... Training loss: 1.0753\n",
      "Epoch: 506/2000... Training loss: 1.0354\n",
      "Epoch: 506/2000... Training loss: 1.1346\n",
      "Epoch: 506/2000... Training loss: 0.9071\n",
      "Epoch: 506/2000... Training loss: 0.9789\n",
      "Epoch: 506/2000... Training loss: 1.4019\n",
      "Epoch: 506/2000... Training loss: 1.0902\n",
      "Epoch: 506/2000... Training loss: 0.9466\n",
      "Epoch: 506/2000... Training loss: 1.1870\n",
      "Epoch: 506/2000... Training loss: 0.9794\n",
      "Epoch: 506/2000... Training loss: 1.0814\n",
      "Epoch: 506/2000... Training loss: 1.0350\n",
      "Epoch: 506/2000... Training loss: 1.1644\n",
      "Epoch: 506/2000... Training loss: 1.2240\n",
      "Epoch: 506/2000... Training loss: 0.8739\n",
      "Epoch: 506/2000... Training loss: 1.0600\n",
      "Epoch: 506/2000... Training loss: 1.0401\n",
      "Epoch: 507/2000... Training loss: 1.0822\n",
      "Epoch: 507/2000... Training loss: 0.8745\n",
      "Epoch: 507/2000... Training loss: 1.1989\n",
      "Epoch: 507/2000... Training loss: 0.9252\n",
      "Epoch: 507/2000... Training loss: 1.0418\n",
      "Epoch: 507/2000... Training loss: 0.9987\n",
      "Epoch: 507/2000... Training loss: 0.9866\n",
      "Epoch: 507/2000... Training loss: 1.0687\n",
      "Epoch: 507/2000... Training loss: 0.8823\n",
      "Epoch: 507/2000... Training loss: 1.1045\n",
      "Epoch: 507/2000... Training loss: 0.9803\n",
      "Epoch: 507/2000... Training loss: 1.0765\n",
      "Epoch: 507/2000... Training loss: 1.0274\n",
      "Epoch: 507/2000... Training loss: 0.9015\n",
      "Epoch: 507/2000... Training loss: 1.0251\n",
      "Epoch: 507/2000... Training loss: 0.9458\n",
      "Epoch: 507/2000... Training loss: 1.2826\n",
      "Epoch: 507/2000... Training loss: 1.0270\n",
      "Epoch: 507/2000... Training loss: 1.1641\n",
      "Epoch: 507/2000... Training loss: 0.8815\n",
      "Epoch: 507/2000... Training loss: 0.9232\n",
      "Epoch: 507/2000... Training loss: 0.9251\n",
      "Epoch: 507/2000... Training loss: 1.1242\n",
      "Epoch: 507/2000... Training loss: 1.0259\n",
      "Epoch: 507/2000... Training loss: 1.2732\n",
      "Epoch: 507/2000... Training loss: 0.9175\n",
      "Epoch: 507/2000... Training loss: 0.9683\n",
      "Epoch: 507/2000... Training loss: 0.9855\n",
      "Epoch: 507/2000... Training loss: 1.3418\n",
      "Epoch: 507/2000... Training loss: 0.9736\n",
      "Epoch: 507/2000... Training loss: 1.1562\n",
      "Epoch: 508/2000... Training loss: 1.0735\n",
      "Epoch: 508/2000... Training loss: 1.2729\n",
      "Epoch: 508/2000... Training loss: 1.0521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 508/2000... Training loss: 1.1345\n",
      "Epoch: 508/2000... Training loss: 0.9855\n",
      "Epoch: 508/2000... Training loss: 0.9778\n",
      "Epoch: 508/2000... Training loss: 0.9826\n",
      "Epoch: 508/2000... Training loss: 1.0364\n",
      "Epoch: 508/2000... Training loss: 1.3157\n",
      "Epoch: 508/2000... Training loss: 1.1091\n",
      "Epoch: 508/2000... Training loss: 1.3155\n",
      "Epoch: 508/2000... Training loss: 0.8770\n",
      "Epoch: 508/2000... Training loss: 0.8606\n",
      "Epoch: 508/2000... Training loss: 1.1640\n",
      "Epoch: 508/2000... Training loss: 0.9588\n",
      "Epoch: 508/2000... Training loss: 1.1800\n",
      "Epoch: 508/2000... Training loss: 1.0251\n",
      "Epoch: 508/2000... Training loss: 0.8747\n",
      "Epoch: 508/2000... Training loss: 1.0016\n",
      "Epoch: 508/2000... Training loss: 0.9329\n",
      "Epoch: 508/2000... Training loss: 0.9673\n",
      "Epoch: 508/2000... Training loss: 1.0810\n",
      "Epoch: 508/2000... Training loss: 1.1381\n",
      "Epoch: 508/2000... Training loss: 1.1215\n",
      "Epoch: 508/2000... Training loss: 0.9485\n",
      "Epoch: 508/2000... Training loss: 1.0743\n",
      "Epoch: 508/2000... Training loss: 1.1411\n",
      "Epoch: 508/2000... Training loss: 1.0508\n",
      "Epoch: 508/2000... Training loss: 1.0229\n",
      "Epoch: 508/2000... Training loss: 1.0302\n",
      "Epoch: 508/2000... Training loss: 0.9156\n",
      "Epoch: 509/2000... Training loss: 1.1217\n",
      "Epoch: 509/2000... Training loss: 1.0730\n",
      "Epoch: 509/2000... Training loss: 1.0709\n",
      "Epoch: 509/2000... Training loss: 1.1432\n",
      "Epoch: 509/2000... Training loss: 1.0026\n",
      "Epoch: 509/2000... Training loss: 1.1001\n",
      "Epoch: 509/2000... Training loss: 0.9753\n",
      "Epoch: 509/2000... Training loss: 1.2397\n",
      "Epoch: 509/2000... Training loss: 1.0149\n",
      "Epoch: 509/2000... Training loss: 1.0716\n",
      "Epoch: 509/2000... Training loss: 1.2134\n",
      "Epoch: 509/2000... Training loss: 1.0620\n",
      "Epoch: 509/2000... Training loss: 1.0580\n",
      "Epoch: 509/2000... Training loss: 1.3068\n",
      "Epoch: 509/2000... Training loss: 1.4171\n",
      "Epoch: 509/2000... Training loss: 1.0386\n",
      "Epoch: 509/2000... Training loss: 0.9771\n",
      "Epoch: 509/2000... Training loss: 1.0746\n",
      "Epoch: 509/2000... Training loss: 1.1348\n",
      "Epoch: 509/2000... Training loss: 1.0108\n",
      "Epoch: 509/2000... Training loss: 1.1421\n",
      "Epoch: 509/2000... Training loss: 1.0037\n",
      "Epoch: 509/2000... Training loss: 1.1044\n",
      "Epoch: 509/2000... Training loss: 1.1560\n",
      "Epoch: 509/2000... Training loss: 0.8458\n",
      "Epoch: 509/2000... Training loss: 0.9316\n",
      "Epoch: 509/2000... Training loss: 0.9548\n",
      "Epoch: 509/2000... Training loss: 1.0858\n",
      "Epoch: 509/2000... Training loss: 1.0464\n",
      "Epoch: 509/2000... Training loss: 1.2192\n",
      "Epoch: 509/2000... Training loss: 1.1844\n",
      "Epoch: 510/2000... Training loss: 1.2386\n",
      "Epoch: 510/2000... Training loss: 1.0621\n",
      "Epoch: 510/2000... Training loss: 1.0862\n",
      "Epoch: 510/2000... Training loss: 1.0415\n",
      "Epoch: 510/2000... Training loss: 1.1220\n",
      "Epoch: 510/2000... Training loss: 1.3110\n",
      "Epoch: 510/2000... Training loss: 1.0033\n",
      "Epoch: 510/2000... Training loss: 0.9908\n",
      "Epoch: 510/2000... Training loss: 1.1585\n",
      "Epoch: 510/2000... Training loss: 1.1232\n",
      "Epoch: 510/2000... Training loss: 1.1803\n",
      "Epoch: 510/2000... Training loss: 1.0530\n",
      "Epoch: 510/2000... Training loss: 1.1892\n",
      "Epoch: 510/2000... Training loss: 0.9958\n",
      "Epoch: 510/2000... Training loss: 0.9262\n",
      "Epoch: 510/2000... Training loss: 0.7413\n",
      "Epoch: 510/2000... Training loss: 0.9972\n",
      "Epoch: 510/2000... Training loss: 1.0631\n",
      "Epoch: 510/2000... Training loss: 1.0333\n",
      "Epoch: 510/2000... Training loss: 0.9476\n",
      "Epoch: 510/2000... Training loss: 1.1297\n",
      "Epoch: 510/2000... Training loss: 0.9585\n",
      "Epoch: 510/2000... Training loss: 1.2273\n",
      "Epoch: 510/2000... Training loss: 0.8525\n",
      "Epoch: 510/2000... Training loss: 0.9312\n",
      "Epoch: 510/2000... Training loss: 1.1442\n",
      "Epoch: 510/2000... Training loss: 0.9924\n",
      "Epoch: 510/2000... Training loss: 1.3667\n",
      "Epoch: 510/2000... Training loss: 1.0501\n",
      "Epoch: 510/2000... Training loss: 1.1729\n",
      "Epoch: 510/2000... Training loss: 1.0333\n",
      "Epoch: 511/2000... Training loss: 0.9316\n",
      "Epoch: 511/2000... Training loss: 1.1916\n",
      "Epoch: 511/2000... Training loss: 0.9240\n",
      "Epoch: 511/2000... Training loss: 0.9092\n",
      "Epoch: 511/2000... Training loss: 0.9879\n",
      "Epoch: 511/2000... Training loss: 1.1510\n",
      "Epoch: 511/2000... Training loss: 1.0292\n",
      "Epoch: 511/2000... Training loss: 0.9281\n",
      "Epoch: 511/2000... Training loss: 0.9204\n",
      "Epoch: 511/2000... Training loss: 0.9289\n",
      "Epoch: 511/2000... Training loss: 0.9222\n",
      "Epoch: 511/2000... Training loss: 0.9967\n",
      "Epoch: 511/2000... Training loss: 1.2615\n",
      "Epoch: 511/2000... Training loss: 1.1737\n",
      "Epoch: 511/2000... Training loss: 0.9492\n",
      "Epoch: 511/2000... Training loss: 0.9976\n",
      "Epoch: 511/2000... Training loss: 0.9022\n",
      "Epoch: 511/2000... Training loss: 1.0822\n",
      "Epoch: 511/2000... Training loss: 1.1511\n",
      "Epoch: 511/2000... Training loss: 0.8753\n",
      "Epoch: 511/2000... Training loss: 1.0080\n",
      "Epoch: 511/2000... Training loss: 0.9397\n",
      "Epoch: 511/2000... Training loss: 1.1190\n",
      "Epoch: 511/2000... Training loss: 1.1702\n",
      "Epoch: 511/2000... Training loss: 0.9384\n",
      "Epoch: 511/2000... Training loss: 1.0706\n",
      "Epoch: 511/2000... Training loss: 0.9442\n",
      "Epoch: 511/2000... Training loss: 1.0083\n",
      "Epoch: 511/2000... Training loss: 1.0406\n",
      "Epoch: 511/2000... Training loss: 1.3383\n",
      "Epoch: 511/2000... Training loss: 0.9664\n",
      "Epoch: 512/2000... Training loss: 1.1562\n",
      "Epoch: 512/2000... Training loss: 1.0820\n",
      "Epoch: 512/2000... Training loss: 1.0035\n",
      "Epoch: 512/2000... Training loss: 1.2527\n",
      "Epoch: 512/2000... Training loss: 1.1364\n",
      "Epoch: 512/2000... Training loss: 1.0155\n",
      "Epoch: 512/2000... Training loss: 1.1400\n",
      "Epoch: 512/2000... Training loss: 1.0128\n",
      "Epoch: 512/2000... Training loss: 1.0273\n",
      "Epoch: 512/2000... Training loss: 1.2696\n",
      "Epoch: 512/2000... Training loss: 1.0134\n",
      "Epoch: 512/2000... Training loss: 0.8935\n",
      "Epoch: 512/2000... Training loss: 1.0079\n",
      "Epoch: 512/2000... Training loss: 1.0908\n",
      "Epoch: 512/2000... Training loss: 0.9559\n",
      "Epoch: 512/2000... Training loss: 1.0478\n",
      "Epoch: 512/2000... Training loss: 1.1138\n",
      "Epoch: 512/2000... Training loss: 0.9867\n",
      "Epoch: 512/2000... Training loss: 1.1086\n",
      "Epoch: 512/2000... Training loss: 1.1312\n",
      "Epoch: 512/2000... Training loss: 1.0264\n",
      "Epoch: 512/2000... Training loss: 0.9867\n",
      "Epoch: 512/2000... Training loss: 1.0196\n",
      "Epoch: 512/2000... Training loss: 0.9146\n",
      "Epoch: 512/2000... Training loss: 1.1174\n",
      "Epoch: 512/2000... Training loss: 0.9607\n",
      "Epoch: 512/2000... Training loss: 0.9548\n",
      "Epoch: 512/2000... Training loss: 0.9896\n",
      "Epoch: 512/2000... Training loss: 1.0358\n",
      "Epoch: 512/2000... Training loss: 0.9199\n",
      "Epoch: 512/2000... Training loss: 0.8761\n",
      "Epoch: 513/2000... Training loss: 1.1235\n",
      "Epoch: 513/2000... Training loss: 0.9753\n",
      "Epoch: 513/2000... Training loss: 1.0810\n",
      "Epoch: 513/2000... Training loss: 0.9262\n",
      "Epoch: 513/2000... Training loss: 0.9897\n",
      "Epoch: 513/2000... Training loss: 1.0774\n",
      "Epoch: 513/2000... Training loss: 1.1328\n",
      "Epoch: 513/2000... Training loss: 1.2511\n",
      "Epoch: 513/2000... Training loss: 1.3393\n",
      "Epoch: 513/2000... Training loss: 1.0122\n",
      "Epoch: 513/2000... Training loss: 1.0059\n",
      "Epoch: 513/2000... Training loss: 0.9581\n",
      "Epoch: 513/2000... Training loss: 1.0595\n",
      "Epoch: 513/2000... Training loss: 1.2422\n",
      "Epoch: 513/2000... Training loss: 1.2213\n",
      "Epoch: 513/2000... Training loss: 0.9494\n",
      "Epoch: 513/2000... Training loss: 1.0726\n",
      "Epoch: 513/2000... Training loss: 1.0099\n",
      "Epoch: 513/2000... Training loss: 1.2304\n",
      "Epoch: 513/2000... Training loss: 0.9160\n",
      "Epoch: 513/2000... Training loss: 1.0547\n",
      "Epoch: 513/2000... Training loss: 0.9729\n",
      "Epoch: 513/2000... Training loss: 0.9521\n",
      "Epoch: 513/2000... Training loss: 0.8493\n",
      "Epoch: 513/2000... Training loss: 1.0696\n",
      "Epoch: 513/2000... Training loss: 1.0916\n",
      "Epoch: 513/2000... Training loss: 1.0763\n",
      "Epoch: 513/2000... Training loss: 0.9959\n",
      "Epoch: 513/2000... Training loss: 0.9375\n",
      "Epoch: 513/2000... Training loss: 1.0138\n",
      "Epoch: 513/2000... Training loss: 1.0471\n",
      "Epoch: 514/2000... Training loss: 0.8784\n",
      "Epoch: 514/2000... Training loss: 0.9326\n",
      "Epoch: 514/2000... Training loss: 0.9537\n",
      "Epoch: 514/2000... Training loss: 1.1017\n",
      "Epoch: 514/2000... Training loss: 1.1503\n",
      "Epoch: 514/2000... Training loss: 1.1762\n",
      "Epoch: 514/2000... Training loss: 1.0820\n",
      "Epoch: 514/2000... Training loss: 1.0194\n",
      "Epoch: 514/2000... Training loss: 0.9941\n",
      "Epoch: 514/2000... Training loss: 1.2973\n",
      "Epoch: 514/2000... Training loss: 1.2465\n",
      "Epoch: 514/2000... Training loss: 0.9807\n",
      "Epoch: 514/2000... Training loss: 1.2245\n",
      "Epoch: 514/2000... Training loss: 0.8949\n",
      "Epoch: 514/2000... Training loss: 0.9781\n",
      "Epoch: 514/2000... Training loss: 1.0977\n",
      "Epoch: 514/2000... Training loss: 1.0685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 514/2000... Training loss: 1.0757\n",
      "Epoch: 514/2000... Training loss: 0.8792\n",
      "Epoch: 514/2000... Training loss: 1.1311\n",
      "Epoch: 514/2000... Training loss: 0.8039\n",
      "Epoch: 514/2000... Training loss: 0.9775\n",
      "Epoch: 514/2000... Training loss: 0.8858\n",
      "Epoch: 514/2000... Training loss: 1.0066\n",
      "Epoch: 514/2000... Training loss: 1.1909\n",
      "Epoch: 514/2000... Training loss: 0.9975\n",
      "Epoch: 514/2000... Training loss: 1.2030\n",
      "Epoch: 514/2000... Training loss: 0.9290\n",
      "Epoch: 514/2000... Training loss: 0.9408\n",
      "Epoch: 514/2000... Training loss: 0.9723\n",
      "Epoch: 514/2000... Training loss: 1.0336\n",
      "Epoch: 515/2000... Training loss: 0.8735\n",
      "Epoch: 515/2000... Training loss: 1.0933\n",
      "Epoch: 515/2000... Training loss: 0.9045\n",
      "Epoch: 515/2000... Training loss: 1.0717\n",
      "Epoch: 515/2000... Training loss: 0.9079\n",
      "Epoch: 515/2000... Training loss: 1.1562\n",
      "Epoch: 515/2000... Training loss: 1.0233\n",
      "Epoch: 515/2000... Training loss: 0.8690\n",
      "Epoch: 515/2000... Training loss: 1.2208\n",
      "Epoch: 515/2000... Training loss: 1.2522\n",
      "Epoch: 515/2000... Training loss: 1.1525\n",
      "Epoch: 515/2000... Training loss: 0.7834\n",
      "Epoch: 515/2000... Training loss: 0.9659\n",
      "Epoch: 515/2000... Training loss: 0.8238\n",
      "Epoch: 515/2000... Training loss: 1.0064\n",
      "Epoch: 515/2000... Training loss: 0.7517\n",
      "Epoch: 515/2000... Training loss: 1.0708\n",
      "Epoch: 515/2000... Training loss: 1.0220\n",
      "Epoch: 515/2000... Training loss: 0.8491\n",
      "Epoch: 515/2000... Training loss: 1.0675\n",
      "Epoch: 515/2000... Training loss: 1.1096\n",
      "Epoch: 515/2000... Training loss: 0.9320\n",
      "Epoch: 515/2000... Training loss: 1.0595\n",
      "Epoch: 515/2000... Training loss: 1.0052\n",
      "Epoch: 515/2000... Training loss: 1.1566\n",
      "Epoch: 515/2000... Training loss: 1.1798\n",
      "Epoch: 515/2000... Training loss: 0.9918\n",
      "Epoch: 515/2000... Training loss: 0.8939\n",
      "Epoch: 515/2000... Training loss: 1.1662\n",
      "Epoch: 515/2000... Training loss: 1.2104\n",
      "Epoch: 515/2000... Training loss: 0.8128\n",
      "Epoch: 516/2000... Training loss: 0.9792\n",
      "Epoch: 516/2000... Training loss: 1.0580\n",
      "Epoch: 516/2000... Training loss: 0.8030\n",
      "Epoch: 516/2000... Training loss: 1.0578\n",
      "Epoch: 516/2000... Training loss: 0.8743\n",
      "Epoch: 516/2000... Training loss: 1.1314\n",
      "Epoch: 516/2000... Training loss: 0.8709\n",
      "Epoch: 516/2000... Training loss: 0.9368\n",
      "Epoch: 516/2000... Training loss: 0.9960\n",
      "Epoch: 516/2000... Training loss: 1.2082\n",
      "Epoch: 516/2000... Training loss: 1.1890\n",
      "Epoch: 516/2000... Training loss: 0.9926\n",
      "Epoch: 516/2000... Training loss: 1.1234\n",
      "Epoch: 516/2000... Training loss: 1.0196\n",
      "Epoch: 516/2000... Training loss: 0.8439\n",
      "Epoch: 516/2000... Training loss: 1.0583\n",
      "Epoch: 516/2000... Training loss: 0.8290\n",
      "Epoch: 516/2000... Training loss: 0.9360\n",
      "Epoch: 516/2000... Training loss: 1.1731\n",
      "Epoch: 516/2000... Training loss: 1.0405\n",
      "Epoch: 516/2000... Training loss: 0.8858\n",
      "Epoch: 516/2000... Training loss: 0.8991\n",
      "Epoch: 516/2000... Training loss: 1.1818\n",
      "Epoch: 516/2000... Training loss: 1.1496\n",
      "Epoch: 516/2000... Training loss: 1.0525\n",
      "Epoch: 516/2000... Training loss: 0.9511\n",
      "Epoch: 516/2000... Training loss: 1.1005\n",
      "Epoch: 516/2000... Training loss: 0.9360\n",
      "Epoch: 516/2000... Training loss: 1.0161\n",
      "Epoch: 516/2000... Training loss: 0.9666\n",
      "Epoch: 516/2000... Training loss: 1.0450\n",
      "Epoch: 517/2000... Training loss: 1.0804\n",
      "Epoch: 517/2000... Training loss: 1.0334\n",
      "Epoch: 517/2000... Training loss: 1.1278\n",
      "Epoch: 517/2000... Training loss: 0.9561\n",
      "Epoch: 517/2000... Training loss: 0.9210\n",
      "Epoch: 517/2000... Training loss: 0.8268\n",
      "Epoch: 517/2000... Training loss: 1.3399\n",
      "Epoch: 517/2000... Training loss: 1.1151\n",
      "Epoch: 517/2000... Training loss: 1.0240\n",
      "Epoch: 517/2000... Training loss: 1.0691\n",
      "Epoch: 517/2000... Training loss: 1.2603\n",
      "Epoch: 517/2000... Training loss: 0.9745\n",
      "Epoch: 517/2000... Training loss: 0.9046\n",
      "Epoch: 517/2000... Training loss: 1.2117\n",
      "Epoch: 517/2000... Training loss: 1.2101\n",
      "Epoch: 517/2000... Training loss: 1.0006\n",
      "Epoch: 517/2000... Training loss: 1.3831\n",
      "Epoch: 517/2000... Training loss: 0.9897\n",
      "Epoch: 517/2000... Training loss: 0.8961\n",
      "Epoch: 517/2000... Training loss: 1.0047\n",
      "Epoch: 517/2000... Training loss: 1.0019\n",
      "Epoch: 517/2000... Training loss: 1.1552\n",
      "Epoch: 517/2000... Training loss: 1.1773\n",
      "Epoch: 517/2000... Training loss: 1.0449\n",
      "Epoch: 517/2000... Training loss: 1.1776\n",
      "Epoch: 517/2000... Training loss: 1.0988\n",
      "Epoch: 517/2000... Training loss: 0.9170\n",
      "Epoch: 517/2000... Training loss: 1.1838\n",
      "Epoch: 517/2000... Training loss: 1.0460\n",
      "Epoch: 517/2000... Training loss: 1.0031\n",
      "Epoch: 517/2000... Training loss: 1.0671\n",
      "Epoch: 518/2000... Training loss: 1.0224\n",
      "Epoch: 518/2000... Training loss: 1.0541\n",
      "Epoch: 518/2000... Training loss: 1.0808\n",
      "Epoch: 518/2000... Training loss: 0.9320\n",
      "Epoch: 518/2000... Training loss: 1.1252\n",
      "Epoch: 518/2000... Training loss: 1.0530\n",
      "Epoch: 518/2000... Training loss: 1.0242\n",
      "Epoch: 518/2000... Training loss: 1.2256\n",
      "Epoch: 518/2000... Training loss: 0.8821\n",
      "Epoch: 518/2000... Training loss: 1.1139\n",
      "Epoch: 518/2000... Training loss: 1.2364\n",
      "Epoch: 518/2000... Training loss: 0.9880\n",
      "Epoch: 518/2000... Training loss: 1.0856\n",
      "Epoch: 518/2000... Training loss: 1.0093\n",
      "Epoch: 518/2000... Training loss: 1.1250\n",
      "Epoch: 518/2000... Training loss: 0.9942\n",
      "Epoch: 518/2000... Training loss: 1.2084\n",
      "Epoch: 518/2000... Training loss: 0.9007\n",
      "Epoch: 518/2000... Training loss: 1.3535\n",
      "Epoch: 518/2000... Training loss: 1.0151\n",
      "Epoch: 518/2000... Training loss: 0.9078\n",
      "Epoch: 518/2000... Training loss: 0.7575\n",
      "Epoch: 518/2000... Training loss: 1.0828\n",
      "Epoch: 518/2000... Training loss: 1.0920\n",
      "Epoch: 518/2000... Training loss: 1.1274\n",
      "Epoch: 518/2000... Training loss: 0.9790\n",
      "Epoch: 518/2000... Training loss: 1.0980\n",
      "Epoch: 518/2000... Training loss: 0.9315\n",
      "Epoch: 518/2000... Training loss: 1.1102\n",
      "Epoch: 518/2000... Training loss: 1.0473\n",
      "Epoch: 518/2000... Training loss: 0.9931\n",
      "Epoch: 519/2000... Training loss: 1.0237\n",
      "Epoch: 519/2000... Training loss: 1.0449\n",
      "Epoch: 519/2000... Training loss: 1.0631\n",
      "Epoch: 519/2000... Training loss: 1.1299\n",
      "Epoch: 519/2000... Training loss: 1.0381\n",
      "Epoch: 519/2000... Training loss: 0.8852\n",
      "Epoch: 519/2000... Training loss: 1.1004\n",
      "Epoch: 519/2000... Training loss: 1.0246\n",
      "Epoch: 519/2000... Training loss: 1.1429\n",
      "Epoch: 519/2000... Training loss: 1.2050\n",
      "Epoch: 519/2000... Training loss: 1.0005\n",
      "Epoch: 519/2000... Training loss: 1.0376\n",
      "Epoch: 519/2000... Training loss: 1.1548\n",
      "Epoch: 519/2000... Training loss: 0.7909\n",
      "Epoch: 519/2000... Training loss: 1.0926\n",
      "Epoch: 519/2000... Training loss: 0.9319\n",
      "Epoch: 519/2000... Training loss: 0.9356\n",
      "Epoch: 519/2000... Training loss: 0.9123\n",
      "Epoch: 519/2000... Training loss: 0.9818\n",
      "Epoch: 519/2000... Training loss: 0.9324\n",
      "Epoch: 519/2000... Training loss: 0.9008\n",
      "Epoch: 519/2000... Training loss: 0.9726\n",
      "Epoch: 519/2000... Training loss: 1.2824\n",
      "Epoch: 519/2000... Training loss: 1.2195\n",
      "Epoch: 519/2000... Training loss: 0.8797\n",
      "Epoch: 519/2000... Training loss: 1.0248\n",
      "Epoch: 519/2000... Training loss: 0.9490\n",
      "Epoch: 519/2000... Training loss: 1.1277\n",
      "Epoch: 519/2000... Training loss: 1.1533\n",
      "Epoch: 519/2000... Training loss: 1.1674\n",
      "Epoch: 519/2000... Training loss: 1.0814\n",
      "Epoch: 520/2000... Training loss: 1.0068\n",
      "Epoch: 520/2000... Training loss: 0.9913\n",
      "Epoch: 520/2000... Training loss: 1.1043\n",
      "Epoch: 520/2000... Training loss: 0.9124\n",
      "Epoch: 520/2000... Training loss: 0.8700\n",
      "Epoch: 520/2000... Training loss: 1.2755\n",
      "Epoch: 520/2000... Training loss: 1.1523\n",
      "Epoch: 520/2000... Training loss: 1.2163\n",
      "Epoch: 520/2000... Training loss: 0.8392\n",
      "Epoch: 520/2000... Training loss: 1.5514\n",
      "Epoch: 520/2000... Training loss: 1.0388\n",
      "Epoch: 520/2000... Training loss: 0.7645\n",
      "Epoch: 520/2000... Training loss: 1.0445\n",
      "Epoch: 520/2000... Training loss: 1.0055\n",
      "Epoch: 520/2000... Training loss: 0.8675\n",
      "Epoch: 520/2000... Training loss: 0.9414\n",
      "Epoch: 520/2000... Training loss: 1.0508\n",
      "Epoch: 520/2000... Training loss: 1.2270\n",
      "Epoch: 520/2000... Training loss: 1.0601\n",
      "Epoch: 520/2000... Training loss: 1.0726\n",
      "Epoch: 520/2000... Training loss: 1.1278\n",
      "Epoch: 520/2000... Training loss: 1.0787\n",
      "Epoch: 520/2000... Training loss: 1.1015\n",
      "Epoch: 520/2000... Training loss: 1.0326\n",
      "Epoch: 520/2000... Training loss: 1.1943\n",
      "Epoch: 520/2000... Training loss: 0.9318\n",
      "Epoch: 520/2000... Training loss: 1.2052\n",
      "Epoch: 520/2000... Training loss: 1.0407\n",
      "Epoch: 520/2000... Training loss: 1.0701\n",
      "Epoch: 520/2000... Training loss: 1.0800\n",
      "Epoch: 520/2000... Training loss: 1.1342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 521/2000... Training loss: 1.0362\n",
      "Epoch: 521/2000... Training loss: 0.9512\n",
      "Epoch: 521/2000... Training loss: 0.9730\n",
      "Epoch: 521/2000... Training loss: 0.9752\n",
      "Epoch: 521/2000... Training loss: 1.0454\n",
      "Epoch: 521/2000... Training loss: 1.3662\n",
      "Epoch: 521/2000... Training loss: 1.2356\n",
      "Epoch: 521/2000... Training loss: 1.0352\n",
      "Epoch: 521/2000... Training loss: 1.2412\n",
      "Epoch: 521/2000... Training loss: 1.0870\n",
      "Epoch: 521/2000... Training loss: 0.9978\n",
      "Epoch: 521/2000... Training loss: 1.0060\n",
      "Epoch: 521/2000... Training loss: 1.0850\n",
      "Epoch: 521/2000... Training loss: 1.0952\n",
      "Epoch: 521/2000... Training loss: 1.0673\n",
      "Epoch: 521/2000... Training loss: 1.0361\n",
      "Epoch: 521/2000... Training loss: 0.8711\n",
      "Epoch: 521/2000... Training loss: 0.9744\n",
      "Epoch: 521/2000... Training loss: 1.2806\n",
      "Epoch: 521/2000... Training loss: 0.9384\n",
      "Epoch: 521/2000... Training loss: 0.9696\n",
      "Epoch: 521/2000... Training loss: 1.1433\n",
      "Epoch: 521/2000... Training loss: 0.9791\n",
      "Epoch: 521/2000... Training loss: 0.9754\n",
      "Epoch: 521/2000... Training loss: 1.0756\n",
      "Epoch: 521/2000... Training loss: 0.9882\n",
      "Epoch: 521/2000... Training loss: 0.9958\n",
      "Epoch: 521/2000... Training loss: 1.1253\n",
      "Epoch: 521/2000... Training loss: 0.9380\n",
      "Epoch: 521/2000... Training loss: 1.1531\n",
      "Epoch: 521/2000... Training loss: 0.9268\n",
      "Epoch: 522/2000... Training loss: 1.2807\n",
      "Epoch: 522/2000... Training loss: 0.8881\n",
      "Epoch: 522/2000... Training loss: 0.9006\n",
      "Epoch: 522/2000... Training loss: 1.1126\n",
      "Epoch: 522/2000... Training loss: 0.9239\n",
      "Epoch: 522/2000... Training loss: 0.9142\n",
      "Epoch: 522/2000... Training loss: 1.0384\n",
      "Epoch: 522/2000... Training loss: 1.3937\n",
      "Epoch: 522/2000... Training loss: 0.8881\n",
      "Epoch: 522/2000... Training loss: 1.0853\n",
      "Epoch: 522/2000... Training loss: 1.0786\n",
      "Epoch: 522/2000... Training loss: 1.1245\n",
      "Epoch: 522/2000... Training loss: 0.8892\n",
      "Epoch: 522/2000... Training loss: 0.9817\n",
      "Epoch: 522/2000... Training loss: 0.9131\n",
      "Epoch: 522/2000... Training loss: 1.1878\n",
      "Epoch: 522/2000... Training loss: 0.9413\n",
      "Epoch: 522/2000... Training loss: 0.8393\n",
      "Epoch: 522/2000... Training loss: 0.7808\n",
      "Epoch: 522/2000... Training loss: 1.1652\n",
      "Epoch: 522/2000... Training loss: 0.9839\n",
      "Epoch: 522/2000... Training loss: 0.8149\n",
      "Epoch: 522/2000... Training loss: 1.0496\n",
      "Epoch: 522/2000... Training loss: 0.8583\n",
      "Epoch: 522/2000... Training loss: 1.1489\n",
      "Epoch: 522/2000... Training loss: 1.0347\n",
      "Epoch: 522/2000... Training loss: 1.1225\n",
      "Epoch: 522/2000... Training loss: 0.7647\n",
      "Epoch: 522/2000... Training loss: 0.8479\n",
      "Epoch: 522/2000... Training loss: 1.1818\n",
      "Epoch: 522/2000... Training loss: 0.9064\n",
      "Epoch: 523/2000... Training loss: 1.0260\n",
      "Epoch: 523/2000... Training loss: 1.1805\n",
      "Epoch: 523/2000... Training loss: 0.9156\n",
      "Epoch: 523/2000... Training loss: 0.9175\n",
      "Epoch: 523/2000... Training loss: 0.9445\n",
      "Epoch: 523/2000... Training loss: 1.1142\n",
      "Epoch: 523/2000... Training loss: 0.8796\n",
      "Epoch: 523/2000... Training loss: 1.0357\n",
      "Epoch: 523/2000... Training loss: 0.8987\n",
      "Epoch: 523/2000... Training loss: 1.2233\n",
      "Epoch: 523/2000... Training loss: 0.9922\n",
      "Epoch: 523/2000... Training loss: 0.8277\n",
      "Epoch: 523/2000... Training loss: 1.0093\n",
      "Epoch: 523/2000... Training loss: 0.6833\n",
      "Epoch: 523/2000... Training loss: 1.0924\n",
      "Epoch: 523/2000... Training loss: 1.0881\n",
      "Epoch: 523/2000... Training loss: 0.9284\n",
      "Epoch: 523/2000... Training loss: 0.8937\n",
      "Epoch: 523/2000... Training loss: 1.0150\n",
      "Epoch: 523/2000... Training loss: 1.0459\n",
      "Epoch: 523/2000... Training loss: 0.9990\n",
      "Epoch: 523/2000... Training loss: 0.7938\n",
      "Epoch: 523/2000... Training loss: 1.0879\n",
      "Epoch: 523/2000... Training loss: 0.8944\n",
      "Epoch: 523/2000... Training loss: 1.1255\n",
      "Epoch: 523/2000... Training loss: 1.0524\n",
      "Epoch: 523/2000... Training loss: 0.9600\n",
      "Epoch: 523/2000... Training loss: 0.8991\n",
      "Epoch: 523/2000... Training loss: 1.1665\n",
      "Epoch: 523/2000... Training loss: 1.0075\n",
      "Epoch: 523/2000... Training loss: 1.0107\n",
      "Epoch: 524/2000... Training loss: 1.0064\n",
      "Epoch: 524/2000... Training loss: 0.9687\n",
      "Epoch: 524/2000... Training loss: 0.8903\n",
      "Epoch: 524/2000... Training loss: 1.0370\n",
      "Epoch: 524/2000... Training loss: 0.9220\n",
      "Epoch: 524/2000... Training loss: 0.9793\n",
      "Epoch: 524/2000... Training loss: 1.1086\n",
      "Epoch: 524/2000... Training loss: 0.8381\n",
      "Epoch: 524/2000... Training loss: 1.0060\n",
      "Epoch: 524/2000... Training loss: 1.2774\n",
      "Epoch: 524/2000... Training loss: 1.0283\n",
      "Epoch: 524/2000... Training loss: 1.0067\n",
      "Epoch: 524/2000... Training loss: 1.0217\n",
      "Epoch: 524/2000... Training loss: 1.3005\n",
      "Epoch: 524/2000... Training loss: 0.7722\n",
      "Epoch: 524/2000... Training loss: 0.9502\n",
      "Epoch: 524/2000... Training loss: 1.1257\n",
      "Epoch: 524/2000... Training loss: 0.8136\n",
      "Epoch: 524/2000... Training loss: 1.0099\n",
      "Epoch: 524/2000... Training loss: 0.9827\n",
      "Epoch: 524/2000... Training loss: 0.9230\n",
      "Epoch: 524/2000... Training loss: 0.9962\n",
      "Epoch: 524/2000... Training loss: 0.9003\n",
      "Epoch: 524/2000... Training loss: 0.8927\n",
      "Epoch: 524/2000... Training loss: 1.1939\n",
      "Epoch: 524/2000... Training loss: 0.9799\n",
      "Epoch: 524/2000... Training loss: 1.0438\n",
      "Epoch: 524/2000... Training loss: 1.1690\n",
      "Epoch: 524/2000... Training loss: 1.0052\n",
      "Epoch: 524/2000... Training loss: 0.9019\n",
      "Epoch: 524/2000... Training loss: 1.0193\n",
      "Epoch: 525/2000... Training loss: 1.1812\n",
      "Epoch: 525/2000... Training loss: 1.1189\n",
      "Epoch: 525/2000... Training loss: 1.1070\n",
      "Epoch: 525/2000... Training loss: 0.9409\n",
      "Epoch: 525/2000... Training loss: 1.1058\n",
      "Epoch: 525/2000... Training loss: 0.8545\n",
      "Epoch: 525/2000... Training loss: 0.9736\n",
      "Epoch: 525/2000... Training loss: 0.9061\n",
      "Epoch: 525/2000... Training loss: 1.0574\n",
      "Epoch: 525/2000... Training loss: 0.8316\n",
      "Epoch: 525/2000... Training loss: 0.9928\n",
      "Epoch: 525/2000... Training loss: 0.9602\n",
      "Epoch: 525/2000... Training loss: 0.9709\n",
      "Epoch: 525/2000... Training loss: 1.0586\n",
      "Epoch: 525/2000... Training loss: 0.8414\n",
      "Epoch: 525/2000... Training loss: 1.0746\n",
      "Epoch: 525/2000... Training loss: 1.1931\n",
      "Epoch: 525/2000... Training loss: 0.9370\n",
      "Epoch: 525/2000... Training loss: 1.0609\n",
      "Epoch: 525/2000... Training loss: 0.9917\n",
      "Epoch: 525/2000... Training loss: 1.3324\n",
      "Epoch: 525/2000... Training loss: 0.9914\n",
      "Epoch: 525/2000... Training loss: 0.9808\n",
      "Epoch: 525/2000... Training loss: 0.7929\n",
      "Epoch: 525/2000... Training loss: 1.1141\n",
      "Epoch: 525/2000... Training loss: 0.9240\n",
      "Epoch: 525/2000... Training loss: 1.0567\n",
      "Epoch: 525/2000... Training loss: 1.0829\n",
      "Epoch: 525/2000... Training loss: 1.0820\n",
      "Epoch: 525/2000... Training loss: 1.0609\n",
      "Epoch: 525/2000... Training loss: 0.8467\n",
      "Epoch: 526/2000... Training loss: 1.0966\n",
      "Epoch: 526/2000... Training loss: 0.9750\n",
      "Epoch: 526/2000... Training loss: 0.9880\n",
      "Epoch: 526/2000... Training loss: 0.9773\n",
      "Epoch: 526/2000... Training loss: 0.9087\n",
      "Epoch: 526/2000... Training loss: 1.0120\n",
      "Epoch: 526/2000... Training loss: 1.0785\n",
      "Epoch: 526/2000... Training loss: 1.0078\n",
      "Epoch: 526/2000... Training loss: 0.8662\n",
      "Epoch: 526/2000... Training loss: 1.1241\n",
      "Epoch: 526/2000... Training loss: 1.0236\n",
      "Epoch: 526/2000... Training loss: 0.9977\n",
      "Epoch: 526/2000... Training loss: 1.1189\n",
      "Epoch: 526/2000... Training loss: 0.8803\n",
      "Epoch: 526/2000... Training loss: 1.0336\n",
      "Epoch: 526/2000... Training loss: 1.0762\n",
      "Epoch: 526/2000... Training loss: 0.9994\n",
      "Epoch: 526/2000... Training loss: 0.8152\n",
      "Epoch: 526/2000... Training loss: 0.9913\n",
      "Epoch: 526/2000... Training loss: 0.9381\n",
      "Epoch: 526/2000... Training loss: 0.7584\n",
      "Epoch: 526/2000... Training loss: 0.8540\n",
      "Epoch: 526/2000... Training loss: 1.0113\n",
      "Epoch: 526/2000... Training loss: 1.0846\n",
      "Epoch: 526/2000... Training loss: 0.8403\n",
      "Epoch: 526/2000... Training loss: 1.1340\n",
      "Epoch: 526/2000... Training loss: 1.1335\n",
      "Epoch: 526/2000... Training loss: 0.8924\n",
      "Epoch: 526/2000... Training loss: 1.0676\n",
      "Epoch: 526/2000... Training loss: 1.0480\n",
      "Epoch: 526/2000... Training loss: 0.9206\n",
      "Epoch: 527/2000... Training loss: 1.0886\n",
      "Epoch: 527/2000... Training loss: 0.9944\n",
      "Epoch: 527/2000... Training loss: 0.8455\n",
      "Epoch: 527/2000... Training loss: 1.0714\n",
      "Epoch: 527/2000... Training loss: 0.8396\n",
      "Epoch: 527/2000... Training loss: 0.9973\n",
      "Epoch: 527/2000... Training loss: 0.8968\n",
      "Epoch: 527/2000... Training loss: 0.9297\n",
      "Epoch: 527/2000... Training loss: 0.9724\n",
      "Epoch: 527/2000... Training loss: 1.0004\n",
      "Epoch: 527/2000... Training loss: 1.0757\n",
      "Epoch: 527/2000... Training loss: 1.0310\n",
      "Epoch: 527/2000... Training loss: 0.9817\n",
      "Epoch: 527/2000... Training loss: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 527/2000... Training loss: 0.8941\n",
      "Epoch: 527/2000... Training loss: 1.1422\n",
      "Epoch: 527/2000... Training loss: 1.0268\n",
      "Epoch: 527/2000... Training loss: 1.2633\n",
      "Epoch: 527/2000... Training loss: 0.9231\n",
      "Epoch: 527/2000... Training loss: 1.0269\n",
      "Epoch: 527/2000... Training loss: 1.1546\n",
      "Epoch: 527/2000... Training loss: 0.8395\n",
      "Epoch: 527/2000... Training loss: 0.9474\n",
      "Epoch: 527/2000... Training loss: 0.9142\n",
      "Epoch: 527/2000... Training loss: 1.1270\n",
      "Epoch: 527/2000... Training loss: 0.9279\n",
      "Epoch: 527/2000... Training loss: 0.9264\n",
      "Epoch: 527/2000... Training loss: 0.9998\n",
      "Epoch: 527/2000... Training loss: 0.7994\n",
      "Epoch: 527/2000... Training loss: 1.0221\n",
      "Epoch: 527/2000... Training loss: 1.1714\n",
      "Epoch: 528/2000... Training loss: 0.9576\n",
      "Epoch: 528/2000... Training loss: 1.1479\n",
      "Epoch: 528/2000... Training loss: 0.9831\n",
      "Epoch: 528/2000... Training loss: 0.9699\n",
      "Epoch: 528/2000... Training loss: 1.0119\n",
      "Epoch: 528/2000... Training loss: 0.9239\n",
      "Epoch: 528/2000... Training loss: 1.0927\n",
      "Epoch: 528/2000... Training loss: 1.0980\n",
      "Epoch: 528/2000... Training loss: 0.8279\n",
      "Epoch: 528/2000... Training loss: 1.2171\n",
      "Epoch: 528/2000... Training loss: 0.9611\n",
      "Epoch: 528/2000... Training loss: 0.8846\n",
      "Epoch: 528/2000... Training loss: 0.8646\n",
      "Epoch: 528/2000... Training loss: 0.9887\n",
      "Epoch: 528/2000... Training loss: 1.1915\n",
      "Epoch: 528/2000... Training loss: 1.0550\n",
      "Epoch: 528/2000... Training loss: 1.1840\n",
      "Epoch: 528/2000... Training loss: 1.0593\n",
      "Epoch: 528/2000... Training loss: 0.8617\n",
      "Epoch: 528/2000... Training loss: 1.2220\n",
      "Epoch: 528/2000... Training loss: 1.0920\n",
      "Epoch: 528/2000... Training loss: 1.0853\n",
      "Epoch: 528/2000... Training loss: 1.1266\n",
      "Epoch: 528/2000... Training loss: 1.0010\n",
      "Epoch: 528/2000... Training loss: 1.1185\n",
      "Epoch: 528/2000... Training loss: 0.9766\n",
      "Epoch: 528/2000... Training loss: 1.1178\n",
      "Epoch: 528/2000... Training loss: 1.0577\n",
      "Epoch: 528/2000... Training loss: 1.1574\n",
      "Epoch: 528/2000... Training loss: 1.0392\n",
      "Epoch: 528/2000... Training loss: 0.9569\n",
      "Epoch: 529/2000... Training loss: 0.9938\n",
      "Epoch: 529/2000... Training loss: 0.9228\n",
      "Epoch: 529/2000... Training loss: 0.9514\n",
      "Epoch: 529/2000... Training loss: 1.3257\n",
      "Epoch: 529/2000... Training loss: 0.9747\n",
      "Epoch: 529/2000... Training loss: 0.9675\n",
      "Epoch: 529/2000... Training loss: 1.1025\n",
      "Epoch: 529/2000... Training loss: 0.9754\n",
      "Epoch: 529/2000... Training loss: 1.2159\n",
      "Epoch: 529/2000... Training loss: 0.9441\n",
      "Epoch: 529/2000... Training loss: 0.8925\n",
      "Epoch: 529/2000... Training loss: 0.9272\n",
      "Epoch: 529/2000... Training loss: 1.0107\n",
      "Epoch: 529/2000... Training loss: 0.9391\n",
      "Epoch: 529/2000... Training loss: 0.8862\n",
      "Epoch: 529/2000... Training loss: 0.9667\n",
      "Epoch: 529/2000... Training loss: 1.0244\n",
      "Epoch: 529/2000... Training loss: 1.1577\n",
      "Epoch: 529/2000... Training loss: 1.0448\n",
      "Epoch: 529/2000... Training loss: 1.0268\n",
      "Epoch: 529/2000... Training loss: 0.9863\n",
      "Epoch: 529/2000... Training loss: 0.8678\n",
      "Epoch: 529/2000... Training loss: 1.0850\n",
      "Epoch: 529/2000... Training loss: 1.0865\n",
      "Epoch: 529/2000... Training loss: 0.7941\n",
      "Epoch: 529/2000... Training loss: 1.0367\n",
      "Epoch: 529/2000... Training loss: 0.9570\n",
      "Epoch: 529/2000... Training loss: 0.9813\n",
      "Epoch: 529/2000... Training loss: 0.7352\n",
      "Epoch: 529/2000... Training loss: 0.8501\n",
      "Epoch: 529/2000... Training loss: 1.2904\n",
      "Epoch: 530/2000... Training loss: 1.1035\n",
      "Epoch: 530/2000... Training loss: 0.7563\n",
      "Epoch: 530/2000... Training loss: 1.1595\n",
      "Epoch: 530/2000... Training loss: 1.1178\n",
      "Epoch: 530/2000... Training loss: 1.1785\n",
      "Epoch: 530/2000... Training loss: 1.1177\n",
      "Epoch: 530/2000... Training loss: 0.9463\n",
      "Epoch: 530/2000... Training loss: 1.2418\n",
      "Epoch: 530/2000... Training loss: 0.8156\n",
      "Epoch: 530/2000... Training loss: 0.8660\n",
      "Epoch: 530/2000... Training loss: 0.8285\n",
      "Epoch: 530/2000... Training loss: 1.0466\n",
      "Epoch: 530/2000... Training loss: 0.9019\n",
      "Epoch: 530/2000... Training loss: 0.9717\n",
      "Epoch: 530/2000... Training loss: 0.8674\n",
      "Epoch: 530/2000... Training loss: 1.0603\n",
      "Epoch: 530/2000... Training loss: 0.9954\n",
      "Epoch: 530/2000... Training loss: 1.0087\n",
      "Epoch: 530/2000... Training loss: 0.9295\n",
      "Epoch: 530/2000... Training loss: 1.1995\n",
      "Epoch: 530/2000... Training loss: 0.9777\n",
      "Epoch: 530/2000... Training loss: 0.8717\n",
      "Epoch: 530/2000... Training loss: 1.0934\n",
      "Epoch: 530/2000... Training loss: 1.1237\n",
      "Epoch: 530/2000... Training loss: 0.8767\n",
      "Epoch: 530/2000... Training loss: 1.0180\n",
      "Epoch: 530/2000... Training loss: 1.0329\n",
      "Epoch: 530/2000... Training loss: 1.2343\n",
      "Epoch: 530/2000... Training loss: 0.9977\n",
      "Epoch: 530/2000... Training loss: 1.0091\n",
      "Epoch: 530/2000... Training loss: 0.9993\n",
      "Epoch: 531/2000... Training loss: 1.0520\n",
      "Epoch: 531/2000... Training loss: 1.1786\n",
      "Epoch: 531/2000... Training loss: 1.1716\n",
      "Epoch: 531/2000... Training loss: 0.7872\n",
      "Epoch: 531/2000... Training loss: 0.8866\n",
      "Epoch: 531/2000... Training loss: 1.0781\n",
      "Epoch: 531/2000... Training loss: 1.1264\n",
      "Epoch: 531/2000... Training loss: 1.0095\n",
      "Epoch: 531/2000... Training loss: 1.0298\n",
      "Epoch: 531/2000... Training loss: 1.1029\n",
      "Epoch: 531/2000... Training loss: 0.9042\n",
      "Epoch: 531/2000... Training loss: 1.0522\n",
      "Epoch: 531/2000... Training loss: 0.9698\n",
      "Epoch: 531/2000... Training loss: 1.1006\n",
      "Epoch: 531/2000... Training loss: 1.0977\n",
      "Epoch: 531/2000... Training loss: 1.0590\n",
      "Epoch: 531/2000... Training loss: 0.9335\n",
      "Epoch: 531/2000... Training loss: 0.9037\n",
      "Epoch: 531/2000... Training loss: 1.0151\n",
      "Epoch: 531/2000... Training loss: 0.9473\n",
      "Epoch: 531/2000... Training loss: 0.8589\n",
      "Epoch: 531/2000... Training loss: 0.9882\n",
      "Epoch: 531/2000... Training loss: 1.0098\n",
      "Epoch: 531/2000... Training loss: 0.8599\n",
      "Epoch: 531/2000... Training loss: 0.9733\n",
      "Epoch: 531/2000... Training loss: 1.0568\n",
      "Epoch: 531/2000... Training loss: 0.9456\n",
      "Epoch: 531/2000... Training loss: 0.9150\n",
      "Epoch: 531/2000... Training loss: 0.9888\n",
      "Epoch: 531/2000... Training loss: 1.0804\n",
      "Epoch: 531/2000... Training loss: 0.9180\n",
      "Epoch: 532/2000... Training loss: 0.7910\n",
      "Epoch: 532/2000... Training loss: 0.8260\n",
      "Epoch: 532/2000... Training loss: 0.7625\n",
      "Epoch: 532/2000... Training loss: 1.0131\n",
      "Epoch: 532/2000... Training loss: 0.8956\n",
      "Epoch: 532/2000... Training loss: 1.2171\n",
      "Epoch: 532/2000... Training loss: 0.9775\n",
      "Epoch: 532/2000... Training loss: 1.2895\n",
      "Epoch: 532/2000... Training loss: 0.9605\n",
      "Epoch: 532/2000... Training loss: 0.9481\n",
      "Epoch: 532/2000... Training loss: 1.0453\n",
      "Epoch: 532/2000... Training loss: 1.0355\n",
      "Epoch: 532/2000... Training loss: 1.2552\n",
      "Epoch: 532/2000... Training loss: 1.0110\n",
      "Epoch: 532/2000... Training loss: 0.7923\n",
      "Epoch: 532/2000... Training loss: 1.1473\n",
      "Epoch: 532/2000... Training loss: 0.7651\n",
      "Epoch: 532/2000... Training loss: 0.8299\n",
      "Epoch: 532/2000... Training loss: 0.9196\n",
      "Epoch: 532/2000... Training loss: 0.9620\n",
      "Epoch: 532/2000... Training loss: 0.8307\n",
      "Epoch: 532/2000... Training loss: 1.2039\n",
      "Epoch: 532/2000... Training loss: 1.1873\n",
      "Epoch: 532/2000... Training loss: 0.9287\n",
      "Epoch: 532/2000... Training loss: 0.9615\n",
      "Epoch: 532/2000... Training loss: 0.7459\n",
      "Epoch: 532/2000... Training loss: 1.0625\n",
      "Epoch: 532/2000... Training loss: 1.2455\n",
      "Epoch: 532/2000... Training loss: 1.0946\n",
      "Epoch: 532/2000... Training loss: 1.0837\n",
      "Epoch: 532/2000... Training loss: 0.7637\n",
      "Epoch: 533/2000... Training loss: 1.1253\n",
      "Epoch: 533/2000... Training loss: 0.8296\n",
      "Epoch: 533/2000... Training loss: 1.0315\n",
      "Epoch: 533/2000... Training loss: 0.9466\n",
      "Epoch: 533/2000... Training loss: 0.9020\n",
      "Epoch: 533/2000... Training loss: 1.1783\n",
      "Epoch: 533/2000... Training loss: 1.0198\n",
      "Epoch: 533/2000... Training loss: 0.9368\n",
      "Epoch: 533/2000... Training loss: 0.8692\n",
      "Epoch: 533/2000... Training loss: 0.7987\n",
      "Epoch: 533/2000... Training loss: 0.9844\n",
      "Epoch: 533/2000... Training loss: 1.0876\n",
      "Epoch: 533/2000... Training loss: 1.1109\n",
      "Epoch: 533/2000... Training loss: 1.1406\n",
      "Epoch: 533/2000... Training loss: 0.9298\n",
      "Epoch: 533/2000... Training loss: 1.0737\n",
      "Epoch: 533/2000... Training loss: 1.0574\n",
      "Epoch: 533/2000... Training loss: 0.9701\n",
      "Epoch: 533/2000... Training loss: 0.9579\n",
      "Epoch: 533/2000... Training loss: 0.9599\n",
      "Epoch: 533/2000... Training loss: 0.9594\n",
      "Epoch: 533/2000... Training loss: 1.1380\n",
      "Epoch: 533/2000... Training loss: 0.9867\n",
      "Epoch: 533/2000... Training loss: 1.0569\n",
      "Epoch: 533/2000... Training loss: 0.9916\n",
      "Epoch: 533/2000... Training loss: 0.9348\n",
      "Epoch: 533/2000... Training loss: 0.9835\n",
      "Epoch: 533/2000... Training loss: 1.2395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 533/2000... Training loss: 1.2048\n",
      "Epoch: 533/2000... Training loss: 1.0777\n",
      "Epoch: 533/2000... Training loss: 1.0549\n",
      "Epoch: 534/2000... Training loss: 0.9007\n",
      "Epoch: 534/2000... Training loss: 0.8064\n",
      "Epoch: 534/2000... Training loss: 0.8430\n",
      "Epoch: 534/2000... Training loss: 1.1519\n",
      "Epoch: 534/2000... Training loss: 1.1481\n",
      "Epoch: 534/2000... Training loss: 1.1082\n",
      "Epoch: 534/2000... Training loss: 1.0726\n",
      "Epoch: 534/2000... Training loss: 1.1776\n",
      "Epoch: 534/2000... Training loss: 1.0576\n",
      "Epoch: 534/2000... Training loss: 0.9826\n",
      "Epoch: 534/2000... Training loss: 0.9391\n",
      "Epoch: 534/2000... Training loss: 1.0716\n",
      "Epoch: 534/2000... Training loss: 1.0154\n",
      "Epoch: 534/2000... Training loss: 0.9732\n",
      "Epoch: 534/2000... Training loss: 1.0118\n",
      "Epoch: 534/2000... Training loss: 0.8982\n",
      "Epoch: 534/2000... Training loss: 1.1758\n",
      "Epoch: 534/2000... Training loss: 1.0489\n",
      "Epoch: 534/2000... Training loss: 0.9660\n",
      "Epoch: 534/2000... Training loss: 0.9540\n",
      "Epoch: 534/2000... Training loss: 1.1496\n",
      "Epoch: 534/2000... Training loss: 0.9341\n",
      "Epoch: 534/2000... Training loss: 1.0309\n",
      "Epoch: 534/2000... Training loss: 0.9502\n",
      "Epoch: 534/2000... Training loss: 0.9873\n",
      "Epoch: 534/2000... Training loss: 0.9995\n",
      "Epoch: 534/2000... Training loss: 0.8733\n",
      "Epoch: 534/2000... Training loss: 1.1708\n",
      "Epoch: 534/2000... Training loss: 0.8839\n",
      "Epoch: 534/2000... Training loss: 1.0463\n",
      "Epoch: 534/2000... Training loss: 1.1676\n",
      "Epoch: 535/2000... Training loss: 0.9683\n",
      "Epoch: 535/2000... Training loss: 1.0659\n",
      "Epoch: 535/2000... Training loss: 1.0487\n",
      "Epoch: 535/2000... Training loss: 1.0658\n",
      "Epoch: 535/2000... Training loss: 0.9458\n",
      "Epoch: 535/2000... Training loss: 0.8151\n",
      "Epoch: 535/2000... Training loss: 0.9775\n",
      "Epoch: 535/2000... Training loss: 1.1818\n",
      "Epoch: 535/2000... Training loss: 1.0918\n",
      "Epoch: 535/2000... Training loss: 1.0127\n",
      "Epoch: 535/2000... Training loss: 1.0805\n",
      "Epoch: 535/2000... Training loss: 1.0760\n",
      "Epoch: 535/2000... Training loss: 0.9460\n",
      "Epoch: 535/2000... Training loss: 1.0446\n",
      "Epoch: 535/2000... Training loss: 1.0725\n",
      "Epoch: 535/2000... Training loss: 0.9272\n",
      "Epoch: 535/2000... Training loss: 0.8565\n",
      "Epoch: 535/2000... Training loss: 0.6629\n",
      "Epoch: 535/2000... Training loss: 1.1483\n",
      "Epoch: 535/2000... Training loss: 1.1544\n",
      "Epoch: 535/2000... Training loss: 0.6935\n",
      "Epoch: 535/2000... Training loss: 1.0760\n",
      "Epoch: 535/2000... Training loss: 0.9391\n",
      "Epoch: 535/2000... Training loss: 1.0687\n",
      "Epoch: 535/2000... Training loss: 0.9016\n",
      "Epoch: 535/2000... Training loss: 1.1338\n",
      "Epoch: 535/2000... Training loss: 1.0553\n",
      "Epoch: 535/2000... Training loss: 1.1761\n",
      "Epoch: 535/2000... Training loss: 1.0044\n",
      "Epoch: 535/2000... Training loss: 1.1996\n",
      "Epoch: 535/2000... Training loss: 0.8667\n",
      "Epoch: 536/2000... Training loss: 0.8563\n",
      "Epoch: 536/2000... Training loss: 1.0009\n",
      "Epoch: 536/2000... Training loss: 0.9795\n",
      "Epoch: 536/2000... Training loss: 0.6562\n",
      "Epoch: 536/2000... Training loss: 0.8385\n",
      "Epoch: 536/2000... Training loss: 1.0110\n",
      "Epoch: 536/2000... Training loss: 0.8911\n",
      "Epoch: 536/2000... Training loss: 1.0274\n",
      "Epoch: 536/2000... Training loss: 1.0987\n",
      "Epoch: 536/2000... Training loss: 0.8948\n",
      "Epoch: 536/2000... Training loss: 1.3121\n",
      "Epoch: 536/2000... Training loss: 1.0163\n",
      "Epoch: 536/2000... Training loss: 1.0057\n",
      "Epoch: 536/2000... Training loss: 1.0120\n",
      "Epoch: 536/2000... Training loss: 1.0857\n",
      "Epoch: 536/2000... Training loss: 0.9507\n",
      "Epoch: 536/2000... Training loss: 1.1311\n",
      "Epoch: 536/2000... Training loss: 0.8736\n",
      "Epoch: 536/2000... Training loss: 0.9525\n",
      "Epoch: 536/2000... Training loss: 1.0307\n",
      "Epoch: 536/2000... Training loss: 1.0337\n",
      "Epoch: 536/2000... Training loss: 0.9873\n",
      "Epoch: 536/2000... Training loss: 1.1944\n",
      "Epoch: 536/2000... Training loss: 0.9598\n",
      "Epoch: 536/2000... Training loss: 1.1670\n",
      "Epoch: 536/2000... Training loss: 1.1314\n",
      "Epoch: 536/2000... Training loss: 1.0758\n",
      "Epoch: 536/2000... Training loss: 1.1248\n",
      "Epoch: 536/2000... Training loss: 1.1127\n",
      "Epoch: 536/2000... Training loss: 0.9476\n",
      "Epoch: 536/2000... Training loss: 1.0085\n",
      "Epoch: 537/2000... Training loss: 0.9249\n",
      "Epoch: 537/2000... Training loss: 1.0604\n",
      "Epoch: 537/2000... Training loss: 0.9590\n",
      "Epoch: 537/2000... Training loss: 0.9982\n",
      "Epoch: 537/2000... Training loss: 0.9058\n",
      "Epoch: 537/2000... Training loss: 1.0558\n",
      "Epoch: 537/2000... Training loss: 1.0081\n",
      "Epoch: 537/2000... Training loss: 1.0133\n",
      "Epoch: 537/2000... Training loss: 0.9304\n",
      "Epoch: 537/2000... Training loss: 0.9026\n",
      "Epoch: 537/2000... Training loss: 0.9455\n",
      "Epoch: 537/2000... Training loss: 1.2389\n",
      "Epoch: 537/2000... Training loss: 0.8970\n",
      "Epoch: 537/2000... Training loss: 0.9145\n",
      "Epoch: 537/2000... Training loss: 0.8140\n",
      "Epoch: 537/2000... Training loss: 1.0000\n",
      "Epoch: 537/2000... Training loss: 1.0380\n",
      "Epoch: 537/2000... Training loss: 0.9241\n",
      "Epoch: 537/2000... Training loss: 0.9948\n",
      "Epoch: 537/2000... Training loss: 0.8641\n",
      "Epoch: 537/2000... Training loss: 1.0246\n",
      "Epoch: 537/2000... Training loss: 0.9738\n",
      "Epoch: 537/2000... Training loss: 1.1036\n",
      "Epoch: 537/2000... Training loss: 0.9147\n",
      "Epoch: 537/2000... Training loss: 1.1729\n",
      "Epoch: 537/2000... Training loss: 0.9429\n",
      "Epoch: 537/2000... Training loss: 0.8931\n",
      "Epoch: 537/2000... Training loss: 1.0660\n",
      "Epoch: 537/2000... Training loss: 0.7740\n",
      "Epoch: 537/2000... Training loss: 0.7562\n",
      "Epoch: 537/2000... Training loss: 1.1477\n",
      "Epoch: 538/2000... Training loss: 1.0157\n",
      "Epoch: 538/2000... Training loss: 0.9614\n",
      "Epoch: 538/2000... Training loss: 1.0614\n",
      "Epoch: 538/2000... Training loss: 1.1052\n",
      "Epoch: 538/2000... Training loss: 0.9958\n",
      "Epoch: 538/2000... Training loss: 1.1727\n",
      "Epoch: 538/2000... Training loss: 0.9703\n",
      "Epoch: 538/2000... Training loss: 0.8035\n",
      "Epoch: 538/2000... Training loss: 0.9156\n",
      "Epoch: 538/2000... Training loss: 0.9716\n",
      "Epoch: 538/2000... Training loss: 1.0638\n",
      "Epoch: 538/2000... Training loss: 1.1841\n",
      "Epoch: 538/2000... Training loss: 1.3206\n",
      "Epoch: 538/2000... Training loss: 0.8075\n",
      "Epoch: 538/2000... Training loss: 1.0259\n",
      "Epoch: 538/2000... Training loss: 1.0581\n",
      "Epoch: 538/2000... Training loss: 1.0386\n",
      "Epoch: 538/2000... Training loss: 0.8521\n",
      "Epoch: 538/2000... Training loss: 0.8623\n",
      "Epoch: 538/2000... Training loss: 0.7231\n",
      "Epoch: 538/2000... Training loss: 0.8546\n",
      "Epoch: 538/2000... Training loss: 1.0773\n",
      "Epoch: 538/2000... Training loss: 0.9821\n",
      "Epoch: 538/2000... Training loss: 0.9774\n",
      "Epoch: 538/2000... Training loss: 0.9693\n",
      "Epoch: 538/2000... Training loss: 1.0731\n",
      "Epoch: 538/2000... Training loss: 1.0023\n",
      "Epoch: 538/2000... Training loss: 1.0107\n",
      "Epoch: 538/2000... Training loss: 1.0088\n",
      "Epoch: 538/2000... Training loss: 1.0063\n",
      "Epoch: 538/2000... Training loss: 1.3259\n",
      "Epoch: 539/2000... Training loss: 0.8516\n",
      "Epoch: 539/2000... Training loss: 0.8613\n",
      "Epoch: 539/2000... Training loss: 0.8821\n",
      "Epoch: 539/2000... Training loss: 0.9723\n",
      "Epoch: 539/2000... Training loss: 0.8883\n",
      "Epoch: 539/2000... Training loss: 0.8798\n",
      "Epoch: 539/2000... Training loss: 0.7863\n",
      "Epoch: 539/2000... Training loss: 1.0496\n",
      "Epoch: 539/2000... Training loss: 1.3357\n",
      "Epoch: 539/2000... Training loss: 0.9546\n",
      "Epoch: 539/2000... Training loss: 0.9165\n",
      "Epoch: 539/2000... Training loss: 0.9278\n",
      "Epoch: 539/2000... Training loss: 0.9611\n",
      "Epoch: 539/2000... Training loss: 1.1579\n",
      "Epoch: 539/2000... Training loss: 1.0418\n",
      "Epoch: 539/2000... Training loss: 0.9681\n",
      "Epoch: 539/2000... Training loss: 0.9358\n",
      "Epoch: 539/2000... Training loss: 0.9395\n",
      "Epoch: 539/2000... Training loss: 0.9128\n",
      "Epoch: 539/2000... Training loss: 1.1806\n",
      "Epoch: 539/2000... Training loss: 1.1021\n",
      "Epoch: 539/2000... Training loss: 0.8514\n",
      "Epoch: 539/2000... Training loss: 0.9368\n",
      "Epoch: 539/2000... Training loss: 1.0042\n",
      "Epoch: 539/2000... Training loss: 0.9103\n",
      "Epoch: 539/2000... Training loss: 0.9022\n",
      "Epoch: 539/2000... Training loss: 0.8309\n",
      "Epoch: 539/2000... Training loss: 1.0222\n",
      "Epoch: 539/2000... Training loss: 0.9030\n",
      "Epoch: 539/2000... Training loss: 0.9129\n",
      "Epoch: 539/2000... Training loss: 0.8961\n",
      "Epoch: 540/2000... Training loss: 0.9158\n",
      "Epoch: 540/2000... Training loss: 1.1045\n",
      "Epoch: 540/2000... Training loss: 1.0403\n",
      "Epoch: 540/2000... Training loss: 0.7813\n",
      "Epoch: 540/2000... Training loss: 0.9039\n",
      "Epoch: 540/2000... Training loss: 1.1707\n",
      "Epoch: 540/2000... Training loss: 0.8830\n",
      "Epoch: 540/2000... Training loss: 0.7280\n",
      "Epoch: 540/2000... Training loss: 0.9353\n",
      "Epoch: 540/2000... Training loss: 0.9902\n",
      "Epoch: 540/2000... Training loss: 0.9809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 540/2000... Training loss: 0.9867\n",
      "Epoch: 540/2000... Training loss: 0.9810\n",
      "Epoch: 540/2000... Training loss: 0.8741\n",
      "Epoch: 540/2000... Training loss: 0.9377\n",
      "Epoch: 540/2000... Training loss: 0.9303\n",
      "Epoch: 540/2000... Training loss: 0.9831\n",
      "Epoch: 540/2000... Training loss: 0.8533\n",
      "Epoch: 540/2000... Training loss: 0.8949\n",
      "Epoch: 540/2000... Training loss: 1.0350\n",
      "Epoch: 540/2000... Training loss: 0.9880\n",
      "Epoch: 540/2000... Training loss: 0.8982\n",
      "Epoch: 540/2000... Training loss: 0.8171\n",
      "Epoch: 540/2000... Training loss: 0.9916\n",
      "Epoch: 540/2000... Training loss: 1.1015\n",
      "Epoch: 540/2000... Training loss: 1.1105\n",
      "Epoch: 540/2000... Training loss: 0.8878\n",
      "Epoch: 540/2000... Training loss: 1.1279\n",
      "Epoch: 540/2000... Training loss: 0.8967\n",
      "Epoch: 540/2000... Training loss: 0.8947\n",
      "Epoch: 540/2000... Training loss: 0.9992\n",
      "Epoch: 541/2000... Training loss: 0.8990\n",
      "Epoch: 541/2000... Training loss: 0.9050\n",
      "Epoch: 541/2000... Training loss: 1.1592\n",
      "Epoch: 541/2000... Training loss: 0.8075\n",
      "Epoch: 541/2000... Training loss: 0.8267\n",
      "Epoch: 541/2000... Training loss: 0.9982\n",
      "Epoch: 541/2000... Training loss: 0.7867\n",
      "Epoch: 541/2000... Training loss: 1.0902\n",
      "Epoch: 541/2000... Training loss: 0.9762\n",
      "Epoch: 541/2000... Training loss: 1.0504\n",
      "Epoch: 541/2000... Training loss: 0.8016\n",
      "Epoch: 541/2000... Training loss: 0.8903\n",
      "Epoch: 541/2000... Training loss: 0.8154\n",
      "Epoch: 541/2000... Training loss: 0.7829\n",
      "Epoch: 541/2000... Training loss: 0.9293\n",
      "Epoch: 541/2000... Training loss: 0.9941\n",
      "Epoch: 541/2000... Training loss: 0.8816\n",
      "Epoch: 541/2000... Training loss: 0.9611\n",
      "Epoch: 541/2000... Training loss: 0.7050\n",
      "Epoch: 541/2000... Training loss: 0.9280\n",
      "Epoch: 541/2000... Training loss: 1.3272\n",
      "Epoch: 541/2000... Training loss: 0.9165\n",
      "Epoch: 541/2000... Training loss: 1.0385\n",
      "Epoch: 541/2000... Training loss: 0.9495\n",
      "Epoch: 541/2000... Training loss: 0.8535\n",
      "Epoch: 541/2000... Training loss: 0.9532\n",
      "Epoch: 541/2000... Training loss: 1.0563\n",
      "Epoch: 541/2000... Training loss: 0.8909\n",
      "Epoch: 541/2000... Training loss: 1.1231\n",
      "Epoch: 541/2000... Training loss: 0.8083\n",
      "Epoch: 541/2000... Training loss: 0.8891\n",
      "Epoch: 542/2000... Training loss: 0.9662\n",
      "Epoch: 542/2000... Training loss: 0.8713\n",
      "Epoch: 542/2000... Training loss: 0.8586\n",
      "Epoch: 542/2000... Training loss: 0.9568\n",
      "Epoch: 542/2000... Training loss: 0.8525\n",
      "Epoch: 542/2000... Training loss: 0.8385\n",
      "Epoch: 542/2000... Training loss: 1.0596\n",
      "Epoch: 542/2000... Training loss: 1.0116\n",
      "Epoch: 542/2000... Training loss: 1.1565\n",
      "Epoch: 542/2000... Training loss: 0.8838\n",
      "Epoch: 542/2000... Training loss: 0.9672\n",
      "Epoch: 542/2000... Training loss: 0.9143\n",
      "Epoch: 542/2000... Training loss: 0.9622\n",
      "Epoch: 542/2000... Training loss: 0.9939\n",
      "Epoch: 542/2000... Training loss: 0.7274\n",
      "Epoch: 542/2000... Training loss: 0.9613\n",
      "Epoch: 542/2000... Training loss: 0.9914\n",
      "Epoch: 542/2000... Training loss: 0.9642\n",
      "Epoch: 542/2000... Training loss: 0.8709\n",
      "Epoch: 542/2000... Training loss: 0.9471\n",
      "Epoch: 542/2000... Training loss: 0.8664\n",
      "Epoch: 542/2000... Training loss: 0.9524\n",
      "Epoch: 542/2000... Training loss: 0.8800\n",
      "Epoch: 542/2000... Training loss: 1.1671\n",
      "Epoch: 542/2000... Training loss: 0.8675\n",
      "Epoch: 542/2000... Training loss: 0.9742\n",
      "Epoch: 542/2000... Training loss: 0.8822\n",
      "Epoch: 542/2000... Training loss: 1.0684\n",
      "Epoch: 542/2000... Training loss: 0.8357\n",
      "Epoch: 542/2000... Training loss: 1.1059\n",
      "Epoch: 542/2000... Training loss: 1.0034\n",
      "Epoch: 543/2000... Training loss: 1.0290\n",
      "Epoch: 543/2000... Training loss: 0.9492\n",
      "Epoch: 543/2000... Training loss: 1.0001\n",
      "Epoch: 543/2000... Training loss: 0.8151\n",
      "Epoch: 543/2000... Training loss: 1.0049\n",
      "Epoch: 543/2000... Training loss: 1.0112\n",
      "Epoch: 543/2000... Training loss: 0.8423\n",
      "Epoch: 543/2000... Training loss: 1.1571\n",
      "Epoch: 543/2000... Training loss: 1.0385\n",
      "Epoch: 543/2000... Training loss: 1.0349\n",
      "Epoch: 543/2000... Training loss: 0.9895\n",
      "Epoch: 543/2000... Training loss: 0.9311\n",
      "Epoch: 543/2000... Training loss: 1.1621\n",
      "Epoch: 543/2000... Training loss: 0.9924\n",
      "Epoch: 543/2000... Training loss: 0.9285\n",
      "Epoch: 543/2000... Training loss: 1.0864\n",
      "Epoch: 543/2000... Training loss: 0.8502\n",
      "Epoch: 543/2000... Training loss: 1.0223\n",
      "Epoch: 543/2000... Training loss: 0.8077\n",
      "Epoch: 543/2000... Training loss: 1.0516\n",
      "Epoch: 543/2000... Training loss: 0.8718\n",
      "Epoch: 543/2000... Training loss: 0.8068\n",
      "Epoch: 543/2000... Training loss: 0.9745\n",
      "Epoch: 543/2000... Training loss: 0.8067\n",
      "Epoch: 543/2000... Training loss: 1.0537\n",
      "Epoch: 543/2000... Training loss: 0.9881\n",
      "Epoch: 543/2000... Training loss: 1.1380\n",
      "Epoch: 543/2000... Training loss: 0.9318\n",
      "Epoch: 543/2000... Training loss: 1.0327\n",
      "Epoch: 543/2000... Training loss: 0.8142\n",
      "Epoch: 543/2000... Training loss: 0.9962\n",
      "Epoch: 544/2000... Training loss: 1.0339\n",
      "Epoch: 544/2000... Training loss: 0.8170\n",
      "Epoch: 544/2000... Training loss: 0.8444\n",
      "Epoch: 544/2000... Training loss: 1.2553\n",
      "Epoch: 544/2000... Training loss: 0.8976\n",
      "Epoch: 544/2000... Training loss: 1.1114\n",
      "Epoch: 544/2000... Training loss: 0.9750\n",
      "Epoch: 544/2000... Training loss: 0.9524\n",
      "Epoch: 544/2000... Training loss: 1.1140\n",
      "Epoch: 544/2000... Training loss: 0.7279\n",
      "Epoch: 544/2000... Training loss: 0.8282\n",
      "Epoch: 544/2000... Training loss: 0.9099\n",
      "Epoch: 544/2000... Training loss: 0.9303\n",
      "Epoch: 544/2000... Training loss: 0.9552\n",
      "Epoch: 544/2000... Training loss: 0.9554\n",
      "Epoch: 544/2000... Training loss: 0.8142\n",
      "Epoch: 544/2000... Training loss: 0.8970\n",
      "Epoch: 544/2000... Training loss: 0.9303\n",
      "Epoch: 544/2000... Training loss: 0.9958\n",
      "Epoch: 544/2000... Training loss: 1.1464\n",
      "Epoch: 544/2000... Training loss: 0.8433\n",
      "Epoch: 544/2000... Training loss: 0.9055\n",
      "Epoch: 544/2000... Training loss: 1.0437\n",
      "Epoch: 544/2000... Training loss: 0.7630\n",
      "Epoch: 544/2000... Training loss: 0.9861\n",
      "Epoch: 544/2000... Training loss: 1.0457\n",
      "Epoch: 544/2000... Training loss: 0.9049\n",
      "Epoch: 544/2000... Training loss: 0.8662\n",
      "Epoch: 544/2000... Training loss: 1.1704\n",
      "Epoch: 544/2000... Training loss: 0.7590\n",
      "Epoch: 544/2000... Training loss: 1.1170\n",
      "Epoch: 545/2000... Training loss: 1.0406\n",
      "Epoch: 545/2000... Training loss: 0.9851\n",
      "Epoch: 545/2000... Training loss: 1.0434\n",
      "Epoch: 545/2000... Training loss: 0.9673\n",
      "Epoch: 545/2000... Training loss: 1.2259\n",
      "Epoch: 545/2000... Training loss: 0.9294\n",
      "Epoch: 545/2000... Training loss: 0.8727\n",
      "Epoch: 545/2000... Training loss: 0.9213\n",
      "Epoch: 545/2000... Training loss: 0.9766\n",
      "Epoch: 545/2000... Training loss: 1.0526\n",
      "Epoch: 545/2000... Training loss: 1.2208\n",
      "Epoch: 545/2000... Training loss: 0.9705\n",
      "Epoch: 545/2000... Training loss: 0.9324\n",
      "Epoch: 545/2000... Training loss: 1.1791\n",
      "Epoch: 545/2000... Training loss: 0.9026\n",
      "Epoch: 545/2000... Training loss: 1.0099\n",
      "Epoch: 545/2000... Training loss: 1.1520\n",
      "Epoch: 545/2000... Training loss: 0.7835\n",
      "Epoch: 545/2000... Training loss: 1.0787\n",
      "Epoch: 545/2000... Training loss: 1.1696\n",
      "Epoch: 545/2000... Training loss: 0.6665\n",
      "Epoch: 545/2000... Training loss: 1.0697\n",
      "Epoch: 545/2000... Training loss: 0.8845\n",
      "Epoch: 545/2000... Training loss: 0.8278\n",
      "Epoch: 545/2000... Training loss: 0.9585\n",
      "Epoch: 545/2000... Training loss: 0.9474\n",
      "Epoch: 545/2000... Training loss: 1.0168\n",
      "Epoch: 545/2000... Training loss: 1.0800\n",
      "Epoch: 545/2000... Training loss: 1.0872\n",
      "Epoch: 545/2000... Training loss: 0.9397\n",
      "Epoch: 545/2000... Training loss: 1.0384\n",
      "Epoch: 546/2000... Training loss: 0.8364\n",
      "Epoch: 546/2000... Training loss: 0.8592\n",
      "Epoch: 546/2000... Training loss: 0.8508\n",
      "Epoch: 546/2000... Training loss: 0.9623\n",
      "Epoch: 546/2000... Training loss: 1.3467\n",
      "Epoch: 546/2000... Training loss: 0.8648\n",
      "Epoch: 546/2000... Training loss: 0.8268\n",
      "Epoch: 546/2000... Training loss: 0.9782\n",
      "Epoch: 546/2000... Training loss: 0.8882\n",
      "Epoch: 546/2000... Training loss: 0.9039\n",
      "Epoch: 546/2000... Training loss: 1.0380\n",
      "Epoch: 546/2000... Training loss: 0.9386\n",
      "Epoch: 546/2000... Training loss: 1.0083\n",
      "Epoch: 546/2000... Training loss: 0.9257\n",
      "Epoch: 546/2000... Training loss: 0.9957\n",
      "Epoch: 546/2000... Training loss: 1.0986\n",
      "Epoch: 546/2000... Training loss: 0.7516\n",
      "Epoch: 546/2000... Training loss: 0.9011\n",
      "Epoch: 546/2000... Training loss: 0.8134\n",
      "Epoch: 546/2000... Training loss: 0.5523\n",
      "Epoch: 546/2000... Training loss: 0.8944\n",
      "Epoch: 546/2000... Training loss: 0.9163\n",
      "Epoch: 546/2000... Training loss: 0.9336\n",
      "Epoch: 546/2000... Training loss: 0.9145\n",
      "Epoch: 546/2000... Training loss: 1.2424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 546/2000... Training loss: 1.0987\n",
      "Epoch: 546/2000... Training loss: 0.9402\n",
      "Epoch: 546/2000... Training loss: 0.8123\n",
      "Epoch: 546/2000... Training loss: 0.9615\n",
      "Epoch: 546/2000... Training loss: 0.8276\n",
      "Epoch: 546/2000... Training loss: 1.0233\n",
      "Epoch: 547/2000... Training loss: 1.2016\n",
      "Epoch: 547/2000... Training loss: 0.9605\n",
      "Epoch: 547/2000... Training loss: 0.9180\n",
      "Epoch: 547/2000... Training loss: 0.9657\n",
      "Epoch: 547/2000... Training loss: 0.6870\n",
      "Epoch: 547/2000... Training loss: 0.8662\n",
      "Epoch: 547/2000... Training loss: 1.0202\n",
      "Epoch: 547/2000... Training loss: 0.9808\n",
      "Epoch: 547/2000... Training loss: 1.1741\n",
      "Epoch: 547/2000... Training loss: 0.9890\n",
      "Epoch: 547/2000... Training loss: 1.0663\n",
      "Epoch: 547/2000... Training loss: 1.0075\n",
      "Epoch: 547/2000... Training loss: 0.8334\n",
      "Epoch: 547/2000... Training loss: 0.9498\n",
      "Epoch: 547/2000... Training loss: 0.9513\n",
      "Epoch: 547/2000... Training loss: 0.9948\n",
      "Epoch: 547/2000... Training loss: 0.9443\n",
      "Epoch: 547/2000... Training loss: 1.1464\n",
      "Epoch: 547/2000... Training loss: 0.9526\n",
      "Epoch: 547/2000... Training loss: 0.9064\n",
      "Epoch: 547/2000... Training loss: 0.9034\n",
      "Epoch: 547/2000... Training loss: 0.9619\n",
      "Epoch: 547/2000... Training loss: 0.9300\n",
      "Epoch: 547/2000... Training loss: 0.9376\n",
      "Epoch: 547/2000... Training loss: 0.8930\n",
      "Epoch: 547/2000... Training loss: 0.9130\n",
      "Epoch: 547/2000... Training loss: 0.9496\n",
      "Epoch: 547/2000... Training loss: 0.9420\n",
      "Epoch: 547/2000... Training loss: 0.9761\n",
      "Epoch: 547/2000... Training loss: 1.0196\n",
      "Epoch: 547/2000... Training loss: 0.8118\n",
      "Epoch: 548/2000... Training loss: 0.9499\n",
      "Epoch: 548/2000... Training loss: 1.0648\n",
      "Epoch: 548/2000... Training loss: 0.8404\n",
      "Epoch: 548/2000... Training loss: 0.8361\n",
      "Epoch: 548/2000... Training loss: 1.0397\n",
      "Epoch: 548/2000... Training loss: 0.9389\n",
      "Epoch: 548/2000... Training loss: 0.8798\n",
      "Epoch: 548/2000... Training loss: 0.7240\n",
      "Epoch: 548/2000... Training loss: 0.8751\n",
      "Epoch: 548/2000... Training loss: 0.7918\n",
      "Epoch: 548/2000... Training loss: 0.8248\n",
      "Epoch: 548/2000... Training loss: 0.9999\n",
      "Epoch: 548/2000... Training loss: 1.0651\n",
      "Epoch: 548/2000... Training loss: 0.9943\n",
      "Epoch: 548/2000... Training loss: 0.8461\n",
      "Epoch: 548/2000... Training loss: 0.8696\n",
      "Epoch: 548/2000... Training loss: 0.8454\n",
      "Epoch: 548/2000... Training loss: 0.9489\n",
      "Epoch: 548/2000... Training loss: 0.9228\n",
      "Epoch: 548/2000... Training loss: 0.7698\n",
      "Epoch: 548/2000... Training loss: 0.8993\n",
      "Epoch: 548/2000... Training loss: 0.9390\n",
      "Epoch: 548/2000... Training loss: 0.9478\n",
      "Epoch: 548/2000... Training loss: 1.0341\n",
      "Epoch: 548/2000... Training loss: 1.0239\n",
      "Epoch: 548/2000... Training loss: 0.8681\n",
      "Epoch: 548/2000... Training loss: 1.0590\n",
      "Epoch: 548/2000... Training loss: 1.0022\n",
      "Epoch: 548/2000... Training loss: 0.9820\n",
      "Epoch: 548/2000... Training loss: 0.8870\n",
      "Epoch: 548/2000... Training loss: 0.9957\n",
      "Epoch: 549/2000... Training loss: 0.8826\n",
      "Epoch: 549/2000... Training loss: 0.7807\n",
      "Epoch: 549/2000... Training loss: 0.8317\n",
      "Epoch: 549/2000... Training loss: 0.8791\n",
      "Epoch: 549/2000... Training loss: 0.9171\n",
      "Epoch: 549/2000... Training loss: 0.9842\n",
      "Epoch: 549/2000... Training loss: 1.0582\n",
      "Epoch: 549/2000... Training loss: 1.0933\n",
      "Epoch: 549/2000... Training loss: 1.0011\n",
      "Epoch: 549/2000... Training loss: 0.9538\n",
      "Epoch: 549/2000... Training loss: 0.8667\n",
      "Epoch: 549/2000... Training loss: 1.0152\n",
      "Epoch: 549/2000... Training loss: 0.9025\n",
      "Epoch: 549/2000... Training loss: 0.9012\n",
      "Epoch: 549/2000... Training loss: 0.9361\n",
      "Epoch: 549/2000... Training loss: 0.9671\n",
      "Epoch: 549/2000... Training loss: 1.0055\n",
      "Epoch: 549/2000... Training loss: 0.7841\n",
      "Epoch: 549/2000... Training loss: 0.9511\n",
      "Epoch: 549/2000... Training loss: 0.7159\n",
      "Epoch: 549/2000... Training loss: 0.9797\n",
      "Epoch: 549/2000... Training loss: 0.8426\n",
      "Epoch: 549/2000... Training loss: 1.1845\n",
      "Epoch: 549/2000... Training loss: 0.8534\n",
      "Epoch: 549/2000... Training loss: 0.7020\n",
      "Epoch: 549/2000... Training loss: 1.0862\n",
      "Epoch: 549/2000... Training loss: 0.9861\n",
      "Epoch: 549/2000... Training loss: 1.0108\n",
      "Epoch: 549/2000... Training loss: 0.8212\n",
      "Epoch: 549/2000... Training loss: 0.7044\n",
      "Epoch: 549/2000... Training loss: 0.9711\n",
      "Epoch: 550/2000... Training loss: 0.9268\n",
      "Epoch: 550/2000... Training loss: 1.1260\n",
      "Epoch: 550/2000... Training loss: 0.7877\n",
      "Epoch: 550/2000... Training loss: 0.9011\n",
      "Epoch: 550/2000... Training loss: 0.8685\n",
      "Epoch: 550/2000... Training loss: 1.0499\n",
      "Epoch: 550/2000... Training loss: 1.1632\n",
      "Epoch: 550/2000... Training loss: 0.9113\n",
      "Epoch: 550/2000... Training loss: 1.0127\n",
      "Epoch: 550/2000... Training loss: 0.9260\n",
      "Epoch: 550/2000... Training loss: 0.9996\n",
      "Epoch: 550/2000... Training loss: 0.9570\n",
      "Epoch: 550/2000... Training loss: 0.8701\n",
      "Epoch: 550/2000... Training loss: 1.0390\n",
      "Epoch: 550/2000... Training loss: 0.7123\n",
      "Epoch: 550/2000... Training loss: 0.9637\n",
      "Epoch: 550/2000... Training loss: 0.7719\n",
      "Epoch: 550/2000... Training loss: 0.9481\n",
      "Epoch: 550/2000... Training loss: 0.8372\n",
      "Epoch: 550/2000... Training loss: 0.8555\n",
      "Epoch: 550/2000... Training loss: 1.0276\n",
      "Epoch: 550/2000... Training loss: 0.8716\n",
      "Epoch: 550/2000... Training loss: 0.8006\n",
      "Epoch: 550/2000... Training loss: 1.1821\n",
      "Epoch: 550/2000... Training loss: 0.7837\n",
      "Epoch: 550/2000... Training loss: 0.8226\n",
      "Epoch: 550/2000... Training loss: 0.9906\n",
      "Epoch: 550/2000... Training loss: 0.8412\n",
      "Epoch: 550/2000... Training loss: 1.0476\n",
      "Epoch: 550/2000... Training loss: 0.8735\n",
      "Epoch: 550/2000... Training loss: 0.9474\n",
      "Epoch: 551/2000... Training loss: 0.8103\n",
      "Epoch: 551/2000... Training loss: 1.1078\n",
      "Epoch: 551/2000... Training loss: 0.9030\n",
      "Epoch: 551/2000... Training loss: 1.0196\n",
      "Epoch: 551/2000... Training loss: 0.8812\n",
      "Epoch: 551/2000... Training loss: 0.9231\n",
      "Epoch: 551/2000... Training loss: 0.9867\n",
      "Epoch: 551/2000... Training loss: 0.7346\n",
      "Epoch: 551/2000... Training loss: 1.0427\n",
      "Epoch: 551/2000... Training loss: 0.7524\n",
      "Epoch: 551/2000... Training loss: 0.7909\n",
      "Epoch: 551/2000... Training loss: 1.0444\n",
      "Epoch: 551/2000... Training loss: 1.1285\n",
      "Epoch: 551/2000... Training loss: 0.9495\n",
      "Epoch: 551/2000... Training loss: 0.9592\n",
      "Epoch: 551/2000... Training loss: 1.1616\n",
      "Epoch: 551/2000... Training loss: 1.0021\n",
      "Epoch: 551/2000... Training loss: 0.8706\n",
      "Epoch: 551/2000... Training loss: 0.8239\n",
      "Epoch: 551/2000... Training loss: 0.7434\n",
      "Epoch: 551/2000... Training loss: 0.8336\n",
      "Epoch: 551/2000... Training loss: 1.0725\n",
      "Epoch: 551/2000... Training loss: 1.0531\n",
      "Epoch: 551/2000... Training loss: 1.3121\n",
      "Epoch: 551/2000... Training loss: 0.9820\n",
      "Epoch: 551/2000... Training loss: 0.9733\n",
      "Epoch: 551/2000... Training loss: 0.9469\n",
      "Epoch: 551/2000... Training loss: 0.9109\n",
      "Epoch: 551/2000... Training loss: 1.1190\n",
      "Epoch: 551/2000... Training loss: 1.1181\n",
      "Epoch: 551/2000... Training loss: 1.1454\n",
      "Epoch: 552/2000... Training loss: 1.1122\n",
      "Epoch: 552/2000... Training loss: 1.0036\n",
      "Epoch: 552/2000... Training loss: 0.9023\n",
      "Epoch: 552/2000... Training loss: 0.9675\n",
      "Epoch: 552/2000... Training loss: 0.9732\n",
      "Epoch: 552/2000... Training loss: 1.0790\n",
      "Epoch: 552/2000... Training loss: 1.0613\n",
      "Epoch: 552/2000... Training loss: 1.0377\n",
      "Epoch: 552/2000... Training loss: 0.8857\n",
      "Epoch: 552/2000... Training loss: 0.8939\n",
      "Epoch: 552/2000... Training loss: 0.7781\n",
      "Epoch: 552/2000... Training loss: 0.8537\n",
      "Epoch: 552/2000... Training loss: 0.8044\n",
      "Epoch: 552/2000... Training loss: 0.7901\n",
      "Epoch: 552/2000... Training loss: 0.6478\n",
      "Epoch: 552/2000... Training loss: 0.9757\n",
      "Epoch: 552/2000... Training loss: 0.9032\n",
      "Epoch: 552/2000... Training loss: 0.9056\n",
      "Epoch: 552/2000... Training loss: 1.0525\n",
      "Epoch: 552/2000... Training loss: 1.0200\n",
      "Epoch: 552/2000... Training loss: 1.1395\n",
      "Epoch: 552/2000... Training loss: 0.7872\n",
      "Epoch: 552/2000... Training loss: 1.0079\n",
      "Epoch: 552/2000... Training loss: 0.8404\n",
      "Epoch: 552/2000... Training loss: 0.9058\n",
      "Epoch: 552/2000... Training loss: 1.0013\n",
      "Epoch: 552/2000... Training loss: 0.9316\n",
      "Epoch: 552/2000... Training loss: 1.1149\n",
      "Epoch: 552/2000... Training loss: 1.0609\n",
      "Epoch: 552/2000... Training loss: 0.9889\n",
      "Epoch: 552/2000... Training loss: 0.9704\n",
      "Epoch: 553/2000... Training loss: 1.2404\n",
      "Epoch: 553/2000... Training loss: 0.9430\n",
      "Epoch: 553/2000... Training loss: 1.1376\n",
      "Epoch: 553/2000... Training loss: 0.7783\n",
      "Epoch: 553/2000... Training loss: 0.7726\n",
      "Epoch: 553/2000... Training loss: 0.8299\n",
      "Epoch: 553/2000... Training loss: 1.0679\n",
      "Epoch: 553/2000... Training loss: 1.0092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 553/2000... Training loss: 1.0242\n",
      "Epoch: 553/2000... Training loss: 1.1723\n",
      "Epoch: 553/2000... Training loss: 1.1975\n",
      "Epoch: 553/2000... Training loss: 1.0716\n",
      "Epoch: 553/2000... Training loss: 1.0285\n",
      "Epoch: 553/2000... Training loss: 0.8026\n",
      "Epoch: 553/2000... Training loss: 1.0533\n",
      "Epoch: 553/2000... Training loss: 1.0159\n",
      "Epoch: 553/2000... Training loss: 0.8941\n",
      "Epoch: 553/2000... Training loss: 0.7672\n",
      "Epoch: 553/2000... Training loss: 0.9689\n",
      "Epoch: 553/2000... Training loss: 0.8895\n",
      "Epoch: 553/2000... Training loss: 0.7803\n",
      "Epoch: 553/2000... Training loss: 0.9873\n",
      "Epoch: 553/2000... Training loss: 1.0428\n",
      "Epoch: 553/2000... Training loss: 0.9666\n",
      "Epoch: 553/2000... Training loss: 0.7404\n",
      "Epoch: 553/2000... Training loss: 0.9910\n",
      "Epoch: 553/2000... Training loss: 0.8353\n",
      "Epoch: 553/2000... Training loss: 0.9677\n",
      "Epoch: 553/2000... Training loss: 1.0004\n",
      "Epoch: 553/2000... Training loss: 0.8480\n",
      "Epoch: 553/2000... Training loss: 0.9345\n",
      "Epoch: 554/2000... Training loss: 1.0371\n",
      "Epoch: 554/2000... Training loss: 1.2264\n",
      "Epoch: 554/2000... Training loss: 0.7687\n",
      "Epoch: 554/2000... Training loss: 0.9381\n",
      "Epoch: 554/2000... Training loss: 0.7957\n",
      "Epoch: 554/2000... Training loss: 0.8316\n",
      "Epoch: 554/2000... Training loss: 1.0360\n",
      "Epoch: 554/2000... Training loss: 0.8099\n",
      "Epoch: 554/2000... Training loss: 0.8164\n",
      "Epoch: 554/2000... Training loss: 0.9157\n",
      "Epoch: 554/2000... Training loss: 0.9166\n",
      "Epoch: 554/2000... Training loss: 0.9375\n",
      "Epoch: 554/2000... Training loss: 0.9994\n",
      "Epoch: 554/2000... Training loss: 0.7981\n",
      "Epoch: 554/2000... Training loss: 0.8508\n",
      "Epoch: 554/2000... Training loss: 0.8839\n",
      "Epoch: 554/2000... Training loss: 0.8085\n",
      "Epoch: 554/2000... Training loss: 0.9332\n",
      "Epoch: 554/2000... Training loss: 0.9878\n",
      "Epoch: 554/2000... Training loss: 0.9236\n",
      "Epoch: 554/2000... Training loss: 1.0010\n",
      "Epoch: 554/2000... Training loss: 1.0259\n",
      "Epoch: 554/2000... Training loss: 0.8322\n",
      "Epoch: 554/2000... Training loss: 0.7487\n",
      "Epoch: 554/2000... Training loss: 0.9893\n",
      "Epoch: 554/2000... Training loss: 0.8036\n",
      "Epoch: 554/2000... Training loss: 0.9219\n",
      "Epoch: 554/2000... Training loss: 1.0819\n",
      "Epoch: 554/2000... Training loss: 0.9593\n",
      "Epoch: 554/2000... Training loss: 0.9805\n",
      "Epoch: 554/2000... Training loss: 0.9701\n",
      "Epoch: 555/2000... Training loss: 1.1148\n",
      "Epoch: 555/2000... Training loss: 0.8765\n",
      "Epoch: 555/2000... Training loss: 0.9260\n",
      "Epoch: 555/2000... Training loss: 0.8801\n",
      "Epoch: 555/2000... Training loss: 0.8941\n",
      "Epoch: 555/2000... Training loss: 0.7533\n",
      "Epoch: 555/2000... Training loss: 0.7427\n",
      "Epoch: 555/2000... Training loss: 1.1228\n",
      "Epoch: 555/2000... Training loss: 1.0288\n",
      "Epoch: 555/2000... Training loss: 0.9953\n",
      "Epoch: 555/2000... Training loss: 0.9983\n",
      "Epoch: 555/2000... Training loss: 1.0210\n",
      "Epoch: 555/2000... Training loss: 0.7832\n",
      "Epoch: 555/2000... Training loss: 1.0819\n",
      "Epoch: 555/2000... Training loss: 0.6951\n",
      "Epoch: 555/2000... Training loss: 1.0135\n",
      "Epoch: 555/2000... Training loss: 0.8816\n",
      "Epoch: 555/2000... Training loss: 1.0004\n",
      "Epoch: 555/2000... Training loss: 0.8735\n",
      "Epoch: 555/2000... Training loss: 0.9055\n",
      "Epoch: 555/2000... Training loss: 0.8062\n",
      "Epoch: 555/2000... Training loss: 0.9236\n",
      "Epoch: 555/2000... Training loss: 1.0842\n",
      "Epoch: 555/2000... Training loss: 1.0373\n",
      "Epoch: 555/2000... Training loss: 1.1320\n",
      "Epoch: 555/2000... Training loss: 0.8041\n",
      "Epoch: 555/2000... Training loss: 0.8161\n",
      "Epoch: 555/2000... Training loss: 1.0070\n",
      "Epoch: 555/2000... Training loss: 0.9308\n",
      "Epoch: 555/2000... Training loss: 0.9185\n",
      "Epoch: 555/2000... Training loss: 1.0314\n",
      "Epoch: 556/2000... Training loss: 0.8950\n",
      "Epoch: 556/2000... Training loss: 0.9570\n",
      "Epoch: 556/2000... Training loss: 1.0296\n",
      "Epoch: 556/2000... Training loss: 1.1071\n",
      "Epoch: 556/2000... Training loss: 0.8801\n",
      "Epoch: 556/2000... Training loss: 0.8766\n",
      "Epoch: 556/2000... Training loss: 0.9866\n",
      "Epoch: 556/2000... Training loss: 0.9651\n",
      "Epoch: 556/2000... Training loss: 0.9754\n",
      "Epoch: 556/2000... Training loss: 1.1378\n",
      "Epoch: 556/2000... Training loss: 0.9724\n",
      "Epoch: 556/2000... Training loss: 0.7725\n",
      "Epoch: 556/2000... Training loss: 0.9947\n",
      "Epoch: 556/2000... Training loss: 1.0094\n",
      "Epoch: 556/2000... Training loss: 1.2628\n",
      "Epoch: 556/2000... Training loss: 0.9384\n",
      "Epoch: 556/2000... Training loss: 0.9804\n",
      "Epoch: 556/2000... Training loss: 1.0544\n",
      "Epoch: 556/2000... Training loss: 1.0649\n",
      "Epoch: 556/2000... Training loss: 0.9279\n",
      "Epoch: 556/2000... Training loss: 0.8900\n",
      "Epoch: 556/2000... Training loss: 0.8627\n",
      "Epoch: 556/2000... Training loss: 1.0159\n",
      "Epoch: 556/2000... Training loss: 0.8773\n",
      "Epoch: 556/2000... Training loss: 1.1256\n",
      "Epoch: 556/2000... Training loss: 0.8046\n",
      "Epoch: 556/2000... Training loss: 0.9344\n",
      "Epoch: 556/2000... Training loss: 0.6730\n",
      "Epoch: 556/2000... Training loss: 0.8420\n",
      "Epoch: 556/2000... Training loss: 0.7754\n",
      "Epoch: 556/2000... Training loss: 1.0875\n",
      "Epoch: 557/2000... Training loss: 0.8366\n",
      "Epoch: 557/2000... Training loss: 1.0305\n",
      "Epoch: 557/2000... Training loss: 1.0567\n",
      "Epoch: 557/2000... Training loss: 0.9578\n",
      "Epoch: 557/2000... Training loss: 0.8955\n",
      "Epoch: 557/2000... Training loss: 1.1246\n",
      "Epoch: 557/2000... Training loss: 1.0175\n",
      "Epoch: 557/2000... Training loss: 0.8557\n",
      "Epoch: 557/2000... Training loss: 0.9008\n",
      "Epoch: 557/2000... Training loss: 0.8486\n",
      "Epoch: 557/2000... Training loss: 0.8608\n",
      "Epoch: 557/2000... Training loss: 0.8444\n",
      "Epoch: 557/2000... Training loss: 1.0701\n",
      "Epoch: 557/2000... Training loss: 0.8670\n",
      "Epoch: 557/2000... Training loss: 0.9518\n",
      "Epoch: 557/2000... Training loss: 1.0216\n",
      "Epoch: 557/2000... Training loss: 0.9075\n",
      "Epoch: 557/2000... Training loss: 0.9855\n",
      "Epoch: 557/2000... Training loss: 1.1086\n",
      "Epoch: 557/2000... Training loss: 1.1613\n",
      "Epoch: 557/2000... Training loss: 0.9034\n",
      "Epoch: 557/2000... Training loss: 0.9682\n",
      "Epoch: 557/2000... Training loss: 0.9198\n",
      "Epoch: 557/2000... Training loss: 0.9247\n",
      "Epoch: 557/2000... Training loss: 0.8995\n",
      "Epoch: 557/2000... Training loss: 0.8683\n",
      "Epoch: 557/2000... Training loss: 1.0060\n",
      "Epoch: 557/2000... Training loss: 0.9657\n",
      "Epoch: 557/2000... Training loss: 1.0761\n",
      "Epoch: 557/2000... Training loss: 0.8057\n",
      "Epoch: 557/2000... Training loss: 0.9503\n",
      "Epoch: 558/2000... Training loss: 0.8543\n",
      "Epoch: 558/2000... Training loss: 1.1948\n",
      "Epoch: 558/2000... Training loss: 0.9425\n",
      "Epoch: 558/2000... Training loss: 0.9108\n",
      "Epoch: 558/2000... Training loss: 1.0245\n",
      "Epoch: 558/2000... Training loss: 0.9641\n",
      "Epoch: 558/2000... Training loss: 0.6893\n",
      "Epoch: 558/2000... Training loss: 0.7537\n",
      "Epoch: 558/2000... Training loss: 0.9572\n",
      "Epoch: 558/2000... Training loss: 1.0237\n",
      "Epoch: 558/2000... Training loss: 1.1048\n",
      "Epoch: 558/2000... Training loss: 0.8550\n",
      "Epoch: 558/2000... Training loss: 0.8536\n",
      "Epoch: 558/2000... Training loss: 0.9468\n",
      "Epoch: 558/2000... Training loss: 0.6862\n",
      "Epoch: 558/2000... Training loss: 0.9977\n",
      "Epoch: 558/2000... Training loss: 0.9246\n",
      "Epoch: 558/2000... Training loss: 0.8421\n",
      "Epoch: 558/2000... Training loss: 0.9214\n",
      "Epoch: 558/2000... Training loss: 0.9122\n",
      "Epoch: 558/2000... Training loss: 1.0965\n",
      "Epoch: 558/2000... Training loss: 0.7463\n",
      "Epoch: 558/2000... Training loss: 0.9333\n",
      "Epoch: 558/2000... Training loss: 0.9600\n",
      "Epoch: 558/2000... Training loss: 0.8981\n",
      "Epoch: 558/2000... Training loss: 0.9306\n",
      "Epoch: 558/2000... Training loss: 0.8622\n",
      "Epoch: 558/2000... Training loss: 0.9356\n",
      "Epoch: 558/2000... Training loss: 0.6873\n",
      "Epoch: 558/2000... Training loss: 0.9910\n",
      "Epoch: 558/2000... Training loss: 0.8418\n",
      "Epoch: 559/2000... Training loss: 0.8386\n",
      "Epoch: 559/2000... Training loss: 0.9544\n",
      "Epoch: 559/2000... Training loss: 0.7475\n",
      "Epoch: 559/2000... Training loss: 0.7846\n",
      "Epoch: 559/2000... Training loss: 1.0900\n",
      "Epoch: 559/2000... Training loss: 0.7274\n",
      "Epoch: 559/2000... Training loss: 0.8134\n",
      "Epoch: 559/2000... Training loss: 0.7382\n",
      "Epoch: 559/2000... Training loss: 1.1013\n",
      "Epoch: 559/2000... Training loss: 1.0989\n",
      "Epoch: 559/2000... Training loss: 0.8857\n",
      "Epoch: 559/2000... Training loss: 0.9120\n",
      "Epoch: 559/2000... Training loss: 0.9840\n",
      "Epoch: 559/2000... Training loss: 0.8412\n",
      "Epoch: 559/2000... Training loss: 0.7492\n",
      "Epoch: 559/2000... Training loss: 1.0222\n",
      "Epoch: 559/2000... Training loss: 0.9126\n",
      "Epoch: 559/2000... Training loss: 0.8991\n",
      "Epoch: 559/2000... Training loss: 0.7968\n",
      "Epoch: 559/2000... Training loss: 1.0671\n",
      "Epoch: 559/2000... Training loss: 1.0310\n",
      "Epoch: 559/2000... Training loss: 0.8456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 559/2000... Training loss: 1.1092\n",
      "Epoch: 559/2000... Training loss: 0.7522\n",
      "Epoch: 559/2000... Training loss: 0.9740\n",
      "Epoch: 559/2000... Training loss: 1.0687\n",
      "Epoch: 559/2000... Training loss: 0.9731\n",
      "Epoch: 559/2000... Training loss: 0.8168\n",
      "Epoch: 559/2000... Training loss: 0.9074\n",
      "Epoch: 559/2000... Training loss: 0.8311\n",
      "Epoch: 559/2000... Training loss: 0.9878\n",
      "Epoch: 560/2000... Training loss: 0.7842\n",
      "Epoch: 560/2000... Training loss: 0.8257\n",
      "Epoch: 560/2000... Training loss: 1.1710\n",
      "Epoch: 560/2000... Training loss: 1.0786\n",
      "Epoch: 560/2000... Training loss: 0.8733\n",
      "Epoch: 560/2000... Training loss: 0.9963\n",
      "Epoch: 560/2000... Training loss: 0.8294\n",
      "Epoch: 560/2000... Training loss: 1.0328\n",
      "Epoch: 560/2000... Training loss: 1.2300\n",
      "Epoch: 560/2000... Training loss: 1.0028\n",
      "Epoch: 560/2000... Training loss: 0.9723\n",
      "Epoch: 560/2000... Training loss: 0.9573\n",
      "Epoch: 560/2000... Training loss: 0.9163\n",
      "Epoch: 560/2000... Training loss: 1.0137\n",
      "Epoch: 560/2000... Training loss: 0.8054\n",
      "Epoch: 560/2000... Training loss: 0.9988\n",
      "Epoch: 560/2000... Training loss: 0.8294\n",
      "Epoch: 560/2000... Training loss: 0.7254\n",
      "Epoch: 560/2000... Training loss: 1.1955\n",
      "Epoch: 560/2000... Training loss: 0.8276\n",
      "Epoch: 560/2000... Training loss: 0.8057\n",
      "Epoch: 560/2000... Training loss: 0.7773\n",
      "Epoch: 560/2000... Training loss: 0.9872\n",
      "Epoch: 560/2000... Training loss: 0.9094\n",
      "Epoch: 560/2000... Training loss: 0.9926\n",
      "Epoch: 560/2000... Training loss: 0.7384\n",
      "Epoch: 560/2000... Training loss: 0.9333\n",
      "Epoch: 560/2000... Training loss: 1.1809\n",
      "Epoch: 560/2000... Training loss: 0.9338\n",
      "Epoch: 560/2000... Training loss: 0.8599\n",
      "Epoch: 560/2000... Training loss: 1.0144\n",
      "Epoch: 561/2000... Training loss: 1.2284\n",
      "Epoch: 561/2000... Training loss: 1.0636\n",
      "Epoch: 561/2000... Training loss: 0.8981\n",
      "Epoch: 561/2000... Training loss: 0.8887\n",
      "Epoch: 561/2000... Training loss: 1.0548\n",
      "Epoch: 561/2000... Training loss: 0.8703\n",
      "Epoch: 561/2000... Training loss: 0.9564\n",
      "Epoch: 561/2000... Training loss: 0.8073\n",
      "Epoch: 561/2000... Training loss: 0.9136\n",
      "Epoch: 561/2000... Training loss: 1.1592\n",
      "Epoch: 561/2000... Training loss: 1.0698\n",
      "Epoch: 561/2000... Training loss: 0.7932\n",
      "Epoch: 561/2000... Training loss: 0.9462\n",
      "Epoch: 561/2000... Training loss: 0.8941\n",
      "Epoch: 561/2000... Training loss: 1.2535\n",
      "Epoch: 561/2000... Training loss: 0.7985\n",
      "Epoch: 561/2000... Training loss: 0.8823\n",
      "Epoch: 561/2000... Training loss: 0.8757\n",
      "Epoch: 561/2000... Training loss: 0.7744\n",
      "Epoch: 561/2000... Training loss: 0.9788\n",
      "Epoch: 561/2000... Training loss: 0.8382\n",
      "Epoch: 561/2000... Training loss: 0.8946\n",
      "Epoch: 561/2000... Training loss: 1.0180\n",
      "Epoch: 561/2000... Training loss: 0.9275\n",
      "Epoch: 561/2000... Training loss: 0.5700\n",
      "Epoch: 561/2000... Training loss: 0.9839\n",
      "Epoch: 561/2000... Training loss: 1.0841\n",
      "Epoch: 561/2000... Training loss: 0.8979\n",
      "Epoch: 561/2000... Training loss: 1.0004\n",
      "Epoch: 561/2000... Training loss: 1.0101\n",
      "Epoch: 561/2000... Training loss: 0.8018\n",
      "Epoch: 562/2000... Training loss: 0.9569\n",
      "Epoch: 562/2000... Training loss: 0.7127\n",
      "Epoch: 562/2000... Training loss: 0.9882\n",
      "Epoch: 562/2000... Training loss: 0.9587\n",
      "Epoch: 562/2000... Training loss: 0.8374\n",
      "Epoch: 562/2000... Training loss: 0.7559\n",
      "Epoch: 562/2000... Training loss: 0.8071\n",
      "Epoch: 562/2000... Training loss: 0.7673\n",
      "Epoch: 562/2000... Training loss: 0.9388\n",
      "Epoch: 562/2000... Training loss: 0.8560\n",
      "Epoch: 562/2000... Training loss: 0.8768\n",
      "Epoch: 562/2000... Training loss: 0.9340\n",
      "Epoch: 562/2000... Training loss: 0.9382\n",
      "Epoch: 562/2000... Training loss: 1.2872\n",
      "Epoch: 562/2000... Training loss: 0.9863\n",
      "Epoch: 562/2000... Training loss: 1.1426\n",
      "Epoch: 562/2000... Training loss: 0.9395\n",
      "Epoch: 562/2000... Training loss: 1.0924\n",
      "Epoch: 562/2000... Training loss: 1.0406\n",
      "Epoch: 562/2000... Training loss: 0.7314\n",
      "Epoch: 562/2000... Training loss: 0.9026\n",
      "Epoch: 562/2000... Training loss: 0.8310\n",
      "Epoch: 562/2000... Training loss: 0.9058\n",
      "Epoch: 562/2000... Training loss: 1.0398\n",
      "Epoch: 562/2000... Training loss: 0.8863\n",
      "Epoch: 562/2000... Training loss: 1.0896\n",
      "Epoch: 562/2000... Training loss: 0.9081\n",
      "Epoch: 562/2000... Training loss: 0.9803\n",
      "Epoch: 562/2000... Training loss: 1.0415\n",
      "Epoch: 562/2000... Training loss: 0.9145\n",
      "Epoch: 562/2000... Training loss: 0.8115\n",
      "Epoch: 563/2000... Training loss: 0.9824\n",
      "Epoch: 563/2000... Training loss: 0.8592\n",
      "Epoch: 563/2000... Training loss: 0.9269\n",
      "Epoch: 563/2000... Training loss: 0.9255\n",
      "Epoch: 563/2000... Training loss: 0.7551\n",
      "Epoch: 563/2000... Training loss: 0.9920\n",
      "Epoch: 563/2000... Training loss: 0.8617\n",
      "Epoch: 563/2000... Training loss: 1.0462\n",
      "Epoch: 563/2000... Training loss: 1.1625\n",
      "Epoch: 563/2000... Training loss: 1.0428\n",
      "Epoch: 563/2000... Training loss: 0.8325\n",
      "Epoch: 563/2000... Training loss: 1.0935\n",
      "Epoch: 563/2000... Training loss: 0.9524\n",
      "Epoch: 563/2000... Training loss: 1.1213\n",
      "Epoch: 563/2000... Training loss: 1.1450\n",
      "Epoch: 563/2000... Training loss: 0.7717\n",
      "Epoch: 563/2000... Training loss: 1.0655\n",
      "Epoch: 563/2000... Training loss: 0.7527\n",
      "Epoch: 563/2000... Training loss: 0.8862\n",
      "Epoch: 563/2000... Training loss: 0.9113\n",
      "Epoch: 563/2000... Training loss: 0.9253\n",
      "Epoch: 563/2000... Training loss: 0.9388\n",
      "Epoch: 563/2000... Training loss: 0.9387\n",
      "Epoch: 563/2000... Training loss: 0.9289\n",
      "Epoch: 563/2000... Training loss: 1.0891\n",
      "Epoch: 563/2000... Training loss: 0.8566\n",
      "Epoch: 563/2000... Training loss: 0.9748\n",
      "Epoch: 563/2000... Training loss: 0.9584\n",
      "Epoch: 563/2000... Training loss: 0.9091\n",
      "Epoch: 563/2000... Training loss: 0.9182\n",
      "Epoch: 563/2000... Training loss: 0.9504\n",
      "Epoch: 564/2000... Training loss: 1.0790\n",
      "Epoch: 564/2000... Training loss: 0.7892\n",
      "Epoch: 564/2000... Training loss: 1.0095\n",
      "Epoch: 564/2000... Training loss: 0.9266\n",
      "Epoch: 564/2000... Training loss: 0.9813\n",
      "Epoch: 564/2000... Training loss: 1.0194\n",
      "Epoch: 564/2000... Training loss: 1.0494\n",
      "Epoch: 564/2000... Training loss: 0.9565\n",
      "Epoch: 564/2000... Training loss: 0.7967\n",
      "Epoch: 564/2000... Training loss: 0.9479\n",
      "Epoch: 564/2000... Training loss: 0.9615\n",
      "Epoch: 564/2000... Training loss: 1.1117\n",
      "Epoch: 564/2000... Training loss: 0.9062\n",
      "Epoch: 564/2000... Training loss: 0.7362\n",
      "Epoch: 564/2000... Training loss: 0.7969\n",
      "Epoch: 564/2000... Training loss: 0.9143\n",
      "Epoch: 564/2000... Training loss: 1.0098\n",
      "Epoch: 564/2000... Training loss: 0.9161\n",
      "Epoch: 564/2000... Training loss: 0.7563\n",
      "Epoch: 564/2000... Training loss: 1.1345\n",
      "Epoch: 564/2000... Training loss: 0.8113\n",
      "Epoch: 564/2000... Training loss: 1.1454\n",
      "Epoch: 564/2000... Training loss: 1.1227\n",
      "Epoch: 564/2000... Training loss: 0.8975\n",
      "Epoch: 564/2000... Training loss: 0.8922\n",
      "Epoch: 564/2000... Training loss: 1.0655\n",
      "Epoch: 564/2000... Training loss: 0.9082\n",
      "Epoch: 564/2000... Training loss: 1.2574\n",
      "Epoch: 564/2000... Training loss: 1.1751\n",
      "Epoch: 564/2000... Training loss: 0.7453\n",
      "Epoch: 564/2000... Training loss: 0.8256\n",
      "Epoch: 565/2000... Training loss: 0.8932\n",
      "Epoch: 565/2000... Training loss: 0.9525\n",
      "Epoch: 565/2000... Training loss: 0.9340\n",
      "Epoch: 565/2000... Training loss: 0.9611\n",
      "Epoch: 565/2000... Training loss: 1.0189\n",
      "Epoch: 565/2000... Training loss: 0.7840\n",
      "Epoch: 565/2000... Training loss: 0.8781\n",
      "Epoch: 565/2000... Training loss: 1.0108\n",
      "Epoch: 565/2000... Training loss: 0.7585\n",
      "Epoch: 565/2000... Training loss: 1.2271\n",
      "Epoch: 565/2000... Training loss: 0.9074\n",
      "Epoch: 565/2000... Training loss: 0.9862\n",
      "Epoch: 565/2000... Training loss: 1.0763\n",
      "Epoch: 565/2000... Training loss: 0.9055\n",
      "Epoch: 565/2000... Training loss: 0.8977\n",
      "Epoch: 565/2000... Training loss: 1.1438\n",
      "Epoch: 565/2000... Training loss: 0.7579\n",
      "Epoch: 565/2000... Training loss: 0.8076\n",
      "Epoch: 565/2000... Training loss: 0.9975\n",
      "Epoch: 565/2000... Training loss: 0.7644\n",
      "Epoch: 565/2000... Training loss: 0.9222\n",
      "Epoch: 565/2000... Training loss: 0.7528\n",
      "Epoch: 565/2000... Training loss: 1.0534\n",
      "Epoch: 565/2000... Training loss: 0.9554\n",
      "Epoch: 565/2000... Training loss: 0.8425\n",
      "Epoch: 565/2000... Training loss: 0.9750\n",
      "Epoch: 565/2000... Training loss: 1.2432\n",
      "Epoch: 565/2000... Training loss: 0.9175\n",
      "Epoch: 565/2000... Training loss: 1.0342\n",
      "Epoch: 565/2000... Training loss: 1.1322\n",
      "Epoch: 565/2000... Training loss: 0.9696\n",
      "Epoch: 566/2000... Training loss: 0.9890\n",
      "Epoch: 566/2000... Training loss: 1.2589\n",
      "Epoch: 566/2000... Training loss: 0.9687\n",
      "Epoch: 566/2000... Training loss: 1.0133\n",
      "Epoch: 566/2000... Training loss: 0.9423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 566/2000... Training loss: 0.9994\n",
      "Epoch: 566/2000... Training loss: 0.8355\n",
      "Epoch: 566/2000... Training loss: 0.9723\n",
      "Epoch: 566/2000... Training loss: 0.8853\n",
      "Epoch: 566/2000... Training loss: 1.0429\n",
      "Epoch: 566/2000... Training loss: 0.8900\n",
      "Epoch: 566/2000... Training loss: 0.9362\n",
      "Epoch: 566/2000... Training loss: 0.9227\n",
      "Epoch: 566/2000... Training loss: 0.8036\n",
      "Epoch: 566/2000... Training loss: 0.8756\n",
      "Epoch: 566/2000... Training loss: 1.0025\n",
      "Epoch: 566/2000... Training loss: 1.0852\n",
      "Epoch: 566/2000... Training loss: 0.8388\n",
      "Epoch: 566/2000... Training loss: 1.0840\n",
      "Epoch: 566/2000... Training loss: 0.7802\n",
      "Epoch: 566/2000... Training loss: 0.8356\n",
      "Epoch: 566/2000... Training loss: 1.1814\n",
      "Epoch: 566/2000... Training loss: 0.8826\n",
      "Epoch: 566/2000... Training loss: 0.7215\n",
      "Epoch: 566/2000... Training loss: 0.7837\n",
      "Epoch: 566/2000... Training loss: 1.0031\n",
      "Epoch: 566/2000... Training loss: 0.9566\n",
      "Epoch: 566/2000... Training loss: 0.9321\n",
      "Epoch: 566/2000... Training loss: 1.0160\n",
      "Epoch: 566/2000... Training loss: 0.9933\n",
      "Epoch: 566/2000... Training loss: 0.8105\n",
      "Epoch: 567/2000... Training loss: 0.6814\n",
      "Epoch: 567/2000... Training loss: 0.7802\n",
      "Epoch: 567/2000... Training loss: 0.9618\n",
      "Epoch: 567/2000... Training loss: 1.1443\n",
      "Epoch: 567/2000... Training loss: 1.0334\n",
      "Epoch: 567/2000... Training loss: 0.6806\n",
      "Epoch: 567/2000... Training loss: 0.8029\n",
      "Epoch: 567/2000... Training loss: 0.8827\n",
      "Epoch: 567/2000... Training loss: 0.8781\n",
      "Epoch: 567/2000... Training loss: 1.0220\n",
      "Epoch: 567/2000... Training loss: 0.9230\n",
      "Epoch: 567/2000... Training loss: 0.9506\n",
      "Epoch: 567/2000... Training loss: 1.1071\n",
      "Epoch: 567/2000... Training loss: 0.7999\n",
      "Epoch: 567/2000... Training loss: 0.9358\n",
      "Epoch: 567/2000... Training loss: 0.9158\n",
      "Epoch: 567/2000... Training loss: 0.8658\n",
      "Epoch: 567/2000... Training loss: 0.9553\n",
      "Epoch: 567/2000... Training loss: 0.8470\n",
      "Epoch: 567/2000... Training loss: 0.9722\n",
      "Epoch: 567/2000... Training loss: 0.7873\n",
      "Epoch: 567/2000... Training loss: 0.8741\n",
      "Epoch: 567/2000... Training loss: 0.8718\n",
      "Epoch: 567/2000... Training loss: 0.8585\n",
      "Epoch: 567/2000... Training loss: 0.8459\n",
      "Epoch: 567/2000... Training loss: 0.9153\n",
      "Epoch: 567/2000... Training loss: 1.0420\n",
      "Epoch: 567/2000... Training loss: 0.8918\n",
      "Epoch: 567/2000... Training loss: 0.6414\n",
      "Epoch: 567/2000... Training loss: 0.7219\n",
      "Epoch: 567/2000... Training loss: 0.9717\n",
      "Epoch: 568/2000... Training loss: 0.9210\n",
      "Epoch: 568/2000... Training loss: 0.7524\n",
      "Epoch: 568/2000... Training loss: 1.1313\n",
      "Epoch: 568/2000... Training loss: 0.8681\n",
      "Epoch: 568/2000... Training loss: 0.8597\n",
      "Epoch: 568/2000... Training loss: 0.9368\n",
      "Epoch: 568/2000... Training loss: 1.0593\n",
      "Epoch: 568/2000... Training loss: 1.0054\n",
      "Epoch: 568/2000... Training loss: 1.0161\n",
      "Epoch: 568/2000... Training loss: 0.8794\n",
      "Epoch: 568/2000... Training loss: 0.7804\n",
      "Epoch: 568/2000... Training loss: 0.9442\n",
      "Epoch: 568/2000... Training loss: 1.0959\n",
      "Epoch: 568/2000... Training loss: 0.7709\n",
      "Epoch: 568/2000... Training loss: 0.6859\n",
      "Epoch: 568/2000... Training loss: 0.8518\n",
      "Epoch: 568/2000... Training loss: 1.0506\n",
      "Epoch: 568/2000... Training loss: 0.9209\n",
      "Epoch: 568/2000... Training loss: 0.7177\n",
      "Epoch: 568/2000... Training loss: 0.7119\n",
      "Epoch: 568/2000... Training loss: 0.8349\n",
      "Epoch: 568/2000... Training loss: 0.8567\n",
      "Epoch: 568/2000... Training loss: 0.8951\n",
      "Epoch: 568/2000... Training loss: 1.1485\n",
      "Epoch: 568/2000... Training loss: 0.6787\n",
      "Epoch: 568/2000... Training loss: 0.9320\n",
      "Epoch: 568/2000... Training loss: 0.6943\n",
      "Epoch: 568/2000... Training loss: 0.7304\n",
      "Epoch: 568/2000... Training loss: 1.0503\n",
      "Epoch: 568/2000... Training loss: 0.7686\n",
      "Epoch: 568/2000... Training loss: 0.9110\n",
      "Epoch: 569/2000... Training loss: 0.7403\n",
      "Epoch: 569/2000... Training loss: 0.8395\n",
      "Epoch: 569/2000... Training loss: 0.9737\n",
      "Epoch: 569/2000... Training loss: 1.0562\n",
      "Epoch: 569/2000... Training loss: 0.9485\n",
      "Epoch: 569/2000... Training loss: 0.8484\n",
      "Epoch: 569/2000... Training loss: 0.9822\n",
      "Epoch: 569/2000... Training loss: 0.8242\n",
      "Epoch: 569/2000... Training loss: 1.0047\n",
      "Epoch: 569/2000... Training loss: 0.9223\n",
      "Epoch: 569/2000... Training loss: 1.0253\n",
      "Epoch: 569/2000... Training loss: 1.1235\n",
      "Epoch: 569/2000... Training loss: 0.9056\n",
      "Epoch: 569/2000... Training loss: 1.0006\n",
      "Epoch: 569/2000... Training loss: 1.0530\n",
      "Epoch: 569/2000... Training loss: 0.6531\n",
      "Epoch: 569/2000... Training loss: 0.8131\n",
      "Epoch: 569/2000... Training loss: 0.9771\n",
      "Epoch: 569/2000... Training loss: 1.0495\n",
      "Epoch: 569/2000... Training loss: 0.9967\n",
      "Epoch: 569/2000... Training loss: 0.7304\n",
      "Epoch: 569/2000... Training loss: 0.8458\n",
      "Epoch: 569/2000... Training loss: 0.7998\n",
      "Epoch: 569/2000... Training loss: 0.8735\n",
      "Epoch: 569/2000... Training loss: 0.8278\n",
      "Epoch: 569/2000... Training loss: 0.9496\n",
      "Epoch: 569/2000... Training loss: 0.7973\n",
      "Epoch: 569/2000... Training loss: 0.9962\n",
      "Epoch: 569/2000... Training loss: 0.9997\n",
      "Epoch: 569/2000... Training loss: 0.9172\n",
      "Epoch: 569/2000... Training loss: 0.8304\n",
      "Epoch: 570/2000... Training loss: 0.8227\n",
      "Epoch: 570/2000... Training loss: 0.9708\n",
      "Epoch: 570/2000... Training loss: 0.8616\n",
      "Epoch: 570/2000... Training loss: 1.0881\n",
      "Epoch: 570/2000... Training loss: 0.9424\n",
      "Epoch: 570/2000... Training loss: 0.8463\n",
      "Epoch: 570/2000... Training loss: 0.7609\n",
      "Epoch: 570/2000... Training loss: 0.9425\n",
      "Epoch: 570/2000... Training loss: 0.9549\n",
      "Epoch: 570/2000... Training loss: 0.7940\n",
      "Epoch: 570/2000... Training loss: 0.7711\n",
      "Epoch: 570/2000... Training loss: 0.8715\n",
      "Epoch: 570/2000... Training loss: 0.9207\n",
      "Epoch: 570/2000... Training loss: 1.2785\n",
      "Epoch: 570/2000... Training loss: 0.7934\n",
      "Epoch: 570/2000... Training loss: 1.0311\n",
      "Epoch: 570/2000... Training loss: 0.9410\n",
      "Epoch: 570/2000... Training loss: 0.8114\n",
      "Epoch: 570/2000... Training loss: 0.9423\n",
      "Epoch: 570/2000... Training loss: 0.9754\n",
      "Epoch: 570/2000... Training loss: 0.7155\n",
      "Epoch: 570/2000... Training loss: 0.6520\n",
      "Epoch: 570/2000... Training loss: 1.0439\n",
      "Epoch: 570/2000... Training loss: 0.7908\n",
      "Epoch: 570/2000... Training loss: 0.9724\n",
      "Epoch: 570/2000... Training loss: 0.9225\n",
      "Epoch: 570/2000... Training loss: 0.8328\n",
      "Epoch: 570/2000... Training loss: 0.9278\n",
      "Epoch: 570/2000... Training loss: 0.7913\n",
      "Epoch: 570/2000... Training loss: 0.8985\n",
      "Epoch: 570/2000... Training loss: 0.7563\n",
      "Epoch: 571/2000... Training loss: 0.8403\n",
      "Epoch: 571/2000... Training loss: 0.7952\n",
      "Epoch: 571/2000... Training loss: 0.9193\n",
      "Epoch: 571/2000... Training loss: 0.9436\n",
      "Epoch: 571/2000... Training loss: 0.8966\n",
      "Epoch: 571/2000... Training loss: 0.9720\n",
      "Epoch: 571/2000... Training loss: 1.0971\n",
      "Epoch: 571/2000... Training loss: 0.7184\n",
      "Epoch: 571/2000... Training loss: 0.9197\n",
      "Epoch: 571/2000... Training loss: 1.2819\n",
      "Epoch: 571/2000... Training loss: 0.7642\n",
      "Epoch: 571/2000... Training loss: 1.1609\n",
      "Epoch: 571/2000... Training loss: 1.0898\n",
      "Epoch: 571/2000... Training loss: 1.0806\n",
      "Epoch: 571/2000... Training loss: 0.7887\n",
      "Epoch: 571/2000... Training loss: 0.7339\n",
      "Epoch: 571/2000... Training loss: 1.0509\n",
      "Epoch: 571/2000... Training loss: 0.8665\n",
      "Epoch: 571/2000... Training loss: 0.9923\n",
      "Epoch: 571/2000... Training loss: 0.8448\n",
      "Epoch: 571/2000... Training loss: 0.8272\n",
      "Epoch: 571/2000... Training loss: 0.9406\n",
      "Epoch: 571/2000... Training loss: 0.6490\n",
      "Epoch: 571/2000... Training loss: 0.6935\n",
      "Epoch: 571/2000... Training loss: 0.9880\n",
      "Epoch: 571/2000... Training loss: 0.9570\n",
      "Epoch: 571/2000... Training loss: 0.8875\n",
      "Epoch: 571/2000... Training loss: 0.9848\n",
      "Epoch: 571/2000... Training loss: 0.9214\n",
      "Epoch: 571/2000... Training loss: 0.9003\n",
      "Epoch: 571/2000... Training loss: 1.0527\n",
      "Epoch: 572/2000... Training loss: 0.9301\n",
      "Epoch: 572/2000... Training loss: 1.1997\n",
      "Epoch: 572/2000... Training loss: 0.8681\n",
      "Epoch: 572/2000... Training loss: 0.7368\n",
      "Epoch: 572/2000... Training loss: 0.7765\n",
      "Epoch: 572/2000... Training loss: 0.8224\n",
      "Epoch: 572/2000... Training loss: 0.8517\n",
      "Epoch: 572/2000... Training loss: 0.9361\n",
      "Epoch: 572/2000... Training loss: 1.0041\n",
      "Epoch: 572/2000... Training loss: 1.0763\n",
      "Epoch: 572/2000... Training loss: 0.9246\n",
      "Epoch: 572/2000... Training loss: 0.7754\n",
      "Epoch: 572/2000... Training loss: 1.1163\n",
      "Epoch: 572/2000... Training loss: 1.0589\n",
      "Epoch: 572/2000... Training loss: 1.0739\n",
      "Epoch: 572/2000... Training loss: 0.7262\n",
      "Epoch: 572/2000... Training loss: 0.8241\n",
      "Epoch: 572/2000... Training loss: 0.8663\n",
      "Epoch: 572/2000... Training loss: 0.8229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 572/2000... Training loss: 0.9930\n",
      "Epoch: 572/2000... Training loss: 0.8557\n",
      "Epoch: 572/2000... Training loss: 0.7271\n",
      "Epoch: 572/2000... Training loss: 1.0060\n",
      "Epoch: 572/2000... Training loss: 0.9949\n",
      "Epoch: 572/2000... Training loss: 0.8829\n",
      "Epoch: 572/2000... Training loss: 0.9858\n",
      "Epoch: 572/2000... Training loss: 1.0259\n",
      "Epoch: 572/2000... Training loss: 1.0537\n",
      "Epoch: 572/2000... Training loss: 0.6363\n",
      "Epoch: 572/2000... Training loss: 0.9936\n",
      "Epoch: 572/2000... Training loss: 0.8173\n",
      "Epoch: 573/2000... Training loss: 0.7954\n",
      "Epoch: 573/2000... Training loss: 1.0797\n",
      "Epoch: 573/2000... Training loss: 0.8056\n",
      "Epoch: 573/2000... Training loss: 1.1993\n",
      "Epoch: 573/2000... Training loss: 0.6919\n",
      "Epoch: 573/2000... Training loss: 1.0524\n",
      "Epoch: 573/2000... Training loss: 1.0169\n",
      "Epoch: 573/2000... Training loss: 0.8682\n",
      "Epoch: 573/2000... Training loss: 1.1373\n",
      "Epoch: 573/2000... Training loss: 0.9339\n",
      "Epoch: 573/2000... Training loss: 0.7471\n",
      "Epoch: 573/2000... Training loss: 0.9550\n",
      "Epoch: 573/2000... Training loss: 1.0043\n",
      "Epoch: 573/2000... Training loss: 0.8305\n",
      "Epoch: 573/2000... Training loss: 1.0158\n",
      "Epoch: 573/2000... Training loss: 0.8536\n",
      "Epoch: 573/2000... Training loss: 0.8360\n",
      "Epoch: 573/2000... Training loss: 0.8803\n",
      "Epoch: 573/2000... Training loss: 1.1308\n",
      "Epoch: 573/2000... Training loss: 0.7611\n",
      "Epoch: 573/2000... Training loss: 0.7928\n",
      "Epoch: 573/2000... Training loss: 0.8639\n",
      "Epoch: 573/2000... Training loss: 0.9997\n",
      "Epoch: 573/2000... Training loss: 0.8323\n",
      "Epoch: 573/2000... Training loss: 0.8328\n",
      "Epoch: 573/2000... Training loss: 0.8973\n",
      "Epoch: 573/2000... Training loss: 1.0544\n",
      "Epoch: 573/2000... Training loss: 0.9717\n",
      "Epoch: 573/2000... Training loss: 0.8327\n",
      "Epoch: 573/2000... Training loss: 0.8469\n",
      "Epoch: 573/2000... Training loss: 0.9258\n",
      "Epoch: 574/2000... Training loss: 0.9989\n",
      "Epoch: 574/2000... Training loss: 1.1950\n",
      "Epoch: 574/2000... Training loss: 0.8944\n",
      "Epoch: 574/2000... Training loss: 0.8526\n",
      "Epoch: 574/2000... Training loss: 0.7918\n",
      "Epoch: 574/2000... Training loss: 0.8837\n",
      "Epoch: 574/2000... Training loss: 0.9987\n",
      "Epoch: 574/2000... Training loss: 0.8297\n",
      "Epoch: 574/2000... Training loss: 0.8425\n",
      "Epoch: 574/2000... Training loss: 0.9050\n",
      "Epoch: 574/2000... Training loss: 0.9654\n",
      "Epoch: 574/2000... Training loss: 1.0558\n",
      "Epoch: 574/2000... Training loss: 1.0630\n",
      "Epoch: 574/2000... Training loss: 0.8043\n",
      "Epoch: 574/2000... Training loss: 0.7771\n",
      "Epoch: 574/2000... Training loss: 0.8633\n",
      "Epoch: 574/2000... Training loss: 0.9164\n",
      "Epoch: 574/2000... Training loss: 0.7230\n",
      "Epoch: 574/2000... Training loss: 0.8160\n",
      "Epoch: 574/2000... Training loss: 0.9361\n",
      "Epoch: 574/2000... Training loss: 0.8162\n",
      "Epoch: 574/2000... Training loss: 0.7889\n",
      "Epoch: 574/2000... Training loss: 0.9167\n",
      "Epoch: 574/2000... Training loss: 0.9995\n",
      "Epoch: 574/2000... Training loss: 0.8269\n",
      "Epoch: 574/2000... Training loss: 0.9114\n",
      "Epoch: 574/2000... Training loss: 1.0548\n",
      "Epoch: 574/2000... Training loss: 0.9122\n",
      "Epoch: 574/2000... Training loss: 0.8569\n",
      "Epoch: 574/2000... Training loss: 1.0416\n",
      "Epoch: 574/2000... Training loss: 0.8165\n",
      "Epoch: 575/2000... Training loss: 0.9150\n",
      "Epoch: 575/2000... Training loss: 0.8417\n",
      "Epoch: 575/2000... Training loss: 0.8454\n",
      "Epoch: 575/2000... Training loss: 0.9230\n",
      "Epoch: 575/2000... Training loss: 0.8364\n",
      "Epoch: 575/2000... Training loss: 0.7194\n",
      "Epoch: 575/2000... Training loss: 0.7457\n",
      "Epoch: 575/2000... Training loss: 0.8223\n",
      "Epoch: 575/2000... Training loss: 0.8277\n",
      "Epoch: 575/2000... Training loss: 1.0459\n",
      "Epoch: 575/2000... Training loss: 1.0451\n",
      "Epoch: 575/2000... Training loss: 1.0428\n",
      "Epoch: 575/2000... Training loss: 0.9353\n",
      "Epoch: 575/2000... Training loss: 0.8671\n",
      "Epoch: 575/2000... Training loss: 0.8815\n",
      "Epoch: 575/2000... Training loss: 0.9453\n",
      "Epoch: 575/2000... Training loss: 0.8043\n",
      "Epoch: 575/2000... Training loss: 0.8547\n",
      "Epoch: 575/2000... Training loss: 1.0227\n",
      "Epoch: 575/2000... Training loss: 1.0180\n",
      "Epoch: 575/2000... Training loss: 0.8053\n",
      "Epoch: 575/2000... Training loss: 0.8462\n",
      "Epoch: 575/2000... Training loss: 0.8844\n",
      "Epoch: 575/2000... Training loss: 0.8867\n",
      "Epoch: 575/2000... Training loss: 0.8612\n",
      "Epoch: 575/2000... Training loss: 1.1006\n",
      "Epoch: 575/2000... Training loss: 0.7882\n",
      "Epoch: 575/2000... Training loss: 0.8809\n",
      "Epoch: 575/2000... Training loss: 0.7617\n",
      "Epoch: 575/2000... Training loss: 0.6238\n",
      "Epoch: 575/2000... Training loss: 0.8947\n",
      "Epoch: 576/2000... Training loss: 0.8837\n",
      "Epoch: 576/2000... Training loss: 1.1417\n",
      "Epoch: 576/2000... Training loss: 0.9248\n",
      "Epoch: 576/2000... Training loss: 0.8815\n",
      "Epoch: 576/2000... Training loss: 0.9886\n",
      "Epoch: 576/2000... Training loss: 0.9512\n",
      "Epoch: 576/2000... Training loss: 0.9334\n",
      "Epoch: 576/2000... Training loss: 0.8807\n",
      "Epoch: 576/2000... Training loss: 0.8543\n",
      "Epoch: 576/2000... Training loss: 0.8500\n",
      "Epoch: 576/2000... Training loss: 0.6286\n",
      "Epoch: 576/2000... Training loss: 0.9264\n",
      "Epoch: 576/2000... Training loss: 0.8997\n",
      "Epoch: 576/2000... Training loss: 1.0198\n",
      "Epoch: 576/2000... Training loss: 0.9291\n",
      "Epoch: 576/2000... Training loss: 1.0908\n",
      "Epoch: 576/2000... Training loss: 0.9434\n",
      "Epoch: 576/2000... Training loss: 1.1671\n",
      "Epoch: 576/2000... Training loss: 0.8579\n",
      "Epoch: 576/2000... Training loss: 0.9681\n",
      "Epoch: 576/2000... Training loss: 0.8752\n",
      "Epoch: 576/2000... Training loss: 0.6607\n",
      "Epoch: 576/2000... Training loss: 1.0066\n",
      "Epoch: 576/2000... Training loss: 0.8427\n",
      "Epoch: 576/2000... Training loss: 0.8532\n",
      "Epoch: 576/2000... Training loss: 0.8259\n",
      "Epoch: 576/2000... Training loss: 0.7379\n",
      "Epoch: 576/2000... Training loss: 0.8140\n",
      "Epoch: 576/2000... Training loss: 0.8008\n",
      "Epoch: 576/2000... Training loss: 0.8056\n",
      "Epoch: 576/2000... Training loss: 0.9870\n",
      "Epoch: 577/2000... Training loss: 0.8210\n",
      "Epoch: 577/2000... Training loss: 0.8414\n",
      "Epoch: 577/2000... Training loss: 0.9065\n",
      "Epoch: 577/2000... Training loss: 0.7681\n",
      "Epoch: 577/2000... Training loss: 0.9998\n",
      "Epoch: 577/2000... Training loss: 1.0132\n",
      "Epoch: 577/2000... Training loss: 0.7465\n",
      "Epoch: 577/2000... Training loss: 0.9041\n",
      "Epoch: 577/2000... Training loss: 1.0072\n",
      "Epoch: 577/2000... Training loss: 0.8735\n",
      "Epoch: 577/2000... Training loss: 1.0025\n",
      "Epoch: 577/2000... Training loss: 1.0309\n",
      "Epoch: 577/2000... Training loss: 0.7275\n",
      "Epoch: 577/2000... Training loss: 1.0163\n",
      "Epoch: 577/2000... Training loss: 0.7922\n",
      "Epoch: 577/2000... Training loss: 0.6784\n",
      "Epoch: 577/2000... Training loss: 0.7622\n",
      "Epoch: 577/2000... Training loss: 0.7640\n",
      "Epoch: 577/2000... Training loss: 0.8911\n",
      "Epoch: 577/2000... Training loss: 0.9891\n",
      "Epoch: 577/2000... Training loss: 0.9239\n",
      "Epoch: 577/2000... Training loss: 1.1029\n",
      "Epoch: 577/2000... Training loss: 0.7888\n",
      "Epoch: 577/2000... Training loss: 1.0846\n",
      "Epoch: 577/2000... Training loss: 0.9832\n",
      "Epoch: 577/2000... Training loss: 1.0843\n",
      "Epoch: 577/2000... Training loss: 0.9724\n",
      "Epoch: 577/2000... Training loss: 0.8910\n",
      "Epoch: 577/2000... Training loss: 0.8784\n",
      "Epoch: 577/2000... Training loss: 0.7599\n",
      "Epoch: 577/2000... Training loss: 0.9063\n",
      "Epoch: 578/2000... Training loss: 1.0293\n",
      "Epoch: 578/2000... Training loss: 1.0056\n",
      "Epoch: 578/2000... Training loss: 0.7454\n",
      "Epoch: 578/2000... Training loss: 0.6320\n",
      "Epoch: 578/2000... Training loss: 1.0237\n",
      "Epoch: 578/2000... Training loss: 0.7968\n",
      "Epoch: 578/2000... Training loss: 1.1223\n",
      "Epoch: 578/2000... Training loss: 0.8010\n",
      "Epoch: 578/2000... Training loss: 1.2234\n",
      "Epoch: 578/2000... Training loss: 0.6753\n",
      "Epoch: 578/2000... Training loss: 1.1286\n",
      "Epoch: 578/2000... Training loss: 0.8808\n",
      "Epoch: 578/2000... Training loss: 0.7081\n",
      "Epoch: 578/2000... Training loss: 0.8857\n",
      "Epoch: 578/2000... Training loss: 1.0772\n",
      "Epoch: 578/2000... Training loss: 0.9318\n",
      "Epoch: 578/2000... Training loss: 1.0091\n",
      "Epoch: 578/2000... Training loss: 0.9428\n",
      "Epoch: 578/2000... Training loss: 0.7711\n",
      "Epoch: 578/2000... Training loss: 0.8469\n",
      "Epoch: 578/2000... Training loss: 0.8037\n",
      "Epoch: 578/2000... Training loss: 0.8794\n",
      "Epoch: 578/2000... Training loss: 0.9089\n",
      "Epoch: 578/2000... Training loss: 0.9681\n",
      "Epoch: 578/2000... Training loss: 1.0134\n",
      "Epoch: 578/2000... Training loss: 1.0521\n",
      "Epoch: 578/2000... Training loss: 0.7627\n",
      "Epoch: 578/2000... Training loss: 1.0010\n",
      "Epoch: 578/2000... Training loss: 1.0462\n",
      "Epoch: 578/2000... Training loss: 0.8827\n",
      "Epoch: 578/2000... Training loss: 0.8872\n",
      "Epoch: 579/2000... Training loss: 0.8751\n",
      "Epoch: 579/2000... Training loss: 0.7475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 579/2000... Training loss: 0.9114\n",
      "Epoch: 579/2000... Training loss: 0.7934\n",
      "Epoch: 579/2000... Training loss: 0.8098\n",
      "Epoch: 579/2000... Training loss: 1.1046\n",
      "Epoch: 579/2000... Training loss: 0.8425\n",
      "Epoch: 579/2000... Training loss: 0.8319\n",
      "Epoch: 579/2000... Training loss: 0.9158\n",
      "Epoch: 579/2000... Training loss: 1.0355\n",
      "Epoch: 579/2000... Training loss: 0.7831\n",
      "Epoch: 579/2000... Training loss: 0.8743\n",
      "Epoch: 579/2000... Training loss: 0.9762\n",
      "Epoch: 579/2000... Training loss: 0.6998\n",
      "Epoch: 579/2000... Training loss: 0.8804\n",
      "Epoch: 579/2000... Training loss: 0.8121\n",
      "Epoch: 579/2000... Training loss: 0.8738\n",
      "Epoch: 579/2000... Training loss: 0.7504\n",
      "Epoch: 579/2000... Training loss: 0.8813\n",
      "Epoch: 579/2000... Training loss: 0.9921\n",
      "Epoch: 579/2000... Training loss: 0.7089\n",
      "Epoch: 579/2000... Training loss: 0.7710\n",
      "Epoch: 579/2000... Training loss: 0.8989\n",
      "Epoch: 579/2000... Training loss: 0.8192\n",
      "Epoch: 579/2000... Training loss: 0.9086\n",
      "Epoch: 579/2000... Training loss: 0.9118\n",
      "Epoch: 579/2000... Training loss: 0.8873\n",
      "Epoch: 579/2000... Training loss: 0.8624\n",
      "Epoch: 579/2000... Training loss: 0.7484\n",
      "Epoch: 579/2000... Training loss: 0.9789\n",
      "Epoch: 579/2000... Training loss: 0.9670\n",
      "Epoch: 580/2000... Training loss: 0.7955\n",
      "Epoch: 580/2000... Training loss: 0.9751\n",
      "Epoch: 580/2000... Training loss: 0.8613\n",
      "Epoch: 580/2000... Training loss: 0.9681\n",
      "Epoch: 580/2000... Training loss: 0.8071\n",
      "Epoch: 580/2000... Training loss: 0.8446\n",
      "Epoch: 580/2000... Training loss: 0.7482\n",
      "Epoch: 580/2000... Training loss: 0.9801\n",
      "Epoch: 580/2000... Training loss: 0.9334\n",
      "Epoch: 580/2000... Training loss: 1.0444\n",
      "Epoch: 580/2000... Training loss: 0.9174\n",
      "Epoch: 580/2000... Training loss: 0.9955\n",
      "Epoch: 580/2000... Training loss: 0.9254\n",
      "Epoch: 580/2000... Training loss: 0.9682\n",
      "Epoch: 580/2000... Training loss: 0.7993\n",
      "Epoch: 580/2000... Training loss: 0.8416\n",
      "Epoch: 580/2000... Training loss: 1.0042\n",
      "Epoch: 580/2000... Training loss: 0.8275\n",
      "Epoch: 580/2000... Training loss: 0.7952\n",
      "Epoch: 580/2000... Training loss: 0.8120\n",
      "Epoch: 580/2000... Training loss: 1.0989\n",
      "Epoch: 580/2000... Training loss: 0.8174\n",
      "Epoch: 580/2000... Training loss: 0.7912\n",
      "Epoch: 580/2000... Training loss: 1.1269\n",
      "Epoch: 580/2000... Training loss: 0.9900\n",
      "Epoch: 580/2000... Training loss: 0.8359\n",
      "Epoch: 580/2000... Training loss: 0.8945\n",
      "Epoch: 580/2000... Training loss: 0.9736\n",
      "Epoch: 580/2000... Training loss: 0.8715\n",
      "Epoch: 580/2000... Training loss: 0.8361\n",
      "Epoch: 580/2000... Training loss: 0.7815\n",
      "Epoch: 581/2000... Training loss: 0.8539\n",
      "Epoch: 581/2000... Training loss: 0.8048\n",
      "Epoch: 581/2000... Training loss: 0.9179\n",
      "Epoch: 581/2000... Training loss: 0.6457\n",
      "Epoch: 581/2000... Training loss: 0.8735\n",
      "Epoch: 581/2000... Training loss: 0.9859\n",
      "Epoch: 581/2000... Training loss: 0.8269\n",
      "Epoch: 581/2000... Training loss: 0.9134\n",
      "Epoch: 581/2000... Training loss: 0.7961\n",
      "Epoch: 581/2000... Training loss: 0.9004\n",
      "Epoch: 581/2000... Training loss: 0.6980\n",
      "Epoch: 581/2000... Training loss: 0.6947\n",
      "Epoch: 581/2000... Training loss: 1.1095\n",
      "Epoch: 581/2000... Training loss: 0.9199\n",
      "Epoch: 581/2000... Training loss: 0.7599\n",
      "Epoch: 581/2000... Training loss: 0.8531\n",
      "Epoch: 581/2000... Training loss: 0.7963\n",
      "Epoch: 581/2000... Training loss: 0.6942\n",
      "Epoch: 581/2000... Training loss: 0.9485\n",
      "Epoch: 581/2000... Training loss: 0.9758\n",
      "Epoch: 581/2000... Training loss: 0.7975\n",
      "Epoch: 581/2000... Training loss: 0.7994\n",
      "Epoch: 581/2000... Training loss: 0.7704\n",
      "Epoch: 581/2000... Training loss: 1.0638\n",
      "Epoch: 581/2000... Training loss: 0.8407\n",
      "Epoch: 581/2000... Training loss: 0.8047\n",
      "Epoch: 581/2000... Training loss: 1.0638\n",
      "Epoch: 581/2000... Training loss: 0.9891\n",
      "Epoch: 581/2000... Training loss: 0.9681\n",
      "Epoch: 581/2000... Training loss: 0.9076\n",
      "Epoch: 581/2000... Training loss: 0.7154\n",
      "Epoch: 582/2000... Training loss: 0.9244\n",
      "Epoch: 582/2000... Training loss: 0.9048\n",
      "Epoch: 582/2000... Training loss: 0.8340\n",
      "Epoch: 582/2000... Training loss: 1.0447\n",
      "Epoch: 582/2000... Training loss: 0.7864\n",
      "Epoch: 582/2000... Training loss: 0.5987\n",
      "Epoch: 582/2000... Training loss: 0.7209\n",
      "Epoch: 582/2000... Training loss: 0.8284\n",
      "Epoch: 582/2000... Training loss: 1.0927\n",
      "Epoch: 582/2000... Training loss: 0.8766\n",
      "Epoch: 582/2000... Training loss: 0.9161\n",
      "Epoch: 582/2000... Training loss: 1.0667\n",
      "Epoch: 582/2000... Training loss: 0.9419\n",
      "Epoch: 582/2000... Training loss: 0.8242\n",
      "Epoch: 582/2000... Training loss: 1.0809\n",
      "Epoch: 582/2000... Training loss: 0.7263\n",
      "Epoch: 582/2000... Training loss: 0.9394\n",
      "Epoch: 582/2000... Training loss: 0.8593\n",
      "Epoch: 582/2000... Training loss: 1.0350\n",
      "Epoch: 582/2000... Training loss: 1.0065\n",
      "Epoch: 582/2000... Training loss: 0.8162\n",
      "Epoch: 582/2000... Training loss: 0.6624\n",
      "Epoch: 582/2000... Training loss: 0.8875\n",
      "Epoch: 582/2000... Training loss: 0.8724\n",
      "Epoch: 582/2000... Training loss: 0.8532\n",
      "Epoch: 582/2000... Training loss: 1.0827\n",
      "Epoch: 582/2000... Training loss: 0.9044\n",
      "Epoch: 582/2000... Training loss: 0.8734\n",
      "Epoch: 582/2000... Training loss: 0.6811\n",
      "Epoch: 582/2000... Training loss: 0.7715\n",
      "Epoch: 582/2000... Training loss: 0.8372\n",
      "Epoch: 583/2000... Training loss: 1.0616\n",
      "Epoch: 583/2000... Training loss: 0.9676\n",
      "Epoch: 583/2000... Training loss: 0.9612\n",
      "Epoch: 583/2000... Training loss: 0.7322\n",
      "Epoch: 583/2000... Training loss: 0.7332\n",
      "Epoch: 583/2000... Training loss: 0.8320\n",
      "Epoch: 583/2000... Training loss: 0.8367\n",
      "Epoch: 583/2000... Training loss: 0.7138\n",
      "Epoch: 583/2000... Training loss: 0.9091\n",
      "Epoch: 583/2000... Training loss: 0.9549\n",
      "Epoch: 583/2000... Training loss: 0.9386\n",
      "Epoch: 583/2000... Training loss: 1.0707\n",
      "Epoch: 583/2000... Training loss: 0.9419\n",
      "Epoch: 583/2000... Training loss: 0.9674\n",
      "Epoch: 583/2000... Training loss: 0.8674\n",
      "Epoch: 583/2000... Training loss: 0.9977\n",
      "Epoch: 583/2000... Training loss: 0.9949\n",
      "Epoch: 583/2000... Training loss: 0.7717\n",
      "Epoch: 583/2000... Training loss: 0.9727\n",
      "Epoch: 583/2000... Training loss: 0.9574\n",
      "Epoch: 583/2000... Training loss: 0.7169\n",
      "Epoch: 583/2000... Training loss: 0.8575\n",
      "Epoch: 583/2000... Training loss: 0.7909\n",
      "Epoch: 583/2000... Training loss: 0.6821\n",
      "Epoch: 583/2000... Training loss: 0.8414\n",
      "Epoch: 583/2000... Training loss: 1.0848\n",
      "Epoch: 583/2000... Training loss: 0.8242\n",
      "Epoch: 583/2000... Training loss: 0.8422\n",
      "Epoch: 583/2000... Training loss: 0.7581\n",
      "Epoch: 583/2000... Training loss: 0.8422\n",
      "Epoch: 583/2000... Training loss: 0.9095\n",
      "Epoch: 584/2000... Training loss: 0.8069\n",
      "Epoch: 584/2000... Training loss: 0.8016\n",
      "Epoch: 584/2000... Training loss: 0.9330\n",
      "Epoch: 584/2000... Training loss: 0.9782\n",
      "Epoch: 584/2000... Training loss: 0.7851\n",
      "Epoch: 584/2000... Training loss: 0.9030\n",
      "Epoch: 584/2000... Training loss: 0.7402\n",
      "Epoch: 584/2000... Training loss: 0.8649\n",
      "Epoch: 584/2000... Training loss: 0.8999\n",
      "Epoch: 584/2000... Training loss: 1.0482\n",
      "Epoch: 584/2000... Training loss: 1.0833\n",
      "Epoch: 584/2000... Training loss: 0.9298\n",
      "Epoch: 584/2000... Training loss: 0.8582\n",
      "Epoch: 584/2000... Training loss: 0.6184\n",
      "Epoch: 584/2000... Training loss: 0.8593\n",
      "Epoch: 584/2000... Training loss: 0.7290\n",
      "Epoch: 584/2000... Training loss: 0.8416\n",
      "Epoch: 584/2000... Training loss: 0.7198\n",
      "Epoch: 584/2000... Training loss: 0.9878\n",
      "Epoch: 584/2000... Training loss: 0.8868\n",
      "Epoch: 584/2000... Training loss: 0.9288\n",
      "Epoch: 584/2000... Training loss: 0.8595\n",
      "Epoch: 584/2000... Training loss: 1.0355\n",
      "Epoch: 584/2000... Training loss: 0.7050\n",
      "Epoch: 584/2000... Training loss: 1.0559\n",
      "Epoch: 584/2000... Training loss: 0.9342\n",
      "Epoch: 584/2000... Training loss: 1.1468\n",
      "Epoch: 584/2000... Training loss: 1.0833\n",
      "Epoch: 584/2000... Training loss: 1.0114\n",
      "Epoch: 584/2000... Training loss: 0.7920\n",
      "Epoch: 584/2000... Training loss: 0.8979\n",
      "Epoch: 585/2000... Training loss: 0.9602\n",
      "Epoch: 585/2000... Training loss: 0.8691\n",
      "Epoch: 585/2000... Training loss: 0.9013\n",
      "Epoch: 585/2000... Training loss: 0.9491\n",
      "Epoch: 585/2000... Training loss: 0.9301\n",
      "Epoch: 585/2000... Training loss: 1.0356\n",
      "Epoch: 585/2000... Training loss: 0.8667\n",
      "Epoch: 585/2000... Training loss: 0.6780\n",
      "Epoch: 585/2000... Training loss: 0.8379\n",
      "Epoch: 585/2000... Training loss: 1.0265\n",
      "Epoch: 585/2000... Training loss: 0.9147\n",
      "Epoch: 585/2000... Training loss: 1.0215\n",
      "Epoch: 585/2000... Training loss: 0.9878\n",
      "Epoch: 585/2000... Training loss: 1.0310\n",
      "Epoch: 585/2000... Training loss: 0.9324\n",
      "Epoch: 585/2000... Training loss: 0.8216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 585/2000... Training loss: 0.9638\n",
      "Epoch: 585/2000... Training loss: 0.7373\n",
      "Epoch: 585/2000... Training loss: 0.8605\n",
      "Epoch: 585/2000... Training loss: 0.8174\n",
      "Epoch: 585/2000... Training loss: 0.9256\n",
      "Epoch: 585/2000... Training loss: 0.7760\n",
      "Epoch: 585/2000... Training loss: 0.7880\n",
      "Epoch: 585/2000... Training loss: 0.7863\n",
      "Epoch: 585/2000... Training loss: 1.0267\n",
      "Epoch: 585/2000... Training loss: 0.8497\n",
      "Epoch: 585/2000... Training loss: 0.7855\n",
      "Epoch: 585/2000... Training loss: 0.8683\n",
      "Epoch: 585/2000... Training loss: 0.7959\n",
      "Epoch: 585/2000... Training loss: 0.9537\n",
      "Epoch: 585/2000... Training loss: 0.8575\n",
      "Epoch: 586/2000... Training loss: 0.9747\n",
      "Epoch: 586/2000... Training loss: 1.0388\n",
      "Epoch: 586/2000... Training loss: 0.9760\n",
      "Epoch: 586/2000... Training loss: 1.0849\n",
      "Epoch: 586/2000... Training loss: 1.0383\n",
      "Epoch: 586/2000... Training loss: 1.0744\n",
      "Epoch: 586/2000... Training loss: 0.8867\n",
      "Epoch: 586/2000... Training loss: 0.9073\n",
      "Epoch: 586/2000... Training loss: 0.6731\n",
      "Epoch: 586/2000... Training loss: 1.0016\n",
      "Epoch: 586/2000... Training loss: 0.7512\n",
      "Epoch: 586/2000... Training loss: 1.0673\n",
      "Epoch: 586/2000... Training loss: 0.9722\n",
      "Epoch: 586/2000... Training loss: 1.0137\n",
      "Epoch: 586/2000... Training loss: 0.9601\n",
      "Epoch: 586/2000... Training loss: 0.8846\n",
      "Epoch: 586/2000... Training loss: 0.9399\n",
      "Epoch: 586/2000... Training loss: 0.8458\n",
      "Epoch: 586/2000... Training loss: 1.0994\n",
      "Epoch: 586/2000... Training loss: 0.5944\n",
      "Epoch: 586/2000... Training loss: 0.8933\n",
      "Epoch: 586/2000... Training loss: 0.9208\n",
      "Epoch: 586/2000... Training loss: 0.8072\n",
      "Epoch: 586/2000... Training loss: 0.7392\n",
      "Epoch: 586/2000... Training loss: 0.9109\n",
      "Epoch: 586/2000... Training loss: 1.0926\n",
      "Epoch: 586/2000... Training loss: 0.6926\n",
      "Epoch: 586/2000... Training loss: 0.9233\n",
      "Epoch: 586/2000... Training loss: 0.7786\n",
      "Epoch: 586/2000... Training loss: 0.8829\n",
      "Epoch: 586/2000... Training loss: 0.8248\n",
      "Epoch: 587/2000... Training loss: 0.7116\n",
      "Epoch: 587/2000... Training loss: 1.0502\n",
      "Epoch: 587/2000... Training loss: 0.9549\n",
      "Epoch: 587/2000... Training loss: 0.9626\n",
      "Epoch: 587/2000... Training loss: 0.7178\n",
      "Epoch: 587/2000... Training loss: 0.6965\n",
      "Epoch: 587/2000... Training loss: 0.6790\n",
      "Epoch: 587/2000... Training loss: 0.7243\n",
      "Epoch: 587/2000... Training loss: 0.8384\n",
      "Epoch: 587/2000... Training loss: 0.8319\n",
      "Epoch: 587/2000... Training loss: 0.9408\n",
      "Epoch: 587/2000... Training loss: 0.6116\n",
      "Epoch: 587/2000... Training loss: 0.8865\n",
      "Epoch: 587/2000... Training loss: 0.7924\n",
      "Epoch: 587/2000... Training loss: 0.5825\n",
      "Epoch: 587/2000... Training loss: 1.0048\n",
      "Epoch: 587/2000... Training loss: 0.8041\n",
      "Epoch: 587/2000... Training loss: 0.8318\n",
      "Epoch: 587/2000... Training loss: 0.7882\n",
      "Epoch: 587/2000... Training loss: 0.9405\n",
      "Epoch: 587/2000... Training loss: 0.7096\n",
      "Epoch: 587/2000... Training loss: 0.9025\n",
      "Epoch: 587/2000... Training loss: 0.9321\n",
      "Epoch: 587/2000... Training loss: 0.9005\n",
      "Epoch: 587/2000... Training loss: 0.9013\n",
      "Epoch: 587/2000... Training loss: 1.0432\n",
      "Epoch: 587/2000... Training loss: 1.0555\n",
      "Epoch: 587/2000... Training loss: 1.0047\n",
      "Epoch: 587/2000... Training loss: 0.8824\n",
      "Epoch: 587/2000... Training loss: 0.8313\n",
      "Epoch: 587/2000... Training loss: 0.8314\n",
      "Epoch: 588/2000... Training loss: 0.7340\n",
      "Epoch: 588/2000... Training loss: 0.8918\n",
      "Epoch: 588/2000... Training loss: 0.9444\n",
      "Epoch: 588/2000... Training loss: 0.7980\n",
      "Epoch: 588/2000... Training loss: 0.7839\n",
      "Epoch: 588/2000... Training loss: 0.9796\n",
      "Epoch: 588/2000... Training loss: 0.9878\n",
      "Epoch: 588/2000... Training loss: 0.8226\n",
      "Epoch: 588/2000... Training loss: 1.2176\n",
      "Epoch: 588/2000... Training loss: 0.8412\n",
      "Epoch: 588/2000... Training loss: 0.8505\n",
      "Epoch: 588/2000... Training loss: 0.8532\n",
      "Epoch: 588/2000... Training loss: 0.8460\n",
      "Epoch: 588/2000... Training loss: 0.9850\n",
      "Epoch: 588/2000... Training loss: 0.9198\n",
      "Epoch: 588/2000... Training loss: 0.8384\n",
      "Epoch: 588/2000... Training loss: 0.8670\n",
      "Epoch: 588/2000... Training loss: 0.7163\n",
      "Epoch: 588/2000... Training loss: 0.9312\n",
      "Epoch: 588/2000... Training loss: 0.7272\n",
      "Epoch: 588/2000... Training loss: 0.8470\n",
      "Epoch: 588/2000... Training loss: 0.6951\n",
      "Epoch: 588/2000... Training loss: 1.0294\n",
      "Epoch: 588/2000... Training loss: 0.9798\n",
      "Epoch: 588/2000... Training loss: 0.7463\n",
      "Epoch: 588/2000... Training loss: 0.6957\n",
      "Epoch: 588/2000... Training loss: 1.0512\n",
      "Epoch: 588/2000... Training loss: 0.9501\n",
      "Epoch: 588/2000... Training loss: 0.8346\n",
      "Epoch: 588/2000... Training loss: 0.8045\n",
      "Epoch: 588/2000... Training loss: 1.2036\n",
      "Epoch: 589/2000... Training loss: 1.0697\n",
      "Epoch: 589/2000... Training loss: 0.8559\n",
      "Epoch: 589/2000... Training loss: 0.8569\n",
      "Epoch: 589/2000... Training loss: 0.8350\n",
      "Epoch: 589/2000... Training loss: 0.8164\n",
      "Epoch: 589/2000... Training loss: 0.9171\n",
      "Epoch: 589/2000... Training loss: 0.9386\n",
      "Epoch: 589/2000... Training loss: 0.7519\n",
      "Epoch: 589/2000... Training loss: 0.9381\n",
      "Epoch: 589/2000... Training loss: 1.0399\n",
      "Epoch: 589/2000... Training loss: 0.6560\n",
      "Epoch: 589/2000... Training loss: 0.8342\n",
      "Epoch: 589/2000... Training loss: 1.0813\n",
      "Epoch: 589/2000... Training loss: 0.8743\n",
      "Epoch: 589/2000... Training loss: 0.8434\n",
      "Epoch: 589/2000... Training loss: 1.0693\n",
      "Epoch: 589/2000... Training loss: 0.8295\n",
      "Epoch: 589/2000... Training loss: 1.0270\n",
      "Epoch: 589/2000... Training loss: 0.6046\n",
      "Epoch: 589/2000... Training loss: 0.9125\n",
      "Epoch: 589/2000... Training loss: 0.6915\n",
      "Epoch: 589/2000... Training loss: 1.0474\n",
      "Epoch: 589/2000... Training loss: 0.8370\n",
      "Epoch: 589/2000... Training loss: 0.9298\n",
      "Epoch: 589/2000... Training loss: 1.0281\n",
      "Epoch: 589/2000... Training loss: 0.6750\n",
      "Epoch: 589/2000... Training loss: 0.7441\n",
      "Epoch: 589/2000... Training loss: 1.0311\n",
      "Epoch: 589/2000... Training loss: 0.8787\n",
      "Epoch: 589/2000... Training loss: 0.7515\n",
      "Epoch: 589/2000... Training loss: 0.7121\n",
      "Epoch: 590/2000... Training loss: 0.9853\n",
      "Epoch: 590/2000... Training loss: 0.6807\n",
      "Epoch: 590/2000... Training loss: 0.8202\n",
      "Epoch: 590/2000... Training loss: 0.7394\n",
      "Epoch: 590/2000... Training loss: 0.9323\n",
      "Epoch: 590/2000... Training loss: 0.9408\n",
      "Epoch: 590/2000... Training loss: 0.9301\n",
      "Epoch: 590/2000... Training loss: 0.8012\n",
      "Epoch: 590/2000... Training loss: 0.9201\n",
      "Epoch: 590/2000... Training loss: 0.8290\n",
      "Epoch: 590/2000... Training loss: 0.9546\n",
      "Epoch: 590/2000... Training loss: 0.7869\n",
      "Epoch: 590/2000... Training loss: 0.9263\n",
      "Epoch: 590/2000... Training loss: 0.8234\n",
      "Epoch: 590/2000... Training loss: 0.8937\n",
      "Epoch: 590/2000... Training loss: 0.9643\n",
      "Epoch: 590/2000... Training loss: 0.9332\n",
      "Epoch: 590/2000... Training loss: 0.6524\n",
      "Epoch: 590/2000... Training loss: 0.9586\n",
      "Epoch: 590/2000... Training loss: 1.0244\n",
      "Epoch: 590/2000... Training loss: 0.7544\n",
      "Epoch: 590/2000... Training loss: 0.7530\n",
      "Epoch: 590/2000... Training loss: 0.9444\n",
      "Epoch: 590/2000... Training loss: 0.7287\n",
      "Epoch: 590/2000... Training loss: 0.9710\n",
      "Epoch: 590/2000... Training loss: 0.8414\n",
      "Epoch: 590/2000... Training loss: 0.9859\n",
      "Epoch: 590/2000... Training loss: 0.8036\n",
      "Epoch: 590/2000... Training loss: 0.7553\n",
      "Epoch: 590/2000... Training loss: 1.1408\n",
      "Epoch: 590/2000... Training loss: 0.7623\n",
      "Epoch: 591/2000... Training loss: 0.7518\n",
      "Epoch: 591/2000... Training loss: 0.8438\n",
      "Epoch: 591/2000... Training loss: 0.8225\n",
      "Epoch: 591/2000... Training loss: 0.8676\n",
      "Epoch: 591/2000... Training loss: 0.7549\n",
      "Epoch: 591/2000... Training loss: 0.8913\n",
      "Epoch: 591/2000... Training loss: 0.8246\n",
      "Epoch: 591/2000... Training loss: 0.8429\n",
      "Epoch: 591/2000... Training loss: 0.8418\n",
      "Epoch: 591/2000... Training loss: 1.1203\n",
      "Epoch: 591/2000... Training loss: 0.6037\n",
      "Epoch: 591/2000... Training loss: 0.9268\n",
      "Epoch: 591/2000... Training loss: 0.8428\n",
      "Epoch: 591/2000... Training loss: 0.7769\n",
      "Epoch: 591/2000... Training loss: 0.6564\n",
      "Epoch: 591/2000... Training loss: 0.9865\n",
      "Epoch: 591/2000... Training loss: 0.7416\n",
      "Epoch: 591/2000... Training loss: 1.0073\n",
      "Epoch: 591/2000... Training loss: 0.7753\n",
      "Epoch: 591/2000... Training loss: 0.9733\n",
      "Epoch: 591/2000... Training loss: 0.8767\n",
      "Epoch: 591/2000... Training loss: 0.9264\n",
      "Epoch: 591/2000... Training loss: 0.9666\n",
      "Epoch: 591/2000... Training loss: 0.9017\n",
      "Epoch: 591/2000... Training loss: 0.9537\n",
      "Epoch: 591/2000... Training loss: 0.7650\n",
      "Epoch: 591/2000... Training loss: 0.8048\n",
      "Epoch: 591/2000... Training loss: 0.7689\n",
      "Epoch: 591/2000... Training loss: 0.9462\n",
      "Epoch: 591/2000... Training loss: 0.9835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 591/2000... Training loss: 0.8918\n",
      "Epoch: 592/2000... Training loss: 1.0197\n",
      "Epoch: 592/2000... Training loss: 0.5787\n",
      "Epoch: 592/2000... Training loss: 0.9332\n",
      "Epoch: 592/2000... Training loss: 0.8736\n",
      "Epoch: 592/2000... Training loss: 0.9560\n",
      "Epoch: 592/2000... Training loss: 0.8480\n",
      "Epoch: 592/2000... Training loss: 0.7739\n",
      "Epoch: 592/2000... Training loss: 0.8362\n",
      "Epoch: 592/2000... Training loss: 0.9855\n",
      "Epoch: 592/2000... Training loss: 0.8095\n",
      "Epoch: 592/2000... Training loss: 0.7196\n",
      "Epoch: 592/2000... Training loss: 0.8784\n",
      "Epoch: 592/2000... Training loss: 0.8763\n",
      "Epoch: 592/2000... Training loss: 0.8573\n",
      "Epoch: 592/2000... Training loss: 0.8858\n",
      "Epoch: 592/2000... Training loss: 0.9588\n",
      "Epoch: 592/2000... Training loss: 1.0195\n",
      "Epoch: 592/2000... Training loss: 0.9460\n",
      "Epoch: 592/2000... Training loss: 0.8389\n",
      "Epoch: 592/2000... Training loss: 0.8238\n",
      "Epoch: 592/2000... Training loss: 0.6597\n",
      "Epoch: 592/2000... Training loss: 1.0810\n",
      "Epoch: 592/2000... Training loss: 0.8930\n",
      "Epoch: 592/2000... Training loss: 0.9259\n",
      "Epoch: 592/2000... Training loss: 0.9876\n",
      "Epoch: 592/2000... Training loss: 0.9798\n",
      "Epoch: 592/2000... Training loss: 0.9131\n",
      "Epoch: 592/2000... Training loss: 0.8041\n",
      "Epoch: 592/2000... Training loss: 0.7674\n",
      "Epoch: 592/2000... Training loss: 0.7333\n",
      "Epoch: 592/2000... Training loss: 0.9601\n",
      "Epoch: 593/2000... Training loss: 0.8919\n",
      "Epoch: 593/2000... Training loss: 0.9508\n",
      "Epoch: 593/2000... Training loss: 0.8731\n",
      "Epoch: 593/2000... Training loss: 0.6659\n",
      "Epoch: 593/2000... Training loss: 0.9357\n",
      "Epoch: 593/2000... Training loss: 0.8319\n",
      "Epoch: 593/2000... Training loss: 0.8474\n",
      "Epoch: 593/2000... Training loss: 0.9471\n",
      "Epoch: 593/2000... Training loss: 0.8859\n",
      "Epoch: 593/2000... Training loss: 0.9479\n",
      "Epoch: 593/2000... Training loss: 0.9568\n",
      "Epoch: 593/2000... Training loss: 1.1320\n",
      "Epoch: 593/2000... Training loss: 0.7748\n",
      "Epoch: 593/2000... Training loss: 0.8231\n",
      "Epoch: 593/2000... Training loss: 0.8230\n",
      "Epoch: 593/2000... Training loss: 1.0644\n",
      "Epoch: 593/2000... Training loss: 0.9683\n",
      "Epoch: 593/2000... Training loss: 0.6574\n",
      "Epoch: 593/2000... Training loss: 0.8962\n",
      "Epoch: 593/2000... Training loss: 0.9832\n",
      "Epoch: 593/2000... Training loss: 0.8458\n",
      "Epoch: 593/2000... Training loss: 0.7297\n",
      "Epoch: 593/2000... Training loss: 0.7998\n",
      "Epoch: 593/2000... Training loss: 0.8714\n",
      "Epoch: 593/2000... Training loss: 1.0793\n",
      "Epoch: 593/2000... Training loss: 0.7258\n",
      "Epoch: 593/2000... Training loss: 0.9352\n",
      "Epoch: 593/2000... Training loss: 0.9779\n",
      "Epoch: 593/2000... Training loss: 0.8683\n",
      "Epoch: 593/2000... Training loss: 0.8340\n",
      "Epoch: 593/2000... Training loss: 0.8904\n",
      "Epoch: 594/2000... Training loss: 0.8840\n",
      "Epoch: 594/2000... Training loss: 0.7722\n",
      "Epoch: 594/2000... Training loss: 0.8801\n",
      "Epoch: 594/2000... Training loss: 0.6745\n",
      "Epoch: 594/2000... Training loss: 0.8231\n",
      "Epoch: 594/2000... Training loss: 0.9386\n",
      "Epoch: 594/2000... Training loss: 0.7723\n",
      "Epoch: 594/2000... Training loss: 0.8999\n",
      "Epoch: 594/2000... Training loss: 0.8850\n",
      "Epoch: 594/2000... Training loss: 0.9369\n",
      "Epoch: 594/2000... Training loss: 0.9542\n",
      "Epoch: 594/2000... Training loss: 0.8716\n",
      "Epoch: 594/2000... Training loss: 0.8521\n",
      "Epoch: 594/2000... Training loss: 0.9172\n",
      "Epoch: 594/2000... Training loss: 0.6798\n",
      "Epoch: 594/2000... Training loss: 0.7116\n",
      "Epoch: 594/2000... Training loss: 0.9096\n",
      "Epoch: 594/2000... Training loss: 0.9077\n",
      "Epoch: 594/2000... Training loss: 0.9839\n",
      "Epoch: 594/2000... Training loss: 0.9554\n",
      "Epoch: 594/2000... Training loss: 1.0768\n",
      "Epoch: 594/2000... Training loss: 0.7463\n",
      "Epoch: 594/2000... Training loss: 0.8004\n",
      "Epoch: 594/2000... Training loss: 0.9612\n",
      "Epoch: 594/2000... Training loss: 0.9297\n",
      "Epoch: 594/2000... Training loss: 0.9073\n",
      "Epoch: 594/2000... Training loss: 0.8700\n",
      "Epoch: 594/2000... Training loss: 0.9323\n",
      "Epoch: 594/2000... Training loss: 1.0144\n",
      "Epoch: 594/2000... Training loss: 0.8701\n",
      "Epoch: 594/2000... Training loss: 0.8837\n",
      "Epoch: 595/2000... Training loss: 0.8391\n",
      "Epoch: 595/2000... Training loss: 0.7789\n",
      "Epoch: 595/2000... Training loss: 0.9730\n",
      "Epoch: 595/2000... Training loss: 1.1620\n",
      "Epoch: 595/2000... Training loss: 0.9082\n",
      "Epoch: 595/2000... Training loss: 1.0322\n",
      "Epoch: 595/2000... Training loss: 0.7455\n",
      "Epoch: 595/2000... Training loss: 0.7216\n",
      "Epoch: 595/2000... Training loss: 1.1127\n",
      "Epoch: 595/2000... Training loss: 0.8448\n",
      "Epoch: 595/2000... Training loss: 0.9187\n",
      "Epoch: 595/2000... Training loss: 0.6962\n",
      "Epoch: 595/2000... Training loss: 0.7166\n",
      "Epoch: 595/2000... Training loss: 1.0700\n",
      "Epoch: 595/2000... Training loss: 0.6903\n",
      "Epoch: 595/2000... Training loss: 0.7289\n",
      "Epoch: 595/2000... Training loss: 1.0867\n",
      "Epoch: 595/2000... Training loss: 0.8066\n",
      "Epoch: 595/2000... Training loss: 0.8204\n",
      "Epoch: 595/2000... Training loss: 0.9678\n",
      "Epoch: 595/2000... Training loss: 0.7603\n",
      "Epoch: 595/2000... Training loss: 0.8113\n",
      "Epoch: 595/2000... Training loss: 1.2135\n",
      "Epoch: 595/2000... Training loss: 0.8371\n",
      "Epoch: 595/2000... Training loss: 0.8980\n",
      "Epoch: 595/2000... Training loss: 0.9402\n",
      "Epoch: 595/2000... Training loss: 0.7806\n",
      "Epoch: 595/2000... Training loss: 0.9146\n",
      "Epoch: 595/2000... Training loss: 0.8485\n",
      "Epoch: 595/2000... Training loss: 0.5537\n",
      "Epoch: 595/2000... Training loss: 0.8035\n",
      "Epoch: 596/2000... Training loss: 0.7074\n",
      "Epoch: 596/2000... Training loss: 0.9607\n",
      "Epoch: 596/2000... Training loss: 0.8326\n",
      "Epoch: 596/2000... Training loss: 0.8461\n",
      "Epoch: 596/2000... Training loss: 1.0141\n",
      "Epoch: 596/2000... Training loss: 0.6905\n",
      "Epoch: 596/2000... Training loss: 0.8213\n",
      "Epoch: 596/2000... Training loss: 0.8163\n",
      "Epoch: 596/2000... Training loss: 1.0188\n",
      "Epoch: 596/2000... Training loss: 0.8764\n",
      "Epoch: 596/2000... Training loss: 0.7849\n",
      "Epoch: 596/2000... Training loss: 0.6873\n",
      "Epoch: 596/2000... Training loss: 0.9762\n",
      "Epoch: 596/2000... Training loss: 1.0999\n",
      "Epoch: 596/2000... Training loss: 0.9372\n",
      "Epoch: 596/2000... Training loss: 0.9965\n",
      "Epoch: 596/2000... Training loss: 0.9724\n",
      "Epoch: 596/2000... Training loss: 0.7439\n",
      "Epoch: 596/2000... Training loss: 0.8256\n",
      "Epoch: 596/2000... Training loss: 0.7776\n",
      "Epoch: 596/2000... Training loss: 0.7394\n",
      "Epoch: 596/2000... Training loss: 0.7008\n",
      "Epoch: 596/2000... Training loss: 0.8137\n",
      "Epoch: 596/2000... Training loss: 0.7063\n",
      "Epoch: 596/2000... Training loss: 0.8612\n",
      "Epoch: 596/2000... Training loss: 1.1288\n",
      "Epoch: 596/2000... Training loss: 0.7984\n",
      "Epoch: 596/2000... Training loss: 0.7028\n",
      "Epoch: 596/2000... Training loss: 0.8962\n",
      "Epoch: 596/2000... Training loss: 1.0082\n",
      "Epoch: 596/2000... Training loss: 0.8248\n",
      "Epoch: 597/2000... Training loss: 0.7425\n",
      "Epoch: 597/2000... Training loss: 1.0303\n",
      "Epoch: 597/2000... Training loss: 0.6649\n",
      "Epoch: 597/2000... Training loss: 0.8359\n",
      "Epoch: 597/2000... Training loss: 0.9092\n",
      "Epoch: 597/2000... Training loss: 1.0061\n",
      "Epoch: 597/2000... Training loss: 0.8286\n",
      "Epoch: 597/2000... Training loss: 1.0268\n",
      "Epoch: 597/2000... Training loss: 0.8306\n",
      "Epoch: 597/2000... Training loss: 0.7680\n",
      "Epoch: 597/2000... Training loss: 0.8773\n",
      "Epoch: 597/2000... Training loss: 1.1037\n",
      "Epoch: 597/2000... Training loss: 0.5984\n",
      "Epoch: 597/2000... Training loss: 0.8979\n",
      "Epoch: 597/2000... Training loss: 0.9175\n",
      "Epoch: 597/2000... Training loss: 0.7983\n",
      "Epoch: 597/2000... Training loss: 0.7393\n",
      "Epoch: 597/2000... Training loss: 0.6431\n",
      "Epoch: 597/2000... Training loss: 0.8281\n",
      "Epoch: 597/2000... Training loss: 0.7351\n",
      "Epoch: 597/2000... Training loss: 1.0371\n",
      "Epoch: 597/2000... Training loss: 0.9406\n",
      "Epoch: 597/2000... Training loss: 1.3405\n",
      "Epoch: 597/2000... Training loss: 0.8363\n",
      "Epoch: 597/2000... Training loss: 0.8709\n",
      "Epoch: 597/2000... Training loss: 0.9080\n",
      "Epoch: 597/2000... Training loss: 0.9593\n",
      "Epoch: 597/2000... Training loss: 0.9234\n",
      "Epoch: 597/2000... Training loss: 0.8444\n",
      "Epoch: 597/2000... Training loss: 0.7391\n",
      "Epoch: 597/2000... Training loss: 1.0969\n",
      "Epoch: 598/2000... Training loss: 0.9195\n",
      "Epoch: 598/2000... Training loss: 0.7161\n",
      "Epoch: 598/2000... Training loss: 1.1790\n",
      "Epoch: 598/2000... Training loss: 0.8580\n",
      "Epoch: 598/2000... Training loss: 1.0042\n",
      "Epoch: 598/2000... Training loss: 0.8699\n",
      "Epoch: 598/2000... Training loss: 0.8222\n",
      "Epoch: 598/2000... Training loss: 0.9452\n",
      "Epoch: 598/2000... Training loss: 1.1824\n",
      "Epoch: 598/2000... Training loss: 0.6478\n",
      "Epoch: 598/2000... Training loss: 0.8524\n",
      "Epoch: 598/2000... Training loss: 0.8507\n",
      "Epoch: 598/2000... Training loss: 0.9267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 598/2000... Training loss: 0.6363\n",
      "Epoch: 598/2000... Training loss: 1.0860\n",
      "Epoch: 598/2000... Training loss: 0.9984\n",
      "Epoch: 598/2000... Training loss: 0.6772\n",
      "Epoch: 598/2000... Training loss: 0.9427\n",
      "Epoch: 598/2000... Training loss: 0.8594\n",
      "Epoch: 598/2000... Training loss: 0.9417\n",
      "Epoch: 598/2000... Training loss: 1.0067\n",
      "Epoch: 598/2000... Training loss: 0.8310\n",
      "Epoch: 598/2000... Training loss: 1.2399\n",
      "Epoch: 598/2000... Training loss: 0.8757\n",
      "Epoch: 598/2000... Training loss: 1.1450\n",
      "Epoch: 598/2000... Training loss: 0.9079\n",
      "Epoch: 598/2000... Training loss: 0.7248\n",
      "Epoch: 598/2000... Training loss: 0.8618\n",
      "Epoch: 598/2000... Training loss: 0.9742\n",
      "Epoch: 598/2000... Training loss: 0.7346\n",
      "Epoch: 598/2000... Training loss: 0.9706\n",
      "Epoch: 599/2000... Training loss: 0.8547\n",
      "Epoch: 599/2000... Training loss: 1.1686\n",
      "Epoch: 599/2000... Training loss: 0.9994\n",
      "Epoch: 599/2000... Training loss: 0.8367\n",
      "Epoch: 599/2000... Training loss: 0.9509\n",
      "Epoch: 599/2000... Training loss: 1.0274\n",
      "Epoch: 599/2000... Training loss: 0.8612\n",
      "Epoch: 599/2000... Training loss: 0.6968\n",
      "Epoch: 599/2000... Training loss: 1.0761\n",
      "Epoch: 599/2000... Training loss: 0.9314\n",
      "Epoch: 599/2000... Training loss: 1.0057\n",
      "Epoch: 599/2000... Training loss: 1.1078\n",
      "Epoch: 599/2000... Training loss: 0.7684\n",
      "Epoch: 599/2000... Training loss: 0.7907\n",
      "Epoch: 599/2000... Training loss: 0.9192\n",
      "Epoch: 599/2000... Training loss: 0.9760\n",
      "Epoch: 599/2000... Training loss: 0.9927\n",
      "Epoch: 599/2000... Training loss: 0.8505\n",
      "Epoch: 599/2000... Training loss: 0.9380\n",
      "Epoch: 599/2000... Training loss: 0.6822\n",
      "Epoch: 599/2000... Training loss: 1.0124\n",
      "Epoch: 599/2000... Training loss: 0.7603\n",
      "Epoch: 599/2000... Training loss: 1.0768\n",
      "Epoch: 599/2000... Training loss: 0.7584\n",
      "Epoch: 599/2000... Training loss: 0.8864\n",
      "Epoch: 599/2000... Training loss: 1.0140\n",
      "Epoch: 599/2000... Training loss: 0.9592\n",
      "Epoch: 599/2000... Training loss: 0.8925\n",
      "Epoch: 599/2000... Training loss: 0.7695\n",
      "Epoch: 599/2000... Training loss: 0.8412\n",
      "Epoch: 599/2000... Training loss: 0.8069\n",
      "Epoch: 600/2000... Training loss: 0.9718\n",
      "Epoch: 600/2000... Training loss: 0.5956\n",
      "Epoch: 600/2000... Training loss: 0.7749\n",
      "Epoch: 600/2000... Training loss: 0.7955\n",
      "Epoch: 600/2000... Training loss: 1.0290\n",
      "Epoch: 600/2000... Training loss: 0.8316\n",
      "Epoch: 600/2000... Training loss: 0.7309\n",
      "Epoch: 600/2000... Training loss: 0.7175\n",
      "Epoch: 600/2000... Training loss: 1.1312\n",
      "Epoch: 600/2000... Training loss: 1.0167\n",
      "Epoch: 600/2000... Training loss: 0.8427\n",
      "Epoch: 600/2000... Training loss: 0.8207\n",
      "Epoch: 600/2000... Training loss: 0.9489\n",
      "Epoch: 600/2000... Training loss: 0.7871\n",
      "Epoch: 600/2000... Training loss: 0.9033\n",
      "Epoch: 600/2000... Training loss: 0.9003\n",
      "Epoch: 600/2000... Training loss: 0.9971\n",
      "Epoch: 600/2000... Training loss: 0.7599\n",
      "Epoch: 600/2000... Training loss: 0.8842\n",
      "Epoch: 600/2000... Training loss: 0.7857\n",
      "Epoch: 600/2000... Training loss: 0.8469\n",
      "Epoch: 600/2000... Training loss: 0.9762\n",
      "Epoch: 600/2000... Training loss: 0.8420\n",
      "Epoch: 600/2000... Training loss: 0.8197\n",
      "Epoch: 600/2000... Training loss: 0.9100\n",
      "Epoch: 600/2000... Training loss: 0.7218\n",
      "Epoch: 600/2000... Training loss: 1.0076\n",
      "Epoch: 600/2000... Training loss: 0.7549\n",
      "Epoch: 600/2000... Training loss: 0.7569\n",
      "Epoch: 600/2000... Training loss: 0.8443\n",
      "Epoch: 600/2000... Training loss: 0.9417\n",
      "Epoch: 601/2000... Training loss: 0.7617\n",
      "Epoch: 601/2000... Training loss: 0.8940\n",
      "Epoch: 601/2000... Training loss: 0.6248\n",
      "Epoch: 601/2000... Training loss: 0.8126\n",
      "Epoch: 601/2000... Training loss: 0.9401\n",
      "Epoch: 601/2000... Training loss: 0.9413\n",
      "Epoch: 601/2000... Training loss: 0.8878\n",
      "Epoch: 601/2000... Training loss: 0.8121\n",
      "Epoch: 601/2000... Training loss: 0.7834\n",
      "Epoch: 601/2000... Training loss: 0.7308\n",
      "Epoch: 601/2000... Training loss: 0.7964\n",
      "Epoch: 601/2000... Training loss: 0.7833\n",
      "Epoch: 601/2000... Training loss: 0.8813\n",
      "Epoch: 601/2000... Training loss: 1.0695\n",
      "Epoch: 601/2000... Training loss: 1.0468\n",
      "Epoch: 601/2000... Training loss: 0.6004\n",
      "Epoch: 601/2000... Training loss: 1.0529\n",
      "Epoch: 601/2000... Training loss: 0.7704\n",
      "Epoch: 601/2000... Training loss: 0.9162\n",
      "Epoch: 601/2000... Training loss: 0.8933\n",
      "Epoch: 601/2000... Training loss: 0.8475\n",
      "Epoch: 601/2000... Training loss: 0.8108\n",
      "Epoch: 601/2000... Training loss: 0.9493\n",
      "Epoch: 601/2000... Training loss: 0.8856\n",
      "Epoch: 601/2000... Training loss: 0.6958\n",
      "Epoch: 601/2000... Training loss: 0.8968\n",
      "Epoch: 601/2000... Training loss: 0.7159\n",
      "Epoch: 601/2000... Training loss: 0.7661\n",
      "Epoch: 601/2000... Training loss: 0.8925\n",
      "Epoch: 601/2000... Training loss: 0.9026\n",
      "Epoch: 601/2000... Training loss: 0.8900\n",
      "Epoch: 602/2000... Training loss: 1.0800\n",
      "Epoch: 602/2000... Training loss: 0.8442\n",
      "Epoch: 602/2000... Training loss: 0.7532\n",
      "Epoch: 602/2000... Training loss: 0.8108\n",
      "Epoch: 602/2000... Training loss: 0.7656\n",
      "Epoch: 602/2000... Training loss: 0.8334\n",
      "Epoch: 602/2000... Training loss: 0.9840\n",
      "Epoch: 602/2000... Training loss: 0.7064\n",
      "Epoch: 602/2000... Training loss: 1.1805\n",
      "Epoch: 602/2000... Training loss: 0.9586\n",
      "Epoch: 602/2000... Training loss: 0.8467\n",
      "Epoch: 602/2000... Training loss: 0.8106\n",
      "Epoch: 602/2000... Training loss: 0.7801\n",
      "Epoch: 602/2000... Training loss: 0.8840\n",
      "Epoch: 602/2000... Training loss: 0.8093\n",
      "Epoch: 602/2000... Training loss: 0.7707\n",
      "Epoch: 602/2000... Training loss: 0.7451\n",
      "Epoch: 602/2000... Training loss: 0.9450\n",
      "Epoch: 602/2000... Training loss: 0.7033\n",
      "Epoch: 602/2000... Training loss: 0.9812\n",
      "Epoch: 602/2000... Training loss: 0.8223\n",
      "Epoch: 602/2000... Training loss: 0.8292\n",
      "Epoch: 602/2000... Training loss: 1.0184\n",
      "Epoch: 602/2000... Training loss: 0.8310\n",
      "Epoch: 602/2000... Training loss: 0.8905\n",
      "Epoch: 602/2000... Training loss: 0.9928\n",
      "Epoch: 602/2000... Training loss: 1.0283\n",
      "Epoch: 602/2000... Training loss: 0.9397\n",
      "Epoch: 602/2000... Training loss: 0.8701\n",
      "Epoch: 602/2000... Training loss: 0.9649\n",
      "Epoch: 602/2000... Training loss: 0.8655\n",
      "Epoch: 603/2000... Training loss: 0.8009\n",
      "Epoch: 603/2000... Training loss: 0.9338\n",
      "Epoch: 603/2000... Training loss: 0.9111\n",
      "Epoch: 603/2000... Training loss: 0.8437\n",
      "Epoch: 603/2000... Training loss: 0.8121\n",
      "Epoch: 603/2000... Training loss: 0.8657\n",
      "Epoch: 603/2000... Training loss: 0.8542\n",
      "Epoch: 603/2000... Training loss: 0.7691\n",
      "Epoch: 603/2000... Training loss: 0.7890\n",
      "Epoch: 603/2000... Training loss: 0.6682\n",
      "Epoch: 603/2000... Training loss: 0.8308\n",
      "Epoch: 603/2000... Training loss: 0.9040\n",
      "Epoch: 603/2000... Training loss: 0.9224\n",
      "Epoch: 603/2000... Training loss: 0.8184\n",
      "Epoch: 603/2000... Training loss: 0.9109\n",
      "Epoch: 603/2000... Training loss: 0.7624\n",
      "Epoch: 603/2000... Training loss: 0.7771\n",
      "Epoch: 603/2000... Training loss: 0.8186\n",
      "Epoch: 603/2000... Training loss: 0.8625\n",
      "Epoch: 603/2000... Training loss: 0.8001\n",
      "Epoch: 603/2000... Training loss: 0.9401\n",
      "Epoch: 603/2000... Training loss: 0.7645\n",
      "Epoch: 603/2000... Training loss: 1.0031\n",
      "Epoch: 603/2000... Training loss: 0.9607\n",
      "Epoch: 603/2000... Training loss: 0.9164\n",
      "Epoch: 603/2000... Training loss: 0.6788\n",
      "Epoch: 603/2000... Training loss: 0.8788\n",
      "Epoch: 603/2000... Training loss: 0.7203\n",
      "Epoch: 603/2000... Training loss: 0.8707\n",
      "Epoch: 603/2000... Training loss: 0.9406\n",
      "Epoch: 603/2000... Training loss: 0.9057\n",
      "Epoch: 604/2000... Training loss: 0.6804\n",
      "Epoch: 604/2000... Training loss: 1.0053\n",
      "Epoch: 604/2000... Training loss: 0.9046\n",
      "Epoch: 604/2000... Training loss: 0.7876\n",
      "Epoch: 604/2000... Training loss: 0.8876\n",
      "Epoch: 604/2000... Training loss: 0.9156\n",
      "Epoch: 604/2000... Training loss: 0.8402\n",
      "Epoch: 604/2000... Training loss: 0.8856\n",
      "Epoch: 604/2000... Training loss: 0.9811\n",
      "Epoch: 604/2000... Training loss: 0.9116\n",
      "Epoch: 604/2000... Training loss: 1.0867\n",
      "Epoch: 604/2000... Training loss: 0.8735\n",
      "Epoch: 604/2000... Training loss: 0.8597\n",
      "Epoch: 604/2000... Training loss: 0.8998\n",
      "Epoch: 604/2000... Training loss: 0.9668\n",
      "Epoch: 604/2000... Training loss: 0.9600\n",
      "Epoch: 604/2000... Training loss: 0.7314\n",
      "Epoch: 604/2000... Training loss: 0.9538\n",
      "Epoch: 604/2000... Training loss: 0.6303\n",
      "Epoch: 604/2000... Training loss: 0.8767\n",
      "Epoch: 604/2000... Training loss: 1.0373\n",
      "Epoch: 604/2000... Training loss: 0.8655\n",
      "Epoch: 604/2000... Training loss: 0.6849\n",
      "Epoch: 604/2000... Training loss: 0.9636\n",
      "Epoch: 604/2000... Training loss: 0.9382\n",
      "Epoch: 604/2000... Training loss: 0.9817\n",
      "Epoch: 604/2000... Training loss: 0.8804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 604/2000... Training loss: 0.8963\n",
      "Epoch: 604/2000... Training loss: 0.8111\n",
      "Epoch: 604/2000... Training loss: 0.9241\n",
      "Epoch: 604/2000... Training loss: 0.9272\n",
      "Epoch: 605/2000... Training loss: 1.0807\n",
      "Epoch: 605/2000... Training loss: 0.8064\n",
      "Epoch: 605/2000... Training loss: 0.8837\n",
      "Epoch: 605/2000... Training loss: 1.0236\n",
      "Epoch: 605/2000... Training loss: 0.7289\n",
      "Epoch: 605/2000... Training loss: 0.9523\n",
      "Epoch: 605/2000... Training loss: 1.0262\n",
      "Epoch: 605/2000... Training loss: 0.7737\n",
      "Epoch: 605/2000... Training loss: 0.9519\n",
      "Epoch: 605/2000... Training loss: 0.8794\n",
      "Epoch: 605/2000... Training loss: 1.0927\n",
      "Epoch: 605/2000... Training loss: 0.7353\n",
      "Epoch: 605/2000... Training loss: 0.9246\n",
      "Epoch: 605/2000... Training loss: 0.9694\n",
      "Epoch: 605/2000... Training loss: 0.7953\n",
      "Epoch: 605/2000... Training loss: 0.9018\n",
      "Epoch: 605/2000... Training loss: 0.8224\n",
      "Epoch: 605/2000... Training loss: 0.8487\n",
      "Epoch: 605/2000... Training loss: 0.8092\n",
      "Epoch: 605/2000... Training loss: 0.7907\n",
      "Epoch: 605/2000... Training loss: 0.9557\n",
      "Epoch: 605/2000... Training loss: 0.7454\n",
      "Epoch: 605/2000... Training loss: 1.0385\n",
      "Epoch: 605/2000... Training loss: 0.8580\n",
      "Epoch: 605/2000... Training loss: 0.8175\n",
      "Epoch: 605/2000... Training loss: 0.8295\n",
      "Epoch: 605/2000... Training loss: 0.8422\n",
      "Epoch: 605/2000... Training loss: 0.9471\n",
      "Epoch: 605/2000... Training loss: 0.8194\n",
      "Epoch: 605/2000... Training loss: 0.8937\n",
      "Epoch: 605/2000... Training loss: 0.7742\n",
      "Epoch: 606/2000... Training loss: 0.9922\n",
      "Epoch: 606/2000... Training loss: 0.8100\n",
      "Epoch: 606/2000... Training loss: 0.9391\n",
      "Epoch: 606/2000... Training loss: 0.7533\n",
      "Epoch: 606/2000... Training loss: 0.9726\n",
      "Epoch: 606/2000... Training loss: 0.7109\n",
      "Epoch: 606/2000... Training loss: 0.6718\n",
      "Epoch: 606/2000... Training loss: 0.7928\n",
      "Epoch: 606/2000... Training loss: 1.0448\n",
      "Epoch: 606/2000... Training loss: 0.8967\n",
      "Epoch: 606/2000... Training loss: 1.0299\n",
      "Epoch: 606/2000... Training loss: 0.9276\n",
      "Epoch: 606/2000... Training loss: 0.7351\n",
      "Epoch: 606/2000... Training loss: 0.8148\n",
      "Epoch: 606/2000... Training loss: 0.7218\n",
      "Epoch: 606/2000... Training loss: 0.9568\n",
      "Epoch: 606/2000... Training loss: 0.9453\n",
      "Epoch: 606/2000... Training loss: 0.7915\n",
      "Epoch: 606/2000... Training loss: 0.8789\n",
      "Epoch: 606/2000... Training loss: 0.9374\n",
      "Epoch: 606/2000... Training loss: 0.7872\n",
      "Epoch: 606/2000... Training loss: 1.0449\n",
      "Epoch: 606/2000... Training loss: 1.0446\n",
      "Epoch: 606/2000... Training loss: 1.0765\n",
      "Epoch: 606/2000... Training loss: 0.8853\n",
      "Epoch: 606/2000... Training loss: 1.0356\n",
      "Epoch: 606/2000... Training loss: 0.8719\n",
      "Epoch: 606/2000... Training loss: 0.8613\n",
      "Epoch: 606/2000... Training loss: 1.0068\n",
      "Epoch: 606/2000... Training loss: 0.6959\n",
      "Epoch: 606/2000... Training loss: 0.8427\n",
      "Epoch: 607/2000... Training loss: 0.8680\n",
      "Epoch: 607/2000... Training loss: 0.8649\n",
      "Epoch: 607/2000... Training loss: 0.9439\n",
      "Epoch: 607/2000... Training loss: 0.9562\n",
      "Epoch: 607/2000... Training loss: 1.1698\n",
      "Epoch: 607/2000... Training loss: 0.7998\n",
      "Epoch: 607/2000... Training loss: 0.9150\n",
      "Epoch: 607/2000... Training loss: 0.8417\n",
      "Epoch: 607/2000... Training loss: 0.9420\n",
      "Epoch: 607/2000... Training loss: 0.7760\n",
      "Epoch: 607/2000... Training loss: 0.7556\n",
      "Epoch: 607/2000... Training loss: 0.7808\n",
      "Epoch: 607/2000... Training loss: 1.0306\n",
      "Epoch: 607/2000... Training loss: 0.9185\n",
      "Epoch: 607/2000... Training loss: 0.8300\n",
      "Epoch: 607/2000... Training loss: 0.9171\n",
      "Epoch: 607/2000... Training loss: 0.7850\n",
      "Epoch: 607/2000... Training loss: 0.7173\n",
      "Epoch: 607/2000... Training loss: 0.8544\n",
      "Epoch: 607/2000... Training loss: 0.9324\n",
      "Epoch: 607/2000... Training loss: 0.9876\n",
      "Epoch: 607/2000... Training loss: 0.6245\n",
      "Epoch: 607/2000... Training loss: 0.9036\n",
      "Epoch: 607/2000... Training loss: 0.8600\n",
      "Epoch: 607/2000... Training loss: 0.8310\n",
      "Epoch: 607/2000... Training loss: 1.0890\n",
      "Epoch: 607/2000... Training loss: 0.6697\n",
      "Epoch: 607/2000... Training loss: 0.8506\n",
      "Epoch: 607/2000... Training loss: 0.9173\n",
      "Epoch: 607/2000... Training loss: 0.7837\n",
      "Epoch: 607/2000... Training loss: 1.0299\n",
      "Epoch: 608/2000... Training loss: 1.1242\n",
      "Epoch: 608/2000... Training loss: 0.9119\n",
      "Epoch: 608/2000... Training loss: 0.7975\n",
      "Epoch: 608/2000... Training loss: 0.8942\n",
      "Epoch: 608/2000... Training loss: 0.7704\n",
      "Epoch: 608/2000... Training loss: 0.8047\n",
      "Epoch: 608/2000... Training loss: 0.9702\n",
      "Epoch: 608/2000... Training loss: 0.8158\n",
      "Epoch: 608/2000... Training loss: 0.6356\n",
      "Epoch: 608/2000... Training loss: 1.0602\n",
      "Epoch: 608/2000... Training loss: 0.6703\n",
      "Epoch: 608/2000... Training loss: 0.9750\n",
      "Epoch: 608/2000... Training loss: 0.7312\n",
      "Epoch: 608/2000... Training loss: 1.0095\n",
      "Epoch: 608/2000... Training loss: 0.8255\n",
      "Epoch: 608/2000... Training loss: 0.8134\n",
      "Epoch: 608/2000... Training loss: 0.7793\n",
      "Epoch: 608/2000... Training loss: 0.8934\n",
      "Epoch: 608/2000... Training loss: 0.7732\n",
      "Epoch: 608/2000... Training loss: 0.7195\n",
      "Epoch: 608/2000... Training loss: 0.8775\n",
      "Epoch: 608/2000... Training loss: 0.6404\n",
      "Epoch: 608/2000... Training loss: 0.8303\n",
      "Epoch: 608/2000... Training loss: 0.7401\n",
      "Epoch: 608/2000... Training loss: 0.6309\n",
      "Epoch: 608/2000... Training loss: 0.9135\n",
      "Epoch: 608/2000... Training loss: 0.6973\n",
      "Epoch: 608/2000... Training loss: 0.9460\n",
      "Epoch: 608/2000... Training loss: 0.7791\n",
      "Epoch: 608/2000... Training loss: 1.1299\n",
      "Epoch: 608/2000... Training loss: 0.6774\n",
      "Epoch: 609/2000... Training loss: 0.8598\n",
      "Epoch: 609/2000... Training loss: 0.9933\n",
      "Epoch: 609/2000... Training loss: 0.8652\n",
      "Epoch: 609/2000... Training loss: 0.7875\n",
      "Epoch: 609/2000... Training loss: 0.9406\n",
      "Epoch: 609/2000... Training loss: 0.7509\n",
      "Epoch: 609/2000... Training loss: 0.7301\n",
      "Epoch: 609/2000... Training loss: 0.9043\n",
      "Epoch: 609/2000... Training loss: 1.0500\n",
      "Epoch: 609/2000... Training loss: 0.8529\n",
      "Epoch: 609/2000... Training loss: 0.9686\n",
      "Epoch: 609/2000... Training loss: 0.9204\n",
      "Epoch: 609/2000... Training loss: 0.6952\n",
      "Epoch: 609/2000... Training loss: 0.8510\n",
      "Epoch: 609/2000... Training loss: 0.7025\n",
      "Epoch: 609/2000... Training loss: 0.7610\n",
      "Epoch: 609/2000... Training loss: 0.8210\n",
      "Epoch: 609/2000... Training loss: 0.8745\n",
      "Epoch: 609/2000... Training loss: 0.9728\n",
      "Epoch: 609/2000... Training loss: 0.9643\n",
      "Epoch: 609/2000... Training loss: 0.8564\n",
      "Epoch: 609/2000... Training loss: 0.8167\n",
      "Epoch: 609/2000... Training loss: 0.9111\n",
      "Epoch: 609/2000... Training loss: 0.6438\n",
      "Epoch: 609/2000... Training loss: 0.8355\n",
      "Epoch: 609/2000... Training loss: 0.6700\n",
      "Epoch: 609/2000... Training loss: 0.9269\n",
      "Epoch: 609/2000... Training loss: 0.9771\n",
      "Epoch: 609/2000... Training loss: 1.2096\n",
      "Epoch: 609/2000... Training loss: 0.7951\n",
      "Epoch: 609/2000... Training loss: 0.9643\n",
      "Epoch: 610/2000... Training loss: 0.7312\n",
      "Epoch: 610/2000... Training loss: 0.8394\n",
      "Epoch: 610/2000... Training loss: 0.9157\n",
      "Epoch: 610/2000... Training loss: 1.1193\n",
      "Epoch: 610/2000... Training loss: 0.8125\n",
      "Epoch: 610/2000... Training loss: 0.7112\n",
      "Epoch: 610/2000... Training loss: 0.9088\n",
      "Epoch: 610/2000... Training loss: 0.8774\n",
      "Epoch: 610/2000... Training loss: 0.8631\n",
      "Epoch: 610/2000... Training loss: 0.6728\n",
      "Epoch: 610/2000... Training loss: 0.7240\n",
      "Epoch: 610/2000... Training loss: 0.7804\n",
      "Epoch: 610/2000... Training loss: 0.8150\n",
      "Epoch: 610/2000... Training loss: 1.0580\n",
      "Epoch: 610/2000... Training loss: 0.8842\n",
      "Epoch: 610/2000... Training loss: 0.8826\n",
      "Epoch: 610/2000... Training loss: 0.8247\n",
      "Epoch: 610/2000... Training loss: 0.8622\n",
      "Epoch: 610/2000... Training loss: 0.8463\n",
      "Epoch: 610/2000... Training loss: 0.8518\n",
      "Epoch: 610/2000... Training loss: 0.9791\n",
      "Epoch: 610/2000... Training loss: 0.8417\n",
      "Epoch: 610/2000... Training loss: 0.9200\n",
      "Epoch: 610/2000... Training loss: 1.1317\n",
      "Epoch: 610/2000... Training loss: 1.0842\n",
      "Epoch: 610/2000... Training loss: 1.0467\n",
      "Epoch: 610/2000... Training loss: 0.6639\n",
      "Epoch: 610/2000... Training loss: 0.7637\n",
      "Epoch: 610/2000... Training loss: 0.7695\n",
      "Epoch: 610/2000... Training loss: 0.6241\n",
      "Epoch: 610/2000... Training loss: 0.8395\n",
      "Epoch: 611/2000... Training loss: 0.9198\n",
      "Epoch: 611/2000... Training loss: 0.9565\n",
      "Epoch: 611/2000... Training loss: 0.8265\n",
      "Epoch: 611/2000... Training loss: 0.8603\n",
      "Epoch: 611/2000... Training loss: 1.0090\n",
      "Epoch: 611/2000... Training loss: 0.8607\n",
      "Epoch: 611/2000... Training loss: 0.9467\n",
      "Epoch: 611/2000... Training loss: 0.8588\n",
      "Epoch: 611/2000... Training loss: 0.9605\n",
      "Epoch: 611/2000... Training loss: 0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 611/2000... Training loss: 0.7827\n",
      "Epoch: 611/2000... Training loss: 0.6279\n",
      "Epoch: 611/2000... Training loss: 0.8751\n",
      "Epoch: 611/2000... Training loss: 0.8488\n",
      "Epoch: 611/2000... Training loss: 0.8654\n",
      "Epoch: 611/2000... Training loss: 0.8323\n",
      "Epoch: 611/2000... Training loss: 0.9106\n",
      "Epoch: 611/2000... Training loss: 0.6338\n",
      "Epoch: 611/2000... Training loss: 0.7168\n",
      "Epoch: 611/2000... Training loss: 0.8066\n",
      "Epoch: 611/2000... Training loss: 0.7735\n",
      "Epoch: 611/2000... Training loss: 0.6929\n",
      "Epoch: 611/2000... Training loss: 1.0267\n",
      "Epoch: 611/2000... Training loss: 1.1308\n",
      "Epoch: 611/2000... Training loss: 0.8953\n",
      "Epoch: 611/2000... Training loss: 0.8655\n",
      "Epoch: 611/2000... Training loss: 0.9820\n",
      "Epoch: 611/2000... Training loss: 0.7698\n",
      "Epoch: 611/2000... Training loss: 0.9108\n",
      "Epoch: 611/2000... Training loss: 0.9290\n",
      "Epoch: 611/2000... Training loss: 0.9406\n",
      "Epoch: 612/2000... Training loss: 0.7201\n",
      "Epoch: 612/2000... Training loss: 1.2142\n",
      "Epoch: 612/2000... Training loss: 0.9232\n",
      "Epoch: 612/2000... Training loss: 0.7264\n",
      "Epoch: 612/2000... Training loss: 0.8086\n",
      "Epoch: 612/2000... Training loss: 0.9395\n",
      "Epoch: 612/2000... Training loss: 0.9438\n",
      "Epoch: 612/2000... Training loss: 0.8664\n",
      "Epoch: 612/2000... Training loss: 0.9678\n",
      "Epoch: 612/2000... Training loss: 0.6902\n",
      "Epoch: 612/2000... Training loss: 0.8564\n",
      "Epoch: 612/2000... Training loss: 0.7782\n",
      "Epoch: 612/2000... Training loss: 0.8804\n",
      "Epoch: 612/2000... Training loss: 0.8772\n",
      "Epoch: 612/2000... Training loss: 0.7106\n",
      "Epoch: 612/2000... Training loss: 0.6777\n",
      "Epoch: 612/2000... Training loss: 0.8666\n",
      "Epoch: 612/2000... Training loss: 0.9311\n",
      "Epoch: 612/2000... Training loss: 0.8682\n",
      "Epoch: 612/2000... Training loss: 0.7226\n",
      "Epoch: 612/2000... Training loss: 0.8689\n",
      "Epoch: 612/2000... Training loss: 0.8871\n",
      "Epoch: 612/2000... Training loss: 0.9127\n",
      "Epoch: 612/2000... Training loss: 0.7064\n",
      "Epoch: 612/2000... Training loss: 1.0141\n",
      "Epoch: 612/2000... Training loss: 0.9296\n",
      "Epoch: 612/2000... Training loss: 0.8982\n",
      "Epoch: 612/2000... Training loss: 0.9284\n",
      "Epoch: 612/2000... Training loss: 0.5980\n",
      "Epoch: 612/2000... Training loss: 0.9243\n",
      "Epoch: 612/2000... Training loss: 1.0733\n",
      "Epoch: 613/2000... Training loss: 0.9293\n",
      "Epoch: 613/2000... Training loss: 0.8435\n",
      "Epoch: 613/2000... Training loss: 0.9199\n",
      "Epoch: 613/2000... Training loss: 0.7456\n",
      "Epoch: 613/2000... Training loss: 0.7965\n",
      "Epoch: 613/2000... Training loss: 1.0362\n",
      "Epoch: 613/2000... Training loss: 0.7601\n",
      "Epoch: 613/2000... Training loss: 0.9839\n",
      "Epoch: 613/2000... Training loss: 0.9397\n",
      "Epoch: 613/2000... Training loss: 1.0204\n",
      "Epoch: 613/2000... Training loss: 0.8265\n",
      "Epoch: 613/2000... Training loss: 1.0079\n",
      "Epoch: 613/2000... Training loss: 0.8422\n",
      "Epoch: 613/2000... Training loss: 0.8249\n",
      "Epoch: 613/2000... Training loss: 0.7570\n",
      "Epoch: 613/2000... Training loss: 0.9941\n",
      "Epoch: 613/2000... Training loss: 0.7206\n",
      "Epoch: 613/2000... Training loss: 0.8074\n",
      "Epoch: 613/2000... Training loss: 0.8298\n",
      "Epoch: 613/2000... Training loss: 1.0101\n",
      "Epoch: 613/2000... Training loss: 0.8441\n",
      "Epoch: 613/2000... Training loss: 0.8568\n",
      "Epoch: 613/2000... Training loss: 0.8649\n",
      "Epoch: 613/2000... Training loss: 0.9203\n",
      "Epoch: 613/2000... Training loss: 0.6963\n",
      "Epoch: 613/2000... Training loss: 0.6258\n",
      "Epoch: 613/2000... Training loss: 0.9906\n",
      "Epoch: 613/2000... Training loss: 0.7050\n",
      "Epoch: 613/2000... Training loss: 0.6950\n",
      "Epoch: 613/2000... Training loss: 0.7680\n",
      "Epoch: 613/2000... Training loss: 0.9690\n",
      "Epoch: 614/2000... Training loss: 0.9344\n",
      "Epoch: 614/2000... Training loss: 0.7609\n",
      "Epoch: 614/2000... Training loss: 0.7998\n",
      "Epoch: 614/2000... Training loss: 1.0201\n",
      "Epoch: 614/2000... Training loss: 0.7275\n",
      "Epoch: 614/2000... Training loss: 0.6124\n",
      "Epoch: 614/2000... Training loss: 0.7106\n",
      "Epoch: 614/2000... Training loss: 0.6706\n",
      "Epoch: 614/2000... Training loss: 0.9177\n",
      "Epoch: 614/2000... Training loss: 0.6896\n",
      "Epoch: 614/2000... Training loss: 0.7432\n",
      "Epoch: 614/2000... Training loss: 0.8912\n",
      "Epoch: 614/2000... Training loss: 0.8038\n",
      "Epoch: 614/2000... Training loss: 0.8692\n",
      "Epoch: 614/2000... Training loss: 0.6255\n",
      "Epoch: 614/2000... Training loss: 0.9359\n",
      "Epoch: 614/2000... Training loss: 0.8017\n",
      "Epoch: 614/2000... Training loss: 0.9865\n",
      "Epoch: 614/2000... Training loss: 0.8810\n",
      "Epoch: 614/2000... Training loss: 0.8163\n",
      "Epoch: 614/2000... Training loss: 0.8630\n",
      "Epoch: 614/2000... Training loss: 0.6818\n",
      "Epoch: 614/2000... Training loss: 0.9422\n",
      "Epoch: 614/2000... Training loss: 0.6835\n",
      "Epoch: 614/2000... Training loss: 0.9978\n",
      "Epoch: 614/2000... Training loss: 1.0054\n",
      "Epoch: 614/2000... Training loss: 0.8062\n",
      "Epoch: 614/2000... Training loss: 0.7543\n",
      "Epoch: 614/2000... Training loss: 0.8493\n",
      "Epoch: 614/2000... Training loss: 0.9321\n",
      "Epoch: 614/2000... Training loss: 0.7904\n",
      "Epoch: 615/2000... Training loss: 0.9423\n",
      "Epoch: 615/2000... Training loss: 1.0042\n",
      "Epoch: 615/2000... Training loss: 0.8622\n",
      "Epoch: 615/2000... Training loss: 0.7892\n",
      "Epoch: 615/2000... Training loss: 0.7043\n",
      "Epoch: 615/2000... Training loss: 0.8822\n",
      "Epoch: 615/2000... Training loss: 0.6844\n",
      "Epoch: 615/2000... Training loss: 0.9386\n",
      "Epoch: 615/2000... Training loss: 0.8075\n",
      "Epoch: 615/2000... Training loss: 0.9497\n",
      "Epoch: 615/2000... Training loss: 0.8284\n",
      "Epoch: 615/2000... Training loss: 0.9566\n",
      "Epoch: 615/2000... Training loss: 0.6960\n",
      "Epoch: 615/2000... Training loss: 1.0395\n",
      "Epoch: 615/2000... Training loss: 0.9475\n",
      "Epoch: 615/2000... Training loss: 0.8106\n",
      "Epoch: 615/2000... Training loss: 0.8344\n",
      "Epoch: 615/2000... Training loss: 0.9109\n",
      "Epoch: 615/2000... Training loss: 0.7386\n",
      "Epoch: 615/2000... Training loss: 0.8438\n",
      "Epoch: 615/2000... Training loss: 0.8164\n",
      "Epoch: 615/2000... Training loss: 0.9107\n",
      "Epoch: 615/2000... Training loss: 0.5752\n",
      "Epoch: 615/2000... Training loss: 0.9713\n",
      "Epoch: 615/2000... Training loss: 0.9389\n",
      "Epoch: 615/2000... Training loss: 0.8847\n",
      "Epoch: 615/2000... Training loss: 0.7291\n",
      "Epoch: 615/2000... Training loss: 0.8704\n",
      "Epoch: 615/2000... Training loss: 0.6484\n",
      "Epoch: 615/2000... Training loss: 0.7667\n",
      "Epoch: 615/2000... Training loss: 0.7329\n",
      "Epoch: 616/2000... Training loss: 0.7679\n",
      "Epoch: 616/2000... Training loss: 0.8139\n",
      "Epoch: 616/2000... Training loss: 0.8392\n",
      "Epoch: 616/2000... Training loss: 0.7722\n",
      "Epoch: 616/2000... Training loss: 0.5687\n",
      "Epoch: 616/2000... Training loss: 0.7474\n",
      "Epoch: 616/2000... Training loss: 0.7983\n",
      "Epoch: 616/2000... Training loss: 0.7571\n",
      "Epoch: 616/2000... Training loss: 0.7153\n",
      "Epoch: 616/2000... Training loss: 0.8832\n",
      "Epoch: 616/2000... Training loss: 0.7400\n",
      "Epoch: 616/2000... Training loss: 0.8833\n",
      "Epoch: 616/2000... Training loss: 0.7343\n",
      "Epoch: 616/2000... Training loss: 1.0280\n",
      "Epoch: 616/2000... Training loss: 0.8502\n",
      "Epoch: 616/2000... Training loss: 0.9440\n",
      "Epoch: 616/2000... Training loss: 0.7433\n",
      "Epoch: 616/2000... Training loss: 0.7183\n",
      "Epoch: 616/2000... Training loss: 0.5277\n",
      "Epoch: 616/2000... Training loss: 0.8554\n",
      "Epoch: 616/2000... Training loss: 0.7811\n",
      "Epoch: 616/2000... Training loss: 0.6547\n",
      "Epoch: 616/2000... Training loss: 0.7974\n",
      "Epoch: 616/2000... Training loss: 1.1350\n",
      "Epoch: 616/2000... Training loss: 0.7534\n",
      "Epoch: 616/2000... Training loss: 0.8793\n",
      "Epoch: 616/2000... Training loss: 0.9308\n",
      "Epoch: 616/2000... Training loss: 0.9278\n",
      "Epoch: 616/2000... Training loss: 0.6835\n",
      "Epoch: 616/2000... Training loss: 0.8838\n",
      "Epoch: 616/2000... Training loss: 0.8405\n",
      "Epoch: 617/2000... Training loss: 0.7161\n",
      "Epoch: 617/2000... Training loss: 0.7357\n",
      "Epoch: 617/2000... Training loss: 0.7424\n",
      "Epoch: 617/2000... Training loss: 0.8756\n",
      "Epoch: 617/2000... Training loss: 0.9208\n",
      "Epoch: 617/2000... Training loss: 0.8395\n",
      "Epoch: 617/2000... Training loss: 0.7570\n",
      "Epoch: 617/2000... Training loss: 0.7017\n",
      "Epoch: 617/2000... Training loss: 0.8959\n",
      "Epoch: 617/2000... Training loss: 0.7062\n",
      "Epoch: 617/2000... Training loss: 0.8122\n",
      "Epoch: 617/2000... Training loss: 0.7889\n",
      "Epoch: 617/2000... Training loss: 0.7607\n",
      "Epoch: 617/2000... Training loss: 0.5884\n",
      "Epoch: 617/2000... Training loss: 0.8881\n",
      "Epoch: 617/2000... Training loss: 0.8702\n",
      "Epoch: 617/2000... Training loss: 0.9280\n",
      "Epoch: 617/2000... Training loss: 0.9223\n",
      "Epoch: 617/2000... Training loss: 0.7421\n",
      "Epoch: 617/2000... Training loss: 0.9313\n",
      "Epoch: 617/2000... Training loss: 0.8513\n",
      "Epoch: 617/2000... Training loss: 0.7615\n",
      "Epoch: 617/2000... Training loss: 0.9734\n",
      "Epoch: 617/2000... Training loss: 0.8632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 617/2000... Training loss: 0.5354\n",
      "Epoch: 617/2000... Training loss: 0.6513\n",
      "Epoch: 617/2000... Training loss: 0.7990\n",
      "Epoch: 617/2000... Training loss: 0.8603\n",
      "Epoch: 617/2000... Training loss: 0.8726\n",
      "Epoch: 617/2000... Training loss: 0.9002\n",
      "Epoch: 617/2000... Training loss: 0.8409\n",
      "Epoch: 618/2000... Training loss: 0.9431\n",
      "Epoch: 618/2000... Training loss: 0.9839\n",
      "Epoch: 618/2000... Training loss: 1.0088\n",
      "Epoch: 618/2000... Training loss: 0.9867\n",
      "Epoch: 618/2000... Training loss: 0.8178\n",
      "Epoch: 618/2000... Training loss: 0.7248\n",
      "Epoch: 618/2000... Training loss: 0.7245\n",
      "Epoch: 618/2000... Training loss: 0.7245\n",
      "Epoch: 618/2000... Training loss: 0.7272\n",
      "Epoch: 618/2000... Training loss: 0.6909\n",
      "Epoch: 618/2000... Training loss: 0.9235\n",
      "Epoch: 618/2000... Training loss: 0.7846\n",
      "Epoch: 618/2000... Training loss: 0.7385\n",
      "Epoch: 618/2000... Training loss: 0.8828\n",
      "Epoch: 618/2000... Training loss: 0.6530\n",
      "Epoch: 618/2000... Training loss: 0.7395\n",
      "Epoch: 618/2000... Training loss: 0.6925\n",
      "Epoch: 618/2000... Training loss: 0.8671\n",
      "Epoch: 618/2000... Training loss: 1.0758\n",
      "Epoch: 618/2000... Training loss: 0.8424\n",
      "Epoch: 618/2000... Training loss: 0.8050\n",
      "Epoch: 618/2000... Training loss: 0.6707\n",
      "Epoch: 618/2000... Training loss: 0.8282\n",
      "Epoch: 618/2000... Training loss: 0.9317\n",
      "Epoch: 618/2000... Training loss: 0.6670\n",
      "Epoch: 618/2000... Training loss: 0.7569\n",
      "Epoch: 618/2000... Training loss: 0.7708\n",
      "Epoch: 618/2000... Training loss: 0.7889\n",
      "Epoch: 618/2000... Training loss: 0.9069\n",
      "Epoch: 618/2000... Training loss: 0.9069\n",
      "Epoch: 618/2000... Training loss: 0.6929\n",
      "Epoch: 619/2000... Training loss: 0.8931\n",
      "Epoch: 619/2000... Training loss: 0.7310\n",
      "Epoch: 619/2000... Training loss: 0.8507\n",
      "Epoch: 619/2000... Training loss: 0.8571\n",
      "Epoch: 619/2000... Training loss: 0.7891\n",
      "Epoch: 619/2000... Training loss: 0.6190\n",
      "Epoch: 619/2000... Training loss: 0.8279\n",
      "Epoch: 619/2000... Training loss: 0.8342\n",
      "Epoch: 619/2000... Training loss: 0.9169\n",
      "Epoch: 619/2000... Training loss: 1.0031\n",
      "Epoch: 619/2000... Training loss: 0.9624\n",
      "Epoch: 619/2000... Training loss: 0.8118\n",
      "Epoch: 619/2000... Training loss: 0.7323\n",
      "Epoch: 619/2000... Training loss: 1.0540\n",
      "Epoch: 619/2000... Training loss: 0.8495\n",
      "Epoch: 619/2000... Training loss: 0.8263\n",
      "Epoch: 619/2000... Training loss: 0.8687\n",
      "Epoch: 619/2000... Training loss: 1.0237\n",
      "Epoch: 619/2000... Training loss: 0.8555\n",
      "Epoch: 619/2000... Training loss: 0.9803\n",
      "Epoch: 619/2000... Training loss: 0.7107\n",
      "Epoch: 619/2000... Training loss: 0.8892\n",
      "Epoch: 619/2000... Training loss: 1.1371\n",
      "Epoch: 619/2000... Training loss: 0.7537\n",
      "Epoch: 619/2000... Training loss: 0.8231\n",
      "Epoch: 619/2000... Training loss: 0.9540\n",
      "Epoch: 619/2000... Training loss: 0.7480\n",
      "Epoch: 619/2000... Training loss: 0.7893\n",
      "Epoch: 619/2000... Training loss: 0.8064\n",
      "Epoch: 619/2000... Training loss: 0.7629\n",
      "Epoch: 619/2000... Training loss: 0.9543\n",
      "Epoch: 620/2000... Training loss: 0.7380\n",
      "Epoch: 620/2000... Training loss: 0.6933\n",
      "Epoch: 620/2000... Training loss: 0.7814\n",
      "Epoch: 620/2000... Training loss: 0.7954\n",
      "Epoch: 620/2000... Training loss: 0.7438\n",
      "Epoch: 620/2000... Training loss: 0.8749\n",
      "Epoch: 620/2000... Training loss: 0.8618\n",
      "Epoch: 620/2000... Training loss: 0.9531\n",
      "Epoch: 620/2000... Training loss: 1.0150\n",
      "Epoch: 620/2000... Training loss: 0.7632\n",
      "Epoch: 620/2000... Training loss: 0.8080\n",
      "Epoch: 620/2000... Training loss: 0.8786\n",
      "Epoch: 620/2000... Training loss: 0.8994\n",
      "Epoch: 620/2000... Training loss: 0.8526\n",
      "Epoch: 620/2000... Training loss: 0.9227\n",
      "Epoch: 620/2000... Training loss: 0.9444\n",
      "Epoch: 620/2000... Training loss: 0.8881\n",
      "Epoch: 620/2000... Training loss: 0.7675\n",
      "Epoch: 620/2000... Training loss: 0.6474\n",
      "Epoch: 620/2000... Training loss: 0.5693\n",
      "Epoch: 620/2000... Training loss: 0.5489\n",
      "Epoch: 620/2000... Training loss: 0.7131\n",
      "Epoch: 620/2000... Training loss: 0.9387\n",
      "Epoch: 620/2000... Training loss: 0.9476\n",
      "Epoch: 620/2000... Training loss: 0.7123\n",
      "Epoch: 620/2000... Training loss: 0.8798\n",
      "Epoch: 620/2000... Training loss: 0.7467\n",
      "Epoch: 620/2000... Training loss: 0.9720\n",
      "Epoch: 620/2000... Training loss: 0.8359\n",
      "Epoch: 620/2000... Training loss: 0.8442\n",
      "Epoch: 620/2000... Training loss: 0.6796\n",
      "Epoch: 621/2000... Training loss: 0.9402\n",
      "Epoch: 621/2000... Training loss: 0.9073\n",
      "Epoch: 621/2000... Training loss: 0.7452\n",
      "Epoch: 621/2000... Training loss: 0.8497\n",
      "Epoch: 621/2000... Training loss: 0.8572\n",
      "Epoch: 621/2000... Training loss: 1.1332\n",
      "Epoch: 621/2000... Training loss: 0.7589\n",
      "Epoch: 621/2000... Training loss: 0.8806\n",
      "Epoch: 621/2000... Training loss: 0.8232\n",
      "Epoch: 621/2000... Training loss: 0.6797\n",
      "Epoch: 621/2000... Training loss: 0.9360\n",
      "Epoch: 621/2000... Training loss: 0.7034\n",
      "Epoch: 621/2000... Training loss: 0.8864\n",
      "Epoch: 621/2000... Training loss: 0.7387\n",
      "Epoch: 621/2000... Training loss: 0.9792\n",
      "Epoch: 621/2000... Training loss: 0.9803\n",
      "Epoch: 621/2000... Training loss: 0.9000\n",
      "Epoch: 621/2000... Training loss: 0.8893\n",
      "Epoch: 621/2000... Training loss: 0.7515\n",
      "Epoch: 621/2000... Training loss: 0.8863\n",
      "Epoch: 621/2000... Training loss: 0.9006\n",
      "Epoch: 621/2000... Training loss: 0.9767\n",
      "Epoch: 621/2000... Training loss: 0.9833\n",
      "Epoch: 621/2000... Training loss: 0.8118\n",
      "Epoch: 621/2000... Training loss: 0.5839\n",
      "Epoch: 621/2000... Training loss: 0.8071\n",
      "Epoch: 621/2000... Training loss: 0.6901\n",
      "Epoch: 621/2000... Training loss: 0.9788\n",
      "Epoch: 621/2000... Training loss: 0.5787\n",
      "Epoch: 621/2000... Training loss: 1.1267\n",
      "Epoch: 621/2000... Training loss: 1.1088\n",
      "Epoch: 622/2000... Training loss: 0.9046\n",
      "Epoch: 622/2000... Training loss: 0.5934\n",
      "Epoch: 622/2000... Training loss: 1.1336\n",
      "Epoch: 622/2000... Training loss: 1.0317\n",
      "Epoch: 622/2000... Training loss: 0.8721\n",
      "Epoch: 622/2000... Training loss: 0.8391\n",
      "Epoch: 622/2000... Training loss: 0.6623\n",
      "Epoch: 622/2000... Training loss: 0.6783\n",
      "Epoch: 622/2000... Training loss: 0.9796\n",
      "Epoch: 622/2000... Training loss: 0.9333\n",
      "Epoch: 622/2000... Training loss: 0.8157\n",
      "Epoch: 622/2000... Training loss: 0.7710\n",
      "Epoch: 622/2000... Training loss: 0.7938\n",
      "Epoch: 622/2000... Training loss: 0.8445\n",
      "Epoch: 622/2000... Training loss: 0.7700\n",
      "Epoch: 622/2000... Training loss: 1.0296\n",
      "Epoch: 622/2000... Training loss: 0.9975\n",
      "Epoch: 622/2000... Training loss: 0.7819\n",
      "Epoch: 622/2000... Training loss: 0.9709\n",
      "Epoch: 622/2000... Training loss: 0.7742\n",
      "Epoch: 622/2000... Training loss: 0.9023\n",
      "Epoch: 622/2000... Training loss: 0.9372\n",
      "Epoch: 622/2000... Training loss: 0.9393\n",
      "Epoch: 622/2000... Training loss: 0.7954\n",
      "Epoch: 622/2000... Training loss: 1.0062\n",
      "Epoch: 622/2000... Training loss: 0.9092\n",
      "Epoch: 622/2000... Training loss: 0.7648\n",
      "Epoch: 622/2000... Training loss: 0.8998\n",
      "Epoch: 622/2000... Training loss: 0.7737\n",
      "Epoch: 622/2000... Training loss: 0.4952\n",
      "Epoch: 622/2000... Training loss: 1.0683\n",
      "Epoch: 623/2000... Training loss: 0.9088\n",
      "Epoch: 623/2000... Training loss: 0.8357\n",
      "Epoch: 623/2000... Training loss: 1.0996\n",
      "Epoch: 623/2000... Training loss: 0.8165\n",
      "Epoch: 623/2000... Training loss: 0.7226\n",
      "Epoch: 623/2000... Training loss: 0.7200\n",
      "Epoch: 623/2000... Training loss: 0.8399\n",
      "Epoch: 623/2000... Training loss: 0.8652\n",
      "Epoch: 623/2000... Training loss: 0.8622\n",
      "Epoch: 623/2000... Training loss: 1.0511\n",
      "Epoch: 623/2000... Training loss: 0.9054\n",
      "Epoch: 623/2000... Training loss: 0.7227\n",
      "Epoch: 623/2000... Training loss: 0.9120\n",
      "Epoch: 623/2000... Training loss: 0.9223\n",
      "Epoch: 623/2000... Training loss: 0.9545\n",
      "Epoch: 623/2000... Training loss: 0.8486\n",
      "Epoch: 623/2000... Training loss: 0.6953\n",
      "Epoch: 623/2000... Training loss: 0.8793\n",
      "Epoch: 623/2000... Training loss: 1.0847\n",
      "Epoch: 623/2000... Training loss: 0.9075\n",
      "Epoch: 623/2000... Training loss: 0.6641\n",
      "Epoch: 623/2000... Training loss: 0.7192\n",
      "Epoch: 623/2000... Training loss: 0.7614\n",
      "Epoch: 623/2000... Training loss: 0.8917\n",
      "Epoch: 623/2000... Training loss: 0.6687\n",
      "Epoch: 623/2000... Training loss: 0.7070\n",
      "Epoch: 623/2000... Training loss: 0.7240\n",
      "Epoch: 623/2000... Training loss: 0.7856\n",
      "Epoch: 623/2000... Training loss: 0.8034\n",
      "Epoch: 623/2000... Training loss: 0.8601\n",
      "Epoch: 623/2000... Training loss: 0.8765\n",
      "Epoch: 624/2000... Training loss: 0.8894\n",
      "Epoch: 624/2000... Training loss: 0.7977\n",
      "Epoch: 624/2000... Training loss: 0.6252\n",
      "Epoch: 624/2000... Training loss: 0.9575\n",
      "Epoch: 624/2000... Training loss: 0.9691\n",
      "Epoch: 624/2000... Training loss: 0.7085\n",
      "Epoch: 624/2000... Training loss: 0.7369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 624/2000... Training loss: 0.7126\n",
      "Epoch: 624/2000... Training loss: 0.9058\n",
      "Epoch: 624/2000... Training loss: 0.9088\n",
      "Epoch: 624/2000... Training loss: 0.9371\n",
      "Epoch: 624/2000... Training loss: 0.6699\n",
      "Epoch: 624/2000... Training loss: 0.8285\n",
      "Epoch: 624/2000... Training loss: 0.7952\n",
      "Epoch: 624/2000... Training loss: 0.8717\n",
      "Epoch: 624/2000... Training loss: 0.9189\n",
      "Epoch: 624/2000... Training loss: 0.7732\n",
      "Epoch: 624/2000... Training loss: 0.6544\n",
      "Epoch: 624/2000... Training loss: 1.0257\n",
      "Epoch: 624/2000... Training loss: 0.8914\n",
      "Epoch: 624/2000... Training loss: 0.9005\n",
      "Epoch: 624/2000... Training loss: 0.8377\n",
      "Epoch: 624/2000... Training loss: 0.8711\n",
      "Epoch: 624/2000... Training loss: 0.6960\n",
      "Epoch: 624/2000... Training loss: 0.8959\n",
      "Epoch: 624/2000... Training loss: 0.8460\n",
      "Epoch: 624/2000... Training loss: 0.8463\n",
      "Epoch: 624/2000... Training loss: 0.9727\n",
      "Epoch: 624/2000... Training loss: 0.8650\n",
      "Epoch: 624/2000... Training loss: 0.8745\n",
      "Epoch: 624/2000... Training loss: 0.9638\n",
      "Epoch: 625/2000... Training loss: 0.7983\n",
      "Epoch: 625/2000... Training loss: 0.7858\n",
      "Epoch: 625/2000... Training loss: 0.8302\n",
      "Epoch: 625/2000... Training loss: 0.9558\n",
      "Epoch: 625/2000... Training loss: 0.8851\n",
      "Epoch: 625/2000... Training loss: 0.7966\n",
      "Epoch: 625/2000... Training loss: 0.6456\n",
      "Epoch: 625/2000... Training loss: 0.8026\n",
      "Epoch: 625/2000... Training loss: 0.8058\n",
      "Epoch: 625/2000... Training loss: 0.8279\n",
      "Epoch: 625/2000... Training loss: 0.6830\n",
      "Epoch: 625/2000... Training loss: 0.8696\n",
      "Epoch: 625/2000... Training loss: 0.7764\n",
      "Epoch: 625/2000... Training loss: 1.0206\n",
      "Epoch: 625/2000... Training loss: 0.8737\n",
      "Epoch: 625/2000... Training loss: 0.6353\n",
      "Epoch: 625/2000... Training loss: 0.6618\n",
      "Epoch: 625/2000... Training loss: 0.7506\n",
      "Epoch: 625/2000... Training loss: 0.7240\n",
      "Epoch: 625/2000... Training loss: 0.7616\n",
      "Epoch: 625/2000... Training loss: 0.9066\n",
      "Epoch: 625/2000... Training loss: 0.7909\n",
      "Epoch: 625/2000... Training loss: 0.7649\n",
      "Epoch: 625/2000... Training loss: 0.7985\n",
      "Epoch: 625/2000... Training loss: 0.8543\n",
      "Epoch: 625/2000... Training loss: 0.8124\n",
      "Epoch: 625/2000... Training loss: 1.0490\n",
      "Epoch: 625/2000... Training loss: 1.0112\n",
      "Epoch: 625/2000... Training loss: 0.7118\n",
      "Epoch: 625/2000... Training loss: 0.9542\n",
      "Epoch: 625/2000... Training loss: 0.7959\n",
      "Epoch: 626/2000... Training loss: 1.0242\n",
      "Epoch: 626/2000... Training loss: 0.7211\n",
      "Epoch: 626/2000... Training loss: 0.8142\n",
      "Epoch: 626/2000... Training loss: 0.8758\n",
      "Epoch: 626/2000... Training loss: 0.7716\n",
      "Epoch: 626/2000... Training loss: 0.9688\n",
      "Epoch: 626/2000... Training loss: 0.7283\n",
      "Epoch: 626/2000... Training loss: 0.7762\n",
      "Epoch: 626/2000... Training loss: 0.6606\n",
      "Epoch: 626/2000... Training loss: 0.7139\n",
      "Epoch: 626/2000... Training loss: 0.8124\n",
      "Epoch: 626/2000... Training loss: 0.6564\n",
      "Epoch: 626/2000... Training loss: 0.6621\n",
      "Epoch: 626/2000... Training loss: 0.6168\n",
      "Epoch: 626/2000... Training loss: 0.7060\n",
      "Epoch: 626/2000... Training loss: 1.0653\n",
      "Epoch: 626/2000... Training loss: 0.8140\n",
      "Epoch: 626/2000... Training loss: 0.8228\n",
      "Epoch: 626/2000... Training loss: 0.7577\n",
      "Epoch: 626/2000... Training loss: 0.8710\n",
      "Epoch: 626/2000... Training loss: 0.7688\n",
      "Epoch: 626/2000... Training loss: 0.6959\n",
      "Epoch: 626/2000... Training loss: 0.8613\n",
      "Epoch: 626/2000... Training loss: 0.9077\n",
      "Epoch: 626/2000... Training loss: 0.7835\n",
      "Epoch: 626/2000... Training loss: 0.7966\n",
      "Epoch: 626/2000... Training loss: 0.7461\n",
      "Epoch: 626/2000... Training loss: 0.8863\n",
      "Epoch: 626/2000... Training loss: 0.8003\n",
      "Epoch: 626/2000... Training loss: 0.9544\n",
      "Epoch: 626/2000... Training loss: 0.8350\n",
      "Epoch: 627/2000... Training loss: 1.0099\n",
      "Epoch: 627/2000... Training loss: 0.7807\n",
      "Epoch: 627/2000... Training loss: 1.0061\n",
      "Epoch: 627/2000... Training loss: 0.6697\n",
      "Epoch: 627/2000... Training loss: 0.9234\n",
      "Epoch: 627/2000... Training loss: 0.8363\n",
      "Epoch: 627/2000... Training loss: 0.9268\n",
      "Epoch: 627/2000... Training loss: 0.8834\n",
      "Epoch: 627/2000... Training loss: 0.9209\n",
      "Epoch: 627/2000... Training loss: 0.9941\n",
      "Epoch: 627/2000... Training loss: 0.8826\n",
      "Epoch: 627/2000... Training loss: 0.6939\n",
      "Epoch: 627/2000... Training loss: 0.8958\n",
      "Epoch: 627/2000... Training loss: 0.8417\n",
      "Epoch: 627/2000... Training loss: 0.9180\n",
      "Epoch: 627/2000... Training loss: 0.6172\n",
      "Epoch: 627/2000... Training loss: 0.8026\n",
      "Epoch: 627/2000... Training loss: 0.6897\n",
      "Epoch: 627/2000... Training loss: 0.7255\n",
      "Epoch: 627/2000... Training loss: 0.8609\n",
      "Epoch: 627/2000... Training loss: 0.6913\n",
      "Epoch: 627/2000... Training loss: 0.6884\n",
      "Epoch: 627/2000... Training loss: 0.8349\n",
      "Epoch: 627/2000... Training loss: 0.6353\n",
      "Epoch: 627/2000... Training loss: 0.8302\n",
      "Epoch: 627/2000... Training loss: 1.1434\n",
      "Epoch: 627/2000... Training loss: 1.0370\n",
      "Epoch: 627/2000... Training loss: 0.7686\n",
      "Epoch: 627/2000... Training loss: 0.7065\n",
      "Epoch: 627/2000... Training loss: 0.7406\n",
      "Epoch: 627/2000... Training loss: 0.8810\n",
      "Epoch: 628/2000... Training loss: 1.0537\n",
      "Epoch: 628/2000... Training loss: 0.7031\n",
      "Epoch: 628/2000... Training loss: 0.7368\n",
      "Epoch: 628/2000... Training loss: 0.6986\n",
      "Epoch: 628/2000... Training loss: 1.0476\n",
      "Epoch: 628/2000... Training loss: 0.9491\n",
      "Epoch: 628/2000... Training loss: 0.8044\n",
      "Epoch: 628/2000... Training loss: 0.7635\n",
      "Epoch: 628/2000... Training loss: 0.9547\n",
      "Epoch: 628/2000... Training loss: 0.7829\n",
      "Epoch: 628/2000... Training loss: 0.8153\n",
      "Epoch: 628/2000... Training loss: 0.9900\n",
      "Epoch: 628/2000... Training loss: 1.0050\n",
      "Epoch: 628/2000... Training loss: 0.8048\n",
      "Epoch: 628/2000... Training loss: 0.7667\n",
      "Epoch: 628/2000... Training loss: 0.8577\n",
      "Epoch: 628/2000... Training loss: 0.8052\n",
      "Epoch: 628/2000... Training loss: 0.6501\n",
      "Epoch: 628/2000... Training loss: 0.7178\n",
      "Epoch: 628/2000... Training loss: 0.7248\n",
      "Epoch: 628/2000... Training loss: 0.9112\n",
      "Epoch: 628/2000... Training loss: 0.7535\n",
      "Epoch: 628/2000... Training loss: 0.8027\n",
      "Epoch: 628/2000... Training loss: 0.6878\n",
      "Epoch: 628/2000... Training loss: 0.7554\n",
      "Epoch: 628/2000... Training loss: 1.0244\n",
      "Epoch: 628/2000... Training loss: 0.8501\n",
      "Epoch: 628/2000... Training loss: 0.6192\n",
      "Epoch: 628/2000... Training loss: 0.7899\n",
      "Epoch: 628/2000... Training loss: 0.9160\n",
      "Epoch: 628/2000... Training loss: 0.7138\n",
      "Epoch: 629/2000... Training loss: 0.6827\n",
      "Epoch: 629/2000... Training loss: 0.7583\n",
      "Epoch: 629/2000... Training loss: 0.9075\n",
      "Epoch: 629/2000... Training loss: 0.7612\n",
      "Epoch: 629/2000... Training loss: 0.6629\n",
      "Epoch: 629/2000... Training loss: 0.9539\n",
      "Epoch: 629/2000... Training loss: 0.8509\n",
      "Epoch: 629/2000... Training loss: 0.7797\n",
      "Epoch: 629/2000... Training loss: 0.8281\n",
      "Epoch: 629/2000... Training loss: 0.8205\n",
      "Epoch: 629/2000... Training loss: 0.7476\n",
      "Epoch: 629/2000... Training loss: 0.7243\n",
      "Epoch: 629/2000... Training loss: 0.7821\n",
      "Epoch: 629/2000... Training loss: 0.6548\n",
      "Epoch: 629/2000... Training loss: 0.7501\n",
      "Epoch: 629/2000... Training loss: 0.8887\n",
      "Epoch: 629/2000... Training loss: 0.6380\n",
      "Epoch: 629/2000... Training loss: 0.8629\n",
      "Epoch: 629/2000... Training loss: 0.8638\n",
      "Epoch: 629/2000... Training loss: 0.8051\n",
      "Epoch: 629/2000... Training loss: 0.5407\n",
      "Epoch: 629/2000... Training loss: 0.9446\n",
      "Epoch: 629/2000... Training loss: 0.8708\n",
      "Epoch: 629/2000... Training loss: 0.8957\n",
      "Epoch: 629/2000... Training loss: 0.5900\n",
      "Epoch: 629/2000... Training loss: 0.7937\n",
      "Epoch: 629/2000... Training loss: 0.8543\n",
      "Epoch: 629/2000... Training loss: 0.7363\n",
      "Epoch: 629/2000... Training loss: 0.6623\n",
      "Epoch: 629/2000... Training loss: 0.7707\n",
      "Epoch: 629/2000... Training loss: 0.8786\n",
      "Epoch: 630/2000... Training loss: 0.8531\n",
      "Epoch: 630/2000... Training loss: 0.6222\n",
      "Epoch: 630/2000... Training loss: 0.9728\n",
      "Epoch: 630/2000... Training loss: 0.7627\n",
      "Epoch: 630/2000... Training loss: 0.7706\n",
      "Epoch: 630/2000... Training loss: 0.8112\n",
      "Epoch: 630/2000... Training loss: 0.8805\n",
      "Epoch: 630/2000... Training loss: 0.7720\n",
      "Epoch: 630/2000... Training loss: 0.8129\n",
      "Epoch: 630/2000... Training loss: 0.8602\n",
      "Epoch: 630/2000... Training loss: 0.7597\n",
      "Epoch: 630/2000... Training loss: 0.7203\n",
      "Epoch: 630/2000... Training loss: 0.7605\n",
      "Epoch: 630/2000... Training loss: 0.5632\n",
      "Epoch: 630/2000... Training loss: 0.8212\n",
      "Epoch: 630/2000... Training loss: 0.7315\n",
      "Epoch: 630/2000... Training loss: 0.9870\n",
      "Epoch: 630/2000... Training loss: 1.0196\n",
      "Epoch: 630/2000... Training loss: 0.9237\n",
      "Epoch: 630/2000... Training loss: 0.8134\n",
      "Epoch: 630/2000... Training loss: 0.7733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 630/2000... Training loss: 0.7280\n",
      "Epoch: 630/2000... Training loss: 0.9120\n",
      "Epoch: 630/2000... Training loss: 0.8306\n",
      "Epoch: 630/2000... Training loss: 0.7763\n",
      "Epoch: 630/2000... Training loss: 0.7962\n",
      "Epoch: 630/2000... Training loss: 0.9462\n",
      "Epoch: 630/2000... Training loss: 0.8518\n",
      "Epoch: 630/2000... Training loss: 0.6643\n",
      "Epoch: 630/2000... Training loss: 0.9656\n",
      "Epoch: 630/2000... Training loss: 0.9065\n",
      "Epoch: 631/2000... Training loss: 0.7247\n",
      "Epoch: 631/2000... Training loss: 0.7549\n",
      "Epoch: 631/2000... Training loss: 0.8489\n",
      "Epoch: 631/2000... Training loss: 1.0081\n",
      "Epoch: 631/2000... Training loss: 0.9817\n",
      "Epoch: 631/2000... Training loss: 0.7328\n",
      "Epoch: 631/2000... Training loss: 0.7275\n",
      "Epoch: 631/2000... Training loss: 0.9288\n",
      "Epoch: 631/2000... Training loss: 0.7394\n",
      "Epoch: 631/2000... Training loss: 0.7236\n",
      "Epoch: 631/2000... Training loss: 0.8168\n",
      "Epoch: 631/2000... Training loss: 0.7388\n",
      "Epoch: 631/2000... Training loss: 0.8786\n",
      "Epoch: 631/2000... Training loss: 0.8900\n",
      "Epoch: 631/2000... Training loss: 0.7246\n",
      "Epoch: 631/2000... Training loss: 0.7352\n",
      "Epoch: 631/2000... Training loss: 0.9466\n",
      "Epoch: 631/2000... Training loss: 1.0780\n",
      "Epoch: 631/2000... Training loss: 0.7994\n",
      "Epoch: 631/2000... Training loss: 0.7755\n",
      "Epoch: 631/2000... Training loss: 0.7466\n",
      "Epoch: 631/2000... Training loss: 0.6831\n",
      "Epoch: 631/2000... Training loss: 0.9184\n",
      "Epoch: 631/2000... Training loss: 0.6884\n",
      "Epoch: 631/2000... Training loss: 0.9155\n",
      "Epoch: 631/2000... Training loss: 0.8016\n",
      "Epoch: 631/2000... Training loss: 1.0152\n",
      "Epoch: 631/2000... Training loss: 0.8167\n",
      "Epoch: 631/2000... Training loss: 0.8787\n",
      "Epoch: 631/2000... Training loss: 0.8823\n",
      "Epoch: 631/2000... Training loss: 0.7625\n",
      "Epoch: 632/2000... Training loss: 0.8588\n",
      "Epoch: 632/2000... Training loss: 0.7547\n",
      "Epoch: 632/2000... Training loss: 0.8284\n",
      "Epoch: 632/2000... Training loss: 0.8924\n",
      "Epoch: 632/2000... Training loss: 1.0202\n",
      "Epoch: 632/2000... Training loss: 0.9513\n",
      "Epoch: 632/2000... Training loss: 0.5944\n",
      "Epoch: 632/2000... Training loss: 0.8756\n",
      "Epoch: 632/2000... Training loss: 0.8350\n",
      "Epoch: 632/2000... Training loss: 0.7126\n",
      "Epoch: 632/2000... Training loss: 0.8165\n",
      "Epoch: 632/2000... Training loss: 0.7742\n",
      "Epoch: 632/2000... Training loss: 0.6189\n",
      "Epoch: 632/2000... Training loss: 0.8044\n",
      "Epoch: 632/2000... Training loss: 0.8095\n",
      "Epoch: 632/2000... Training loss: 0.8835\n",
      "Epoch: 632/2000... Training loss: 0.8220\n",
      "Epoch: 632/2000... Training loss: 0.8253\n",
      "Epoch: 632/2000... Training loss: 0.8410\n",
      "Epoch: 632/2000... Training loss: 0.8608\n",
      "Epoch: 632/2000... Training loss: 0.7535\n",
      "Epoch: 632/2000... Training loss: 0.6853\n",
      "Epoch: 632/2000... Training loss: 1.0072\n",
      "Epoch: 632/2000... Training loss: 0.7549\n",
      "Epoch: 632/2000... Training loss: 0.8846\n",
      "Epoch: 632/2000... Training loss: 0.7351\n",
      "Epoch: 632/2000... Training loss: 0.7812\n",
      "Epoch: 632/2000... Training loss: 0.9946\n",
      "Epoch: 632/2000... Training loss: 1.1131\n",
      "Epoch: 632/2000... Training loss: 0.8234\n",
      "Epoch: 632/2000... Training loss: 0.9289\n",
      "Epoch: 633/2000... Training loss: 0.6984\n",
      "Epoch: 633/2000... Training loss: 0.8438\n",
      "Epoch: 633/2000... Training loss: 0.9036\n",
      "Epoch: 633/2000... Training loss: 0.8707\n",
      "Epoch: 633/2000... Training loss: 0.8156\n",
      "Epoch: 633/2000... Training loss: 0.8222\n",
      "Epoch: 633/2000... Training loss: 0.6081\n",
      "Epoch: 633/2000... Training loss: 1.0364\n",
      "Epoch: 633/2000... Training loss: 0.8325\n",
      "Epoch: 633/2000... Training loss: 1.0128\n",
      "Epoch: 633/2000... Training loss: 0.7707\n",
      "Epoch: 633/2000... Training loss: 0.8199\n",
      "Epoch: 633/2000... Training loss: 0.8241\n",
      "Epoch: 633/2000... Training loss: 0.8129\n",
      "Epoch: 633/2000... Training loss: 0.7856\n",
      "Epoch: 633/2000... Training loss: 0.6198\n",
      "Epoch: 633/2000... Training loss: 0.8072\n",
      "Epoch: 633/2000... Training loss: 0.7066\n",
      "Epoch: 633/2000... Training loss: 0.6588\n",
      "Epoch: 633/2000... Training loss: 0.6164\n",
      "Epoch: 633/2000... Training loss: 0.6822\n",
      "Epoch: 633/2000... Training loss: 0.8130\n",
      "Epoch: 633/2000... Training loss: 0.7500\n",
      "Epoch: 633/2000... Training loss: 0.8590\n",
      "Epoch: 633/2000... Training loss: 0.7122\n",
      "Epoch: 633/2000... Training loss: 0.9655\n",
      "Epoch: 633/2000... Training loss: 0.6037\n",
      "Epoch: 633/2000... Training loss: 0.7533\n",
      "Epoch: 633/2000... Training loss: 0.8825\n",
      "Epoch: 633/2000... Training loss: 0.8908\n",
      "Epoch: 633/2000... Training loss: 0.6614\n",
      "Epoch: 634/2000... Training loss: 0.9538\n",
      "Epoch: 634/2000... Training loss: 0.6875\n",
      "Epoch: 634/2000... Training loss: 0.7215\n",
      "Epoch: 634/2000... Training loss: 0.9535\n",
      "Epoch: 634/2000... Training loss: 0.6209\n",
      "Epoch: 634/2000... Training loss: 0.8399\n",
      "Epoch: 634/2000... Training loss: 0.7900\n",
      "Epoch: 634/2000... Training loss: 0.7644\n",
      "Epoch: 634/2000... Training loss: 0.9540\n",
      "Epoch: 634/2000... Training loss: 0.9642\n",
      "Epoch: 634/2000... Training loss: 0.7026\n",
      "Epoch: 634/2000... Training loss: 0.9422\n",
      "Epoch: 634/2000... Training loss: 0.7575\n",
      "Epoch: 634/2000... Training loss: 0.7788\n",
      "Epoch: 634/2000... Training loss: 0.8823\n",
      "Epoch: 634/2000... Training loss: 0.8270\n",
      "Epoch: 634/2000... Training loss: 0.9159\n",
      "Epoch: 634/2000... Training loss: 0.7490\n",
      "Epoch: 634/2000... Training loss: 0.9300\n",
      "Epoch: 634/2000... Training loss: 0.7511\n",
      "Epoch: 634/2000... Training loss: 0.8496\n",
      "Epoch: 634/2000... Training loss: 0.7641\n",
      "Epoch: 634/2000... Training loss: 0.6890\n",
      "Epoch: 634/2000... Training loss: 0.7651\n",
      "Epoch: 634/2000... Training loss: 0.8556\n",
      "Epoch: 634/2000... Training loss: 0.8011\n",
      "Epoch: 634/2000... Training loss: 0.9997\n",
      "Epoch: 634/2000... Training loss: 0.6638\n",
      "Epoch: 634/2000... Training loss: 0.6458\n",
      "Epoch: 634/2000... Training loss: 0.9694\n",
      "Epoch: 634/2000... Training loss: 0.8385\n",
      "Epoch: 635/2000... Training loss: 0.7039\n",
      "Epoch: 635/2000... Training loss: 0.9089\n",
      "Epoch: 635/2000... Training loss: 0.7275\n",
      "Epoch: 635/2000... Training loss: 0.5058\n",
      "Epoch: 635/2000... Training loss: 0.7768\n",
      "Epoch: 635/2000... Training loss: 0.8888\n",
      "Epoch: 635/2000... Training loss: 0.8990\n",
      "Epoch: 635/2000... Training loss: 0.7507\n",
      "Epoch: 635/2000... Training loss: 0.6435\n",
      "Epoch: 635/2000... Training loss: 0.8734\n",
      "Epoch: 635/2000... Training loss: 0.9844\n",
      "Epoch: 635/2000... Training loss: 0.7136\n",
      "Epoch: 635/2000... Training loss: 0.7428\n",
      "Epoch: 635/2000... Training loss: 0.7860\n",
      "Epoch: 635/2000... Training loss: 0.8530\n",
      "Epoch: 635/2000... Training loss: 0.9321\n",
      "Epoch: 635/2000... Training loss: 0.8083\n",
      "Epoch: 635/2000... Training loss: 0.5820\n",
      "Epoch: 635/2000... Training loss: 0.8757\n",
      "Epoch: 635/2000... Training loss: 0.8390\n",
      "Epoch: 635/2000... Training loss: 0.6224\n",
      "Epoch: 635/2000... Training loss: 0.9155\n",
      "Epoch: 635/2000... Training loss: 0.7294\n",
      "Epoch: 635/2000... Training loss: 0.6793\n",
      "Epoch: 635/2000... Training loss: 0.8542\n",
      "Epoch: 635/2000... Training loss: 0.7202\n",
      "Epoch: 635/2000... Training loss: 0.9568\n",
      "Epoch: 635/2000... Training loss: 0.7933\n",
      "Epoch: 635/2000... Training loss: 0.8124\n",
      "Epoch: 635/2000... Training loss: 0.7788\n",
      "Epoch: 635/2000... Training loss: 0.7412\n",
      "Epoch: 636/2000... Training loss: 1.0300\n",
      "Epoch: 636/2000... Training loss: 0.8278\n",
      "Epoch: 636/2000... Training loss: 0.7397\n",
      "Epoch: 636/2000... Training loss: 0.7615\n",
      "Epoch: 636/2000... Training loss: 0.7239\n",
      "Epoch: 636/2000... Training loss: 0.7363\n",
      "Epoch: 636/2000... Training loss: 0.7390\n",
      "Epoch: 636/2000... Training loss: 0.9568\n",
      "Epoch: 636/2000... Training loss: 0.7690\n",
      "Epoch: 636/2000... Training loss: 1.0149\n",
      "Epoch: 636/2000... Training loss: 0.7870\n",
      "Epoch: 636/2000... Training loss: 0.7845\n",
      "Epoch: 636/2000... Training loss: 0.7241\n",
      "Epoch: 636/2000... Training loss: 0.8314\n",
      "Epoch: 636/2000... Training loss: 0.9840\n",
      "Epoch: 636/2000... Training loss: 0.6699\n",
      "Epoch: 636/2000... Training loss: 0.6584\n",
      "Epoch: 636/2000... Training loss: 0.6990\n",
      "Epoch: 636/2000... Training loss: 0.7671\n",
      "Epoch: 636/2000... Training loss: 0.9287\n",
      "Epoch: 636/2000... Training loss: 0.7535\n",
      "Epoch: 636/2000... Training loss: 0.5817\n",
      "Epoch: 636/2000... Training loss: 0.8440\n",
      "Epoch: 636/2000... Training loss: 0.6712\n",
      "Epoch: 636/2000... Training loss: 0.7313\n",
      "Epoch: 636/2000... Training loss: 0.8724\n",
      "Epoch: 636/2000... Training loss: 0.7362\n",
      "Epoch: 636/2000... Training loss: 0.7760\n",
      "Epoch: 636/2000... Training loss: 0.6744\n",
      "Epoch: 636/2000... Training loss: 0.9226\n",
      "Epoch: 636/2000... Training loss: 0.7313\n",
      "Epoch: 637/2000... Training loss: 0.8042\n",
      "Epoch: 637/2000... Training loss: 0.8232\n",
      "Epoch: 637/2000... Training loss: 0.7647\n",
      "Epoch: 637/2000... Training loss: 0.7068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 637/2000... Training loss: 1.0995\n",
      "Epoch: 637/2000... Training loss: 0.9998\n",
      "Epoch: 637/2000... Training loss: 0.7118\n",
      "Epoch: 637/2000... Training loss: 0.6964\n",
      "Epoch: 637/2000... Training loss: 0.9829\n",
      "Epoch: 637/2000... Training loss: 0.7873\n",
      "Epoch: 637/2000... Training loss: 0.8681\n",
      "Epoch: 637/2000... Training loss: 0.7260\n",
      "Epoch: 637/2000... Training loss: 0.7346\n",
      "Epoch: 637/2000... Training loss: 0.5865\n",
      "Epoch: 637/2000... Training loss: 0.8058\n",
      "Epoch: 637/2000... Training loss: 0.9064\n",
      "Epoch: 637/2000... Training loss: 0.7804\n",
      "Epoch: 637/2000... Training loss: 0.5673\n",
      "Epoch: 637/2000... Training loss: 0.8997\n",
      "Epoch: 637/2000... Training loss: 1.0448\n",
      "Epoch: 637/2000... Training loss: 0.5963\n",
      "Epoch: 637/2000... Training loss: 0.8595\n",
      "Epoch: 637/2000... Training loss: 1.0289\n",
      "Epoch: 637/2000... Training loss: 0.5958\n",
      "Epoch: 637/2000... Training loss: 0.7561\n",
      "Epoch: 637/2000... Training loss: 0.8976\n",
      "Epoch: 637/2000... Training loss: 0.6769\n",
      "Epoch: 637/2000... Training loss: 0.6478\n",
      "Epoch: 637/2000... Training loss: 0.8715\n",
      "Epoch: 637/2000... Training loss: 0.8946\n",
      "Epoch: 637/2000... Training loss: 0.9443\n",
      "Epoch: 638/2000... Training loss: 0.7210\n",
      "Epoch: 638/2000... Training loss: 0.7626\n",
      "Epoch: 638/2000... Training loss: 0.8074\n",
      "Epoch: 638/2000... Training loss: 0.9342\n",
      "Epoch: 638/2000... Training loss: 0.7354\n",
      "Epoch: 638/2000... Training loss: 1.1077\n",
      "Epoch: 638/2000... Training loss: 0.8094\n",
      "Epoch: 638/2000... Training loss: 0.8625\n",
      "Epoch: 638/2000... Training loss: 0.8807\n",
      "Epoch: 638/2000... Training loss: 0.7318\n",
      "Epoch: 638/2000... Training loss: 0.7633\n",
      "Epoch: 638/2000... Training loss: 0.8612\n",
      "Epoch: 638/2000... Training loss: 0.7171\n",
      "Epoch: 638/2000... Training loss: 0.7540\n",
      "Epoch: 638/2000... Training loss: 0.5685\n",
      "Epoch: 638/2000... Training loss: 0.9854\n",
      "Epoch: 638/2000... Training loss: 0.8531\n",
      "Epoch: 638/2000... Training loss: 0.8398\n",
      "Epoch: 638/2000... Training loss: 0.6273\n",
      "Epoch: 638/2000... Training loss: 0.6771\n",
      "Epoch: 638/2000... Training loss: 0.7947\n",
      "Epoch: 638/2000... Training loss: 0.6945\n",
      "Epoch: 638/2000... Training loss: 0.8723\n",
      "Epoch: 638/2000... Training loss: 0.7333\n",
      "Epoch: 638/2000... Training loss: 0.7553\n",
      "Epoch: 638/2000... Training loss: 0.8639\n",
      "Epoch: 638/2000... Training loss: 0.8548\n",
      "Epoch: 638/2000... Training loss: 0.7737\n",
      "Epoch: 638/2000... Training loss: 0.7682\n",
      "Epoch: 638/2000... Training loss: 0.8379\n",
      "Epoch: 638/2000... Training loss: 0.8503\n",
      "Epoch: 639/2000... Training loss: 0.8007\n",
      "Epoch: 639/2000... Training loss: 0.7537\n",
      "Epoch: 639/2000... Training loss: 0.9673\n",
      "Epoch: 639/2000... Training loss: 0.8802\n",
      "Epoch: 639/2000... Training loss: 0.6415\n",
      "Epoch: 639/2000... Training loss: 0.7758\n",
      "Epoch: 639/2000... Training loss: 0.6041\n",
      "Epoch: 639/2000... Training loss: 0.8105\n",
      "Epoch: 639/2000... Training loss: 1.0752\n",
      "Epoch: 639/2000... Training loss: 0.6730\n",
      "Epoch: 639/2000... Training loss: 0.7859\n",
      "Epoch: 639/2000... Training loss: 0.8769\n",
      "Epoch: 639/2000... Training loss: 0.8744\n",
      "Epoch: 639/2000... Training loss: 0.6876\n",
      "Epoch: 639/2000... Training loss: 0.8369\n",
      "Epoch: 639/2000... Training loss: 0.7719\n",
      "Epoch: 639/2000... Training loss: 0.6573\n",
      "Epoch: 639/2000... Training loss: 0.6777\n",
      "Epoch: 639/2000... Training loss: 1.0815\n",
      "Epoch: 639/2000... Training loss: 0.6758\n",
      "Epoch: 639/2000... Training loss: 0.8572\n",
      "Epoch: 639/2000... Training loss: 0.8408\n",
      "Epoch: 639/2000... Training loss: 0.6668\n",
      "Epoch: 639/2000... Training loss: 0.7825\n",
      "Epoch: 639/2000... Training loss: 0.8422\n",
      "Epoch: 639/2000... Training loss: 0.7908\n",
      "Epoch: 639/2000... Training loss: 1.0534\n",
      "Epoch: 639/2000... Training loss: 0.8919\n",
      "Epoch: 639/2000... Training loss: 0.6771\n",
      "Epoch: 639/2000... Training loss: 0.7705\n",
      "Epoch: 639/2000... Training loss: 0.7637\n",
      "Epoch: 640/2000... Training loss: 0.7993\n",
      "Epoch: 640/2000... Training loss: 0.8852\n",
      "Epoch: 640/2000... Training loss: 1.1718\n",
      "Epoch: 640/2000... Training loss: 0.6124\n",
      "Epoch: 640/2000... Training loss: 0.9461\n",
      "Epoch: 640/2000... Training loss: 0.8041\n",
      "Epoch: 640/2000... Training loss: 0.8487\n",
      "Epoch: 640/2000... Training loss: 0.8690\n",
      "Epoch: 640/2000... Training loss: 0.8967\n",
      "Epoch: 640/2000... Training loss: 0.6839\n",
      "Epoch: 640/2000... Training loss: 0.9255\n",
      "Epoch: 640/2000... Training loss: 0.9251\n",
      "Epoch: 640/2000... Training loss: 0.7663\n",
      "Epoch: 640/2000... Training loss: 0.6624\n",
      "Epoch: 640/2000... Training loss: 0.8462\n",
      "Epoch: 640/2000... Training loss: 0.7017\n",
      "Epoch: 640/2000... Training loss: 0.6384\n",
      "Epoch: 640/2000... Training loss: 0.8510\n",
      "Epoch: 640/2000... Training loss: 0.7400\n",
      "Epoch: 640/2000... Training loss: 0.8095\n",
      "Epoch: 640/2000... Training loss: 0.7742\n",
      "Epoch: 640/2000... Training loss: 0.7013\n",
      "Epoch: 640/2000... Training loss: 0.8902\n",
      "Epoch: 640/2000... Training loss: 0.8685\n",
      "Epoch: 640/2000... Training loss: 0.9051\n",
      "Epoch: 640/2000... Training loss: 0.7788\n",
      "Epoch: 640/2000... Training loss: 0.7511\n",
      "Epoch: 640/2000... Training loss: 1.2098\n",
      "Epoch: 640/2000... Training loss: 0.8810\n",
      "Epoch: 640/2000... Training loss: 1.1175\n",
      "Epoch: 640/2000... Training loss: 0.6706\n",
      "Epoch: 641/2000... Training loss: 0.6445\n",
      "Epoch: 641/2000... Training loss: 0.6325\n",
      "Epoch: 641/2000... Training loss: 0.9093\n",
      "Epoch: 641/2000... Training loss: 0.8439\n",
      "Epoch: 641/2000... Training loss: 0.7491\n",
      "Epoch: 641/2000... Training loss: 1.0295\n",
      "Epoch: 641/2000... Training loss: 0.9688\n",
      "Epoch: 641/2000... Training loss: 0.7396\n",
      "Epoch: 641/2000... Training loss: 1.1413\n",
      "Epoch: 641/2000... Training loss: 0.6972\n",
      "Epoch: 641/2000... Training loss: 0.9915\n",
      "Epoch: 641/2000... Training loss: 0.9340\n",
      "Epoch: 641/2000... Training loss: 0.8671\n",
      "Epoch: 641/2000... Training loss: 0.7647\n",
      "Epoch: 641/2000... Training loss: 0.6622\n",
      "Epoch: 641/2000... Training loss: 0.6214\n",
      "Epoch: 641/2000... Training loss: 0.9692\n",
      "Epoch: 641/2000... Training loss: 0.7559\n",
      "Epoch: 641/2000... Training loss: 0.7626\n",
      "Epoch: 641/2000... Training loss: 0.9298\n",
      "Epoch: 641/2000... Training loss: 0.6997\n",
      "Epoch: 641/2000... Training loss: 0.6423\n",
      "Epoch: 641/2000... Training loss: 0.8207\n",
      "Epoch: 641/2000... Training loss: 0.9213\n",
      "Epoch: 641/2000... Training loss: 0.6999\n",
      "Epoch: 641/2000... Training loss: 0.8235\n",
      "Epoch: 641/2000... Training loss: 0.9272\n",
      "Epoch: 641/2000... Training loss: 0.7164\n",
      "Epoch: 641/2000... Training loss: 0.8954\n",
      "Epoch: 641/2000... Training loss: 0.8058\n",
      "Epoch: 641/2000... Training loss: 0.8624\n",
      "Epoch: 642/2000... Training loss: 0.9983\n",
      "Epoch: 642/2000... Training loss: 1.0049\n",
      "Epoch: 642/2000... Training loss: 0.9154\n",
      "Epoch: 642/2000... Training loss: 0.8614\n",
      "Epoch: 642/2000... Training loss: 0.8781\n",
      "Epoch: 642/2000... Training loss: 0.7470\n",
      "Epoch: 642/2000... Training loss: 0.7336\n",
      "Epoch: 642/2000... Training loss: 0.7939\n",
      "Epoch: 642/2000... Training loss: 0.7886\n",
      "Epoch: 642/2000... Training loss: 0.7973\n",
      "Epoch: 642/2000... Training loss: 0.7500\n",
      "Epoch: 642/2000... Training loss: 0.8943\n",
      "Epoch: 642/2000... Training loss: 0.7427\n",
      "Epoch: 642/2000... Training loss: 0.6121\n",
      "Epoch: 642/2000... Training loss: 0.9027\n",
      "Epoch: 642/2000... Training loss: 0.8028\n",
      "Epoch: 642/2000... Training loss: 0.7140\n",
      "Epoch: 642/2000... Training loss: 0.8328\n",
      "Epoch: 642/2000... Training loss: 0.6447\n",
      "Epoch: 642/2000... Training loss: 0.5972\n",
      "Epoch: 642/2000... Training loss: 0.7549\n",
      "Epoch: 642/2000... Training loss: 0.8544\n",
      "Epoch: 642/2000... Training loss: 0.7267\n",
      "Epoch: 642/2000... Training loss: 0.8798\n",
      "Epoch: 642/2000... Training loss: 1.0075\n",
      "Epoch: 642/2000... Training loss: 0.6674\n",
      "Epoch: 642/2000... Training loss: 0.8753\n",
      "Epoch: 642/2000... Training loss: 0.6454\n",
      "Epoch: 642/2000... Training loss: 0.6719\n",
      "Epoch: 642/2000... Training loss: 0.9894\n",
      "Epoch: 642/2000... Training loss: 0.8586\n",
      "Epoch: 643/2000... Training loss: 0.6401\n",
      "Epoch: 643/2000... Training loss: 0.9322\n",
      "Epoch: 643/2000... Training loss: 0.8629\n",
      "Epoch: 643/2000... Training loss: 0.7855\n",
      "Epoch: 643/2000... Training loss: 0.6805\n",
      "Epoch: 643/2000... Training loss: 0.7328\n",
      "Epoch: 643/2000... Training loss: 0.8858\n",
      "Epoch: 643/2000... Training loss: 0.7223\n",
      "Epoch: 643/2000... Training loss: 0.9178\n",
      "Epoch: 643/2000... Training loss: 0.8474\n",
      "Epoch: 643/2000... Training loss: 0.9787\n",
      "Epoch: 643/2000... Training loss: 0.7086\n",
      "Epoch: 643/2000... Training loss: 0.8231\n",
      "Epoch: 643/2000... Training loss: 0.9083\n",
      "Epoch: 643/2000... Training loss: 0.7582\n",
      "Epoch: 643/2000... Training loss: 0.8676\n",
      "Epoch: 643/2000... Training loss: 0.8631\n",
      "Epoch: 643/2000... Training loss: 0.6227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 643/2000... Training loss: 0.8038\n",
      "Epoch: 643/2000... Training loss: 0.8735\n",
      "Epoch: 643/2000... Training loss: 0.9916\n",
      "Epoch: 643/2000... Training loss: 0.8056\n",
      "Epoch: 643/2000... Training loss: 1.1436\n",
      "Epoch: 643/2000... Training loss: 0.7403\n",
      "Epoch: 643/2000... Training loss: 0.7490\n",
      "Epoch: 643/2000... Training loss: 0.8140\n",
      "Epoch: 643/2000... Training loss: 0.7581\n",
      "Epoch: 643/2000... Training loss: 0.8603\n",
      "Epoch: 643/2000... Training loss: 0.8429\n",
      "Epoch: 643/2000... Training loss: 0.7390\n",
      "Epoch: 643/2000... Training loss: 0.7850\n",
      "Epoch: 644/2000... Training loss: 0.8125\n",
      "Epoch: 644/2000... Training loss: 0.7920\n",
      "Epoch: 644/2000... Training loss: 0.8209\n",
      "Epoch: 644/2000... Training loss: 0.8196\n",
      "Epoch: 644/2000... Training loss: 0.7336\n",
      "Epoch: 644/2000... Training loss: 0.8086\n",
      "Epoch: 644/2000... Training loss: 0.6711\n",
      "Epoch: 644/2000... Training loss: 0.8223\n",
      "Epoch: 644/2000... Training loss: 0.9539\n",
      "Epoch: 644/2000... Training loss: 0.6563\n",
      "Epoch: 644/2000... Training loss: 0.7064\n",
      "Epoch: 644/2000... Training loss: 0.7385\n",
      "Epoch: 644/2000... Training loss: 0.9629\n",
      "Epoch: 644/2000... Training loss: 0.7634\n",
      "Epoch: 644/2000... Training loss: 0.7998\n",
      "Epoch: 644/2000... Training loss: 0.5730\n",
      "Epoch: 644/2000... Training loss: 0.8007\n",
      "Epoch: 644/2000... Training loss: 0.6827\n",
      "Epoch: 644/2000... Training loss: 0.8602\n",
      "Epoch: 644/2000... Training loss: 0.8782\n",
      "Epoch: 644/2000... Training loss: 0.6855\n",
      "Epoch: 644/2000... Training loss: 0.7037\n",
      "Epoch: 644/2000... Training loss: 0.7384\n",
      "Epoch: 644/2000... Training loss: 0.8018\n",
      "Epoch: 644/2000... Training loss: 0.7595\n",
      "Epoch: 644/2000... Training loss: 0.7286\n",
      "Epoch: 644/2000... Training loss: 0.6002\n",
      "Epoch: 644/2000... Training loss: 0.7545\n",
      "Epoch: 644/2000... Training loss: 0.7880\n",
      "Epoch: 644/2000... Training loss: 0.9986\n",
      "Epoch: 644/2000... Training loss: 1.0754\n",
      "Epoch: 645/2000... Training loss: 0.6291\n",
      "Epoch: 645/2000... Training loss: 0.8332\n",
      "Epoch: 645/2000... Training loss: 0.8828\n",
      "Epoch: 645/2000... Training loss: 0.7962\n",
      "Epoch: 645/2000... Training loss: 0.9364\n",
      "Epoch: 645/2000... Training loss: 0.8131\n",
      "Epoch: 645/2000... Training loss: 0.7740\n",
      "Epoch: 645/2000... Training loss: 1.0012\n",
      "Epoch: 645/2000... Training loss: 0.7762\n",
      "Epoch: 645/2000... Training loss: 1.0027\n",
      "Epoch: 645/2000... Training loss: 0.8074\n",
      "Epoch: 645/2000... Training loss: 0.9222\n",
      "Epoch: 645/2000... Training loss: 0.8960\n",
      "Epoch: 645/2000... Training loss: 0.7353\n",
      "Epoch: 645/2000... Training loss: 0.8158\n",
      "Epoch: 645/2000... Training loss: 0.6217\n",
      "Epoch: 645/2000... Training loss: 0.7459\n",
      "Epoch: 645/2000... Training loss: 0.8170\n",
      "Epoch: 645/2000... Training loss: 0.8601\n",
      "Epoch: 645/2000... Training loss: 0.5830\n",
      "Epoch: 645/2000... Training loss: 0.7832\n",
      "Epoch: 645/2000... Training loss: 0.8047\n",
      "Epoch: 645/2000... Training loss: 0.7678\n",
      "Epoch: 645/2000... Training loss: 0.9103\n",
      "Epoch: 645/2000... Training loss: 0.8538\n",
      "Epoch: 645/2000... Training loss: 0.8864\n",
      "Epoch: 645/2000... Training loss: 0.7137\n",
      "Epoch: 645/2000... Training loss: 1.0734\n",
      "Epoch: 645/2000... Training loss: 0.9371\n",
      "Epoch: 645/2000... Training loss: 0.8163\n",
      "Epoch: 645/2000... Training loss: 0.7385\n",
      "Epoch: 646/2000... Training loss: 0.7717\n",
      "Epoch: 646/2000... Training loss: 0.8436\n",
      "Epoch: 646/2000... Training loss: 0.9576\n",
      "Epoch: 646/2000... Training loss: 0.7376\n",
      "Epoch: 646/2000... Training loss: 0.9736\n",
      "Epoch: 646/2000... Training loss: 1.0178\n",
      "Epoch: 646/2000... Training loss: 0.6245\n",
      "Epoch: 646/2000... Training loss: 0.8749\n",
      "Epoch: 646/2000... Training loss: 0.8515\n",
      "Epoch: 646/2000... Training loss: 0.7904\n",
      "Epoch: 646/2000... Training loss: 0.9463\n",
      "Epoch: 646/2000... Training loss: 0.8111\n",
      "Epoch: 646/2000... Training loss: 0.8660\n",
      "Epoch: 646/2000... Training loss: 0.7933\n",
      "Epoch: 646/2000... Training loss: 1.0530\n",
      "Epoch: 646/2000... Training loss: 0.6680\n",
      "Epoch: 646/2000... Training loss: 0.8024\n",
      "Epoch: 646/2000... Training loss: 0.6933\n",
      "Epoch: 646/2000... Training loss: 0.8419\n",
      "Epoch: 646/2000... Training loss: 0.6871\n",
      "Epoch: 646/2000... Training loss: 0.7759\n",
      "Epoch: 646/2000... Training loss: 0.7382\n",
      "Epoch: 646/2000... Training loss: 0.6330\n",
      "Epoch: 646/2000... Training loss: 0.7462\n",
      "Epoch: 646/2000... Training loss: 0.7385\n",
      "Epoch: 646/2000... Training loss: 0.8423\n",
      "Epoch: 646/2000... Training loss: 0.7057\n",
      "Epoch: 646/2000... Training loss: 0.9481\n",
      "Epoch: 646/2000... Training loss: 0.8573\n",
      "Epoch: 646/2000... Training loss: 0.8888\n",
      "Epoch: 646/2000... Training loss: 0.9208\n",
      "Epoch: 647/2000... Training loss: 0.9065\n",
      "Epoch: 647/2000... Training loss: 0.7031\n",
      "Epoch: 647/2000... Training loss: 0.6553\n",
      "Epoch: 647/2000... Training loss: 1.0500\n",
      "Epoch: 647/2000... Training loss: 0.5985\n",
      "Epoch: 647/2000... Training loss: 0.8029\n",
      "Epoch: 647/2000... Training loss: 0.7446\n",
      "Epoch: 647/2000... Training loss: 0.7367\n",
      "Epoch: 647/2000... Training loss: 0.8917\n",
      "Epoch: 647/2000... Training loss: 0.8334\n",
      "Epoch: 647/2000... Training loss: 0.7930\n",
      "Epoch: 647/2000... Training loss: 0.9159\n",
      "Epoch: 647/2000... Training loss: 0.6642\n",
      "Epoch: 647/2000... Training loss: 0.6664\n",
      "Epoch: 647/2000... Training loss: 0.7236\n",
      "Epoch: 647/2000... Training loss: 0.7146\n",
      "Epoch: 647/2000... Training loss: 1.0396\n",
      "Epoch: 647/2000... Training loss: 0.7428\n",
      "Epoch: 647/2000... Training loss: 0.8764\n",
      "Epoch: 647/2000... Training loss: 0.8969\n",
      "Epoch: 647/2000... Training loss: 0.7458\n",
      "Epoch: 647/2000... Training loss: 0.8440\n",
      "Epoch: 647/2000... Training loss: 0.7761\n",
      "Epoch: 647/2000... Training loss: 0.7858\n",
      "Epoch: 647/2000... Training loss: 0.7994\n",
      "Epoch: 647/2000... Training loss: 0.8671\n",
      "Epoch: 647/2000... Training loss: 0.7977\n",
      "Epoch: 647/2000... Training loss: 0.7545\n",
      "Epoch: 647/2000... Training loss: 0.7180\n",
      "Epoch: 647/2000... Training loss: 0.8354\n",
      "Epoch: 647/2000... Training loss: 0.8883\n",
      "Epoch: 648/2000... Training loss: 0.7244\n",
      "Epoch: 648/2000... Training loss: 0.5836\n",
      "Epoch: 648/2000... Training loss: 0.9289\n",
      "Epoch: 648/2000... Training loss: 0.4643\n",
      "Epoch: 648/2000... Training loss: 0.9067\n",
      "Epoch: 648/2000... Training loss: 0.8647\n",
      "Epoch: 648/2000... Training loss: 0.7640\n",
      "Epoch: 648/2000... Training loss: 0.7313\n",
      "Epoch: 648/2000... Training loss: 0.6869\n",
      "Epoch: 648/2000... Training loss: 1.0486\n",
      "Epoch: 648/2000... Training loss: 0.7758\n",
      "Epoch: 648/2000... Training loss: 0.6674\n",
      "Epoch: 648/2000... Training loss: 0.8875\n",
      "Epoch: 648/2000... Training loss: 0.7890\n",
      "Epoch: 648/2000... Training loss: 0.6859\n",
      "Epoch: 648/2000... Training loss: 0.6111\n",
      "Epoch: 648/2000... Training loss: 0.6038\n",
      "Epoch: 648/2000... Training loss: 0.8494\n",
      "Epoch: 648/2000... Training loss: 0.8275\n",
      "Epoch: 648/2000... Training loss: 0.7798\n",
      "Epoch: 648/2000... Training loss: 0.7924\n",
      "Epoch: 648/2000... Training loss: 0.8487\n",
      "Epoch: 648/2000... Training loss: 0.9115\n",
      "Epoch: 648/2000... Training loss: 0.8513\n",
      "Epoch: 648/2000... Training loss: 0.7298\n",
      "Epoch: 648/2000... Training loss: 0.8609\n",
      "Epoch: 648/2000... Training loss: 0.8461\n",
      "Epoch: 648/2000... Training loss: 0.7109\n",
      "Epoch: 648/2000... Training loss: 0.7364\n",
      "Epoch: 648/2000... Training loss: 0.9171\n",
      "Epoch: 648/2000... Training loss: 0.8193\n",
      "Epoch: 649/2000... Training loss: 1.0105\n",
      "Epoch: 649/2000... Training loss: 0.5789\n",
      "Epoch: 649/2000... Training loss: 0.8452\n",
      "Epoch: 649/2000... Training loss: 0.8910\n",
      "Epoch: 649/2000... Training loss: 0.7523\n",
      "Epoch: 649/2000... Training loss: 0.8540\n",
      "Epoch: 649/2000... Training loss: 0.8724\n",
      "Epoch: 649/2000... Training loss: 0.9354\n",
      "Epoch: 649/2000... Training loss: 0.9344\n",
      "Epoch: 649/2000... Training loss: 0.8209\n",
      "Epoch: 649/2000... Training loss: 0.8396\n",
      "Epoch: 649/2000... Training loss: 0.9168\n",
      "Epoch: 649/2000... Training loss: 0.7353\n",
      "Epoch: 649/2000... Training loss: 0.9860\n",
      "Epoch: 649/2000... Training loss: 0.7137\n",
      "Epoch: 649/2000... Training loss: 0.8595\n",
      "Epoch: 649/2000... Training loss: 0.9024\n",
      "Epoch: 649/2000... Training loss: 0.8088\n",
      "Epoch: 649/2000... Training loss: 0.5834\n",
      "Epoch: 649/2000... Training loss: 0.7284\n",
      "Epoch: 649/2000... Training loss: 0.6511\n",
      "Epoch: 649/2000... Training loss: 1.0807\n",
      "Epoch: 649/2000... Training loss: 1.0149\n",
      "Epoch: 649/2000... Training loss: 0.9084\n",
      "Epoch: 649/2000... Training loss: 0.8616\n",
      "Epoch: 649/2000... Training loss: 0.7444\n",
      "Epoch: 649/2000... Training loss: 0.6700\n",
      "Epoch: 649/2000... Training loss: 0.8410\n",
      "Epoch: 649/2000... Training loss: 0.8446\n",
      "Epoch: 649/2000... Training loss: 0.8040\n",
      "Epoch: 649/2000... Training loss: 1.0802\n",
      "Epoch: 650/2000... Training loss: 0.7638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 650/2000... Training loss: 0.5637\n",
      "Epoch: 650/2000... Training loss: 0.9889\n",
      "Epoch: 650/2000... Training loss: 0.9124\n",
      "Epoch: 650/2000... Training loss: 0.7394\n",
      "Epoch: 650/2000... Training loss: 0.9149\n",
      "Epoch: 650/2000... Training loss: 0.8565\n",
      "Epoch: 650/2000... Training loss: 0.7422\n",
      "Epoch: 650/2000... Training loss: 0.8852\n",
      "Epoch: 650/2000... Training loss: 0.6897\n",
      "Epoch: 650/2000... Training loss: 0.6244\n",
      "Epoch: 650/2000... Training loss: 0.7257\n",
      "Epoch: 650/2000... Training loss: 0.6845\n",
      "Epoch: 650/2000... Training loss: 0.7006\n",
      "Epoch: 650/2000... Training loss: 0.7688\n",
      "Epoch: 650/2000... Training loss: 0.7037\n",
      "Epoch: 650/2000... Training loss: 0.8379\n",
      "Epoch: 650/2000... Training loss: 0.8969\n",
      "Epoch: 650/2000... Training loss: 0.5716\n",
      "Epoch: 650/2000... Training loss: 0.9106\n",
      "Epoch: 650/2000... Training loss: 0.8247\n",
      "Epoch: 650/2000... Training loss: 0.6571\n",
      "Epoch: 650/2000... Training loss: 0.8471\n",
      "Epoch: 650/2000... Training loss: 0.7995\n",
      "Epoch: 650/2000... Training loss: 0.8564\n",
      "Epoch: 650/2000... Training loss: 0.7756\n",
      "Epoch: 650/2000... Training loss: 0.8597\n",
      "Epoch: 650/2000... Training loss: 0.7849\n",
      "Epoch: 650/2000... Training loss: 0.8101\n",
      "Epoch: 650/2000... Training loss: 0.8062\n",
      "Epoch: 650/2000... Training loss: 0.9159\n",
      "Epoch: 651/2000... Training loss: 0.8327\n",
      "Epoch: 651/2000... Training loss: 0.7612\n",
      "Epoch: 651/2000... Training loss: 0.8926\n",
      "Epoch: 651/2000... Training loss: 0.9861\n",
      "Epoch: 651/2000... Training loss: 0.9221\n",
      "Epoch: 651/2000... Training loss: 0.8042\n",
      "Epoch: 651/2000... Training loss: 0.6942\n",
      "Epoch: 651/2000... Training loss: 0.7988\n",
      "Epoch: 651/2000... Training loss: 0.8520\n",
      "Epoch: 651/2000... Training loss: 0.8014\n",
      "Epoch: 651/2000... Training loss: 0.5979\n",
      "Epoch: 651/2000... Training loss: 0.7208\n",
      "Epoch: 651/2000... Training loss: 1.0263\n",
      "Epoch: 651/2000... Training loss: 0.7492\n",
      "Epoch: 651/2000... Training loss: 0.7467\n",
      "Epoch: 651/2000... Training loss: 0.7403\n",
      "Epoch: 651/2000... Training loss: 0.7043\n",
      "Epoch: 651/2000... Training loss: 0.8143\n",
      "Epoch: 651/2000... Training loss: 0.9303\n",
      "Epoch: 651/2000... Training loss: 0.7544\n",
      "Epoch: 651/2000... Training loss: 0.6056\n",
      "Epoch: 651/2000... Training loss: 0.8641\n",
      "Epoch: 651/2000... Training loss: 1.0904\n",
      "Epoch: 651/2000... Training loss: 0.7093\n",
      "Epoch: 651/2000... Training loss: 0.6740\n",
      "Epoch: 651/2000... Training loss: 0.9487\n",
      "Epoch: 651/2000... Training loss: 0.7677\n",
      "Epoch: 651/2000... Training loss: 0.6592\n",
      "Epoch: 651/2000... Training loss: 0.7254\n",
      "Epoch: 651/2000... Training loss: 0.8771\n",
      "Epoch: 651/2000... Training loss: 0.8585\n",
      "Epoch: 652/2000... Training loss: 0.7290\n",
      "Epoch: 652/2000... Training loss: 0.6395\n",
      "Epoch: 652/2000... Training loss: 0.9247\n",
      "Epoch: 652/2000... Training loss: 0.6987\n",
      "Epoch: 652/2000... Training loss: 0.7721\n",
      "Epoch: 652/2000... Training loss: 0.5866\n",
      "Epoch: 652/2000... Training loss: 0.8390\n",
      "Epoch: 652/2000... Training loss: 0.8496\n",
      "Epoch: 652/2000... Training loss: 0.7004\n",
      "Epoch: 652/2000... Training loss: 0.7226\n",
      "Epoch: 652/2000... Training loss: 0.7775\n",
      "Epoch: 652/2000... Training loss: 0.8262\n",
      "Epoch: 652/2000... Training loss: 0.7571\n",
      "Epoch: 652/2000... Training loss: 0.8871\n",
      "Epoch: 652/2000... Training loss: 0.8334\n",
      "Epoch: 652/2000... Training loss: 0.8047\n",
      "Epoch: 652/2000... Training loss: 0.7185\n",
      "Epoch: 652/2000... Training loss: 0.8147\n",
      "Epoch: 652/2000... Training loss: 1.0788\n",
      "Epoch: 652/2000... Training loss: 1.0401\n",
      "Epoch: 652/2000... Training loss: 0.7995\n",
      "Epoch: 652/2000... Training loss: 0.7869\n",
      "Epoch: 652/2000... Training loss: 0.5918\n",
      "Epoch: 652/2000... Training loss: 0.8811\n",
      "Epoch: 652/2000... Training loss: 0.7148\n",
      "Epoch: 652/2000... Training loss: 0.6808\n",
      "Epoch: 652/2000... Training loss: 1.0297\n",
      "Epoch: 652/2000... Training loss: 0.8633\n",
      "Epoch: 652/2000... Training loss: 0.8747\n",
      "Epoch: 652/2000... Training loss: 0.5794\n",
      "Epoch: 652/2000... Training loss: 0.6620\n",
      "Epoch: 653/2000... Training loss: 0.5332\n",
      "Epoch: 653/2000... Training loss: 0.8798\n",
      "Epoch: 653/2000... Training loss: 0.8770\n",
      "Epoch: 653/2000... Training loss: 0.6737\n",
      "Epoch: 653/2000... Training loss: 0.8547\n",
      "Epoch: 653/2000... Training loss: 0.7902\n",
      "Epoch: 653/2000... Training loss: 0.6509\n",
      "Epoch: 653/2000... Training loss: 0.8209\n",
      "Epoch: 653/2000... Training loss: 0.6476\n",
      "Epoch: 653/2000... Training loss: 1.0759\n",
      "Epoch: 653/2000... Training loss: 0.9072\n",
      "Epoch: 653/2000... Training loss: 0.7947\n",
      "Epoch: 653/2000... Training loss: 0.6777\n",
      "Epoch: 653/2000... Training loss: 0.8897\n",
      "Epoch: 653/2000... Training loss: 0.7701\n",
      "Epoch: 653/2000... Training loss: 0.8743\n",
      "Epoch: 653/2000... Training loss: 0.6726\n",
      "Epoch: 653/2000... Training loss: 0.7650\n",
      "Epoch: 653/2000... Training loss: 0.6110\n",
      "Epoch: 653/2000... Training loss: 0.7905\n",
      "Epoch: 653/2000... Training loss: 0.8747\n",
      "Epoch: 653/2000... Training loss: 0.5792\n",
      "Epoch: 653/2000... Training loss: 1.0314\n",
      "Epoch: 653/2000... Training loss: 0.7925\n",
      "Epoch: 653/2000... Training loss: 0.6631\n",
      "Epoch: 653/2000... Training loss: 0.6617\n",
      "Epoch: 653/2000... Training loss: 0.8931\n",
      "Epoch: 653/2000... Training loss: 0.7796\n",
      "Epoch: 653/2000... Training loss: 0.4931\n",
      "Epoch: 653/2000... Training loss: 1.0606\n",
      "Epoch: 653/2000... Training loss: 0.7909\n",
      "Epoch: 654/2000... Training loss: 0.7119\n",
      "Epoch: 654/2000... Training loss: 0.8310\n",
      "Epoch: 654/2000... Training loss: 0.5550\n",
      "Epoch: 654/2000... Training loss: 0.9335\n",
      "Epoch: 654/2000... Training loss: 0.6628\n",
      "Epoch: 654/2000... Training loss: 0.6954\n",
      "Epoch: 654/2000... Training loss: 1.0029\n",
      "Epoch: 654/2000... Training loss: 0.8620\n",
      "Epoch: 654/2000... Training loss: 0.9042\n",
      "Epoch: 654/2000... Training loss: 0.7938\n",
      "Epoch: 654/2000... Training loss: 0.9637\n",
      "Epoch: 654/2000... Training loss: 1.0578\n",
      "Epoch: 654/2000... Training loss: 0.6671\n",
      "Epoch: 654/2000... Training loss: 0.8579\n",
      "Epoch: 654/2000... Training loss: 0.9346\n",
      "Epoch: 654/2000... Training loss: 0.9759\n",
      "Epoch: 654/2000... Training loss: 0.5767\n",
      "Epoch: 654/2000... Training loss: 0.7301\n",
      "Epoch: 654/2000... Training loss: 0.8060\n",
      "Epoch: 654/2000... Training loss: 0.6621\n",
      "Epoch: 654/2000... Training loss: 0.7183\n",
      "Epoch: 654/2000... Training loss: 0.8422\n",
      "Epoch: 654/2000... Training loss: 0.6106\n",
      "Epoch: 654/2000... Training loss: 0.6458\n",
      "Epoch: 654/2000... Training loss: 0.6136\n",
      "Epoch: 654/2000... Training loss: 0.6677\n",
      "Epoch: 654/2000... Training loss: 0.8518\n",
      "Epoch: 654/2000... Training loss: 0.8152\n",
      "Epoch: 654/2000... Training loss: 0.7604\n",
      "Epoch: 654/2000... Training loss: 0.8806\n",
      "Epoch: 654/2000... Training loss: 0.9146\n",
      "Epoch: 655/2000... Training loss: 0.7893\n",
      "Epoch: 655/2000... Training loss: 0.9399\n",
      "Epoch: 655/2000... Training loss: 0.9149\n",
      "Epoch: 655/2000... Training loss: 0.6246\n",
      "Epoch: 655/2000... Training loss: 0.6679\n",
      "Epoch: 655/2000... Training loss: 0.6698\n",
      "Epoch: 655/2000... Training loss: 0.7076\n",
      "Epoch: 655/2000... Training loss: 0.8066\n",
      "Epoch: 655/2000... Training loss: 0.5637\n",
      "Epoch: 655/2000... Training loss: 0.8193\n",
      "Epoch: 655/2000... Training loss: 0.8452\n",
      "Epoch: 655/2000... Training loss: 0.7144\n",
      "Epoch: 655/2000... Training loss: 0.8180\n",
      "Epoch: 655/2000... Training loss: 0.8056\n",
      "Epoch: 655/2000... Training loss: 0.9576\n",
      "Epoch: 655/2000... Training loss: 0.7817\n",
      "Epoch: 655/2000... Training loss: 0.5312\n",
      "Epoch: 655/2000... Training loss: 0.6830\n",
      "Epoch: 655/2000... Training loss: 0.7596\n",
      "Epoch: 655/2000... Training loss: 0.7318\n",
      "Epoch: 655/2000... Training loss: 0.8355\n",
      "Epoch: 655/2000... Training loss: 0.7453\n",
      "Epoch: 655/2000... Training loss: 0.7599\n",
      "Epoch: 655/2000... Training loss: 0.6415\n",
      "Epoch: 655/2000... Training loss: 1.0108\n",
      "Epoch: 655/2000... Training loss: 0.7863\n",
      "Epoch: 655/2000... Training loss: 0.8357\n",
      "Epoch: 655/2000... Training loss: 0.7765\n",
      "Epoch: 655/2000... Training loss: 0.9253\n",
      "Epoch: 655/2000... Training loss: 0.9017\n",
      "Epoch: 655/2000... Training loss: 0.8189\n",
      "Epoch: 656/2000... Training loss: 0.8344\n",
      "Epoch: 656/2000... Training loss: 0.6316\n",
      "Epoch: 656/2000... Training loss: 0.7550\n",
      "Epoch: 656/2000... Training loss: 0.8315\n",
      "Epoch: 656/2000... Training loss: 0.6886\n",
      "Epoch: 656/2000... Training loss: 0.8307\n",
      "Epoch: 656/2000... Training loss: 0.7782\n",
      "Epoch: 656/2000... Training loss: 0.9609\n",
      "Epoch: 656/2000... Training loss: 0.8792\n",
      "Epoch: 656/2000... Training loss: 0.6275\n",
      "Epoch: 656/2000... Training loss: 0.9195\n",
      "Epoch: 656/2000... Training loss: 0.7263\n",
      "Epoch: 656/2000... Training loss: 0.6910\n",
      "Epoch: 656/2000... Training loss: 0.6430\n",
      "Epoch: 656/2000... Training loss: 1.0834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 656/2000... Training loss: 0.6316\n",
      "Epoch: 656/2000... Training loss: 0.6787\n",
      "Epoch: 656/2000... Training loss: 0.7326\n",
      "Epoch: 656/2000... Training loss: 0.8201\n",
      "Epoch: 656/2000... Training loss: 0.8449\n",
      "Epoch: 656/2000... Training loss: 0.9125\n",
      "Epoch: 656/2000... Training loss: 0.5257\n",
      "Epoch: 656/2000... Training loss: 0.7831\n",
      "Epoch: 656/2000... Training loss: 0.7527\n",
      "Epoch: 656/2000... Training loss: 0.8828\n",
      "Epoch: 656/2000... Training loss: 0.5795\n",
      "Epoch: 656/2000... Training loss: 0.8586\n",
      "Epoch: 656/2000... Training loss: 0.8606\n",
      "Epoch: 656/2000... Training loss: 1.0529\n",
      "Epoch: 656/2000... Training loss: 0.7953\n",
      "Epoch: 656/2000... Training loss: 0.8096\n",
      "Epoch: 657/2000... Training loss: 0.7537\n",
      "Epoch: 657/2000... Training loss: 0.9080\n",
      "Epoch: 657/2000... Training loss: 0.7635\n",
      "Epoch: 657/2000... Training loss: 0.5724\n",
      "Epoch: 657/2000... Training loss: 0.7343\n",
      "Epoch: 657/2000... Training loss: 0.8408\n",
      "Epoch: 657/2000... Training loss: 0.5908\n",
      "Epoch: 657/2000... Training loss: 0.6679\n",
      "Epoch: 657/2000... Training loss: 0.6973\n",
      "Epoch: 657/2000... Training loss: 0.7113\n",
      "Epoch: 657/2000... Training loss: 0.8173\n",
      "Epoch: 657/2000... Training loss: 0.6626\n",
      "Epoch: 657/2000... Training loss: 0.7269\n",
      "Epoch: 657/2000... Training loss: 0.6766\n",
      "Epoch: 657/2000... Training loss: 0.7697\n",
      "Epoch: 657/2000... Training loss: 0.8085\n",
      "Epoch: 657/2000... Training loss: 0.8791\n",
      "Epoch: 657/2000... Training loss: 0.7419\n",
      "Epoch: 657/2000... Training loss: 0.6124\n",
      "Epoch: 657/2000... Training loss: 0.7447\n",
      "Epoch: 657/2000... Training loss: 0.6366\n",
      "Epoch: 657/2000... Training loss: 0.7695\n",
      "Epoch: 657/2000... Training loss: 0.7263\n",
      "Epoch: 657/2000... Training loss: 0.7259\n",
      "Epoch: 657/2000... Training loss: 0.6867\n",
      "Epoch: 657/2000... Training loss: 1.0693\n",
      "Epoch: 657/2000... Training loss: 0.5820\n",
      "Epoch: 657/2000... Training loss: 0.8028\n",
      "Epoch: 657/2000... Training loss: 0.9658\n",
      "Epoch: 657/2000... Training loss: 0.7849\n",
      "Epoch: 657/2000... Training loss: 0.8460\n",
      "Epoch: 658/2000... Training loss: 0.7818\n",
      "Epoch: 658/2000... Training loss: 0.8476\n",
      "Epoch: 658/2000... Training loss: 0.6160\n",
      "Epoch: 658/2000... Training loss: 0.6708\n",
      "Epoch: 658/2000... Training loss: 0.8732\n",
      "Epoch: 658/2000... Training loss: 0.9096\n",
      "Epoch: 658/2000... Training loss: 0.6765\n",
      "Epoch: 658/2000... Training loss: 0.9139\n",
      "Epoch: 658/2000... Training loss: 1.0401\n",
      "Epoch: 658/2000... Training loss: 0.7315\n",
      "Epoch: 658/2000... Training loss: 0.6073\n",
      "Epoch: 658/2000... Training loss: 0.8310\n",
      "Epoch: 658/2000... Training loss: 0.7098\n",
      "Epoch: 658/2000... Training loss: 0.7836\n",
      "Epoch: 658/2000... Training loss: 0.7268\n",
      "Epoch: 658/2000... Training loss: 1.0210\n",
      "Epoch: 658/2000... Training loss: 0.9841\n",
      "Epoch: 658/2000... Training loss: 0.8270\n",
      "Epoch: 658/2000... Training loss: 0.7697\n",
      "Epoch: 658/2000... Training loss: 0.7708\n",
      "Epoch: 658/2000... Training loss: 0.8584\n",
      "Epoch: 658/2000... Training loss: 0.7777\n",
      "Epoch: 658/2000... Training loss: 0.7772\n",
      "Epoch: 658/2000... Training loss: 0.9525\n",
      "Epoch: 658/2000... Training loss: 0.7249\n",
      "Epoch: 658/2000... Training loss: 0.7008\n",
      "Epoch: 658/2000... Training loss: 0.7428\n",
      "Epoch: 658/2000... Training loss: 0.7066\n",
      "Epoch: 658/2000... Training loss: 0.5493\n",
      "Epoch: 658/2000... Training loss: 0.8470\n",
      "Epoch: 658/2000... Training loss: 0.7868\n",
      "Epoch: 659/2000... Training loss: 0.9619\n",
      "Epoch: 659/2000... Training loss: 0.5590\n",
      "Epoch: 659/2000... Training loss: 0.6369\n",
      "Epoch: 659/2000... Training loss: 0.9094\n",
      "Epoch: 659/2000... Training loss: 0.7486\n",
      "Epoch: 659/2000... Training loss: 0.9408\n",
      "Epoch: 659/2000... Training loss: 0.8777\n",
      "Epoch: 659/2000... Training loss: 0.7264\n",
      "Epoch: 659/2000... Training loss: 0.7891\n",
      "Epoch: 659/2000... Training loss: 0.7934\n",
      "Epoch: 659/2000... Training loss: 0.6826\n",
      "Epoch: 659/2000... Training loss: 0.7708\n",
      "Epoch: 659/2000... Training loss: 0.7797\n",
      "Epoch: 659/2000... Training loss: 0.8409\n",
      "Epoch: 659/2000... Training loss: 0.8340\n",
      "Epoch: 659/2000... Training loss: 0.8505\n",
      "Epoch: 659/2000... Training loss: 0.7154\n",
      "Epoch: 659/2000... Training loss: 0.8317\n",
      "Epoch: 659/2000... Training loss: 0.6116\n",
      "Epoch: 659/2000... Training loss: 0.6891\n",
      "Epoch: 659/2000... Training loss: 0.7042\n",
      "Epoch: 659/2000... Training loss: 0.8958\n",
      "Epoch: 659/2000... Training loss: 0.6279\n",
      "Epoch: 659/2000... Training loss: 0.8411\n",
      "Epoch: 659/2000... Training loss: 1.0695\n",
      "Epoch: 659/2000... Training loss: 0.9410\n",
      "Epoch: 659/2000... Training loss: 1.0154\n",
      "Epoch: 659/2000... Training loss: 0.8270\n",
      "Epoch: 659/2000... Training loss: 0.7289\n",
      "Epoch: 659/2000... Training loss: 0.6633\n",
      "Epoch: 659/2000... Training loss: 0.8151\n",
      "Epoch: 660/2000... Training loss: 0.8571\n",
      "Epoch: 660/2000... Training loss: 1.0935\n",
      "Epoch: 660/2000... Training loss: 1.0057\n",
      "Epoch: 660/2000... Training loss: 0.7024\n",
      "Epoch: 660/2000... Training loss: 1.0121\n",
      "Epoch: 660/2000... Training loss: 0.9303\n",
      "Epoch: 660/2000... Training loss: 0.6309\n",
      "Epoch: 660/2000... Training loss: 0.8082\n",
      "Epoch: 660/2000... Training loss: 0.7467\n",
      "Epoch: 660/2000... Training loss: 0.5949\n",
      "Epoch: 660/2000... Training loss: 0.9521\n",
      "Epoch: 660/2000... Training loss: 0.6634\n",
      "Epoch: 660/2000... Training loss: 0.7113\n",
      "Epoch: 660/2000... Training loss: 0.6669\n",
      "Epoch: 660/2000... Training loss: 0.9116\n",
      "Epoch: 660/2000... Training loss: 0.7508\n",
      "Epoch: 660/2000... Training loss: 0.6131\n",
      "Epoch: 660/2000... Training loss: 0.9327\n",
      "Epoch: 660/2000... Training loss: 0.7163\n",
      "Epoch: 660/2000... Training loss: 0.9005\n",
      "Epoch: 660/2000... Training loss: 0.7623\n",
      "Epoch: 660/2000... Training loss: 0.9369\n",
      "Epoch: 660/2000... Training loss: 0.7578\n",
      "Epoch: 660/2000... Training loss: 0.9284\n",
      "Epoch: 660/2000... Training loss: 0.9199\n",
      "Epoch: 660/2000... Training loss: 0.7659\n",
      "Epoch: 660/2000... Training loss: 0.6432\n",
      "Epoch: 660/2000... Training loss: 0.8487\n",
      "Epoch: 660/2000... Training loss: 0.8038\n",
      "Epoch: 660/2000... Training loss: 0.7582\n",
      "Epoch: 660/2000... Training loss: 0.8004\n",
      "Epoch: 661/2000... Training loss: 0.6821\n",
      "Epoch: 661/2000... Training loss: 0.7934\n",
      "Epoch: 661/2000... Training loss: 0.6397\n",
      "Epoch: 661/2000... Training loss: 0.7409\n",
      "Epoch: 661/2000... Training loss: 0.9299\n",
      "Epoch: 661/2000... Training loss: 0.8323\n",
      "Epoch: 661/2000... Training loss: 0.8899\n",
      "Epoch: 661/2000... Training loss: 0.8800\n",
      "Epoch: 661/2000... Training loss: 0.8050\n",
      "Epoch: 661/2000... Training loss: 0.7212\n",
      "Epoch: 661/2000... Training loss: 0.6906\n",
      "Epoch: 661/2000... Training loss: 0.9178\n",
      "Epoch: 661/2000... Training loss: 0.7483\n",
      "Epoch: 661/2000... Training loss: 0.7406\n",
      "Epoch: 661/2000... Training loss: 0.6387\n",
      "Epoch: 661/2000... Training loss: 0.8827\n",
      "Epoch: 661/2000... Training loss: 0.8168\n",
      "Epoch: 661/2000... Training loss: 0.8246\n",
      "Epoch: 661/2000... Training loss: 0.7925\n",
      "Epoch: 661/2000... Training loss: 0.7982\n",
      "Epoch: 661/2000... Training loss: 0.7002\n",
      "Epoch: 661/2000... Training loss: 0.6816\n",
      "Epoch: 661/2000... Training loss: 0.7610\n",
      "Epoch: 661/2000... Training loss: 0.7444\n",
      "Epoch: 661/2000... Training loss: 1.0094\n",
      "Epoch: 661/2000... Training loss: 0.8226\n",
      "Epoch: 661/2000... Training loss: 0.7853\n",
      "Epoch: 661/2000... Training loss: 0.8023\n",
      "Epoch: 661/2000... Training loss: 0.5809\n",
      "Epoch: 661/2000... Training loss: 1.0071\n",
      "Epoch: 661/2000... Training loss: 0.8673\n",
      "Epoch: 662/2000... Training loss: 0.7825\n",
      "Epoch: 662/2000... Training loss: 0.7251\n",
      "Epoch: 662/2000... Training loss: 0.7669\n",
      "Epoch: 662/2000... Training loss: 0.6803\n",
      "Epoch: 662/2000... Training loss: 0.7032\n",
      "Epoch: 662/2000... Training loss: 0.8521\n",
      "Epoch: 662/2000... Training loss: 0.8320\n",
      "Epoch: 662/2000... Training loss: 0.9815\n",
      "Epoch: 662/2000... Training loss: 0.8833\n",
      "Epoch: 662/2000... Training loss: 0.6127\n",
      "Epoch: 662/2000... Training loss: 0.8246\n",
      "Epoch: 662/2000... Training loss: 0.6350\n",
      "Epoch: 662/2000... Training loss: 0.9810\n",
      "Epoch: 662/2000... Training loss: 1.0639\n",
      "Epoch: 662/2000... Training loss: 0.7320\n",
      "Epoch: 662/2000... Training loss: 0.6122\n",
      "Epoch: 662/2000... Training loss: 0.7793\n",
      "Epoch: 662/2000... Training loss: 0.9869\n",
      "Epoch: 662/2000... Training loss: 0.6655\n",
      "Epoch: 662/2000... Training loss: 0.7052\n",
      "Epoch: 662/2000... Training loss: 0.8978\n",
      "Epoch: 662/2000... Training loss: 0.9033\n",
      "Epoch: 662/2000... Training loss: 0.6840\n",
      "Epoch: 662/2000... Training loss: 0.9858\n",
      "Epoch: 662/2000... Training loss: 0.8202\n",
      "Epoch: 662/2000... Training loss: 0.8521\n",
      "Epoch: 662/2000... Training loss: 0.5804\n",
      "Epoch: 662/2000... Training loss: 0.9247\n",
      "Epoch: 662/2000... Training loss: 0.7422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 662/2000... Training loss: 0.8170\n",
      "Epoch: 662/2000... Training loss: 0.6955\n",
      "Epoch: 663/2000... Training loss: 0.8478\n",
      "Epoch: 663/2000... Training loss: 0.6074\n",
      "Epoch: 663/2000... Training loss: 0.7534\n",
      "Epoch: 663/2000... Training loss: 0.6341\n",
      "Epoch: 663/2000... Training loss: 0.8574\n",
      "Epoch: 663/2000... Training loss: 0.7320\n",
      "Epoch: 663/2000... Training loss: 0.7574\n",
      "Epoch: 663/2000... Training loss: 0.7043\n",
      "Epoch: 663/2000... Training loss: 0.8828\n",
      "Epoch: 663/2000... Training loss: 0.9244\n",
      "Epoch: 663/2000... Training loss: 0.7221\n",
      "Epoch: 663/2000... Training loss: 0.7049\n",
      "Epoch: 663/2000... Training loss: 0.6720\n",
      "Epoch: 663/2000... Training loss: 0.7818\n",
      "Epoch: 663/2000... Training loss: 0.8363\n",
      "Epoch: 663/2000... Training loss: 0.8640\n",
      "Epoch: 663/2000... Training loss: 0.7176\n",
      "Epoch: 663/2000... Training loss: 0.7131\n",
      "Epoch: 663/2000... Training loss: 0.7334\n",
      "Epoch: 663/2000... Training loss: 0.8369\n",
      "Epoch: 663/2000... Training loss: 0.5868\n",
      "Epoch: 663/2000... Training loss: 0.8301\n",
      "Epoch: 663/2000... Training loss: 0.8886\n",
      "Epoch: 663/2000... Training loss: 0.7638\n",
      "Epoch: 663/2000... Training loss: 0.7711\n",
      "Epoch: 663/2000... Training loss: 0.7043\n",
      "Epoch: 663/2000... Training loss: 0.8153\n",
      "Epoch: 663/2000... Training loss: 0.7026\n",
      "Epoch: 663/2000... Training loss: 0.8372\n",
      "Epoch: 663/2000... Training loss: 0.9058\n",
      "Epoch: 663/2000... Training loss: 0.8855\n",
      "Epoch: 664/2000... Training loss: 0.8384\n",
      "Epoch: 664/2000... Training loss: 0.6648\n",
      "Epoch: 664/2000... Training loss: 1.0297\n",
      "Epoch: 664/2000... Training loss: 0.8290\n",
      "Epoch: 664/2000... Training loss: 0.7225\n",
      "Epoch: 664/2000... Training loss: 0.6477\n",
      "Epoch: 664/2000... Training loss: 0.6450\n",
      "Epoch: 664/2000... Training loss: 0.8172\n",
      "Epoch: 664/2000... Training loss: 0.8000\n",
      "Epoch: 664/2000... Training loss: 0.6903\n",
      "Epoch: 664/2000... Training loss: 0.7727\n",
      "Epoch: 664/2000... Training loss: 0.9188\n",
      "Epoch: 664/2000... Training loss: 0.5855\n",
      "Epoch: 664/2000... Training loss: 0.6469\n",
      "Epoch: 664/2000... Training loss: 0.6639\n",
      "Epoch: 664/2000... Training loss: 0.7181\n",
      "Epoch: 664/2000... Training loss: 0.9577\n",
      "Epoch: 664/2000... Training loss: 0.7304\n",
      "Epoch: 664/2000... Training loss: 0.7657\n",
      "Epoch: 664/2000... Training loss: 0.7669\n",
      "Epoch: 664/2000... Training loss: 0.6938\n",
      "Epoch: 664/2000... Training loss: 0.4873\n",
      "Epoch: 664/2000... Training loss: 0.7829\n",
      "Epoch: 664/2000... Training loss: 0.7794\n",
      "Epoch: 664/2000... Training loss: 0.8211\n",
      "Epoch: 664/2000... Training loss: 0.7389\n",
      "Epoch: 664/2000... Training loss: 0.8158\n",
      "Epoch: 664/2000... Training loss: 0.8509\n",
      "Epoch: 664/2000... Training loss: 0.7432\n",
      "Epoch: 664/2000... Training loss: 0.7325\n",
      "Epoch: 664/2000... Training loss: 0.9082\n",
      "Epoch: 665/2000... Training loss: 0.8472\n",
      "Epoch: 665/2000... Training loss: 0.8654\n",
      "Epoch: 665/2000... Training loss: 0.7436\n",
      "Epoch: 665/2000... Training loss: 0.6267\n",
      "Epoch: 665/2000... Training loss: 0.8424\n",
      "Epoch: 665/2000... Training loss: 0.8614\n",
      "Epoch: 665/2000... Training loss: 0.7893\n",
      "Epoch: 665/2000... Training loss: 0.9913\n",
      "Epoch: 665/2000... Training loss: 0.8654\n",
      "Epoch: 665/2000... Training loss: 0.7805\n",
      "Epoch: 665/2000... Training loss: 0.7466\n",
      "Epoch: 665/2000... Training loss: 0.8991\n",
      "Epoch: 665/2000... Training loss: 0.6419\n",
      "Epoch: 665/2000... Training loss: 0.8366\n",
      "Epoch: 665/2000... Training loss: 0.7114\n",
      "Epoch: 665/2000... Training loss: 0.9157\n",
      "Epoch: 665/2000... Training loss: 0.6425\n",
      "Epoch: 665/2000... Training loss: 0.8007\n",
      "Epoch: 665/2000... Training loss: 0.7137\n",
      "Epoch: 665/2000... Training loss: 0.8280\n",
      "Epoch: 665/2000... Training loss: 0.9559\n",
      "Epoch: 665/2000... Training loss: 0.8439\n",
      "Epoch: 665/2000... Training loss: 0.6338\n",
      "Epoch: 665/2000... Training loss: 1.0471\n",
      "Epoch: 665/2000... Training loss: 0.8468\n",
      "Epoch: 665/2000... Training loss: 0.8489\n",
      "Epoch: 665/2000... Training loss: 0.9301\n",
      "Epoch: 665/2000... Training loss: 0.8598\n",
      "Epoch: 665/2000... Training loss: 0.8094\n",
      "Epoch: 665/2000... Training loss: 1.0157\n",
      "Epoch: 665/2000... Training loss: 0.7700\n",
      "Epoch: 666/2000... Training loss: 0.7322\n",
      "Epoch: 666/2000... Training loss: 0.9802\n",
      "Epoch: 666/2000... Training loss: 0.6467\n",
      "Epoch: 666/2000... Training loss: 0.7257\n",
      "Epoch: 666/2000... Training loss: 0.9023\n",
      "Epoch: 666/2000... Training loss: 0.6687\n",
      "Epoch: 666/2000... Training loss: 0.7657\n",
      "Epoch: 666/2000... Training loss: 0.8037\n",
      "Epoch: 666/2000... Training loss: 0.8118\n",
      "Epoch: 666/2000... Training loss: 0.8711\n",
      "Epoch: 666/2000... Training loss: 0.6767\n",
      "Epoch: 666/2000... Training loss: 0.7298\n",
      "Epoch: 666/2000... Training loss: 0.8446\n",
      "Epoch: 666/2000... Training loss: 0.8727\n",
      "Epoch: 666/2000... Training loss: 0.7707\n",
      "Epoch: 666/2000... Training loss: 0.8268\n",
      "Epoch: 666/2000... Training loss: 0.9554\n",
      "Epoch: 666/2000... Training loss: 0.8104\n",
      "Epoch: 666/2000... Training loss: 0.9108\n",
      "Epoch: 666/2000... Training loss: 0.6042\n",
      "Epoch: 666/2000... Training loss: 0.7318\n",
      "Epoch: 666/2000... Training loss: 0.8518\n",
      "Epoch: 666/2000... Training loss: 0.8714\n",
      "Epoch: 666/2000... Training loss: 0.8352\n",
      "Epoch: 666/2000... Training loss: 0.7717\n",
      "Epoch: 666/2000... Training loss: 0.7940\n",
      "Epoch: 666/2000... Training loss: 0.9847\n",
      "Epoch: 666/2000... Training loss: 1.0091\n",
      "Epoch: 666/2000... Training loss: 0.6536\n",
      "Epoch: 666/2000... Training loss: 0.7728\n",
      "Epoch: 666/2000... Training loss: 0.9822\n",
      "Epoch: 667/2000... Training loss: 1.1499\n",
      "Epoch: 667/2000... Training loss: 0.8641\n",
      "Epoch: 667/2000... Training loss: 0.9247\n",
      "Epoch: 667/2000... Training loss: 0.7114\n",
      "Epoch: 667/2000... Training loss: 0.6958\n",
      "Epoch: 667/2000... Training loss: 0.7326\n",
      "Epoch: 667/2000... Training loss: 0.8207\n",
      "Epoch: 667/2000... Training loss: 0.6926\n",
      "Epoch: 667/2000... Training loss: 0.8404\n",
      "Epoch: 667/2000... Training loss: 0.8539\n",
      "Epoch: 667/2000... Training loss: 0.6636\n",
      "Epoch: 667/2000... Training loss: 0.7244\n",
      "Epoch: 667/2000... Training loss: 0.8862\n",
      "Epoch: 667/2000... Training loss: 0.8190\n",
      "Epoch: 667/2000... Training loss: 0.6576\n",
      "Epoch: 667/2000... Training loss: 0.7423\n",
      "Epoch: 667/2000... Training loss: 0.7979\n",
      "Epoch: 667/2000... Training loss: 0.8202\n",
      "Epoch: 667/2000... Training loss: 0.9246\n",
      "Epoch: 667/2000... Training loss: 0.8390\n",
      "Epoch: 667/2000... Training loss: 0.7232\n",
      "Epoch: 667/2000... Training loss: 0.8929\n",
      "Epoch: 667/2000... Training loss: 0.6532\n",
      "Epoch: 667/2000... Training loss: 0.7052\n",
      "Epoch: 667/2000... Training loss: 1.0114\n",
      "Epoch: 667/2000... Training loss: 0.8284\n",
      "Epoch: 667/2000... Training loss: 0.7030\n",
      "Epoch: 667/2000... Training loss: 0.5801\n",
      "Epoch: 667/2000... Training loss: 0.8946\n",
      "Epoch: 667/2000... Training loss: 0.7723\n",
      "Epoch: 667/2000... Training loss: 0.6959\n",
      "Epoch: 668/2000... Training loss: 0.6725\n",
      "Epoch: 668/2000... Training loss: 0.8796\n",
      "Epoch: 668/2000... Training loss: 0.8901\n",
      "Epoch: 668/2000... Training loss: 0.9396\n",
      "Epoch: 668/2000... Training loss: 0.6931\n",
      "Epoch: 668/2000... Training loss: 1.0657\n",
      "Epoch: 668/2000... Training loss: 0.7382\n",
      "Epoch: 668/2000... Training loss: 1.0093\n",
      "Epoch: 668/2000... Training loss: 0.8105\n",
      "Epoch: 668/2000... Training loss: 0.6258\n",
      "Epoch: 668/2000... Training loss: 0.8985\n",
      "Epoch: 668/2000... Training loss: 0.9122\n",
      "Epoch: 668/2000... Training loss: 0.7072\n",
      "Epoch: 668/2000... Training loss: 0.9408\n",
      "Epoch: 668/2000... Training loss: 0.7786\n",
      "Epoch: 668/2000... Training loss: 0.8329\n",
      "Epoch: 668/2000... Training loss: 0.7161\n",
      "Epoch: 668/2000... Training loss: 0.7288\n",
      "Epoch: 668/2000... Training loss: 0.7422\n",
      "Epoch: 668/2000... Training loss: 0.5728\n",
      "Epoch: 668/2000... Training loss: 0.5843\n",
      "Epoch: 668/2000... Training loss: 0.6203\n",
      "Epoch: 668/2000... Training loss: 0.8001\n",
      "Epoch: 668/2000... Training loss: 0.9489\n",
      "Epoch: 668/2000... Training loss: 0.8032\n",
      "Epoch: 668/2000... Training loss: 0.9145\n",
      "Epoch: 668/2000... Training loss: 0.8937\n",
      "Epoch: 668/2000... Training loss: 0.7609\n",
      "Epoch: 668/2000... Training loss: 0.7991\n",
      "Epoch: 668/2000... Training loss: 0.6635\n",
      "Epoch: 668/2000... Training loss: 0.4965\n",
      "Epoch: 669/2000... Training loss: 0.8709\n",
      "Epoch: 669/2000... Training loss: 0.7515\n",
      "Epoch: 669/2000... Training loss: 0.7923\n",
      "Epoch: 669/2000... Training loss: 0.7825\n",
      "Epoch: 669/2000... Training loss: 0.7160\n",
      "Epoch: 669/2000... Training loss: 0.8846\n",
      "Epoch: 669/2000... Training loss: 0.9441\n",
      "Epoch: 669/2000... Training loss: 0.7660\n",
      "Epoch: 669/2000... Training loss: 0.8572\n",
      "Epoch: 669/2000... Training loss: 0.7064\n",
      "Epoch: 669/2000... Training loss: 1.1837\n",
      "Epoch: 669/2000... Training loss: 0.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 669/2000... Training loss: 0.8213\n",
      "Epoch: 669/2000... Training loss: 0.7177\n",
      "Epoch: 669/2000... Training loss: 0.8567\n",
      "Epoch: 669/2000... Training loss: 0.7454\n",
      "Epoch: 669/2000... Training loss: 0.8307\n",
      "Epoch: 669/2000... Training loss: 0.7240\n",
      "Epoch: 669/2000... Training loss: 0.8083\n",
      "Epoch: 669/2000... Training loss: 0.7007\n",
      "Epoch: 669/2000... Training loss: 0.7598\n",
      "Epoch: 669/2000... Training loss: 0.7701\n",
      "Epoch: 669/2000... Training loss: 0.5974\n",
      "Epoch: 669/2000... Training loss: 0.8542\n",
      "Epoch: 669/2000... Training loss: 0.7264\n",
      "Epoch: 669/2000... Training loss: 0.8298\n",
      "Epoch: 669/2000... Training loss: 0.9552\n",
      "Epoch: 669/2000... Training loss: 0.7470\n",
      "Epoch: 669/2000... Training loss: 0.7139\n",
      "Epoch: 669/2000... Training loss: 0.8580\n",
      "Epoch: 669/2000... Training loss: 0.8272\n",
      "Epoch: 670/2000... Training loss: 0.7059\n",
      "Epoch: 670/2000... Training loss: 0.6703\n",
      "Epoch: 670/2000... Training loss: 0.7488\n",
      "Epoch: 670/2000... Training loss: 0.5678\n",
      "Epoch: 670/2000... Training loss: 0.9600\n",
      "Epoch: 670/2000... Training loss: 0.8665\n",
      "Epoch: 670/2000... Training loss: 0.6378\n",
      "Epoch: 670/2000... Training loss: 0.9303\n",
      "Epoch: 670/2000... Training loss: 0.7794\n",
      "Epoch: 670/2000... Training loss: 0.8518\n",
      "Epoch: 670/2000... Training loss: 0.6807\n",
      "Epoch: 670/2000... Training loss: 0.6805\n",
      "Epoch: 670/2000... Training loss: 0.5649\n",
      "Epoch: 670/2000... Training loss: 0.7247\n",
      "Epoch: 670/2000... Training loss: 0.7471\n",
      "Epoch: 670/2000... Training loss: 0.6775\n",
      "Epoch: 670/2000... Training loss: 0.8314\n",
      "Epoch: 670/2000... Training loss: 0.9155\n",
      "Epoch: 670/2000... Training loss: 0.7050\n",
      "Epoch: 670/2000... Training loss: 0.7838\n",
      "Epoch: 670/2000... Training loss: 0.6149\n",
      "Epoch: 670/2000... Training loss: 0.7159\n",
      "Epoch: 670/2000... Training loss: 0.9052\n",
      "Epoch: 670/2000... Training loss: 1.0367\n",
      "Epoch: 670/2000... Training loss: 0.6238\n",
      "Epoch: 670/2000... Training loss: 0.8893\n",
      "Epoch: 670/2000... Training loss: 0.8335\n",
      "Epoch: 670/2000... Training loss: 0.8320\n",
      "Epoch: 670/2000... Training loss: 0.7902\n",
      "Epoch: 670/2000... Training loss: 0.9894\n",
      "Epoch: 670/2000... Training loss: 0.8377\n",
      "Epoch: 671/2000... Training loss: 0.9111\n",
      "Epoch: 671/2000... Training loss: 0.8443\n",
      "Epoch: 671/2000... Training loss: 0.7302\n",
      "Epoch: 671/2000... Training loss: 0.6112\n",
      "Epoch: 671/2000... Training loss: 0.7925\n",
      "Epoch: 671/2000... Training loss: 0.9115\n",
      "Epoch: 671/2000... Training loss: 0.7760\n",
      "Epoch: 671/2000... Training loss: 0.6455\n",
      "Epoch: 671/2000... Training loss: 0.7136\n",
      "Epoch: 671/2000... Training loss: 0.7653\n",
      "Epoch: 671/2000... Training loss: 0.7141\n",
      "Epoch: 671/2000... Training loss: 0.6979\n",
      "Epoch: 671/2000... Training loss: 0.8134\n",
      "Epoch: 671/2000... Training loss: 0.9438\n",
      "Epoch: 671/2000... Training loss: 0.8111\n",
      "Epoch: 671/2000... Training loss: 0.7269\n",
      "Epoch: 671/2000... Training loss: 1.0208\n",
      "Epoch: 671/2000... Training loss: 0.8323\n",
      "Epoch: 671/2000... Training loss: 0.7988\n",
      "Epoch: 671/2000... Training loss: 0.5813\n",
      "Epoch: 671/2000... Training loss: 0.4013\n",
      "Epoch: 671/2000... Training loss: 0.9278\n",
      "Epoch: 671/2000... Training loss: 1.1180\n",
      "Epoch: 671/2000... Training loss: 0.9491\n",
      "Epoch: 671/2000... Training loss: 0.7502\n",
      "Epoch: 671/2000... Training loss: 0.7935\n",
      "Epoch: 671/2000... Training loss: 0.6524\n",
      "Epoch: 671/2000... Training loss: 0.5860\n",
      "Epoch: 671/2000... Training loss: 0.7314\n",
      "Epoch: 671/2000... Training loss: 0.8627\n",
      "Epoch: 671/2000... Training loss: 0.6397\n",
      "Epoch: 672/2000... Training loss: 0.6784\n",
      "Epoch: 672/2000... Training loss: 0.7979\n",
      "Epoch: 672/2000... Training loss: 0.8603\n",
      "Epoch: 672/2000... Training loss: 0.9677\n",
      "Epoch: 672/2000... Training loss: 0.7614\n",
      "Epoch: 672/2000... Training loss: 0.7206\n",
      "Epoch: 672/2000... Training loss: 0.8225\n",
      "Epoch: 672/2000... Training loss: 0.7987\n",
      "Epoch: 672/2000... Training loss: 1.0006\n",
      "Epoch: 672/2000... Training loss: 0.7110\n",
      "Epoch: 672/2000... Training loss: 0.9712\n",
      "Epoch: 672/2000... Training loss: 0.7525\n",
      "Epoch: 672/2000... Training loss: 0.9527\n",
      "Epoch: 672/2000... Training loss: 0.9185\n",
      "Epoch: 672/2000... Training loss: 0.6463\n",
      "Epoch: 672/2000... Training loss: 0.5533\n",
      "Epoch: 672/2000... Training loss: 0.6566\n",
      "Epoch: 672/2000... Training loss: 0.9259\n",
      "Epoch: 672/2000... Training loss: 0.6585\n",
      "Epoch: 672/2000... Training loss: 0.9464\n",
      "Epoch: 672/2000... Training loss: 0.9208\n",
      "Epoch: 672/2000... Training loss: 0.5882\n",
      "Epoch: 672/2000... Training loss: 0.9188\n",
      "Epoch: 672/2000... Training loss: 0.7096\n",
      "Epoch: 672/2000... Training loss: 0.6278\n",
      "Epoch: 672/2000... Training loss: 0.9558\n",
      "Epoch: 672/2000... Training loss: 0.7113\n",
      "Epoch: 672/2000... Training loss: 0.8509\n",
      "Epoch: 672/2000... Training loss: 0.7278\n",
      "Epoch: 672/2000... Training loss: 0.6254\n",
      "Epoch: 672/2000... Training loss: 0.5309\n",
      "Epoch: 673/2000... Training loss: 0.6800\n",
      "Epoch: 673/2000... Training loss: 0.8199\n",
      "Epoch: 673/2000... Training loss: 0.7159\n",
      "Epoch: 673/2000... Training loss: 0.9230\n",
      "Epoch: 673/2000... Training loss: 0.8058\n",
      "Epoch: 673/2000... Training loss: 0.8437\n",
      "Epoch: 673/2000... Training loss: 0.8254\n",
      "Epoch: 673/2000... Training loss: 1.1035\n",
      "Epoch: 673/2000... Training loss: 0.7726\n",
      "Epoch: 673/2000... Training loss: 0.7156\n",
      "Epoch: 673/2000... Training loss: 0.9987\n",
      "Epoch: 673/2000... Training loss: 0.8021\n",
      "Epoch: 673/2000... Training loss: 0.9627\n",
      "Epoch: 673/2000... Training loss: 0.7334\n",
      "Epoch: 673/2000... Training loss: 0.7808\n",
      "Epoch: 673/2000... Training loss: 0.7337\n",
      "Epoch: 673/2000... Training loss: 0.5555\n",
      "Epoch: 673/2000... Training loss: 0.9259\n",
      "Epoch: 673/2000... Training loss: 0.5915\n",
      "Epoch: 673/2000... Training loss: 0.7075\n",
      "Epoch: 673/2000... Training loss: 0.6923\n",
      "Epoch: 673/2000... Training loss: 0.7058\n",
      "Epoch: 673/2000... Training loss: 0.7216\n",
      "Epoch: 673/2000... Training loss: 0.7574\n",
      "Epoch: 673/2000... Training loss: 1.0369\n",
      "Epoch: 673/2000... Training loss: 0.6188\n",
      "Epoch: 673/2000... Training loss: 0.9094\n",
      "Epoch: 673/2000... Training loss: 0.8829\n",
      "Epoch: 673/2000... Training loss: 1.0204\n",
      "Epoch: 673/2000... Training loss: 0.6624\n",
      "Epoch: 673/2000... Training loss: 0.7209\n",
      "Epoch: 674/2000... Training loss: 0.8253\n",
      "Epoch: 674/2000... Training loss: 0.7777\n",
      "Epoch: 674/2000... Training loss: 0.7673\n",
      "Epoch: 674/2000... Training loss: 0.7206\n",
      "Epoch: 674/2000... Training loss: 0.6556\n",
      "Epoch: 674/2000... Training loss: 0.6232\n",
      "Epoch: 674/2000... Training loss: 1.0357\n",
      "Epoch: 674/2000... Training loss: 0.6373\n",
      "Epoch: 674/2000... Training loss: 0.8216\n",
      "Epoch: 674/2000... Training loss: 0.6419\n",
      "Epoch: 674/2000... Training loss: 0.6869\n",
      "Epoch: 674/2000... Training loss: 0.9209\n",
      "Epoch: 674/2000... Training loss: 0.5960\n",
      "Epoch: 674/2000... Training loss: 0.8359\n",
      "Epoch: 674/2000... Training loss: 0.6236\n",
      "Epoch: 674/2000... Training loss: 0.7483\n",
      "Epoch: 674/2000... Training loss: 0.8175\n",
      "Epoch: 674/2000... Training loss: 0.6090\n",
      "Epoch: 674/2000... Training loss: 0.7146\n",
      "Epoch: 674/2000... Training loss: 0.7907\n",
      "Epoch: 674/2000... Training loss: 0.7591\n",
      "Epoch: 674/2000... Training loss: 0.6985\n",
      "Epoch: 674/2000... Training loss: 0.7616\n",
      "Epoch: 674/2000... Training loss: 0.8307\n",
      "Epoch: 674/2000... Training loss: 0.8261\n",
      "Epoch: 674/2000... Training loss: 0.6039\n",
      "Epoch: 674/2000... Training loss: 0.7665\n",
      "Epoch: 674/2000... Training loss: 0.7724\n",
      "Epoch: 674/2000... Training loss: 0.5827\n",
      "Epoch: 674/2000... Training loss: 0.6646\n",
      "Epoch: 674/2000... Training loss: 0.9136\n",
      "Epoch: 675/2000... Training loss: 1.0086\n",
      "Epoch: 675/2000... Training loss: 0.8174\n",
      "Epoch: 675/2000... Training loss: 0.6209\n",
      "Epoch: 675/2000... Training loss: 0.7838\n",
      "Epoch: 675/2000... Training loss: 0.7244\n",
      "Epoch: 675/2000... Training loss: 0.7059\n",
      "Epoch: 675/2000... Training loss: 0.7096\n",
      "Epoch: 675/2000... Training loss: 0.7309\n",
      "Epoch: 675/2000... Training loss: 0.6436\n",
      "Epoch: 675/2000... Training loss: 0.7983\n",
      "Epoch: 675/2000... Training loss: 0.7516\n",
      "Epoch: 675/2000... Training loss: 0.7507\n",
      "Epoch: 675/2000... Training loss: 0.9660\n",
      "Epoch: 675/2000... Training loss: 0.8342\n",
      "Epoch: 675/2000... Training loss: 0.7892\n",
      "Epoch: 675/2000... Training loss: 0.9304\n",
      "Epoch: 675/2000... Training loss: 0.7816\n",
      "Epoch: 675/2000... Training loss: 0.8417\n",
      "Epoch: 675/2000... Training loss: 0.6184\n",
      "Epoch: 675/2000... Training loss: 0.8433\n",
      "Epoch: 675/2000... Training loss: 0.6779\n",
      "Epoch: 675/2000... Training loss: 0.7782\n",
      "Epoch: 675/2000... Training loss: 0.7290\n",
      "Epoch: 675/2000... Training loss: 0.6648\n",
      "Epoch: 675/2000... Training loss: 0.7027\n",
      "Epoch: 675/2000... Training loss: 0.8798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 675/2000... Training loss: 0.8011\n",
      "Epoch: 675/2000... Training loss: 0.8671\n",
      "Epoch: 675/2000... Training loss: 0.7290\n",
      "Epoch: 675/2000... Training loss: 0.8183\n",
      "Epoch: 675/2000... Training loss: 0.8256\n",
      "Epoch: 676/2000... Training loss: 0.8407\n",
      "Epoch: 676/2000... Training loss: 0.8552\n",
      "Epoch: 676/2000... Training loss: 0.8858\n",
      "Epoch: 676/2000... Training loss: 0.6705\n",
      "Epoch: 676/2000... Training loss: 0.8066\n",
      "Epoch: 676/2000... Training loss: 0.9818\n",
      "Epoch: 676/2000... Training loss: 0.8838\n",
      "Epoch: 676/2000... Training loss: 1.0451\n",
      "Epoch: 676/2000... Training loss: 0.9370\n",
      "Epoch: 676/2000... Training loss: 0.6464\n",
      "Epoch: 676/2000... Training loss: 0.7101\n",
      "Epoch: 676/2000... Training loss: 0.7936\n",
      "Epoch: 676/2000... Training loss: 0.7538\n",
      "Epoch: 676/2000... Training loss: 0.6311\n",
      "Epoch: 676/2000... Training loss: 0.6676\n",
      "Epoch: 676/2000... Training loss: 0.5252\n",
      "Epoch: 676/2000... Training loss: 0.7821\n",
      "Epoch: 676/2000... Training loss: 0.7971\n",
      "Epoch: 676/2000... Training loss: 0.6914\n",
      "Epoch: 676/2000... Training loss: 0.6377\n",
      "Epoch: 676/2000... Training loss: 0.7540\n",
      "Epoch: 676/2000... Training loss: 0.7206\n",
      "Epoch: 676/2000... Training loss: 0.7928\n",
      "Epoch: 676/2000... Training loss: 0.8387\n",
      "Epoch: 676/2000... Training loss: 0.6020\n",
      "Epoch: 676/2000... Training loss: 0.9267\n",
      "Epoch: 676/2000... Training loss: 0.8181\n",
      "Epoch: 676/2000... Training loss: 0.7263\n",
      "Epoch: 676/2000... Training loss: 0.6858\n",
      "Epoch: 676/2000... Training loss: 0.6748\n",
      "Epoch: 676/2000... Training loss: 0.9625\n",
      "Epoch: 677/2000... Training loss: 0.7631\n",
      "Epoch: 677/2000... Training loss: 0.6083\n",
      "Epoch: 677/2000... Training loss: 0.8047\n",
      "Epoch: 677/2000... Training loss: 0.7960\n",
      "Epoch: 677/2000... Training loss: 0.7492\n",
      "Epoch: 677/2000... Training loss: 0.8034\n",
      "Epoch: 677/2000... Training loss: 0.7890\n",
      "Epoch: 677/2000... Training loss: 0.6570\n",
      "Epoch: 677/2000... Training loss: 0.6469\n",
      "Epoch: 677/2000... Training loss: 0.8069\n",
      "Epoch: 677/2000... Training loss: 0.8185\n",
      "Epoch: 677/2000... Training loss: 0.7312\n",
      "Epoch: 677/2000... Training loss: 0.6992\n",
      "Epoch: 677/2000... Training loss: 0.7423\n",
      "Epoch: 677/2000... Training loss: 0.8027\n",
      "Epoch: 677/2000... Training loss: 0.4901\n",
      "Epoch: 677/2000... Training loss: 0.8887\n",
      "Epoch: 677/2000... Training loss: 0.7038\n",
      "Epoch: 677/2000... Training loss: 0.9643\n",
      "Epoch: 677/2000... Training loss: 0.8125\n",
      "Epoch: 677/2000... Training loss: 0.8453\n",
      "Epoch: 677/2000... Training loss: 0.5808\n",
      "Epoch: 677/2000... Training loss: 0.6706\n",
      "Epoch: 677/2000... Training loss: 0.7798\n",
      "Epoch: 677/2000... Training loss: 0.6241\n",
      "Epoch: 677/2000... Training loss: 0.8606\n",
      "Epoch: 677/2000... Training loss: 0.7330\n",
      "Epoch: 677/2000... Training loss: 0.8069\n",
      "Epoch: 677/2000... Training loss: 0.7360\n",
      "Epoch: 677/2000... Training loss: 0.8354\n",
      "Epoch: 677/2000... Training loss: 0.8453\n",
      "Epoch: 678/2000... Training loss: 0.7863\n",
      "Epoch: 678/2000... Training loss: 0.6910\n",
      "Epoch: 678/2000... Training loss: 0.9850\n",
      "Epoch: 678/2000... Training loss: 0.8151\n",
      "Epoch: 678/2000... Training loss: 0.7825\n",
      "Epoch: 678/2000... Training loss: 0.8362\n",
      "Epoch: 678/2000... Training loss: 0.8127\n",
      "Epoch: 678/2000... Training loss: 0.8059\n",
      "Epoch: 678/2000... Training loss: 0.8129\n",
      "Epoch: 678/2000... Training loss: 0.7923\n",
      "Epoch: 678/2000... Training loss: 0.7824\n",
      "Epoch: 678/2000... Training loss: 0.9456\n",
      "Epoch: 678/2000... Training loss: 0.8598\n",
      "Epoch: 678/2000... Training loss: 0.6560\n",
      "Epoch: 678/2000... Training loss: 0.5908\n",
      "Epoch: 678/2000... Training loss: 0.7850\n",
      "Epoch: 678/2000... Training loss: 0.8307\n",
      "Epoch: 678/2000... Training loss: 0.6670\n",
      "Epoch: 678/2000... Training loss: 0.8668\n",
      "Epoch: 678/2000... Training loss: 0.6707\n",
      "Epoch: 678/2000... Training loss: 0.6334\n",
      "Epoch: 678/2000... Training loss: 0.6816\n",
      "Epoch: 678/2000... Training loss: 0.6206\n",
      "Epoch: 678/2000... Training loss: 0.9115\n",
      "Epoch: 678/2000... Training loss: 0.6944\n",
      "Epoch: 678/2000... Training loss: 0.8108\n",
      "Epoch: 678/2000... Training loss: 0.8532\n",
      "Epoch: 678/2000... Training loss: 0.8514\n",
      "Epoch: 678/2000... Training loss: 0.7222\n",
      "Epoch: 678/2000... Training loss: 0.7330\n",
      "Epoch: 678/2000... Training loss: 0.8510\n",
      "Epoch: 679/2000... Training loss: 0.8578\n",
      "Epoch: 679/2000... Training loss: 0.6692\n",
      "Epoch: 679/2000... Training loss: 0.7759\n",
      "Epoch: 679/2000... Training loss: 0.7115\n",
      "Epoch: 679/2000... Training loss: 0.6991\n",
      "Epoch: 679/2000... Training loss: 1.1283\n",
      "Epoch: 679/2000... Training loss: 0.7608\n",
      "Epoch: 679/2000... Training loss: 0.8449\n",
      "Epoch: 679/2000... Training loss: 1.0494\n",
      "Epoch: 679/2000... Training loss: 0.6231\n",
      "Epoch: 679/2000... Training loss: 0.6788\n",
      "Epoch: 679/2000... Training loss: 0.8196\n",
      "Epoch: 679/2000... Training loss: 1.1300\n",
      "Epoch: 679/2000... Training loss: 0.8461\n",
      "Epoch: 679/2000... Training loss: 0.7085\n",
      "Epoch: 679/2000... Training loss: 0.7507\n",
      "Epoch: 679/2000... Training loss: 0.7893\n",
      "Epoch: 679/2000... Training loss: 0.8456\n",
      "Epoch: 679/2000... Training loss: 0.7305\n",
      "Epoch: 679/2000... Training loss: 0.8712\n",
      "Epoch: 679/2000... Training loss: 0.6409\n",
      "Epoch: 679/2000... Training loss: 0.7657\n",
      "Epoch: 679/2000... Training loss: 0.8795\n",
      "Epoch: 679/2000... Training loss: 0.7678\n",
      "Epoch: 679/2000... Training loss: 0.5745\n",
      "Epoch: 679/2000... Training loss: 0.8073\n",
      "Epoch: 679/2000... Training loss: 0.8337\n",
      "Epoch: 679/2000... Training loss: 0.7181\n",
      "Epoch: 679/2000... Training loss: 0.7882\n",
      "Epoch: 679/2000... Training loss: 0.6351\n",
      "Epoch: 679/2000... Training loss: 0.9483\n",
      "Epoch: 680/2000... Training loss: 0.8044\n",
      "Epoch: 680/2000... Training loss: 0.8036\n",
      "Epoch: 680/2000... Training loss: 0.8976\n",
      "Epoch: 680/2000... Training loss: 0.9586\n",
      "Epoch: 680/2000... Training loss: 0.9970\n",
      "Epoch: 680/2000... Training loss: 0.6417\n",
      "Epoch: 680/2000... Training loss: 0.6979\n",
      "Epoch: 680/2000... Training loss: 0.7851\n",
      "Epoch: 680/2000... Training loss: 0.8217\n",
      "Epoch: 680/2000... Training loss: 0.6888\n",
      "Epoch: 680/2000... Training loss: 0.7943\n",
      "Epoch: 680/2000... Training loss: 0.8116\n",
      "Epoch: 680/2000... Training loss: 1.0884\n",
      "Epoch: 680/2000... Training loss: 0.9125\n",
      "Epoch: 680/2000... Training loss: 0.6171\n",
      "Epoch: 680/2000... Training loss: 0.9392\n",
      "Epoch: 680/2000... Training loss: 0.5966\n",
      "Epoch: 680/2000... Training loss: 0.7747\n",
      "Epoch: 680/2000... Training loss: 0.7422\n",
      "Epoch: 680/2000... Training loss: 0.7161\n",
      "Epoch: 680/2000... Training loss: 0.6977\n",
      "Epoch: 680/2000... Training loss: 0.6632\n",
      "Epoch: 680/2000... Training loss: 0.6014\n",
      "Epoch: 680/2000... Training loss: 0.8347\n",
      "Epoch: 680/2000... Training loss: 0.6232\n",
      "Epoch: 680/2000... Training loss: 0.5438\n",
      "Epoch: 680/2000... Training loss: 0.5321\n",
      "Epoch: 680/2000... Training loss: 0.6531\n",
      "Epoch: 680/2000... Training loss: 0.6489\n",
      "Epoch: 680/2000... Training loss: 0.8443\n",
      "Epoch: 680/2000... Training loss: 0.5824\n",
      "Epoch: 681/2000... Training loss: 0.5749\n",
      "Epoch: 681/2000... Training loss: 0.7527\n",
      "Epoch: 681/2000... Training loss: 0.6782\n",
      "Epoch: 681/2000... Training loss: 0.9330\n",
      "Epoch: 681/2000... Training loss: 0.5447\n",
      "Epoch: 681/2000... Training loss: 0.8087\n",
      "Epoch: 681/2000... Training loss: 0.9861\n",
      "Epoch: 681/2000... Training loss: 0.7332\n",
      "Epoch: 681/2000... Training loss: 1.0085\n",
      "Epoch: 681/2000... Training loss: 0.7555\n",
      "Epoch: 681/2000... Training loss: 0.5927\n",
      "Epoch: 681/2000... Training loss: 0.5850\n",
      "Epoch: 681/2000... Training loss: 0.6493\n",
      "Epoch: 681/2000... Training loss: 1.0660\n",
      "Epoch: 681/2000... Training loss: 0.7419\n",
      "Epoch: 681/2000... Training loss: 0.5298\n",
      "Epoch: 681/2000... Training loss: 0.6991\n",
      "Epoch: 681/2000... Training loss: 0.5234\n",
      "Epoch: 681/2000... Training loss: 0.8279\n",
      "Epoch: 681/2000... Training loss: 0.8812\n",
      "Epoch: 681/2000... Training loss: 0.8298\n",
      "Epoch: 681/2000... Training loss: 0.7405\n",
      "Epoch: 681/2000... Training loss: 0.7180\n",
      "Epoch: 681/2000... Training loss: 0.7254\n",
      "Epoch: 681/2000... Training loss: 0.9831\n",
      "Epoch: 681/2000... Training loss: 0.6965\n",
      "Epoch: 681/2000... Training loss: 0.7671\n",
      "Epoch: 681/2000... Training loss: 0.6862\n",
      "Epoch: 681/2000... Training loss: 0.6562\n",
      "Epoch: 681/2000... Training loss: 0.5926\n",
      "Epoch: 681/2000... Training loss: 0.7282\n",
      "Epoch: 682/2000... Training loss: 0.7390\n",
      "Epoch: 682/2000... Training loss: 0.5475\n",
      "Epoch: 682/2000... Training loss: 0.7739\n",
      "Epoch: 682/2000... Training loss: 0.6126\n",
      "Epoch: 682/2000... Training loss: 0.8160\n",
      "Epoch: 682/2000... Training loss: 0.9311\n",
      "Epoch: 682/2000... Training loss: 0.6290\n",
      "Epoch: 682/2000... Training loss: 0.6968\n",
      "Epoch: 682/2000... Training loss: 0.6404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 682/2000... Training loss: 0.6948\n",
      "Epoch: 682/2000... Training loss: 0.8554\n",
      "Epoch: 682/2000... Training loss: 0.9305\n",
      "Epoch: 682/2000... Training loss: 0.7549\n",
      "Epoch: 682/2000... Training loss: 0.9592\n",
      "Epoch: 682/2000... Training loss: 0.6453\n",
      "Epoch: 682/2000... Training loss: 0.6340\n",
      "Epoch: 682/2000... Training loss: 0.9377\n",
      "Epoch: 682/2000... Training loss: 0.8762\n",
      "Epoch: 682/2000... Training loss: 0.6813\n",
      "Epoch: 682/2000... Training loss: 0.7827\n",
      "Epoch: 682/2000... Training loss: 0.6587\n",
      "Epoch: 682/2000... Training loss: 0.7076\n",
      "Epoch: 682/2000... Training loss: 0.6846\n",
      "Epoch: 682/2000... Training loss: 0.7130\n",
      "Epoch: 682/2000... Training loss: 0.7305\n",
      "Epoch: 682/2000... Training loss: 0.6687\n",
      "Epoch: 682/2000... Training loss: 0.8750\n",
      "Epoch: 682/2000... Training loss: 0.7788\n",
      "Epoch: 682/2000... Training loss: 0.8433\n",
      "Epoch: 682/2000... Training loss: 0.7562\n",
      "Epoch: 682/2000... Training loss: 0.8003\n",
      "Epoch: 683/2000... Training loss: 0.9130\n",
      "Epoch: 683/2000... Training loss: 0.6740\n",
      "Epoch: 683/2000... Training loss: 0.7913\n",
      "Epoch: 683/2000... Training loss: 0.6302\n",
      "Epoch: 683/2000... Training loss: 0.8043\n",
      "Epoch: 683/2000... Training loss: 0.8231\n",
      "Epoch: 683/2000... Training loss: 0.6695\n",
      "Epoch: 683/2000... Training loss: 0.6315\n",
      "Epoch: 683/2000... Training loss: 1.0943\n",
      "Epoch: 683/2000... Training loss: 0.4471\n",
      "Epoch: 683/2000... Training loss: 0.8958\n",
      "Epoch: 683/2000... Training loss: 0.7186\n",
      "Epoch: 683/2000... Training loss: 0.8308\n",
      "Epoch: 683/2000... Training loss: 0.7941\n",
      "Epoch: 683/2000... Training loss: 0.7442\n",
      "Epoch: 683/2000... Training loss: 0.6897\n",
      "Epoch: 683/2000... Training loss: 0.7103\n",
      "Epoch: 683/2000... Training loss: 0.5130\n",
      "Epoch: 683/2000... Training loss: 0.7187\n",
      "Epoch: 683/2000... Training loss: 0.5960\n",
      "Epoch: 683/2000... Training loss: 0.7437\n",
      "Epoch: 683/2000... Training loss: 0.6525\n",
      "Epoch: 683/2000... Training loss: 0.9886\n",
      "Epoch: 683/2000... Training loss: 0.8659\n",
      "Epoch: 683/2000... Training loss: 0.7161\n",
      "Epoch: 683/2000... Training loss: 0.5876\n",
      "Epoch: 683/2000... Training loss: 0.8213\n",
      "Epoch: 683/2000... Training loss: 0.7591\n",
      "Epoch: 683/2000... Training loss: 0.7508\n",
      "Epoch: 683/2000... Training loss: 0.5706\n",
      "Epoch: 683/2000... Training loss: 0.7293\n",
      "Epoch: 684/2000... Training loss: 0.8613\n",
      "Epoch: 684/2000... Training loss: 0.7621\n",
      "Epoch: 684/2000... Training loss: 0.8339\n",
      "Epoch: 684/2000... Training loss: 0.7437\n",
      "Epoch: 684/2000... Training loss: 0.7615\n",
      "Epoch: 684/2000... Training loss: 0.6276\n",
      "Epoch: 684/2000... Training loss: 0.8180\n",
      "Epoch: 684/2000... Training loss: 0.7326\n",
      "Epoch: 684/2000... Training loss: 0.5674\n",
      "Epoch: 684/2000... Training loss: 0.8117\n",
      "Epoch: 684/2000... Training loss: 0.8049\n",
      "Epoch: 684/2000... Training loss: 0.5794\n",
      "Epoch: 684/2000... Training loss: 0.6960\n",
      "Epoch: 684/2000... Training loss: 0.7426\n",
      "Epoch: 684/2000... Training loss: 0.5482\n",
      "Epoch: 684/2000... Training loss: 0.9019\n",
      "Epoch: 684/2000... Training loss: 0.7562\n",
      "Epoch: 684/2000... Training loss: 0.6831\n",
      "Epoch: 684/2000... Training loss: 0.8536\n",
      "Epoch: 684/2000... Training loss: 0.8100\n",
      "Epoch: 684/2000... Training loss: 0.6956\n",
      "Epoch: 684/2000... Training loss: 0.6234\n",
      "Epoch: 684/2000... Training loss: 0.8753\n",
      "Epoch: 684/2000... Training loss: 0.7790\n",
      "Epoch: 684/2000... Training loss: 0.8644\n",
      "Epoch: 684/2000... Training loss: 0.8515\n",
      "Epoch: 684/2000... Training loss: 0.6863\n",
      "Epoch: 684/2000... Training loss: 0.8582\n",
      "Epoch: 684/2000... Training loss: 0.6739\n",
      "Epoch: 684/2000... Training loss: 0.7329\n",
      "Epoch: 684/2000... Training loss: 0.8698\n",
      "Epoch: 685/2000... Training loss: 0.8575\n",
      "Epoch: 685/2000... Training loss: 0.7950\n",
      "Epoch: 685/2000... Training loss: 0.8150\n",
      "Epoch: 685/2000... Training loss: 0.6388\n",
      "Epoch: 685/2000... Training loss: 0.7891\n",
      "Epoch: 685/2000... Training loss: 0.8208\n",
      "Epoch: 685/2000... Training loss: 0.5878\n",
      "Epoch: 685/2000... Training loss: 0.8098\n",
      "Epoch: 685/2000... Training loss: 0.8300\n",
      "Epoch: 685/2000... Training loss: 0.9875\n",
      "Epoch: 685/2000... Training loss: 0.8033\n",
      "Epoch: 685/2000... Training loss: 0.7000\n",
      "Epoch: 685/2000... Training loss: 0.7096\n",
      "Epoch: 685/2000... Training loss: 0.8273\n",
      "Epoch: 685/2000... Training loss: 0.6098\n",
      "Epoch: 685/2000... Training loss: 0.7367\n",
      "Epoch: 685/2000... Training loss: 0.6611\n",
      "Epoch: 685/2000... Training loss: 0.7076\n",
      "Epoch: 685/2000... Training loss: 0.8410\n",
      "Epoch: 685/2000... Training loss: 0.7405\n",
      "Epoch: 685/2000... Training loss: 1.0339\n",
      "Epoch: 685/2000... Training loss: 0.6392\n",
      "Epoch: 685/2000... Training loss: 0.7856\n",
      "Epoch: 685/2000... Training loss: 0.6205\n",
      "Epoch: 685/2000... Training loss: 1.1005\n",
      "Epoch: 685/2000... Training loss: 0.7773\n",
      "Epoch: 685/2000... Training loss: 1.1243\n",
      "Epoch: 685/2000... Training loss: 0.8228\n",
      "Epoch: 685/2000... Training loss: 0.7308\n",
      "Epoch: 685/2000... Training loss: 0.7125\n",
      "Epoch: 685/2000... Training loss: 0.8630\n",
      "Epoch: 686/2000... Training loss: 0.6532\n",
      "Epoch: 686/2000... Training loss: 0.6096\n",
      "Epoch: 686/2000... Training loss: 0.6498\n",
      "Epoch: 686/2000... Training loss: 0.6383\n",
      "Epoch: 686/2000... Training loss: 0.6923\n",
      "Epoch: 686/2000... Training loss: 0.7686\n",
      "Epoch: 686/2000... Training loss: 0.5827\n",
      "Epoch: 686/2000... Training loss: 0.8931\n",
      "Epoch: 686/2000... Training loss: 0.6368\n",
      "Epoch: 686/2000... Training loss: 0.7924\n",
      "Epoch: 686/2000... Training loss: 0.8513\n",
      "Epoch: 686/2000... Training loss: 0.9281\n",
      "Epoch: 686/2000... Training loss: 0.8200\n",
      "Epoch: 686/2000... Training loss: 0.7857\n",
      "Epoch: 686/2000... Training loss: 0.6944\n",
      "Epoch: 686/2000... Training loss: 0.8112\n",
      "Epoch: 686/2000... Training loss: 0.7584\n",
      "Epoch: 686/2000... Training loss: 0.9537\n",
      "Epoch: 686/2000... Training loss: 0.6855\n",
      "Epoch: 686/2000... Training loss: 0.7094\n",
      "Epoch: 686/2000... Training loss: 0.7016\n",
      "Epoch: 686/2000... Training loss: 0.8588\n",
      "Epoch: 686/2000... Training loss: 0.8567\n",
      "Epoch: 686/2000... Training loss: 0.9655\n",
      "Epoch: 686/2000... Training loss: 0.8794\n",
      "Epoch: 686/2000... Training loss: 0.6603\n",
      "Epoch: 686/2000... Training loss: 0.6837\n",
      "Epoch: 686/2000... Training loss: 0.7473\n",
      "Epoch: 686/2000... Training loss: 0.8952\n",
      "Epoch: 686/2000... Training loss: 0.6264\n",
      "Epoch: 686/2000... Training loss: 0.5185\n",
      "Epoch: 687/2000... Training loss: 0.8419\n",
      "Epoch: 687/2000... Training loss: 0.7625\n",
      "Epoch: 687/2000... Training loss: 0.7017\n",
      "Epoch: 687/2000... Training loss: 1.0072\n",
      "Epoch: 687/2000... Training loss: 0.7045\n",
      "Epoch: 687/2000... Training loss: 0.6678\n",
      "Epoch: 687/2000... Training loss: 0.7387\n",
      "Epoch: 687/2000... Training loss: 0.7328\n",
      "Epoch: 687/2000... Training loss: 0.7834\n",
      "Epoch: 687/2000... Training loss: 0.8101\n",
      "Epoch: 687/2000... Training loss: 0.8440\n",
      "Epoch: 687/2000... Training loss: 0.6250\n",
      "Epoch: 687/2000... Training loss: 0.7265\n",
      "Epoch: 687/2000... Training loss: 0.7214\n",
      "Epoch: 687/2000... Training loss: 0.8237\n",
      "Epoch: 687/2000... Training loss: 0.6947\n",
      "Epoch: 687/2000... Training loss: 0.7592\n",
      "Epoch: 687/2000... Training loss: 0.7617\n",
      "Epoch: 687/2000... Training loss: 0.7034\n",
      "Epoch: 687/2000... Training loss: 0.8962\n",
      "Epoch: 687/2000... Training loss: 0.7182\n",
      "Epoch: 687/2000... Training loss: 0.8005\n",
      "Epoch: 687/2000... Training loss: 0.8267\n",
      "Epoch: 687/2000... Training loss: 0.6969\n",
      "Epoch: 687/2000... Training loss: 0.6562\n",
      "Epoch: 687/2000... Training loss: 0.7461\n",
      "Epoch: 687/2000... Training loss: 0.8543\n",
      "Epoch: 687/2000... Training loss: 0.8357\n",
      "Epoch: 687/2000... Training loss: 1.0354\n",
      "Epoch: 687/2000... Training loss: 0.6765\n",
      "Epoch: 687/2000... Training loss: 0.8193\n",
      "Epoch: 688/2000... Training loss: 0.8304\n",
      "Epoch: 688/2000... Training loss: 0.7301\n",
      "Epoch: 688/2000... Training loss: 0.9262\n",
      "Epoch: 688/2000... Training loss: 0.8177\n",
      "Epoch: 688/2000... Training loss: 0.7387\n",
      "Epoch: 688/2000... Training loss: 0.8752\n",
      "Epoch: 688/2000... Training loss: 0.5215\n",
      "Epoch: 688/2000... Training loss: 0.7261\n",
      "Epoch: 688/2000... Training loss: 0.7275\n",
      "Epoch: 688/2000... Training loss: 1.0670\n",
      "Epoch: 688/2000... Training loss: 0.8611\n",
      "Epoch: 688/2000... Training loss: 0.8211\n",
      "Epoch: 688/2000... Training loss: 0.6604\n",
      "Epoch: 688/2000... Training loss: 0.7920\n",
      "Epoch: 688/2000... Training loss: 0.5093\n",
      "Epoch: 688/2000... Training loss: 0.8072\n",
      "Epoch: 688/2000... Training loss: 0.7385\n",
      "Epoch: 688/2000... Training loss: 0.8298\n",
      "Epoch: 688/2000... Training loss: 0.8054\n",
      "Epoch: 688/2000... Training loss: 0.6526\n",
      "Epoch: 688/2000... Training loss: 0.7501\n",
      "Epoch: 688/2000... Training loss: 0.9254\n",
      "Epoch: 688/2000... Training loss: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 688/2000... Training loss: 0.9867\n",
      "Epoch: 688/2000... Training loss: 0.7925\n",
      "Epoch: 688/2000... Training loss: 0.8988\n",
      "Epoch: 688/2000... Training loss: 0.7498\n",
      "Epoch: 688/2000... Training loss: 0.6462\n",
      "Epoch: 688/2000... Training loss: 0.6951\n",
      "Epoch: 688/2000... Training loss: 0.7166\n",
      "Epoch: 688/2000... Training loss: 0.5898\n",
      "Epoch: 689/2000... Training loss: 0.8642\n",
      "Epoch: 689/2000... Training loss: 0.7029\n",
      "Epoch: 689/2000... Training loss: 0.8898\n",
      "Epoch: 689/2000... Training loss: 0.7443\n",
      "Epoch: 689/2000... Training loss: 0.8280\n",
      "Epoch: 689/2000... Training loss: 0.7302\n",
      "Epoch: 689/2000... Training loss: 0.8526\n",
      "Epoch: 689/2000... Training loss: 0.9263\n",
      "Epoch: 689/2000... Training loss: 0.7773\n",
      "Epoch: 689/2000... Training loss: 0.8883\n",
      "Epoch: 689/2000... Training loss: 0.7220\n",
      "Epoch: 689/2000... Training loss: 0.8547\n",
      "Epoch: 689/2000... Training loss: 0.7360\n",
      "Epoch: 689/2000... Training loss: 0.6410\n",
      "Epoch: 689/2000... Training loss: 0.6157\n",
      "Epoch: 689/2000... Training loss: 0.7085\n",
      "Epoch: 689/2000... Training loss: 0.7445\n",
      "Epoch: 689/2000... Training loss: 0.7701\n",
      "Epoch: 689/2000... Training loss: 0.8312\n",
      "Epoch: 689/2000... Training loss: 0.5091\n",
      "Epoch: 689/2000... Training loss: 0.5531\n",
      "Epoch: 689/2000... Training loss: 0.6815\n",
      "Epoch: 689/2000... Training loss: 0.7198\n",
      "Epoch: 689/2000... Training loss: 0.6198\n",
      "Epoch: 689/2000... Training loss: 0.7753\n",
      "Epoch: 689/2000... Training loss: 0.7223\n",
      "Epoch: 689/2000... Training loss: 0.7195\n",
      "Epoch: 689/2000... Training loss: 0.6511\n",
      "Epoch: 689/2000... Training loss: 0.7197\n",
      "Epoch: 689/2000... Training loss: 0.8358\n",
      "Epoch: 689/2000... Training loss: 0.7337\n",
      "Epoch: 690/2000... Training loss: 0.6632\n",
      "Epoch: 690/2000... Training loss: 0.6471\n",
      "Epoch: 690/2000... Training loss: 0.7401\n",
      "Epoch: 690/2000... Training loss: 0.9771\n",
      "Epoch: 690/2000... Training loss: 0.7755\n",
      "Epoch: 690/2000... Training loss: 0.4530\n",
      "Epoch: 690/2000... Training loss: 0.7496\n",
      "Epoch: 690/2000... Training loss: 0.7067\n",
      "Epoch: 690/2000... Training loss: 0.5812\n",
      "Epoch: 690/2000... Training loss: 0.8278\n",
      "Epoch: 690/2000... Training loss: 0.8542\n",
      "Epoch: 690/2000... Training loss: 0.8592\n",
      "Epoch: 690/2000... Training loss: 0.5954\n",
      "Epoch: 690/2000... Training loss: 0.8541\n",
      "Epoch: 690/2000... Training loss: 0.6435\n",
      "Epoch: 690/2000... Training loss: 0.9032\n",
      "Epoch: 690/2000... Training loss: 0.7650\n",
      "Epoch: 690/2000... Training loss: 0.7278\n",
      "Epoch: 690/2000... Training loss: 0.7144\n",
      "Epoch: 690/2000... Training loss: 0.5212\n",
      "Epoch: 690/2000... Training loss: 0.6201\n",
      "Epoch: 690/2000... Training loss: 0.8782\n",
      "Epoch: 690/2000... Training loss: 0.7871\n",
      "Epoch: 690/2000... Training loss: 0.9609\n",
      "Epoch: 690/2000... Training loss: 0.8167\n",
      "Epoch: 690/2000... Training loss: 0.6138\n",
      "Epoch: 690/2000... Training loss: 0.9925\n",
      "Epoch: 690/2000... Training loss: 0.6701\n",
      "Epoch: 690/2000... Training loss: 0.7773\n",
      "Epoch: 690/2000... Training loss: 0.5382\n",
      "Epoch: 690/2000... Training loss: 0.8640\n",
      "Epoch: 691/2000... Training loss: 0.7944\n",
      "Epoch: 691/2000... Training loss: 0.7571\n",
      "Epoch: 691/2000... Training loss: 0.9190\n",
      "Epoch: 691/2000... Training loss: 0.7135\n",
      "Epoch: 691/2000... Training loss: 0.7251\n",
      "Epoch: 691/2000... Training loss: 0.6596\n",
      "Epoch: 691/2000... Training loss: 0.6084\n",
      "Epoch: 691/2000... Training loss: 0.6458\n",
      "Epoch: 691/2000... Training loss: 0.6028\n",
      "Epoch: 691/2000... Training loss: 0.6085\n",
      "Epoch: 691/2000... Training loss: 0.7953\n",
      "Epoch: 691/2000... Training loss: 0.8699\n",
      "Epoch: 691/2000... Training loss: 0.7452\n",
      "Epoch: 691/2000... Training loss: 0.6003\n",
      "Epoch: 691/2000... Training loss: 0.5807\n",
      "Epoch: 691/2000... Training loss: 0.5325\n",
      "Epoch: 691/2000... Training loss: 0.8968\n",
      "Epoch: 691/2000... Training loss: 0.7307\n",
      "Epoch: 691/2000... Training loss: 0.7551\n",
      "Epoch: 691/2000... Training loss: 0.6711\n",
      "Epoch: 691/2000... Training loss: 0.7422\n",
      "Epoch: 691/2000... Training loss: 0.6999\n",
      "Epoch: 691/2000... Training loss: 0.7085\n",
      "Epoch: 691/2000... Training loss: 0.7069\n",
      "Epoch: 691/2000... Training loss: 0.7157\n",
      "Epoch: 691/2000... Training loss: 0.8943\n",
      "Epoch: 691/2000... Training loss: 0.7385\n",
      "Epoch: 691/2000... Training loss: 0.6003\n",
      "Epoch: 691/2000... Training loss: 0.7937\n",
      "Epoch: 691/2000... Training loss: 0.7141\n",
      "Epoch: 691/2000... Training loss: 0.7133\n",
      "Epoch: 692/2000... Training loss: 0.8519\n",
      "Epoch: 692/2000... Training loss: 0.8566\n",
      "Epoch: 692/2000... Training loss: 0.7926\n",
      "Epoch: 692/2000... Training loss: 0.7013\n",
      "Epoch: 692/2000... Training loss: 0.8210\n",
      "Epoch: 692/2000... Training loss: 0.8589\n",
      "Epoch: 692/2000... Training loss: 0.7173\n",
      "Epoch: 692/2000... Training loss: 0.6843\n",
      "Epoch: 692/2000... Training loss: 0.7065\n",
      "Epoch: 692/2000... Training loss: 0.5184\n",
      "Epoch: 692/2000... Training loss: 0.8993\n",
      "Epoch: 692/2000... Training loss: 0.5857\n",
      "Epoch: 692/2000... Training loss: 0.8218\n",
      "Epoch: 692/2000... Training loss: 0.7815\n",
      "Epoch: 692/2000... Training loss: 0.8792\n",
      "Epoch: 692/2000... Training loss: 0.8348\n",
      "Epoch: 692/2000... Training loss: 0.9280\n",
      "Epoch: 692/2000... Training loss: 0.7963\n",
      "Epoch: 692/2000... Training loss: 0.8174\n",
      "Epoch: 692/2000... Training loss: 0.9880\n",
      "Epoch: 692/2000... Training loss: 0.7362\n",
      "Epoch: 692/2000... Training loss: 0.7716\n",
      "Epoch: 692/2000... Training loss: 0.7005\n",
      "Epoch: 692/2000... Training loss: 0.8639\n",
      "Epoch: 692/2000... Training loss: 0.7229\n",
      "Epoch: 692/2000... Training loss: 1.0259\n",
      "Epoch: 692/2000... Training loss: 0.7695\n",
      "Epoch: 692/2000... Training loss: 0.8248\n",
      "Epoch: 692/2000... Training loss: 0.6948\n",
      "Epoch: 692/2000... Training loss: 0.7087\n",
      "Epoch: 692/2000... Training loss: 0.8176\n",
      "Epoch: 693/2000... Training loss: 1.1054\n",
      "Epoch: 693/2000... Training loss: 0.7303\n",
      "Epoch: 693/2000... Training loss: 0.7447\n",
      "Epoch: 693/2000... Training loss: 0.7229\n",
      "Epoch: 693/2000... Training loss: 0.7213\n",
      "Epoch: 693/2000... Training loss: 0.6747\n",
      "Epoch: 693/2000... Training loss: 0.7649\n",
      "Epoch: 693/2000... Training loss: 0.9914\n",
      "Epoch: 693/2000... Training loss: 0.7968\n",
      "Epoch: 693/2000... Training loss: 0.8375\n",
      "Epoch: 693/2000... Training loss: 0.8157\n",
      "Epoch: 693/2000... Training loss: 0.7087\n",
      "Epoch: 693/2000... Training loss: 0.8688\n",
      "Epoch: 693/2000... Training loss: 0.7048\n",
      "Epoch: 693/2000... Training loss: 0.7614\n",
      "Epoch: 693/2000... Training loss: 0.8315\n",
      "Epoch: 693/2000... Training loss: 0.6296\n",
      "Epoch: 693/2000... Training loss: 0.7300\n",
      "Epoch: 693/2000... Training loss: 0.7800\n",
      "Epoch: 693/2000... Training loss: 0.8119\n",
      "Epoch: 693/2000... Training loss: 0.8707\n",
      "Epoch: 693/2000... Training loss: 0.8318\n",
      "Epoch: 693/2000... Training loss: 0.8462\n",
      "Epoch: 693/2000... Training loss: 0.6823\n",
      "Epoch: 693/2000... Training loss: 0.7291\n",
      "Epoch: 693/2000... Training loss: 0.9608\n",
      "Epoch: 693/2000... Training loss: 0.9047\n",
      "Epoch: 693/2000... Training loss: 0.6502\n",
      "Epoch: 693/2000... Training loss: 0.7573\n",
      "Epoch: 693/2000... Training loss: 0.9341\n",
      "Epoch: 693/2000... Training loss: 0.8404\n",
      "Epoch: 694/2000... Training loss: 0.8079\n",
      "Epoch: 694/2000... Training loss: 0.6140\n",
      "Epoch: 694/2000... Training loss: 0.7365\n",
      "Epoch: 694/2000... Training loss: 0.6979\n",
      "Epoch: 694/2000... Training loss: 0.8992\n",
      "Epoch: 694/2000... Training loss: 0.7672\n",
      "Epoch: 694/2000... Training loss: 0.6150\n",
      "Epoch: 694/2000... Training loss: 0.7215\n",
      "Epoch: 694/2000... Training loss: 0.6125\n",
      "Epoch: 694/2000... Training loss: 0.8689\n",
      "Epoch: 694/2000... Training loss: 1.0437\n",
      "Epoch: 694/2000... Training loss: 1.0935\n",
      "Epoch: 694/2000... Training loss: 0.7096\n",
      "Epoch: 694/2000... Training loss: 0.7901\n",
      "Epoch: 694/2000... Training loss: 0.6016\n",
      "Epoch: 694/2000... Training loss: 0.7654\n",
      "Epoch: 694/2000... Training loss: 1.0690\n",
      "Epoch: 694/2000... Training loss: 0.7962\n",
      "Epoch: 694/2000... Training loss: 0.7216\n",
      "Epoch: 694/2000... Training loss: 0.8684\n",
      "Epoch: 694/2000... Training loss: 0.7419\n",
      "Epoch: 694/2000... Training loss: 0.5380\n",
      "Epoch: 694/2000... Training loss: 0.5631\n",
      "Epoch: 694/2000... Training loss: 0.6292\n",
      "Epoch: 694/2000... Training loss: 0.7717\n",
      "Epoch: 694/2000... Training loss: 0.7654\n",
      "Epoch: 694/2000... Training loss: 0.7880\n",
      "Epoch: 694/2000... Training loss: 0.8703\n",
      "Epoch: 694/2000... Training loss: 0.7666\n",
      "Epoch: 694/2000... Training loss: 0.8059\n",
      "Epoch: 694/2000... Training loss: 0.7801\n",
      "Epoch: 695/2000... Training loss: 0.7756\n",
      "Epoch: 695/2000... Training loss: 0.6629\n",
      "Epoch: 695/2000... Training loss: 0.6682\n",
      "Epoch: 695/2000... Training loss: 0.6119\n",
      "Epoch: 695/2000... Training loss: 0.6837\n",
      "Epoch: 695/2000... Training loss: 0.7566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 695/2000... Training loss: 0.7325\n",
      "Epoch: 695/2000... Training loss: 0.7589\n",
      "Epoch: 695/2000... Training loss: 0.6877\n",
      "Epoch: 695/2000... Training loss: 0.6611\n",
      "Epoch: 695/2000... Training loss: 0.8040\n",
      "Epoch: 695/2000... Training loss: 0.7103\n",
      "Epoch: 695/2000... Training loss: 0.7900\n",
      "Epoch: 695/2000... Training loss: 0.6519\n",
      "Epoch: 695/2000... Training loss: 0.8298\n",
      "Epoch: 695/2000... Training loss: 0.6956\n",
      "Epoch: 695/2000... Training loss: 0.6582\n",
      "Epoch: 695/2000... Training loss: 0.5413\n",
      "Epoch: 695/2000... Training loss: 0.8590\n",
      "Epoch: 695/2000... Training loss: 0.7137\n",
      "Epoch: 695/2000... Training loss: 0.7526\n",
      "Epoch: 695/2000... Training loss: 0.6138\n",
      "Epoch: 695/2000... Training loss: 0.7426\n",
      "Epoch: 695/2000... Training loss: 0.7317\n",
      "Epoch: 695/2000... Training loss: 0.7185\n",
      "Epoch: 695/2000... Training loss: 0.4907\n",
      "Epoch: 695/2000... Training loss: 0.5920\n",
      "Epoch: 695/2000... Training loss: 0.6244\n",
      "Epoch: 695/2000... Training loss: 0.9776\n",
      "Epoch: 695/2000... Training loss: 0.5134\n",
      "Epoch: 695/2000... Training loss: 0.7345\n",
      "Epoch: 696/2000... Training loss: 0.8613\n",
      "Epoch: 696/2000... Training loss: 0.5794\n",
      "Epoch: 696/2000... Training loss: 0.7135\n",
      "Epoch: 696/2000... Training loss: 0.7459\n",
      "Epoch: 696/2000... Training loss: 0.7036\n",
      "Epoch: 696/2000... Training loss: 0.5375\n",
      "Epoch: 696/2000... Training loss: 0.8043\n",
      "Epoch: 696/2000... Training loss: 0.7472\n",
      "Epoch: 696/2000... Training loss: 0.8428\n",
      "Epoch: 696/2000... Training loss: 0.7322\n",
      "Epoch: 696/2000... Training loss: 0.6399\n",
      "Epoch: 696/2000... Training loss: 0.8896\n",
      "Epoch: 696/2000... Training loss: 0.8788\n",
      "Epoch: 696/2000... Training loss: 0.7430\n",
      "Epoch: 696/2000... Training loss: 0.7275\n",
      "Epoch: 696/2000... Training loss: 0.8260\n",
      "Epoch: 696/2000... Training loss: 0.7859\n",
      "Epoch: 696/2000... Training loss: 0.8581\n",
      "Epoch: 696/2000... Training loss: 0.6693\n",
      "Epoch: 696/2000... Training loss: 0.6388\n",
      "Epoch: 696/2000... Training loss: 0.6507\n",
      "Epoch: 696/2000... Training loss: 0.9521\n",
      "Epoch: 696/2000... Training loss: 0.6535\n",
      "Epoch: 696/2000... Training loss: 0.9274\n",
      "Epoch: 696/2000... Training loss: 0.9415\n",
      "Epoch: 696/2000... Training loss: 0.8910\n",
      "Epoch: 696/2000... Training loss: 0.7427\n",
      "Epoch: 696/2000... Training loss: 0.8477\n",
      "Epoch: 696/2000... Training loss: 0.5693\n",
      "Epoch: 696/2000... Training loss: 0.7128\n",
      "Epoch: 696/2000... Training loss: 0.6994\n",
      "Epoch: 697/2000... Training loss: 0.6235\n",
      "Epoch: 697/2000... Training loss: 0.7854\n",
      "Epoch: 697/2000... Training loss: 0.8422\n",
      "Epoch: 697/2000... Training loss: 0.7500\n",
      "Epoch: 697/2000... Training loss: 0.6985\n",
      "Epoch: 697/2000... Training loss: 0.7693\n",
      "Epoch: 697/2000... Training loss: 0.7850\n",
      "Epoch: 697/2000... Training loss: 0.7948\n",
      "Epoch: 697/2000... Training loss: 0.7513\n",
      "Epoch: 697/2000... Training loss: 0.7601\n",
      "Epoch: 697/2000... Training loss: 0.5247\n",
      "Epoch: 697/2000... Training loss: 0.8456\n",
      "Epoch: 697/2000... Training loss: 0.8668\n",
      "Epoch: 697/2000... Training loss: 1.1331\n",
      "Epoch: 697/2000... Training loss: 0.5813\n",
      "Epoch: 697/2000... Training loss: 0.9503\n",
      "Epoch: 697/2000... Training loss: 0.6193\n",
      "Epoch: 697/2000... Training loss: 0.6288\n",
      "Epoch: 697/2000... Training loss: 0.7550\n",
      "Epoch: 697/2000... Training loss: 0.7937\n",
      "Epoch: 697/2000... Training loss: 0.7688\n",
      "Epoch: 697/2000... Training loss: 0.5569\n",
      "Epoch: 697/2000... Training loss: 0.9839\n",
      "Epoch: 697/2000... Training loss: 0.9091\n",
      "Epoch: 697/2000... Training loss: 0.8421\n",
      "Epoch: 697/2000... Training loss: 0.8150\n",
      "Epoch: 697/2000... Training loss: 0.8204\n",
      "Epoch: 697/2000... Training loss: 0.7842\n",
      "Epoch: 697/2000... Training loss: 0.6270\n",
      "Epoch: 697/2000... Training loss: 0.7092\n",
      "Epoch: 697/2000... Training loss: 1.0658\n",
      "Epoch: 698/2000... Training loss: 0.8771\n",
      "Epoch: 698/2000... Training loss: 0.6121\n",
      "Epoch: 698/2000... Training loss: 0.7877\n",
      "Epoch: 698/2000... Training loss: 0.6436\n",
      "Epoch: 698/2000... Training loss: 0.6707\n",
      "Epoch: 698/2000... Training loss: 0.7182\n",
      "Epoch: 698/2000... Training loss: 0.8076\n",
      "Epoch: 698/2000... Training loss: 0.6923\n",
      "Epoch: 698/2000... Training loss: 0.7651\n",
      "Epoch: 698/2000... Training loss: 0.6419\n",
      "Epoch: 698/2000... Training loss: 0.4844\n",
      "Epoch: 698/2000... Training loss: 0.7664\n",
      "Epoch: 698/2000... Training loss: 0.4947\n",
      "Epoch: 698/2000... Training loss: 0.6543\n",
      "Epoch: 698/2000... Training loss: 0.8681\n",
      "Epoch: 698/2000... Training loss: 0.8886\n",
      "Epoch: 698/2000... Training loss: 0.7161\n",
      "Epoch: 698/2000... Training loss: 0.9040\n",
      "Epoch: 698/2000... Training loss: 0.6929\n",
      "Epoch: 698/2000... Training loss: 0.7909\n",
      "Epoch: 698/2000... Training loss: 0.7456\n",
      "Epoch: 698/2000... Training loss: 0.6360\n",
      "Epoch: 698/2000... Training loss: 0.9692\n",
      "Epoch: 698/2000... Training loss: 0.8476\n",
      "Epoch: 698/2000... Training loss: 0.8594\n",
      "Epoch: 698/2000... Training loss: 0.6036\n",
      "Epoch: 698/2000... Training loss: 0.7298\n",
      "Epoch: 698/2000... Training loss: 0.6903\n",
      "Epoch: 698/2000... Training loss: 0.6612\n",
      "Epoch: 698/2000... Training loss: 0.6925\n",
      "Epoch: 698/2000... Training loss: 1.0191\n",
      "Epoch: 699/2000... Training loss: 0.7643\n",
      "Epoch: 699/2000... Training loss: 0.7723\n",
      "Epoch: 699/2000... Training loss: 0.8461\n",
      "Epoch: 699/2000... Training loss: 0.8226\n",
      "Epoch: 699/2000... Training loss: 0.7840\n",
      "Epoch: 699/2000... Training loss: 0.9348\n",
      "Epoch: 699/2000... Training loss: 0.7552\n",
      "Epoch: 699/2000... Training loss: 0.7935\n",
      "Epoch: 699/2000... Training loss: 1.1427\n",
      "Epoch: 699/2000... Training loss: 0.7887\n",
      "Epoch: 699/2000... Training loss: 0.9651\n",
      "Epoch: 699/2000... Training loss: 0.7246\n",
      "Epoch: 699/2000... Training loss: 0.7446\n",
      "Epoch: 699/2000... Training loss: 0.7235\n",
      "Epoch: 699/2000... Training loss: 0.6648\n",
      "Epoch: 699/2000... Training loss: 0.6670\n",
      "Epoch: 699/2000... Training loss: 0.8070\n",
      "Epoch: 699/2000... Training loss: 0.5658\n",
      "Epoch: 699/2000... Training loss: 0.7396\n",
      "Epoch: 699/2000... Training loss: 1.0105\n",
      "Epoch: 699/2000... Training loss: 0.7864\n",
      "Epoch: 699/2000... Training loss: 0.9436\n",
      "Epoch: 699/2000... Training loss: 0.6583\n",
      "Epoch: 699/2000... Training loss: 0.7687\n",
      "Epoch: 699/2000... Training loss: 0.9284\n",
      "Epoch: 699/2000... Training loss: 0.6589\n",
      "Epoch: 699/2000... Training loss: 1.0715\n",
      "Epoch: 699/2000... Training loss: 0.5902\n",
      "Epoch: 699/2000... Training loss: 0.8776\n",
      "Epoch: 699/2000... Training loss: 0.7394\n",
      "Epoch: 699/2000... Training loss: 1.0486\n",
      "Epoch: 700/2000... Training loss: 0.9203\n",
      "Epoch: 700/2000... Training loss: 0.8774\n",
      "Epoch: 700/2000... Training loss: 0.8549\n",
      "Epoch: 700/2000... Training loss: 0.7890\n",
      "Epoch: 700/2000... Training loss: 0.5345\n",
      "Epoch: 700/2000... Training loss: 0.6065\n",
      "Epoch: 700/2000... Training loss: 1.1543\n",
      "Epoch: 700/2000... Training loss: 0.7429\n",
      "Epoch: 700/2000... Training loss: 0.8450\n",
      "Epoch: 700/2000... Training loss: 0.8411\n",
      "Epoch: 700/2000... Training loss: 0.8214\n",
      "Epoch: 700/2000... Training loss: 0.8685\n",
      "Epoch: 700/2000... Training loss: 0.8184\n",
      "Epoch: 700/2000... Training loss: 0.6394\n",
      "Epoch: 700/2000... Training loss: 0.8283\n",
      "Epoch: 700/2000... Training loss: 0.6773\n",
      "Epoch: 700/2000... Training loss: 0.7477\n",
      "Epoch: 700/2000... Training loss: 0.7605\n",
      "Epoch: 700/2000... Training loss: 0.7789\n",
      "Epoch: 700/2000... Training loss: 0.6697\n",
      "Epoch: 700/2000... Training loss: 0.7384\n",
      "Epoch: 700/2000... Training loss: 0.7126\n",
      "Epoch: 700/2000... Training loss: 0.7350\n",
      "Epoch: 700/2000... Training loss: 0.8041\n",
      "Epoch: 700/2000... Training loss: 0.5707\n",
      "Epoch: 700/2000... Training loss: 0.9701\n",
      "Epoch: 700/2000... Training loss: 0.7545\n",
      "Epoch: 700/2000... Training loss: 0.6161\n",
      "Epoch: 700/2000... Training loss: 0.7682\n",
      "Epoch: 700/2000... Training loss: 0.8008\n",
      "Epoch: 700/2000... Training loss: 0.7836\n",
      "Epoch: 701/2000... Training loss: 0.8491\n",
      "Epoch: 701/2000... Training loss: 0.8081\n",
      "Epoch: 701/2000... Training loss: 0.6395\n",
      "Epoch: 701/2000... Training loss: 0.6905\n",
      "Epoch: 701/2000... Training loss: 0.6376\n",
      "Epoch: 701/2000... Training loss: 0.7944\n",
      "Epoch: 701/2000... Training loss: 0.6237\n",
      "Epoch: 701/2000... Training loss: 0.6749\n",
      "Epoch: 701/2000... Training loss: 0.8511\n",
      "Epoch: 701/2000... Training loss: 0.6058\n",
      "Epoch: 701/2000... Training loss: 1.2110\n",
      "Epoch: 701/2000... Training loss: 0.8957\n",
      "Epoch: 701/2000... Training loss: 0.7977\n",
      "Epoch: 701/2000... Training loss: 0.8562\n",
      "Epoch: 701/2000... Training loss: 0.6696\n",
      "Epoch: 701/2000... Training loss: 0.6246\n",
      "Epoch: 701/2000... Training loss: 0.6276\n",
      "Epoch: 701/2000... Training loss: 0.9193\n",
      "Epoch: 701/2000... Training loss: 0.6771\n",
      "Epoch: 701/2000... Training loss: 0.8695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 701/2000... Training loss: 0.9852\n",
      "Epoch: 701/2000... Training loss: 0.7222\n",
      "Epoch: 701/2000... Training loss: 0.8030\n",
      "Epoch: 701/2000... Training loss: 0.7110\n",
      "Epoch: 701/2000... Training loss: 0.7016\n",
      "Epoch: 701/2000... Training loss: 0.6003\n",
      "Epoch: 701/2000... Training loss: 0.9491\n",
      "Epoch: 701/2000... Training loss: 0.7130\n",
      "Epoch: 701/2000... Training loss: 0.7210\n",
      "Epoch: 701/2000... Training loss: 0.8442\n",
      "Epoch: 701/2000... Training loss: 0.9516\n",
      "Epoch: 702/2000... Training loss: 0.8117\n",
      "Epoch: 702/2000... Training loss: 0.7796\n",
      "Epoch: 702/2000... Training loss: 0.6237\n",
      "Epoch: 702/2000... Training loss: 0.7330\n",
      "Epoch: 702/2000... Training loss: 0.8954\n",
      "Epoch: 702/2000... Training loss: 0.5715\n",
      "Epoch: 702/2000... Training loss: 0.7969\n",
      "Epoch: 702/2000... Training loss: 0.7984\n",
      "Epoch: 702/2000... Training loss: 0.8931\n",
      "Epoch: 702/2000... Training loss: 0.8211\n",
      "Epoch: 702/2000... Training loss: 0.7862\n",
      "Epoch: 702/2000... Training loss: 0.6742\n",
      "Epoch: 702/2000... Training loss: 0.7736\n",
      "Epoch: 702/2000... Training loss: 0.6938\n",
      "Epoch: 702/2000... Training loss: 0.6572\n",
      "Epoch: 702/2000... Training loss: 0.7280\n",
      "Epoch: 702/2000... Training loss: 0.7615\n",
      "Epoch: 702/2000... Training loss: 0.6982\n",
      "Epoch: 702/2000... Training loss: 0.8760\n",
      "Epoch: 702/2000... Training loss: 0.6506\n",
      "Epoch: 702/2000... Training loss: 0.6811\n",
      "Epoch: 702/2000... Training loss: 0.6360\n",
      "Epoch: 702/2000... Training loss: 0.7036\n",
      "Epoch: 702/2000... Training loss: 0.6261\n",
      "Epoch: 702/2000... Training loss: 0.6449\n",
      "Epoch: 702/2000... Training loss: 0.7418\n",
      "Epoch: 702/2000... Training loss: 0.7393\n",
      "Epoch: 702/2000... Training loss: 0.6610\n",
      "Epoch: 702/2000... Training loss: 0.7715\n",
      "Epoch: 702/2000... Training loss: 0.6619\n",
      "Epoch: 702/2000... Training loss: 0.7148\n",
      "Epoch: 703/2000... Training loss: 0.6995\n",
      "Epoch: 703/2000... Training loss: 0.7362\n",
      "Epoch: 703/2000... Training loss: 0.8092\n",
      "Epoch: 703/2000... Training loss: 0.7270\n",
      "Epoch: 703/2000... Training loss: 0.7481\n",
      "Epoch: 703/2000... Training loss: 0.8659\n",
      "Epoch: 703/2000... Training loss: 0.8520\n",
      "Epoch: 703/2000... Training loss: 0.6647\n",
      "Epoch: 703/2000... Training loss: 0.7002\n",
      "Epoch: 703/2000... Training loss: 0.7709\n",
      "Epoch: 703/2000... Training loss: 0.5984\n",
      "Epoch: 703/2000... Training loss: 0.6572\n",
      "Epoch: 703/2000... Training loss: 0.8254\n",
      "Epoch: 703/2000... Training loss: 0.7654\n",
      "Epoch: 703/2000... Training loss: 0.6725\n",
      "Epoch: 703/2000... Training loss: 0.7629\n",
      "Epoch: 703/2000... Training loss: 0.6668\n",
      "Epoch: 703/2000... Training loss: 0.8322\n",
      "Epoch: 703/2000... Training loss: 0.8154\n",
      "Epoch: 703/2000... Training loss: 0.7347\n",
      "Epoch: 703/2000... Training loss: 0.6565\n",
      "Epoch: 703/2000... Training loss: 0.4745\n",
      "Epoch: 703/2000... Training loss: 0.8178\n",
      "Epoch: 703/2000... Training loss: 0.8871\n",
      "Epoch: 703/2000... Training loss: 0.7080\n",
      "Epoch: 703/2000... Training loss: 0.6769\n",
      "Epoch: 703/2000... Training loss: 0.8571\n",
      "Epoch: 703/2000... Training loss: 0.5703\n",
      "Epoch: 703/2000... Training loss: 0.7846\n",
      "Epoch: 703/2000... Training loss: 0.8008\n",
      "Epoch: 703/2000... Training loss: 0.7294\n",
      "Epoch: 704/2000... Training loss: 0.8061\n",
      "Epoch: 704/2000... Training loss: 0.6625\n",
      "Epoch: 704/2000... Training loss: 0.6543\n",
      "Epoch: 704/2000... Training loss: 0.6529\n",
      "Epoch: 704/2000... Training loss: 0.7656\n",
      "Epoch: 704/2000... Training loss: 0.6709\n",
      "Epoch: 704/2000... Training loss: 0.7887\n",
      "Epoch: 704/2000... Training loss: 0.7895\n",
      "Epoch: 704/2000... Training loss: 0.8946\n",
      "Epoch: 704/2000... Training loss: 0.8271\n",
      "Epoch: 704/2000... Training loss: 0.7064\n",
      "Epoch: 704/2000... Training loss: 0.8510\n",
      "Epoch: 704/2000... Training loss: 0.7631\n",
      "Epoch: 704/2000... Training loss: 0.6812\n",
      "Epoch: 704/2000... Training loss: 0.8037\n",
      "Epoch: 704/2000... Training loss: 0.5724\n",
      "Epoch: 704/2000... Training loss: 0.8380\n",
      "Epoch: 704/2000... Training loss: 0.8140\n",
      "Epoch: 704/2000... Training loss: 0.8015\n",
      "Epoch: 704/2000... Training loss: 0.6895\n",
      "Epoch: 704/2000... Training loss: 0.8310\n",
      "Epoch: 704/2000... Training loss: 0.7197\n",
      "Epoch: 704/2000... Training loss: 0.7474\n",
      "Epoch: 704/2000... Training loss: 0.6677\n",
      "Epoch: 704/2000... Training loss: 0.8637\n",
      "Epoch: 704/2000... Training loss: 0.7230\n",
      "Epoch: 704/2000... Training loss: 0.8531\n",
      "Epoch: 704/2000... Training loss: 0.7822\n",
      "Epoch: 704/2000... Training loss: 0.7663\n",
      "Epoch: 704/2000... Training loss: 0.7345\n",
      "Epoch: 704/2000... Training loss: 0.7107\n",
      "Epoch: 705/2000... Training loss: 0.6800\n",
      "Epoch: 705/2000... Training loss: 0.6115\n",
      "Epoch: 705/2000... Training loss: 0.8748\n",
      "Epoch: 705/2000... Training loss: 0.7717\n",
      "Epoch: 705/2000... Training loss: 0.9962\n",
      "Epoch: 705/2000... Training loss: 0.6132\n",
      "Epoch: 705/2000... Training loss: 0.7649\n",
      "Epoch: 705/2000... Training loss: 0.7591\n",
      "Epoch: 705/2000... Training loss: 0.6635\n",
      "Epoch: 705/2000... Training loss: 0.8453\n",
      "Epoch: 705/2000... Training loss: 0.8511\n",
      "Epoch: 705/2000... Training loss: 0.7403\n",
      "Epoch: 705/2000... Training loss: 0.5779\n",
      "Epoch: 705/2000... Training loss: 0.7973\n",
      "Epoch: 705/2000... Training loss: 0.6155\n",
      "Epoch: 705/2000... Training loss: 0.9026\n",
      "Epoch: 705/2000... Training loss: 0.8389\n",
      "Epoch: 705/2000... Training loss: 0.6681\n",
      "Epoch: 705/2000... Training loss: 0.6781\n",
      "Epoch: 705/2000... Training loss: 0.8298\n",
      "Epoch: 705/2000... Training loss: 0.7064\n",
      "Epoch: 705/2000... Training loss: 0.7960\n",
      "Epoch: 705/2000... Training loss: 0.6783\n",
      "Epoch: 705/2000... Training loss: 0.6975\n",
      "Epoch: 705/2000... Training loss: 0.7435\n",
      "Epoch: 705/2000... Training loss: 0.8555\n",
      "Epoch: 705/2000... Training loss: 0.6801\n",
      "Epoch: 705/2000... Training loss: 0.7806\n",
      "Epoch: 705/2000... Training loss: 0.7586\n",
      "Epoch: 705/2000... Training loss: 0.6586\n",
      "Epoch: 705/2000... Training loss: 0.7554\n",
      "Epoch: 706/2000... Training loss: 0.8678\n",
      "Epoch: 706/2000... Training loss: 0.7375\n",
      "Epoch: 706/2000... Training loss: 0.8113\n",
      "Epoch: 706/2000... Training loss: 0.7634\n",
      "Epoch: 706/2000... Training loss: 0.7465\n",
      "Epoch: 706/2000... Training loss: 0.7316\n",
      "Epoch: 706/2000... Training loss: 0.7511\n",
      "Epoch: 706/2000... Training loss: 0.6528\n",
      "Epoch: 706/2000... Training loss: 0.7555\n",
      "Epoch: 706/2000... Training loss: 0.9314\n",
      "Epoch: 706/2000... Training loss: 0.6060\n",
      "Epoch: 706/2000... Training loss: 0.7906\n",
      "Epoch: 706/2000... Training loss: 0.5571\n",
      "Epoch: 706/2000... Training loss: 0.7982\n",
      "Epoch: 706/2000... Training loss: 0.9979\n",
      "Epoch: 706/2000... Training loss: 0.6136\n",
      "Epoch: 706/2000... Training loss: 0.6669\n",
      "Epoch: 706/2000... Training loss: 0.7047\n",
      "Epoch: 706/2000... Training loss: 0.6160\n",
      "Epoch: 706/2000... Training loss: 0.7774\n",
      "Epoch: 706/2000... Training loss: 0.6646\n",
      "Epoch: 706/2000... Training loss: 1.0947\n",
      "Epoch: 706/2000... Training loss: 0.7288\n",
      "Epoch: 706/2000... Training loss: 0.7792\n",
      "Epoch: 706/2000... Training loss: 0.7136\n",
      "Epoch: 706/2000... Training loss: 0.6857\n",
      "Epoch: 706/2000... Training loss: 0.9591\n",
      "Epoch: 706/2000... Training loss: 0.5773\n",
      "Epoch: 706/2000... Training loss: 0.7040\n",
      "Epoch: 706/2000... Training loss: 0.6428\n",
      "Epoch: 706/2000... Training loss: 0.7362\n",
      "Epoch: 707/2000... Training loss: 0.9295\n",
      "Epoch: 707/2000... Training loss: 0.6738\n",
      "Epoch: 707/2000... Training loss: 0.8719\n",
      "Epoch: 707/2000... Training loss: 0.6630\n",
      "Epoch: 707/2000... Training loss: 0.7746\n",
      "Epoch: 707/2000... Training loss: 0.8242\n",
      "Epoch: 707/2000... Training loss: 0.7919\n",
      "Epoch: 707/2000... Training loss: 0.6467\n",
      "Epoch: 707/2000... Training loss: 0.9215\n",
      "Epoch: 707/2000... Training loss: 0.6643\n",
      "Epoch: 707/2000... Training loss: 0.7323\n",
      "Epoch: 707/2000... Training loss: 0.8598\n",
      "Epoch: 707/2000... Training loss: 0.5959\n",
      "Epoch: 707/2000... Training loss: 0.7468\n",
      "Epoch: 707/2000... Training loss: 0.6945\n",
      "Epoch: 707/2000... Training loss: 0.7362\n",
      "Epoch: 707/2000... Training loss: 0.8595\n",
      "Epoch: 707/2000... Training loss: 0.6590\n",
      "Epoch: 707/2000... Training loss: 0.6015\n",
      "Epoch: 707/2000... Training loss: 0.8983\n",
      "Epoch: 707/2000... Training loss: 0.6443\n",
      "Epoch: 707/2000... Training loss: 0.8698\n",
      "Epoch: 707/2000... Training loss: 0.7994\n",
      "Epoch: 707/2000... Training loss: 0.5849\n",
      "Epoch: 707/2000... Training loss: 0.7637\n",
      "Epoch: 707/2000... Training loss: 0.6788\n",
      "Epoch: 707/2000... Training loss: 0.8583\n",
      "Epoch: 707/2000... Training loss: 0.7606\n",
      "Epoch: 707/2000... Training loss: 0.5602\n",
      "Epoch: 707/2000... Training loss: 0.5588\n",
      "Epoch: 707/2000... Training loss: 1.0965\n",
      "Epoch: 708/2000... Training loss: 0.7196\n",
      "Epoch: 708/2000... Training loss: 0.6516\n",
      "Epoch: 708/2000... Training loss: 0.9456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 708/2000... Training loss: 0.9510\n",
      "Epoch: 708/2000... Training loss: 0.6891\n",
      "Epoch: 708/2000... Training loss: 0.7370\n",
      "Epoch: 708/2000... Training loss: 0.7710\n",
      "Epoch: 708/2000... Training loss: 0.7747\n",
      "Epoch: 708/2000... Training loss: 0.7158\n",
      "Epoch: 708/2000... Training loss: 0.7902\n",
      "Epoch: 708/2000... Training loss: 0.8022\n",
      "Epoch: 708/2000... Training loss: 0.6707\n",
      "Epoch: 708/2000... Training loss: 0.9138\n",
      "Epoch: 708/2000... Training loss: 0.9744\n",
      "Epoch: 708/2000... Training loss: 0.8749\n",
      "Epoch: 708/2000... Training loss: 0.8941\n",
      "Epoch: 708/2000... Training loss: 0.6253\n",
      "Epoch: 708/2000... Training loss: 0.8255\n",
      "Epoch: 708/2000... Training loss: 0.6955\n",
      "Epoch: 708/2000... Training loss: 0.5841\n",
      "Epoch: 708/2000... Training loss: 0.6912\n",
      "Epoch: 708/2000... Training loss: 0.7557\n",
      "Epoch: 708/2000... Training loss: 0.7258\n",
      "Epoch: 708/2000... Training loss: 0.8498\n",
      "Epoch: 708/2000... Training loss: 0.7885\n",
      "Epoch: 708/2000... Training loss: 0.8206\n",
      "Epoch: 708/2000... Training loss: 0.6865\n",
      "Epoch: 708/2000... Training loss: 0.6846\n",
      "Epoch: 708/2000... Training loss: 0.8392\n",
      "Epoch: 708/2000... Training loss: 0.6955\n",
      "Epoch: 708/2000... Training loss: 0.7371\n",
      "Epoch: 709/2000... Training loss: 0.7578\n",
      "Epoch: 709/2000... Training loss: 0.8171\n",
      "Epoch: 709/2000... Training loss: 0.9584\n",
      "Epoch: 709/2000... Training loss: 0.6735\n",
      "Epoch: 709/2000... Training loss: 0.6793\n",
      "Epoch: 709/2000... Training loss: 0.8093\n",
      "Epoch: 709/2000... Training loss: 0.6790\n",
      "Epoch: 709/2000... Training loss: 0.7396\n",
      "Epoch: 709/2000... Training loss: 0.7352\n",
      "Epoch: 709/2000... Training loss: 0.8000\n",
      "Epoch: 709/2000... Training loss: 0.8002\n",
      "Epoch: 709/2000... Training loss: 0.6625\n",
      "Epoch: 709/2000... Training loss: 1.0685\n",
      "Epoch: 709/2000... Training loss: 0.5471\n",
      "Epoch: 709/2000... Training loss: 0.7534\n",
      "Epoch: 709/2000... Training loss: 0.6688\n",
      "Epoch: 709/2000... Training loss: 0.8650\n",
      "Epoch: 709/2000... Training loss: 0.7392\n",
      "Epoch: 709/2000... Training loss: 0.7992\n",
      "Epoch: 709/2000... Training loss: 0.7129\n",
      "Epoch: 709/2000... Training loss: 0.9560\n",
      "Epoch: 709/2000... Training loss: 0.6313\n",
      "Epoch: 709/2000... Training loss: 0.6772\n",
      "Epoch: 709/2000... Training loss: 0.5681\n",
      "Epoch: 709/2000... Training loss: 0.6605\n",
      "Epoch: 709/2000... Training loss: 0.8718\n",
      "Epoch: 709/2000... Training loss: 0.6687\n",
      "Epoch: 709/2000... Training loss: 0.7670\n",
      "Epoch: 709/2000... Training loss: 0.5913\n",
      "Epoch: 709/2000... Training loss: 0.6941\n",
      "Epoch: 709/2000... Training loss: 0.8019\n",
      "Epoch: 710/2000... Training loss: 0.7357\n",
      "Epoch: 710/2000... Training loss: 0.6456\n",
      "Epoch: 710/2000... Training loss: 0.8642\n",
      "Epoch: 710/2000... Training loss: 0.9776\n",
      "Epoch: 710/2000... Training loss: 0.8784\n",
      "Epoch: 710/2000... Training loss: 0.7072\n",
      "Epoch: 710/2000... Training loss: 0.7352\n",
      "Epoch: 710/2000... Training loss: 0.6164\n",
      "Epoch: 710/2000... Training loss: 0.4901\n",
      "Epoch: 710/2000... Training loss: 0.6748\n",
      "Epoch: 710/2000... Training loss: 0.8550\n",
      "Epoch: 710/2000... Training loss: 0.9710\n",
      "Epoch: 710/2000... Training loss: 0.5439\n",
      "Epoch: 710/2000... Training loss: 0.9413\n",
      "Epoch: 710/2000... Training loss: 0.6987\n",
      "Epoch: 710/2000... Training loss: 0.5387\n",
      "Epoch: 710/2000... Training loss: 0.6982\n",
      "Epoch: 710/2000... Training loss: 0.6751\n",
      "Epoch: 710/2000... Training loss: 0.7225\n",
      "Epoch: 710/2000... Training loss: 0.4727\n",
      "Epoch: 710/2000... Training loss: 0.7395\n",
      "Epoch: 710/2000... Training loss: 0.5408\n",
      "Epoch: 710/2000... Training loss: 0.5440\n",
      "Epoch: 710/2000... Training loss: 0.7242\n",
      "Epoch: 710/2000... Training loss: 0.6729\n",
      "Epoch: 710/2000... Training loss: 0.6771\n",
      "Epoch: 710/2000... Training loss: 0.7765\n",
      "Epoch: 710/2000... Training loss: 1.1036\n",
      "Epoch: 710/2000... Training loss: 0.6704\n",
      "Epoch: 710/2000... Training loss: 0.5872\n",
      "Epoch: 710/2000... Training loss: 0.7497\n",
      "Epoch: 711/2000... Training loss: 0.6377\n",
      "Epoch: 711/2000... Training loss: 0.7067\n",
      "Epoch: 711/2000... Training loss: 0.7744\n",
      "Epoch: 711/2000... Training loss: 0.6922\n",
      "Epoch: 711/2000... Training loss: 0.6591\n",
      "Epoch: 711/2000... Training loss: 0.6091\n",
      "Epoch: 711/2000... Training loss: 0.7206\n",
      "Epoch: 711/2000... Training loss: 0.7174\n",
      "Epoch: 711/2000... Training loss: 0.9008\n",
      "Epoch: 711/2000... Training loss: 0.7818\n",
      "Epoch: 711/2000... Training loss: 0.7825\n",
      "Epoch: 711/2000... Training loss: 0.7095\n",
      "Epoch: 711/2000... Training loss: 0.7516\n",
      "Epoch: 711/2000... Training loss: 0.9055\n",
      "Epoch: 711/2000... Training loss: 0.7501\n",
      "Epoch: 711/2000... Training loss: 0.5854\n",
      "Epoch: 711/2000... Training loss: 0.8671\n",
      "Epoch: 711/2000... Training loss: 0.8346\n",
      "Epoch: 711/2000... Training loss: 0.7588\n",
      "Epoch: 711/2000... Training loss: 0.7851\n",
      "Epoch: 711/2000... Training loss: 0.8770\n",
      "Epoch: 711/2000... Training loss: 0.6776\n",
      "Epoch: 711/2000... Training loss: 0.5626\n",
      "Epoch: 711/2000... Training loss: 0.6258\n",
      "Epoch: 711/2000... Training loss: 0.5903\n",
      "Epoch: 711/2000... Training loss: 0.7509\n",
      "Epoch: 711/2000... Training loss: 0.7227\n",
      "Epoch: 711/2000... Training loss: 0.4694\n",
      "Epoch: 711/2000... Training loss: 0.8053\n",
      "Epoch: 711/2000... Training loss: 0.7424\n",
      "Epoch: 711/2000... Training loss: 0.6507\n",
      "Epoch: 712/2000... Training loss: 0.7155\n",
      "Epoch: 712/2000... Training loss: 0.6761\n",
      "Epoch: 712/2000... Training loss: 0.8159\n",
      "Epoch: 712/2000... Training loss: 0.7782\n",
      "Epoch: 712/2000... Training loss: 0.7438\n",
      "Epoch: 712/2000... Training loss: 0.7918\n",
      "Epoch: 712/2000... Training loss: 0.8506\n",
      "Epoch: 712/2000... Training loss: 0.8212\n",
      "Epoch: 712/2000... Training loss: 0.6505\n",
      "Epoch: 712/2000... Training loss: 0.6193\n",
      "Epoch: 712/2000... Training loss: 0.9715\n",
      "Epoch: 712/2000... Training loss: 0.7816\n",
      "Epoch: 712/2000... Training loss: 0.8050\n",
      "Epoch: 712/2000... Training loss: 0.8567\n",
      "Epoch: 712/2000... Training loss: 0.7688\n",
      "Epoch: 712/2000... Training loss: 0.7606\n",
      "Epoch: 712/2000... Training loss: 0.5770\n",
      "Epoch: 712/2000... Training loss: 0.8210\n",
      "Epoch: 712/2000... Training loss: 0.7471\n",
      "Epoch: 712/2000... Training loss: 0.6774\n",
      "Epoch: 712/2000... Training loss: 0.7333\n",
      "Epoch: 712/2000... Training loss: 0.6043\n",
      "Epoch: 712/2000... Training loss: 0.8733\n",
      "Epoch: 712/2000... Training loss: 0.7815\n",
      "Epoch: 712/2000... Training loss: 0.7836\n",
      "Epoch: 712/2000... Training loss: 0.7310\n",
      "Epoch: 712/2000... Training loss: 0.6397\n",
      "Epoch: 712/2000... Training loss: 0.7847\n",
      "Epoch: 712/2000... Training loss: 0.7157\n",
      "Epoch: 712/2000... Training loss: 0.7227\n",
      "Epoch: 712/2000... Training loss: 0.7233\n",
      "Epoch: 713/2000... Training loss: 0.8238\n",
      "Epoch: 713/2000... Training loss: 0.8419\n",
      "Epoch: 713/2000... Training loss: 0.9795\n",
      "Epoch: 713/2000... Training loss: 0.7587\n",
      "Epoch: 713/2000... Training loss: 0.8899\n",
      "Epoch: 713/2000... Training loss: 0.6298\n",
      "Epoch: 713/2000... Training loss: 0.7355\n",
      "Epoch: 713/2000... Training loss: 0.5501\n",
      "Epoch: 713/2000... Training loss: 0.8102\n",
      "Epoch: 713/2000... Training loss: 0.7171\n",
      "Epoch: 713/2000... Training loss: 0.5677\n",
      "Epoch: 713/2000... Training loss: 0.7972\n",
      "Epoch: 713/2000... Training loss: 0.7925\n",
      "Epoch: 713/2000... Training loss: 0.8355\n",
      "Epoch: 713/2000... Training loss: 0.9861\n",
      "Epoch: 713/2000... Training loss: 0.5744\n",
      "Epoch: 713/2000... Training loss: 0.7110\n",
      "Epoch: 713/2000... Training loss: 0.8479\n",
      "Epoch: 713/2000... Training loss: 0.7164\n",
      "Epoch: 713/2000... Training loss: 0.6034\n",
      "Epoch: 713/2000... Training loss: 0.7955\n",
      "Epoch: 713/2000... Training loss: 0.6643\n",
      "Epoch: 713/2000... Training loss: 0.8407\n",
      "Epoch: 713/2000... Training loss: 0.6279\n",
      "Epoch: 713/2000... Training loss: 0.7432\n",
      "Epoch: 713/2000... Training loss: 0.6984\n",
      "Epoch: 713/2000... Training loss: 0.8438\n",
      "Epoch: 713/2000... Training loss: 0.9555\n",
      "Epoch: 713/2000... Training loss: 0.8726\n",
      "Epoch: 713/2000... Training loss: 0.4948\n",
      "Epoch: 713/2000... Training loss: 0.7194\n",
      "Epoch: 714/2000... Training loss: 0.8524\n",
      "Epoch: 714/2000... Training loss: 0.7405\n",
      "Epoch: 714/2000... Training loss: 0.6139\n",
      "Epoch: 714/2000... Training loss: 0.7346\n",
      "Epoch: 714/2000... Training loss: 0.7835\n",
      "Epoch: 714/2000... Training loss: 0.7902\n",
      "Epoch: 714/2000... Training loss: 0.8186\n",
      "Epoch: 714/2000... Training loss: 0.5495\n",
      "Epoch: 714/2000... Training loss: 0.8028\n",
      "Epoch: 714/2000... Training loss: 0.8960\n",
      "Epoch: 714/2000... Training loss: 0.6993\n",
      "Epoch: 714/2000... Training loss: 0.8703\n",
      "Epoch: 714/2000... Training loss: 0.7684\n",
      "Epoch: 714/2000... Training loss: 0.6978\n",
      "Epoch: 714/2000... Training loss: 0.8743\n",
      "Epoch: 714/2000... Training loss: 0.6898\n",
      "Epoch: 714/2000... Training loss: 0.5590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 714/2000... Training loss: 0.8691\n",
      "Epoch: 714/2000... Training loss: 0.5958\n",
      "Epoch: 714/2000... Training loss: 0.6535\n",
      "Epoch: 714/2000... Training loss: 0.5568\n",
      "Epoch: 714/2000... Training loss: 0.6551\n",
      "Epoch: 714/2000... Training loss: 0.4834\n",
      "Epoch: 714/2000... Training loss: 0.5961\n",
      "Epoch: 714/2000... Training loss: 0.6676\n",
      "Epoch: 714/2000... Training loss: 0.7454\n",
      "Epoch: 714/2000... Training loss: 0.6897\n",
      "Epoch: 714/2000... Training loss: 0.7024\n",
      "Epoch: 714/2000... Training loss: 0.4389\n",
      "Epoch: 714/2000... Training loss: 0.7418\n",
      "Epoch: 714/2000... Training loss: 0.8184\n",
      "Epoch: 715/2000... Training loss: 0.4937\n",
      "Epoch: 715/2000... Training loss: 0.4840\n",
      "Epoch: 715/2000... Training loss: 0.9597\n",
      "Epoch: 715/2000... Training loss: 0.8249\n",
      "Epoch: 715/2000... Training loss: 0.6821\n",
      "Epoch: 715/2000... Training loss: 0.7024\n",
      "Epoch: 715/2000... Training loss: 0.8172\n",
      "Epoch: 715/2000... Training loss: 0.5622\n",
      "Epoch: 715/2000... Training loss: 0.6280\n",
      "Epoch: 715/2000... Training loss: 0.7554\n",
      "Epoch: 715/2000... Training loss: 0.4193\n",
      "Epoch: 715/2000... Training loss: 0.6018\n",
      "Epoch: 715/2000... Training loss: 0.5587\n",
      "Epoch: 715/2000... Training loss: 0.8163\n",
      "Epoch: 715/2000... Training loss: 0.6209\n",
      "Epoch: 715/2000... Training loss: 0.6344\n",
      "Epoch: 715/2000... Training loss: 0.8406\n",
      "Epoch: 715/2000... Training loss: 0.5396\n",
      "Epoch: 715/2000... Training loss: 0.7451\n",
      "Epoch: 715/2000... Training loss: 0.8359\n",
      "Epoch: 715/2000... Training loss: 0.8983\n",
      "Epoch: 715/2000... Training loss: 0.7685\n",
      "Epoch: 715/2000... Training loss: 0.5852\n",
      "Epoch: 715/2000... Training loss: 0.7712\n",
      "Epoch: 715/2000... Training loss: 0.8586\n",
      "Epoch: 715/2000... Training loss: 0.7711\n",
      "Epoch: 715/2000... Training loss: 0.7064\n",
      "Epoch: 715/2000... Training loss: 0.5687\n",
      "Epoch: 715/2000... Training loss: 0.8097\n",
      "Epoch: 715/2000... Training loss: 0.8706\n",
      "Epoch: 715/2000... Training loss: 0.8247\n",
      "Epoch: 716/2000... Training loss: 0.6764\n",
      "Epoch: 716/2000... Training loss: 0.6857\n",
      "Epoch: 716/2000... Training loss: 0.8173\n",
      "Epoch: 716/2000... Training loss: 0.5665\n",
      "Epoch: 716/2000... Training loss: 0.5562\n",
      "Epoch: 716/2000... Training loss: 0.6137\n",
      "Epoch: 716/2000... Training loss: 0.9075\n",
      "Epoch: 716/2000... Training loss: 0.6411\n",
      "Epoch: 716/2000... Training loss: 0.6447\n",
      "Epoch: 716/2000... Training loss: 0.6543\n",
      "Epoch: 716/2000... Training loss: 0.8231\n",
      "Epoch: 716/2000... Training loss: 0.7521\n",
      "Epoch: 716/2000... Training loss: 0.5970\n",
      "Epoch: 716/2000... Training loss: 0.7142\n",
      "Epoch: 716/2000... Training loss: 0.7777\n",
      "Epoch: 716/2000... Training loss: 0.6742\n",
      "Epoch: 716/2000... Training loss: 0.5938\n",
      "Epoch: 716/2000... Training loss: 0.9256\n",
      "Epoch: 716/2000... Training loss: 0.8411\n",
      "Epoch: 716/2000... Training loss: 0.8316\n",
      "Epoch: 716/2000... Training loss: 0.8024\n",
      "Epoch: 716/2000... Training loss: 0.7882\n",
      "Epoch: 716/2000... Training loss: 0.6424\n",
      "Epoch: 716/2000... Training loss: 0.7527\n",
      "Epoch: 716/2000... Training loss: 0.5597\n",
      "Epoch: 716/2000... Training loss: 0.8658\n",
      "Epoch: 716/2000... Training loss: 0.6454\n",
      "Epoch: 716/2000... Training loss: 0.8079\n",
      "Epoch: 716/2000... Training loss: 0.9043\n",
      "Epoch: 716/2000... Training loss: 0.7901\n",
      "Epoch: 716/2000... Training loss: 0.6804\n",
      "Epoch: 717/2000... Training loss: 0.7387\n",
      "Epoch: 717/2000... Training loss: 0.6831\n",
      "Epoch: 717/2000... Training loss: 0.7478\n",
      "Epoch: 717/2000... Training loss: 0.6249\n",
      "Epoch: 717/2000... Training loss: 0.8537\n",
      "Epoch: 717/2000... Training loss: 0.7985\n",
      "Epoch: 717/2000... Training loss: 0.6640\n",
      "Epoch: 717/2000... Training loss: 0.7725\n",
      "Epoch: 717/2000... Training loss: 0.6696\n",
      "Epoch: 717/2000... Training loss: 0.7302\n",
      "Epoch: 717/2000... Training loss: 0.6393\n",
      "Epoch: 717/2000... Training loss: 0.6377\n",
      "Epoch: 717/2000... Training loss: 0.6530\n",
      "Epoch: 717/2000... Training loss: 0.8685\n",
      "Epoch: 717/2000... Training loss: 0.5282\n",
      "Epoch: 717/2000... Training loss: 1.0888\n",
      "Epoch: 717/2000... Training loss: 0.5538\n",
      "Epoch: 717/2000... Training loss: 0.6974\n",
      "Epoch: 717/2000... Training loss: 0.7124\n",
      "Epoch: 717/2000... Training loss: 0.7256\n",
      "Epoch: 717/2000... Training loss: 0.7574\n",
      "Epoch: 717/2000... Training loss: 0.6402\n",
      "Epoch: 717/2000... Training loss: 0.7282\n",
      "Epoch: 717/2000... Training loss: 0.7836\n",
      "Epoch: 717/2000... Training loss: 0.6963\n",
      "Epoch: 717/2000... Training loss: 0.6167\n",
      "Epoch: 717/2000... Training loss: 0.9740\n",
      "Epoch: 717/2000... Training loss: 0.6982\n",
      "Epoch: 717/2000... Training loss: 0.6357\n",
      "Epoch: 717/2000... Training loss: 0.7434\n",
      "Epoch: 717/2000... Training loss: 0.7001\n",
      "Epoch: 718/2000... Training loss: 0.7167\n",
      "Epoch: 718/2000... Training loss: 0.8368\n",
      "Epoch: 718/2000... Training loss: 0.7882\n",
      "Epoch: 718/2000... Training loss: 0.6282\n",
      "Epoch: 718/2000... Training loss: 0.7621\n",
      "Epoch: 718/2000... Training loss: 0.6559\n",
      "Epoch: 718/2000... Training loss: 0.8170\n",
      "Epoch: 718/2000... Training loss: 0.7188\n",
      "Epoch: 718/2000... Training loss: 0.6345\n",
      "Epoch: 718/2000... Training loss: 0.6989\n",
      "Epoch: 718/2000... Training loss: 0.6461\n",
      "Epoch: 718/2000... Training loss: 0.6989\n",
      "Epoch: 718/2000... Training loss: 0.8938\n",
      "Epoch: 718/2000... Training loss: 0.9756\n",
      "Epoch: 718/2000... Training loss: 0.6729\n",
      "Epoch: 718/2000... Training loss: 0.5875\n",
      "Epoch: 718/2000... Training loss: 0.5709\n",
      "Epoch: 718/2000... Training loss: 0.7365\n",
      "Epoch: 718/2000... Training loss: 0.7922\n",
      "Epoch: 718/2000... Training loss: 0.6180\n",
      "Epoch: 718/2000... Training loss: 0.6572\n",
      "Epoch: 718/2000... Training loss: 0.8088\n",
      "Epoch: 718/2000... Training loss: 0.8885\n",
      "Epoch: 718/2000... Training loss: 0.8236\n",
      "Epoch: 718/2000... Training loss: 0.6693\n",
      "Epoch: 718/2000... Training loss: 0.6822\n",
      "Epoch: 718/2000... Training loss: 0.6591\n",
      "Epoch: 718/2000... Training loss: 0.8620\n",
      "Epoch: 718/2000... Training loss: 0.6919\n",
      "Epoch: 718/2000... Training loss: 0.7408\n",
      "Epoch: 718/2000... Training loss: 0.6516\n",
      "Epoch: 719/2000... Training loss: 0.8753\n",
      "Epoch: 719/2000... Training loss: 0.5115\n",
      "Epoch: 719/2000... Training loss: 0.7106\n",
      "Epoch: 719/2000... Training loss: 0.6887\n",
      "Epoch: 719/2000... Training loss: 0.9484\n",
      "Epoch: 719/2000... Training loss: 0.6753\n",
      "Epoch: 719/2000... Training loss: 0.6149\n",
      "Epoch: 719/2000... Training loss: 0.5515\n",
      "Epoch: 719/2000... Training loss: 0.5694\n",
      "Epoch: 719/2000... Training loss: 0.8629\n",
      "Epoch: 719/2000... Training loss: 0.5173\n",
      "Epoch: 719/2000... Training loss: 0.6555\n",
      "Epoch: 719/2000... Training loss: 0.6698\n",
      "Epoch: 719/2000... Training loss: 0.6053\n",
      "Epoch: 719/2000... Training loss: 0.7925\n",
      "Epoch: 719/2000... Training loss: 0.7084\n",
      "Epoch: 719/2000... Training loss: 0.5447\n",
      "Epoch: 719/2000... Training loss: 0.7926\n",
      "Epoch: 719/2000... Training loss: 0.5777\n",
      "Epoch: 719/2000... Training loss: 0.7686\n",
      "Epoch: 719/2000... Training loss: 0.6225\n",
      "Epoch: 719/2000... Training loss: 0.7706\n",
      "Epoch: 719/2000... Training loss: 0.7127\n",
      "Epoch: 719/2000... Training loss: 0.6324\n",
      "Epoch: 719/2000... Training loss: 0.6487\n",
      "Epoch: 719/2000... Training loss: 0.4933\n",
      "Epoch: 719/2000... Training loss: 0.7218\n",
      "Epoch: 719/2000... Training loss: 0.7691\n",
      "Epoch: 719/2000... Training loss: 0.7034\n",
      "Epoch: 719/2000... Training loss: 0.7657\n",
      "Epoch: 719/2000... Training loss: 0.6847\n",
      "Epoch: 720/2000... Training loss: 0.7391\n",
      "Epoch: 720/2000... Training loss: 0.9132\n",
      "Epoch: 720/2000... Training loss: 0.7190\n",
      "Epoch: 720/2000... Training loss: 0.6618\n",
      "Epoch: 720/2000... Training loss: 0.5090\n",
      "Epoch: 720/2000... Training loss: 0.6931\n",
      "Epoch: 720/2000... Training loss: 0.8355\n",
      "Epoch: 720/2000... Training loss: 0.7033\n",
      "Epoch: 720/2000... Training loss: 0.6690\n",
      "Epoch: 720/2000... Training loss: 0.7756\n",
      "Epoch: 720/2000... Training loss: 0.9407\n",
      "Epoch: 720/2000... Training loss: 0.5298\n",
      "Epoch: 720/2000... Training loss: 0.6262\n",
      "Epoch: 720/2000... Training loss: 0.7906\n",
      "Epoch: 720/2000... Training loss: 0.6739\n",
      "Epoch: 720/2000... Training loss: 0.7771\n",
      "Epoch: 720/2000... Training loss: 0.7016\n",
      "Epoch: 720/2000... Training loss: 0.5635\n",
      "Epoch: 720/2000... Training loss: 0.8449\n",
      "Epoch: 720/2000... Training loss: 0.7265\n",
      "Epoch: 720/2000... Training loss: 0.6393\n",
      "Epoch: 720/2000... Training loss: 0.7608\n",
      "Epoch: 720/2000... Training loss: 0.6674\n",
      "Epoch: 720/2000... Training loss: 0.5920\n",
      "Epoch: 720/2000... Training loss: 0.7904\n",
      "Epoch: 720/2000... Training loss: 0.8365\n",
      "Epoch: 720/2000... Training loss: 0.6414\n",
      "Epoch: 720/2000... Training loss: 0.9733\n",
      "Epoch: 720/2000... Training loss: 0.7675\n",
      "Epoch: 720/2000... Training loss: 0.7200\n",
      "Epoch: 720/2000... Training loss: 0.6949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 721/2000... Training loss: 0.6789\n",
      "Epoch: 721/2000... Training loss: 0.7405\n",
      "Epoch: 721/2000... Training loss: 0.8083\n",
      "Epoch: 721/2000... Training loss: 0.6913\n",
      "Epoch: 721/2000... Training loss: 0.7797\n",
      "Epoch: 721/2000... Training loss: 0.5418\n",
      "Epoch: 721/2000... Training loss: 0.5808\n",
      "Epoch: 721/2000... Training loss: 0.7748\n",
      "Epoch: 721/2000... Training loss: 0.9411\n",
      "Epoch: 721/2000... Training loss: 0.4981\n",
      "Epoch: 721/2000... Training loss: 0.5133\n",
      "Epoch: 721/2000... Training loss: 0.7718\n",
      "Epoch: 721/2000... Training loss: 0.7544\n",
      "Epoch: 721/2000... Training loss: 0.7846\n",
      "Epoch: 721/2000... Training loss: 0.4838\n",
      "Epoch: 721/2000... Training loss: 0.5516\n",
      "Epoch: 721/2000... Training loss: 0.6302\n",
      "Epoch: 721/2000... Training loss: 0.6360\n",
      "Epoch: 721/2000... Training loss: 0.6277\n",
      "Epoch: 721/2000... Training loss: 0.6039\n",
      "Epoch: 721/2000... Training loss: 0.7651\n",
      "Epoch: 721/2000... Training loss: 0.7154\n",
      "Epoch: 721/2000... Training loss: 0.6650\n",
      "Epoch: 721/2000... Training loss: 0.7929\n",
      "Epoch: 721/2000... Training loss: 0.8323\n",
      "Epoch: 721/2000... Training loss: 0.7196\n",
      "Epoch: 721/2000... Training loss: 0.7619\n",
      "Epoch: 721/2000... Training loss: 0.7868\n",
      "Epoch: 721/2000... Training loss: 0.7363\n",
      "Epoch: 721/2000... Training loss: 0.6836\n",
      "Epoch: 721/2000... Training loss: 0.7867\n",
      "Epoch: 722/2000... Training loss: 0.8164\n",
      "Epoch: 722/2000... Training loss: 0.7804\n",
      "Epoch: 722/2000... Training loss: 0.6256\n",
      "Epoch: 722/2000... Training loss: 0.6327\n",
      "Epoch: 722/2000... Training loss: 0.4692\n",
      "Epoch: 722/2000... Training loss: 0.7649\n",
      "Epoch: 722/2000... Training loss: 0.7022\n",
      "Epoch: 722/2000... Training loss: 0.4798\n",
      "Epoch: 722/2000... Training loss: 0.7889\n",
      "Epoch: 722/2000... Training loss: 0.8866\n",
      "Epoch: 722/2000... Training loss: 0.7215\n",
      "Epoch: 722/2000... Training loss: 0.8483\n",
      "Epoch: 722/2000... Training loss: 0.7765\n",
      "Epoch: 722/2000... Training loss: 0.6456\n",
      "Epoch: 722/2000... Training loss: 0.8852\n",
      "Epoch: 722/2000... Training loss: 0.6118\n",
      "Epoch: 722/2000... Training loss: 0.8446\n",
      "Epoch: 722/2000... Training loss: 0.8041\n",
      "Epoch: 722/2000... Training loss: 0.8047\n",
      "Epoch: 722/2000... Training loss: 0.7813\n",
      "Epoch: 722/2000... Training loss: 0.6131\n",
      "Epoch: 722/2000... Training loss: 0.7976\n",
      "Epoch: 722/2000... Training loss: 0.6775\n",
      "Epoch: 722/2000... Training loss: 0.7102\n",
      "Epoch: 722/2000... Training loss: 0.5519\n",
      "Epoch: 722/2000... Training loss: 0.7821\n",
      "Epoch: 722/2000... Training loss: 0.7907\n",
      "Epoch: 722/2000... Training loss: 0.7678\n",
      "Epoch: 722/2000... Training loss: 0.6373\n",
      "Epoch: 722/2000... Training loss: 0.7061\n",
      "Epoch: 722/2000... Training loss: 0.6057\n",
      "Epoch: 723/2000... Training loss: 0.8002\n",
      "Epoch: 723/2000... Training loss: 0.7367\n",
      "Epoch: 723/2000... Training loss: 0.8274\n",
      "Epoch: 723/2000... Training loss: 0.9897\n",
      "Epoch: 723/2000... Training loss: 0.8594\n",
      "Epoch: 723/2000... Training loss: 0.5778\n",
      "Epoch: 723/2000... Training loss: 0.7440\n",
      "Epoch: 723/2000... Training loss: 0.6702\n",
      "Epoch: 723/2000... Training loss: 0.7462\n",
      "Epoch: 723/2000... Training loss: 0.7306\n",
      "Epoch: 723/2000... Training loss: 0.7992\n",
      "Epoch: 723/2000... Training loss: 0.9364\n",
      "Epoch: 723/2000... Training loss: 0.7508\n",
      "Epoch: 723/2000... Training loss: 0.8370\n",
      "Epoch: 723/2000... Training loss: 0.8196\n",
      "Epoch: 723/2000... Training loss: 0.7861\n",
      "Epoch: 723/2000... Training loss: 0.6886\n",
      "Epoch: 723/2000... Training loss: 0.8034\n",
      "Epoch: 723/2000... Training loss: 0.9460\n",
      "Epoch: 723/2000... Training loss: 0.8995\n",
      "Epoch: 723/2000... Training loss: 0.6978\n",
      "Epoch: 723/2000... Training loss: 0.4901\n",
      "Epoch: 723/2000... Training loss: 0.5930\n",
      "Epoch: 723/2000... Training loss: 0.7680\n",
      "Epoch: 723/2000... Training loss: 0.6771\n",
      "Epoch: 723/2000... Training loss: 0.6750\n",
      "Epoch: 723/2000... Training loss: 0.6799\n",
      "Epoch: 723/2000... Training loss: 0.5018\n",
      "Epoch: 723/2000... Training loss: 0.6712\n",
      "Epoch: 723/2000... Training loss: 0.6530\n",
      "Epoch: 723/2000... Training loss: 0.8106\n",
      "Epoch: 724/2000... Training loss: 0.7905\n",
      "Epoch: 724/2000... Training loss: 0.7348\n",
      "Epoch: 724/2000... Training loss: 0.7222\n",
      "Epoch: 724/2000... Training loss: 0.6759\n",
      "Epoch: 724/2000... Training loss: 0.6345\n",
      "Epoch: 724/2000... Training loss: 0.7875\n",
      "Epoch: 724/2000... Training loss: 0.7880\n",
      "Epoch: 724/2000... Training loss: 0.7803\n",
      "Epoch: 724/2000... Training loss: 0.7955\n",
      "Epoch: 724/2000... Training loss: 0.7658\n",
      "Epoch: 724/2000... Training loss: 0.7787\n",
      "Epoch: 724/2000... Training loss: 0.7048\n",
      "Epoch: 724/2000... Training loss: 0.6313\n",
      "Epoch: 724/2000... Training loss: 0.7720\n",
      "Epoch: 724/2000... Training loss: 0.6147\n",
      "Epoch: 724/2000... Training loss: 0.8942\n",
      "Epoch: 724/2000... Training loss: 0.7911\n",
      "Epoch: 724/2000... Training loss: 0.7224\n",
      "Epoch: 724/2000... Training loss: 0.8356\n",
      "Epoch: 724/2000... Training loss: 0.9514\n",
      "Epoch: 724/2000... Training loss: 0.6967\n",
      "Epoch: 724/2000... Training loss: 0.7444\n",
      "Epoch: 724/2000... Training loss: 0.6136\n",
      "Epoch: 724/2000... Training loss: 0.7954\n",
      "Epoch: 724/2000... Training loss: 0.7135\n",
      "Epoch: 724/2000... Training loss: 0.7557\n",
      "Epoch: 724/2000... Training loss: 0.7572\n",
      "Epoch: 724/2000... Training loss: 0.7724\n",
      "Epoch: 724/2000... Training loss: 0.6053\n",
      "Epoch: 724/2000... Training loss: 0.7714\n",
      "Epoch: 724/2000... Training loss: 0.5486\n",
      "Epoch: 725/2000... Training loss: 0.7625\n",
      "Epoch: 725/2000... Training loss: 0.6491\n",
      "Epoch: 725/2000... Training loss: 0.8656\n",
      "Epoch: 725/2000... Training loss: 0.7209\n",
      "Epoch: 725/2000... Training loss: 0.6767\n",
      "Epoch: 725/2000... Training loss: 0.8826\n",
      "Epoch: 725/2000... Training loss: 0.7138\n",
      "Epoch: 725/2000... Training loss: 0.7389\n",
      "Epoch: 725/2000... Training loss: 0.7442\n",
      "Epoch: 725/2000... Training loss: 0.7032\n",
      "Epoch: 725/2000... Training loss: 0.8749\n",
      "Epoch: 725/2000... Training loss: 0.7833\n",
      "Epoch: 725/2000... Training loss: 0.6089\n",
      "Epoch: 725/2000... Training loss: 0.7502\n",
      "Epoch: 725/2000... Training loss: 0.5716\n",
      "Epoch: 725/2000... Training loss: 0.6989\n",
      "Epoch: 725/2000... Training loss: 0.7272\n",
      "Epoch: 725/2000... Training loss: 0.7563\n",
      "Epoch: 725/2000... Training loss: 0.6259\n",
      "Epoch: 725/2000... Training loss: 0.8842\n",
      "Epoch: 725/2000... Training loss: 0.7925\n",
      "Epoch: 725/2000... Training loss: 0.6719\n",
      "Epoch: 725/2000... Training loss: 0.8202\n",
      "Epoch: 725/2000... Training loss: 0.8537\n",
      "Epoch: 725/2000... Training loss: 0.6465\n",
      "Epoch: 725/2000... Training loss: 0.8265\n",
      "Epoch: 725/2000... Training loss: 0.6141\n",
      "Epoch: 725/2000... Training loss: 0.8012\n",
      "Epoch: 725/2000... Training loss: 0.8274\n",
      "Epoch: 725/2000... Training loss: 0.7064\n",
      "Epoch: 725/2000... Training loss: 0.7989\n",
      "Epoch: 726/2000... Training loss: 0.6769\n",
      "Epoch: 726/2000... Training loss: 0.8216\n",
      "Epoch: 726/2000... Training loss: 0.5229\n",
      "Epoch: 726/2000... Training loss: 0.7817\n",
      "Epoch: 726/2000... Training loss: 0.5805\n",
      "Epoch: 726/2000... Training loss: 0.6396\n",
      "Epoch: 726/2000... Training loss: 0.6723\n",
      "Epoch: 726/2000... Training loss: 0.5627\n",
      "Epoch: 726/2000... Training loss: 0.7256\n",
      "Epoch: 726/2000... Training loss: 0.8559\n",
      "Epoch: 726/2000... Training loss: 0.5319\n",
      "Epoch: 726/2000... Training loss: 0.5954\n",
      "Epoch: 726/2000... Training loss: 0.6428\n",
      "Epoch: 726/2000... Training loss: 0.7083\n",
      "Epoch: 726/2000... Training loss: 0.7537\n",
      "Epoch: 726/2000... Training loss: 0.7438\n",
      "Epoch: 726/2000... Training loss: 0.7790\n",
      "Epoch: 726/2000... Training loss: 0.6175\n",
      "Epoch: 726/2000... Training loss: 0.7670\n",
      "Epoch: 726/2000... Training loss: 0.5460\n",
      "Epoch: 726/2000... Training loss: 0.6236\n",
      "Epoch: 726/2000... Training loss: 0.6935\n",
      "Epoch: 726/2000... Training loss: 0.5845\n",
      "Epoch: 726/2000... Training loss: 0.5906\n",
      "Epoch: 726/2000... Training loss: 0.8459\n",
      "Epoch: 726/2000... Training loss: 0.5198\n",
      "Epoch: 726/2000... Training loss: 0.6075\n",
      "Epoch: 726/2000... Training loss: 0.6998\n",
      "Epoch: 726/2000... Training loss: 0.7651\n",
      "Epoch: 726/2000... Training loss: 0.6573\n",
      "Epoch: 726/2000... Training loss: 0.5431\n",
      "Epoch: 727/2000... Training loss: 0.7633\n",
      "Epoch: 727/2000... Training loss: 0.8084\n",
      "Epoch: 727/2000... Training loss: 0.9331\n",
      "Epoch: 727/2000... Training loss: 0.8967\n",
      "Epoch: 727/2000... Training loss: 0.5957\n",
      "Epoch: 727/2000... Training loss: 0.7171\n",
      "Epoch: 727/2000... Training loss: 0.7748\n",
      "Epoch: 727/2000... Training loss: 0.5445\n",
      "Epoch: 727/2000... Training loss: 0.8207\n",
      "Epoch: 727/2000... Training loss: 0.6151\n",
      "Epoch: 727/2000... Training loss: 0.6803\n",
      "Epoch: 727/2000... Training loss: 0.9010\n",
      "Epoch: 727/2000... Training loss: 0.7735\n",
      "Epoch: 727/2000... Training loss: 0.6971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 727/2000... Training loss: 0.6030\n",
      "Epoch: 727/2000... Training loss: 0.7139\n",
      "Epoch: 727/2000... Training loss: 0.5114\n",
      "Epoch: 727/2000... Training loss: 0.7197\n",
      "Epoch: 727/2000... Training loss: 0.7801\n",
      "Epoch: 727/2000... Training loss: 0.4911\n",
      "Epoch: 727/2000... Training loss: 0.7145\n",
      "Epoch: 727/2000... Training loss: 0.8973\n",
      "Epoch: 727/2000... Training loss: 0.6202\n",
      "Epoch: 727/2000... Training loss: 0.8460\n",
      "Epoch: 727/2000... Training loss: 0.6384\n",
      "Epoch: 727/2000... Training loss: 0.8391\n",
      "Epoch: 727/2000... Training loss: 0.7903\n",
      "Epoch: 727/2000... Training loss: 0.6068\n",
      "Epoch: 727/2000... Training loss: 0.6269\n",
      "Epoch: 727/2000... Training loss: 0.8345\n",
      "Epoch: 727/2000... Training loss: 0.7391\n",
      "Epoch: 728/2000... Training loss: 0.5946\n",
      "Epoch: 728/2000... Training loss: 0.7297\n",
      "Epoch: 728/2000... Training loss: 0.4838\n",
      "Epoch: 728/2000... Training loss: 0.6855\n",
      "Epoch: 728/2000... Training loss: 0.9871\n",
      "Epoch: 728/2000... Training loss: 0.9441\n",
      "Epoch: 728/2000... Training loss: 0.7288\n",
      "Epoch: 728/2000... Training loss: 0.5918\n",
      "Epoch: 728/2000... Training loss: 0.6601\n",
      "Epoch: 728/2000... Training loss: 0.7031\n",
      "Epoch: 728/2000... Training loss: 0.8846\n",
      "Epoch: 728/2000... Training loss: 0.4822\n",
      "Epoch: 728/2000... Training loss: 0.6155\n",
      "Epoch: 728/2000... Training loss: 0.7945\n",
      "Epoch: 728/2000... Training loss: 0.6536\n",
      "Epoch: 728/2000... Training loss: 0.8283\n",
      "Epoch: 728/2000... Training loss: 0.6572\n",
      "Epoch: 728/2000... Training loss: 0.6661\n",
      "Epoch: 728/2000... Training loss: 0.7860\n",
      "Epoch: 728/2000... Training loss: 0.6471\n",
      "Epoch: 728/2000... Training loss: 0.6644\n",
      "Epoch: 728/2000... Training loss: 0.7986\n",
      "Epoch: 728/2000... Training loss: 0.7028\n",
      "Epoch: 728/2000... Training loss: 0.6443\n",
      "Epoch: 728/2000... Training loss: 0.8010\n",
      "Epoch: 728/2000... Training loss: 0.9541\n",
      "Epoch: 728/2000... Training loss: 0.4988\n",
      "Epoch: 728/2000... Training loss: 0.6691\n",
      "Epoch: 728/2000... Training loss: 0.8033\n",
      "Epoch: 728/2000... Training loss: 0.4828\n",
      "Epoch: 728/2000... Training loss: 0.7243\n",
      "Epoch: 729/2000... Training loss: 0.5846\n",
      "Epoch: 729/2000... Training loss: 0.5665\n",
      "Epoch: 729/2000... Training loss: 0.6269\n",
      "Epoch: 729/2000... Training loss: 0.7101\n",
      "Epoch: 729/2000... Training loss: 0.6987\n",
      "Epoch: 729/2000... Training loss: 0.6184\n",
      "Epoch: 729/2000... Training loss: 0.4976\n",
      "Epoch: 729/2000... Training loss: 0.6320\n",
      "Epoch: 729/2000... Training loss: 0.7392\n",
      "Epoch: 729/2000... Training loss: 0.7667\n",
      "Epoch: 729/2000... Training loss: 0.8204\n",
      "Epoch: 729/2000... Training loss: 0.9204\n",
      "Epoch: 729/2000... Training loss: 0.6256\n",
      "Epoch: 729/2000... Training loss: 0.7713\n",
      "Epoch: 729/2000... Training loss: 0.7333\n",
      "Epoch: 729/2000... Training loss: 0.6734\n",
      "Epoch: 729/2000... Training loss: 0.8217\n",
      "Epoch: 729/2000... Training loss: 0.6759\n",
      "Epoch: 729/2000... Training loss: 0.7157\n",
      "Epoch: 729/2000... Training loss: 0.7955\n",
      "Epoch: 729/2000... Training loss: 0.8155\n",
      "Epoch: 729/2000... Training loss: 0.5761\n",
      "Epoch: 729/2000... Training loss: 0.6887\n",
      "Epoch: 729/2000... Training loss: 0.8384\n",
      "Epoch: 729/2000... Training loss: 0.5749\n",
      "Epoch: 729/2000... Training loss: 0.6468\n",
      "Epoch: 729/2000... Training loss: 0.6887\n",
      "Epoch: 729/2000... Training loss: 0.8948\n",
      "Epoch: 729/2000... Training loss: 0.7461\n",
      "Epoch: 729/2000... Training loss: 0.8216\n",
      "Epoch: 729/2000... Training loss: 0.7064\n",
      "Epoch: 730/2000... Training loss: 0.6482\n",
      "Epoch: 730/2000... Training loss: 0.8377\n",
      "Epoch: 730/2000... Training loss: 0.8161\n",
      "Epoch: 730/2000... Training loss: 0.6148\n",
      "Epoch: 730/2000... Training loss: 1.0043\n",
      "Epoch: 730/2000... Training loss: 0.5318\n",
      "Epoch: 730/2000... Training loss: 0.9153\n",
      "Epoch: 730/2000... Training loss: 0.6292\n",
      "Epoch: 730/2000... Training loss: 0.6929\n",
      "Epoch: 730/2000... Training loss: 0.6285\n",
      "Epoch: 730/2000... Training loss: 0.6795\n",
      "Epoch: 730/2000... Training loss: 0.7070\n",
      "Epoch: 730/2000... Training loss: 0.7883\n",
      "Epoch: 730/2000... Training loss: 0.7437\n",
      "Epoch: 730/2000... Training loss: 0.7927\n",
      "Epoch: 730/2000... Training loss: 0.5962\n",
      "Epoch: 730/2000... Training loss: 0.6963\n",
      "Epoch: 730/2000... Training loss: 0.6743\n",
      "Epoch: 730/2000... Training loss: 0.5982\n",
      "Epoch: 730/2000... Training loss: 0.5812\n",
      "Epoch: 730/2000... Training loss: 0.6642\n",
      "Epoch: 730/2000... Training loss: 0.7301\n",
      "Epoch: 730/2000... Training loss: 0.7912\n",
      "Epoch: 730/2000... Training loss: 0.6402\n",
      "Epoch: 730/2000... Training loss: 0.7252\n",
      "Epoch: 730/2000... Training loss: 0.8668\n",
      "Epoch: 730/2000... Training loss: 0.7178\n",
      "Epoch: 730/2000... Training loss: 0.6462\n",
      "Epoch: 730/2000... Training loss: 0.8551\n",
      "Epoch: 730/2000... Training loss: 0.9210\n",
      "Epoch: 730/2000... Training loss: 0.5436\n",
      "Epoch: 731/2000... Training loss: 0.7439\n",
      "Epoch: 731/2000... Training loss: 0.7873\n",
      "Epoch: 731/2000... Training loss: 0.9107\n",
      "Epoch: 731/2000... Training loss: 0.8053\n",
      "Epoch: 731/2000... Training loss: 0.8208\n",
      "Epoch: 731/2000... Training loss: 0.8887\n",
      "Epoch: 731/2000... Training loss: 0.5961\n",
      "Epoch: 731/2000... Training loss: 0.7824\n",
      "Epoch: 731/2000... Training loss: 0.5271\n",
      "Epoch: 731/2000... Training loss: 0.6892\n",
      "Epoch: 731/2000... Training loss: 0.6578\n",
      "Epoch: 731/2000... Training loss: 0.6617\n",
      "Epoch: 731/2000... Training loss: 0.6137\n",
      "Epoch: 731/2000... Training loss: 0.7284\n",
      "Epoch: 731/2000... Training loss: 0.5491\n",
      "Epoch: 731/2000... Training loss: 0.9508\n",
      "Epoch: 731/2000... Training loss: 0.8863\n",
      "Epoch: 731/2000... Training loss: 0.7000\n",
      "Epoch: 731/2000... Training loss: 0.8994\n",
      "Epoch: 731/2000... Training loss: 0.9310\n",
      "Epoch: 731/2000... Training loss: 0.6394\n",
      "Epoch: 731/2000... Training loss: 0.6557\n",
      "Epoch: 731/2000... Training loss: 0.7885\n",
      "Epoch: 731/2000... Training loss: 0.6403\n",
      "Epoch: 731/2000... Training loss: 0.7373\n",
      "Epoch: 731/2000... Training loss: 0.7964\n",
      "Epoch: 731/2000... Training loss: 0.6062\n",
      "Epoch: 731/2000... Training loss: 0.7829\n",
      "Epoch: 731/2000... Training loss: 0.6852\n",
      "Epoch: 731/2000... Training loss: 0.5524\n",
      "Epoch: 731/2000... Training loss: 0.8622\n",
      "Epoch: 732/2000... Training loss: 0.6079\n",
      "Epoch: 732/2000... Training loss: 0.6426\n",
      "Epoch: 732/2000... Training loss: 0.5426\n",
      "Epoch: 732/2000... Training loss: 0.6622\n",
      "Epoch: 732/2000... Training loss: 0.6955\n",
      "Epoch: 732/2000... Training loss: 0.7324\n",
      "Epoch: 732/2000... Training loss: 0.6691\n",
      "Epoch: 732/2000... Training loss: 0.8155\n",
      "Epoch: 732/2000... Training loss: 0.5611\n",
      "Epoch: 732/2000... Training loss: 0.7747\n",
      "Epoch: 732/2000... Training loss: 0.7239\n",
      "Epoch: 732/2000... Training loss: 0.9161\n",
      "Epoch: 732/2000... Training loss: 0.6640\n",
      "Epoch: 732/2000... Training loss: 0.7615\n",
      "Epoch: 732/2000... Training loss: 0.6513\n",
      "Epoch: 732/2000... Training loss: 0.5468\n",
      "Epoch: 732/2000... Training loss: 0.8353\n",
      "Epoch: 732/2000... Training loss: 0.8007\n",
      "Epoch: 732/2000... Training loss: 0.8715\n",
      "Epoch: 732/2000... Training loss: 0.5482\n",
      "Epoch: 732/2000... Training loss: 0.7288\n",
      "Epoch: 732/2000... Training loss: 0.5864\n",
      "Epoch: 732/2000... Training loss: 0.7923\n",
      "Epoch: 732/2000... Training loss: 0.7606\n",
      "Epoch: 732/2000... Training loss: 0.5426\n",
      "Epoch: 732/2000... Training loss: 0.7760\n",
      "Epoch: 732/2000... Training loss: 0.9226\n",
      "Epoch: 732/2000... Training loss: 0.6917\n",
      "Epoch: 732/2000... Training loss: 0.5940\n",
      "Epoch: 732/2000... Training loss: 0.6909\n",
      "Epoch: 732/2000... Training loss: 0.7599\n",
      "Epoch: 733/2000... Training loss: 0.7993\n",
      "Epoch: 733/2000... Training loss: 0.5899\n",
      "Epoch: 733/2000... Training loss: 0.7172\n",
      "Epoch: 733/2000... Training loss: 0.7656\n",
      "Epoch: 733/2000... Training loss: 0.7225\n",
      "Epoch: 733/2000... Training loss: 0.8032\n",
      "Epoch: 733/2000... Training loss: 0.5395\n",
      "Epoch: 733/2000... Training loss: 0.8994\n",
      "Epoch: 733/2000... Training loss: 0.7598\n",
      "Epoch: 733/2000... Training loss: 0.7587\n",
      "Epoch: 733/2000... Training loss: 0.6892\n",
      "Epoch: 733/2000... Training loss: 0.7016\n",
      "Epoch: 733/2000... Training loss: 0.7773\n",
      "Epoch: 733/2000... Training loss: 0.8619\n",
      "Epoch: 733/2000... Training loss: 0.7112\n",
      "Epoch: 733/2000... Training loss: 0.6161\n",
      "Epoch: 733/2000... Training loss: 0.6006\n",
      "Epoch: 733/2000... Training loss: 0.7479\n",
      "Epoch: 733/2000... Training loss: 0.8562\n",
      "Epoch: 733/2000... Training loss: 0.7584\n",
      "Epoch: 733/2000... Training loss: 0.5787\n",
      "Epoch: 733/2000... Training loss: 0.7267\n",
      "Epoch: 733/2000... Training loss: 0.6738\n",
      "Epoch: 733/2000... Training loss: 0.6120\n",
      "Epoch: 733/2000... Training loss: 0.7511\n",
      "Epoch: 733/2000... Training loss: 0.7613\n",
      "Epoch: 733/2000... Training loss: 0.9324\n",
      "Epoch: 733/2000... Training loss: 0.7495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 733/2000... Training loss: 0.6071\n",
      "Epoch: 733/2000... Training loss: 0.6354\n",
      "Epoch: 733/2000... Training loss: 0.6344\n",
      "Epoch: 734/2000... Training loss: 0.6804\n",
      "Epoch: 734/2000... Training loss: 0.5848\n",
      "Epoch: 734/2000... Training loss: 0.7385\n",
      "Epoch: 734/2000... Training loss: 0.8922\n",
      "Epoch: 734/2000... Training loss: 0.7730\n",
      "Epoch: 734/2000... Training loss: 0.7974\n",
      "Epoch: 734/2000... Training loss: 0.8080\n",
      "Epoch: 734/2000... Training loss: 0.5318\n",
      "Epoch: 734/2000... Training loss: 0.7540\n",
      "Epoch: 734/2000... Training loss: 0.7883\n",
      "Epoch: 734/2000... Training loss: 0.6314\n",
      "Epoch: 734/2000... Training loss: 0.5756\n",
      "Epoch: 734/2000... Training loss: 0.6123\n",
      "Epoch: 734/2000... Training loss: 0.6159\n",
      "Epoch: 734/2000... Training loss: 0.6614\n",
      "Epoch: 734/2000... Training loss: 0.9222\n",
      "Epoch: 734/2000... Training loss: 0.8313\n",
      "Epoch: 734/2000... Training loss: 0.8577\n",
      "Epoch: 734/2000... Training loss: 0.5631\n",
      "Epoch: 734/2000... Training loss: 1.0653\n",
      "Epoch: 734/2000... Training loss: 0.6150\n",
      "Epoch: 734/2000... Training loss: 0.5122\n",
      "Epoch: 734/2000... Training loss: 0.8247\n",
      "Epoch: 734/2000... Training loss: 0.6931\n",
      "Epoch: 734/2000... Training loss: 0.7893\n",
      "Epoch: 734/2000... Training loss: 0.6388\n",
      "Epoch: 734/2000... Training loss: 0.7439\n",
      "Epoch: 734/2000... Training loss: 0.5269\n",
      "Epoch: 734/2000... Training loss: 0.8313\n",
      "Epoch: 734/2000... Training loss: 0.7401\n",
      "Epoch: 734/2000... Training loss: 0.7208\n",
      "Epoch: 735/2000... Training loss: 0.6055\n",
      "Epoch: 735/2000... Training loss: 0.8911\n",
      "Epoch: 735/2000... Training loss: 0.7652\n",
      "Epoch: 735/2000... Training loss: 0.5622\n",
      "Epoch: 735/2000... Training loss: 0.7137\n",
      "Epoch: 735/2000... Training loss: 0.6100\n",
      "Epoch: 735/2000... Training loss: 0.5668\n",
      "Epoch: 735/2000... Training loss: 0.6285\n",
      "Epoch: 735/2000... Training loss: 0.6903\n",
      "Epoch: 735/2000... Training loss: 0.7733\n",
      "Epoch: 735/2000... Training loss: 0.8255\n",
      "Epoch: 735/2000... Training loss: 0.7683\n",
      "Epoch: 735/2000... Training loss: 0.5902\n",
      "Epoch: 735/2000... Training loss: 0.7178\n",
      "Epoch: 735/2000... Training loss: 0.7252\n",
      "Epoch: 735/2000... Training loss: 0.5554\n",
      "Epoch: 735/2000... Training loss: 0.6735\n",
      "Epoch: 735/2000... Training loss: 0.7985\n",
      "Epoch: 735/2000... Training loss: 0.7263\n",
      "Epoch: 735/2000... Training loss: 0.5848\n",
      "Epoch: 735/2000... Training loss: 0.6794\n",
      "Epoch: 735/2000... Training loss: 0.8865\n",
      "Epoch: 735/2000... Training loss: 0.5172\n",
      "Epoch: 735/2000... Training loss: 0.6060\n",
      "Epoch: 735/2000... Training loss: 0.7024\n",
      "Epoch: 735/2000... Training loss: 0.5701\n",
      "Epoch: 735/2000... Training loss: 0.8072\n",
      "Epoch: 735/2000... Training loss: 0.6598\n",
      "Epoch: 735/2000... Training loss: 0.5999\n",
      "Epoch: 735/2000... Training loss: 0.7428\n",
      "Epoch: 735/2000... Training loss: 0.8370\n",
      "Epoch: 736/2000... Training loss: 0.5891\n",
      "Epoch: 736/2000... Training loss: 0.7419\n",
      "Epoch: 736/2000... Training loss: 0.7505\n",
      "Epoch: 736/2000... Training loss: 1.0644\n",
      "Epoch: 736/2000... Training loss: 0.8133\n",
      "Epoch: 736/2000... Training loss: 0.5673\n",
      "Epoch: 736/2000... Training loss: 0.6114\n",
      "Epoch: 736/2000... Training loss: 0.6929\n",
      "Epoch: 736/2000... Training loss: 0.6370\n",
      "Epoch: 736/2000... Training loss: 0.5683\n",
      "Epoch: 736/2000... Training loss: 0.7178\n",
      "Epoch: 736/2000... Training loss: 0.8327\n",
      "Epoch: 736/2000... Training loss: 0.5584\n",
      "Epoch: 736/2000... Training loss: 0.7024\n",
      "Epoch: 736/2000... Training loss: 0.7532\n",
      "Epoch: 736/2000... Training loss: 0.5757\n",
      "Epoch: 736/2000... Training loss: 0.7125\n",
      "Epoch: 736/2000... Training loss: 0.7290\n",
      "Epoch: 736/2000... Training loss: 0.8048\n",
      "Epoch: 736/2000... Training loss: 0.8942\n",
      "Epoch: 736/2000... Training loss: 0.6829\n",
      "Epoch: 736/2000... Training loss: 0.7215\n",
      "Epoch: 736/2000... Training loss: 0.6910\n",
      "Epoch: 736/2000... Training loss: 0.7162\n",
      "Epoch: 736/2000... Training loss: 0.8033\n",
      "Epoch: 736/2000... Training loss: 0.6011\n",
      "Epoch: 736/2000... Training loss: 0.5054\n",
      "Epoch: 736/2000... Training loss: 0.7828\n",
      "Epoch: 736/2000... Training loss: 0.7280\n",
      "Epoch: 736/2000... Training loss: 0.9160\n",
      "Epoch: 736/2000... Training loss: 0.5225\n",
      "Epoch: 737/2000... Training loss: 0.5250\n",
      "Epoch: 737/2000... Training loss: 0.7786\n",
      "Epoch: 737/2000... Training loss: 0.8792\n",
      "Epoch: 737/2000... Training loss: 0.6808\n",
      "Epoch: 737/2000... Training loss: 0.6818\n",
      "Epoch: 737/2000... Training loss: 0.7612\n",
      "Epoch: 737/2000... Training loss: 0.8633\n",
      "Epoch: 737/2000... Training loss: 0.5947\n",
      "Epoch: 737/2000... Training loss: 0.6974\n",
      "Epoch: 737/2000... Training loss: 0.7442\n",
      "Epoch: 737/2000... Training loss: 0.7939\n",
      "Epoch: 737/2000... Training loss: 0.6119\n",
      "Epoch: 737/2000... Training loss: 0.7256\n",
      "Epoch: 737/2000... Training loss: 0.6577\n",
      "Epoch: 737/2000... Training loss: 0.8472\n",
      "Epoch: 737/2000... Training loss: 0.6376\n",
      "Epoch: 737/2000... Training loss: 0.8311\n",
      "Epoch: 737/2000... Training loss: 0.6176\n",
      "Epoch: 737/2000... Training loss: 0.6745\n",
      "Epoch: 737/2000... Training loss: 0.7437\n",
      "Epoch: 737/2000... Training loss: 0.7616\n",
      "Epoch: 737/2000... Training loss: 0.9046\n",
      "Epoch: 737/2000... Training loss: 0.6163\n",
      "Epoch: 737/2000... Training loss: 0.7109\n",
      "Epoch: 737/2000... Training loss: 0.6822\n",
      "Epoch: 737/2000... Training loss: 0.9133\n",
      "Epoch: 737/2000... Training loss: 0.5722\n",
      "Epoch: 737/2000... Training loss: 0.7307\n",
      "Epoch: 737/2000... Training loss: 0.7165\n",
      "Epoch: 737/2000... Training loss: 0.6622\n",
      "Epoch: 737/2000... Training loss: 0.5794\n",
      "Epoch: 738/2000... Training loss: 0.7929\n",
      "Epoch: 738/2000... Training loss: 0.7083\n",
      "Epoch: 738/2000... Training loss: 0.6563\n",
      "Epoch: 738/2000... Training loss: 0.7530\n",
      "Epoch: 738/2000... Training loss: 0.7080\n",
      "Epoch: 738/2000... Training loss: 0.6196\n",
      "Epoch: 738/2000... Training loss: 0.6057\n",
      "Epoch: 738/2000... Training loss: 0.7159\n",
      "Epoch: 738/2000... Training loss: 0.7721\n",
      "Epoch: 738/2000... Training loss: 0.5982\n",
      "Epoch: 738/2000... Training loss: 0.6102\n",
      "Epoch: 738/2000... Training loss: 0.7432\n",
      "Epoch: 738/2000... Training loss: 0.7421\n",
      "Epoch: 738/2000... Training loss: 0.5313\n",
      "Epoch: 738/2000... Training loss: 0.8139\n",
      "Epoch: 738/2000... Training loss: 0.8247\n",
      "Epoch: 738/2000... Training loss: 0.7367\n",
      "Epoch: 738/2000... Training loss: 0.7352\n",
      "Epoch: 738/2000... Training loss: 0.4882\n",
      "Epoch: 738/2000... Training loss: 0.7264\n",
      "Epoch: 738/2000... Training loss: 0.7808\n",
      "Epoch: 738/2000... Training loss: 0.7543\n",
      "Epoch: 738/2000... Training loss: 0.8591\n",
      "Epoch: 738/2000... Training loss: 0.6962\n",
      "Epoch: 738/2000... Training loss: 0.7215\n",
      "Epoch: 738/2000... Training loss: 0.6634\n",
      "Epoch: 738/2000... Training loss: 0.6396\n",
      "Epoch: 738/2000... Training loss: 0.6628\n",
      "Epoch: 738/2000... Training loss: 0.9044\n",
      "Epoch: 738/2000... Training loss: 0.6038\n",
      "Epoch: 738/2000... Training loss: 0.7321\n",
      "Epoch: 739/2000... Training loss: 0.6214\n",
      "Epoch: 739/2000... Training loss: 0.6791\n",
      "Epoch: 739/2000... Training loss: 0.8750\n",
      "Epoch: 739/2000... Training loss: 0.5449\n",
      "Epoch: 739/2000... Training loss: 0.7010\n",
      "Epoch: 739/2000... Training loss: 0.8007\n",
      "Epoch: 739/2000... Training loss: 0.6375\n",
      "Epoch: 739/2000... Training loss: 0.5851\n",
      "Epoch: 739/2000... Training loss: 0.7340\n",
      "Epoch: 739/2000... Training loss: 0.6519\n",
      "Epoch: 739/2000... Training loss: 0.4642\n",
      "Epoch: 739/2000... Training loss: 0.7695\n",
      "Epoch: 739/2000... Training loss: 0.5582\n",
      "Epoch: 739/2000... Training loss: 0.7698\n",
      "Epoch: 739/2000... Training loss: 0.5495\n",
      "Epoch: 739/2000... Training loss: 0.6275\n",
      "Epoch: 739/2000... Training loss: 0.6147\n",
      "Epoch: 739/2000... Training loss: 0.8513\n",
      "Epoch: 739/2000... Training loss: 0.7023\n",
      "Epoch: 739/2000... Training loss: 0.8296\n",
      "Epoch: 739/2000... Training loss: 0.7281\n",
      "Epoch: 739/2000... Training loss: 0.7771\n",
      "Epoch: 739/2000... Training loss: 0.7049\n",
      "Epoch: 739/2000... Training loss: 0.8948\n",
      "Epoch: 739/2000... Training loss: 0.6738\n",
      "Epoch: 739/2000... Training loss: 0.6426\n",
      "Epoch: 739/2000... Training loss: 0.5390\n",
      "Epoch: 739/2000... Training loss: 0.6015\n",
      "Epoch: 739/2000... Training loss: 0.7946\n",
      "Epoch: 739/2000... Training loss: 0.8287\n",
      "Epoch: 739/2000... Training loss: 0.8126\n",
      "Epoch: 740/2000... Training loss: 0.7387\n",
      "Epoch: 740/2000... Training loss: 0.6426\n",
      "Epoch: 740/2000... Training loss: 0.6888\n",
      "Epoch: 740/2000... Training loss: 0.6998\n",
      "Epoch: 740/2000... Training loss: 0.8530\n",
      "Epoch: 740/2000... Training loss: 0.5687\n",
      "Epoch: 740/2000... Training loss: 0.9311\n",
      "Epoch: 740/2000... Training loss: 0.7721\n",
      "Epoch: 740/2000... Training loss: 0.7114\n",
      "Epoch: 740/2000... Training loss: 0.8495\n",
      "Epoch: 740/2000... Training loss: 0.6874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 740/2000... Training loss: 0.6427\n",
      "Epoch: 740/2000... Training loss: 0.6309\n",
      "Epoch: 740/2000... Training loss: 0.6474\n",
      "Epoch: 740/2000... Training loss: 0.6567\n",
      "Epoch: 740/2000... Training loss: 0.8139\n",
      "Epoch: 740/2000... Training loss: 0.6997\n",
      "Epoch: 740/2000... Training loss: 0.5285\n",
      "Epoch: 740/2000... Training loss: 0.8818\n",
      "Epoch: 740/2000... Training loss: 0.7003\n",
      "Epoch: 740/2000... Training loss: 0.5045\n",
      "Epoch: 740/2000... Training loss: 0.6424\n",
      "Epoch: 740/2000... Training loss: 0.8602\n",
      "Epoch: 740/2000... Training loss: 0.8199\n",
      "Epoch: 740/2000... Training loss: 0.6567\n",
      "Epoch: 740/2000... Training loss: 0.8017\n",
      "Epoch: 740/2000... Training loss: 0.6895\n",
      "Epoch: 740/2000... Training loss: 0.6080\n",
      "Epoch: 740/2000... Training loss: 0.7720\n",
      "Epoch: 740/2000... Training loss: 0.4781\n",
      "Epoch: 740/2000... Training loss: 0.5364\n",
      "Epoch: 741/2000... Training loss: 0.5050\n",
      "Epoch: 741/2000... Training loss: 0.6323\n",
      "Epoch: 741/2000... Training loss: 0.7414\n",
      "Epoch: 741/2000... Training loss: 0.5698\n",
      "Epoch: 741/2000... Training loss: 0.6803\n",
      "Epoch: 741/2000... Training loss: 0.7606\n",
      "Epoch: 741/2000... Training loss: 0.5171\n",
      "Epoch: 741/2000... Training loss: 0.5800\n",
      "Epoch: 741/2000... Training loss: 0.6620\n",
      "Epoch: 741/2000... Training loss: 0.6104\n",
      "Epoch: 741/2000... Training loss: 0.7914\n",
      "Epoch: 741/2000... Training loss: 0.6319\n",
      "Epoch: 741/2000... Training loss: 0.5507\n",
      "Epoch: 741/2000... Training loss: 0.5536\n",
      "Epoch: 741/2000... Training loss: 0.6413\n",
      "Epoch: 741/2000... Training loss: 0.6556\n",
      "Epoch: 741/2000... Training loss: 0.7531\n",
      "Epoch: 741/2000... Training loss: 0.5797\n",
      "Epoch: 741/2000... Training loss: 0.7759\n",
      "Epoch: 741/2000... Training loss: 0.7577\n",
      "Epoch: 741/2000... Training loss: 0.6833\n",
      "Epoch: 741/2000... Training loss: 0.6919\n",
      "Epoch: 741/2000... Training loss: 0.7355\n",
      "Epoch: 741/2000... Training loss: 0.5230\n",
      "Epoch: 741/2000... Training loss: 0.7026\n",
      "Epoch: 741/2000... Training loss: 1.0383\n",
      "Epoch: 741/2000... Training loss: 0.5375\n",
      "Epoch: 741/2000... Training loss: 0.6224\n",
      "Epoch: 741/2000... Training loss: 0.6070\n",
      "Epoch: 741/2000... Training loss: 0.8012\n",
      "Epoch: 741/2000... Training loss: 0.7285\n",
      "Epoch: 742/2000... Training loss: 0.9349\n",
      "Epoch: 742/2000... Training loss: 0.7246\n",
      "Epoch: 742/2000... Training loss: 0.6566\n",
      "Epoch: 742/2000... Training loss: 0.6799\n",
      "Epoch: 742/2000... Training loss: 0.9404\n",
      "Epoch: 742/2000... Training loss: 0.5828\n",
      "Epoch: 742/2000... Training loss: 0.8644\n",
      "Epoch: 742/2000... Training loss: 0.7597\n",
      "Epoch: 742/2000... Training loss: 0.5619\n",
      "Epoch: 742/2000... Training loss: 0.7940\n",
      "Epoch: 742/2000... Training loss: 0.4715\n",
      "Epoch: 742/2000... Training loss: 0.6622\n",
      "Epoch: 742/2000... Training loss: 0.4608\n",
      "Epoch: 742/2000... Training loss: 0.8210\n",
      "Epoch: 742/2000... Training loss: 0.7037\n",
      "Epoch: 742/2000... Training loss: 0.6870\n",
      "Epoch: 742/2000... Training loss: 0.6278\n",
      "Epoch: 742/2000... Training loss: 0.8945\n",
      "Epoch: 742/2000... Training loss: 0.5743\n",
      "Epoch: 742/2000... Training loss: 0.6054\n",
      "Epoch: 742/2000... Training loss: 0.6201\n",
      "Epoch: 742/2000... Training loss: 0.5713\n",
      "Epoch: 742/2000... Training loss: 0.5473\n",
      "Epoch: 742/2000... Training loss: 0.5952\n",
      "Epoch: 742/2000... Training loss: 0.8650\n",
      "Epoch: 742/2000... Training loss: 0.5578\n",
      "Epoch: 742/2000... Training loss: 0.6555\n",
      "Epoch: 742/2000... Training loss: 0.6714\n",
      "Epoch: 742/2000... Training loss: 0.6912\n",
      "Epoch: 742/2000... Training loss: 0.6781\n",
      "Epoch: 742/2000... Training loss: 0.7274\n",
      "Epoch: 743/2000... Training loss: 0.5757\n",
      "Epoch: 743/2000... Training loss: 0.7451\n",
      "Epoch: 743/2000... Training loss: 0.6489\n",
      "Epoch: 743/2000... Training loss: 0.6237\n",
      "Epoch: 743/2000... Training loss: 0.6423\n",
      "Epoch: 743/2000... Training loss: 0.8134\n",
      "Epoch: 743/2000... Training loss: 0.5826\n",
      "Epoch: 743/2000... Training loss: 0.5722\n",
      "Epoch: 743/2000... Training loss: 0.7608\n",
      "Epoch: 743/2000... Training loss: 0.6161\n",
      "Epoch: 743/2000... Training loss: 0.7688\n",
      "Epoch: 743/2000... Training loss: 0.8624\n",
      "Epoch: 743/2000... Training loss: 0.7563\n",
      "Epoch: 743/2000... Training loss: 0.7818\n",
      "Epoch: 743/2000... Training loss: 0.7338\n",
      "Epoch: 743/2000... Training loss: 0.7979\n",
      "Epoch: 743/2000... Training loss: 0.8121\n",
      "Epoch: 743/2000... Training loss: 0.6445\n",
      "Epoch: 743/2000... Training loss: 0.6279\n",
      "Epoch: 743/2000... Training loss: 0.6687\n",
      "Epoch: 743/2000... Training loss: 0.6256\n",
      "Epoch: 743/2000... Training loss: 0.7904\n",
      "Epoch: 743/2000... Training loss: 0.6745\n",
      "Epoch: 743/2000... Training loss: 0.7731\n",
      "Epoch: 743/2000... Training loss: 0.5529\n",
      "Epoch: 743/2000... Training loss: 0.8048\n",
      "Epoch: 743/2000... Training loss: 0.7318\n",
      "Epoch: 743/2000... Training loss: 0.7344\n",
      "Epoch: 743/2000... Training loss: 0.5437\n",
      "Epoch: 743/2000... Training loss: 0.5262\n",
      "Epoch: 743/2000... Training loss: 0.7838\n",
      "Epoch: 744/2000... Training loss: 0.8426\n",
      "Epoch: 744/2000... Training loss: 0.6113\n",
      "Epoch: 744/2000... Training loss: 0.8382\n",
      "Epoch: 744/2000... Training loss: 0.6700\n",
      "Epoch: 744/2000... Training loss: 0.8346\n",
      "Epoch: 744/2000... Training loss: 0.5524\n",
      "Epoch: 744/2000... Training loss: 0.6984\n",
      "Epoch: 744/2000... Training loss: 0.7670\n",
      "Epoch: 744/2000... Training loss: 0.6147\n",
      "Epoch: 744/2000... Training loss: 0.7460\n",
      "Epoch: 744/2000... Training loss: 0.7916\n",
      "Epoch: 744/2000... Training loss: 0.6501\n",
      "Epoch: 744/2000... Training loss: 0.7100\n",
      "Epoch: 744/2000... Training loss: 0.5293\n",
      "Epoch: 744/2000... Training loss: 0.6499\n",
      "Epoch: 744/2000... Training loss: 0.4574\n",
      "Epoch: 744/2000... Training loss: 0.6160\n",
      "Epoch: 744/2000... Training loss: 0.6593\n",
      "Epoch: 744/2000... Training loss: 0.8758\n",
      "Epoch: 744/2000... Training loss: 0.9747\n",
      "Epoch: 744/2000... Training loss: 0.5966\n",
      "Epoch: 744/2000... Training loss: 0.5086\n",
      "Epoch: 744/2000... Training loss: 0.7524\n",
      "Epoch: 744/2000... Training loss: 0.6878\n",
      "Epoch: 744/2000... Training loss: 0.6045\n",
      "Epoch: 744/2000... Training loss: 0.6229\n",
      "Epoch: 744/2000... Training loss: 0.7609\n",
      "Epoch: 744/2000... Training loss: 0.5132\n",
      "Epoch: 744/2000... Training loss: 0.6772\n",
      "Epoch: 744/2000... Training loss: 0.6811\n",
      "Epoch: 744/2000... Training loss: 0.5912\n",
      "Epoch: 745/2000... Training loss: 0.5857\n",
      "Epoch: 745/2000... Training loss: 0.7394\n",
      "Epoch: 745/2000... Training loss: 0.5889\n",
      "Epoch: 745/2000... Training loss: 0.5154\n",
      "Epoch: 745/2000... Training loss: 0.6458\n",
      "Epoch: 745/2000... Training loss: 0.6986\n",
      "Epoch: 745/2000... Training loss: 0.8446\n",
      "Epoch: 745/2000... Training loss: 0.7080\n",
      "Epoch: 745/2000... Training loss: 0.6293\n",
      "Epoch: 745/2000... Training loss: 0.7495\n",
      "Epoch: 745/2000... Training loss: 0.7416\n",
      "Epoch: 745/2000... Training loss: 0.7110\n",
      "Epoch: 745/2000... Training loss: 0.7329\n",
      "Epoch: 745/2000... Training loss: 0.6888\n",
      "Epoch: 745/2000... Training loss: 0.7770\n",
      "Epoch: 745/2000... Training loss: 0.7462\n",
      "Epoch: 745/2000... Training loss: 0.6558\n",
      "Epoch: 745/2000... Training loss: 0.5380\n",
      "Epoch: 745/2000... Training loss: 0.5910\n",
      "Epoch: 745/2000... Training loss: 0.7273\n",
      "Epoch: 745/2000... Training loss: 0.5863\n",
      "Epoch: 745/2000... Training loss: 0.6094\n",
      "Epoch: 745/2000... Training loss: 0.6854\n",
      "Epoch: 745/2000... Training loss: 0.6942\n",
      "Epoch: 745/2000... Training loss: 0.7738\n",
      "Epoch: 745/2000... Training loss: 0.6095\n",
      "Epoch: 745/2000... Training loss: 0.6213\n",
      "Epoch: 745/2000... Training loss: 0.9461\n",
      "Epoch: 745/2000... Training loss: 0.6212\n",
      "Epoch: 745/2000... Training loss: 0.7960\n",
      "Epoch: 745/2000... Training loss: 0.8926\n",
      "Epoch: 746/2000... Training loss: 0.6631\n",
      "Epoch: 746/2000... Training loss: 0.5839\n",
      "Epoch: 746/2000... Training loss: 0.7802\n",
      "Epoch: 746/2000... Training loss: 0.5313\n",
      "Epoch: 746/2000... Training loss: 0.7865\n",
      "Epoch: 746/2000... Training loss: 0.7607\n",
      "Epoch: 746/2000... Training loss: 0.7712\n",
      "Epoch: 746/2000... Training loss: 0.8651\n",
      "Epoch: 746/2000... Training loss: 0.7599\n",
      "Epoch: 746/2000... Training loss: 0.7439\n",
      "Epoch: 746/2000... Training loss: 0.6719\n",
      "Epoch: 746/2000... Training loss: 0.7049\n",
      "Epoch: 746/2000... Training loss: 0.6708\n",
      "Epoch: 746/2000... Training loss: 0.4678\n",
      "Epoch: 746/2000... Training loss: 0.5861\n",
      "Epoch: 746/2000... Training loss: 0.6622\n",
      "Epoch: 746/2000... Training loss: 0.9183\n",
      "Epoch: 746/2000... Training loss: 0.6325\n",
      "Epoch: 746/2000... Training loss: 0.8993\n",
      "Epoch: 746/2000... Training loss: 0.6761\n",
      "Epoch: 746/2000... Training loss: 0.6781\n",
      "Epoch: 746/2000... Training loss: 0.8550\n",
      "Epoch: 746/2000... Training loss: 0.7168\n",
      "Epoch: 746/2000... Training loss: 0.8515\n",
      "Epoch: 746/2000... Training loss: 0.7761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 746/2000... Training loss: 0.7014\n",
      "Epoch: 746/2000... Training loss: 0.7946\n",
      "Epoch: 746/2000... Training loss: 0.5852\n",
      "Epoch: 746/2000... Training loss: 0.6386\n",
      "Epoch: 746/2000... Training loss: 0.3713\n",
      "Epoch: 746/2000... Training loss: 0.6092\n",
      "Epoch: 747/2000... Training loss: 0.7877\n",
      "Epoch: 747/2000... Training loss: 0.5804\n",
      "Epoch: 747/2000... Training loss: 0.5804\n",
      "Epoch: 747/2000... Training loss: 0.7575\n",
      "Epoch: 747/2000... Training loss: 0.6640\n",
      "Epoch: 747/2000... Training loss: 0.8432\n",
      "Epoch: 747/2000... Training loss: 0.6042\n",
      "Epoch: 747/2000... Training loss: 0.8263\n",
      "Epoch: 747/2000... Training loss: 0.8375\n",
      "Epoch: 747/2000... Training loss: 0.5808\n",
      "Epoch: 747/2000... Training loss: 0.8516\n",
      "Epoch: 747/2000... Training loss: 0.6283\n",
      "Epoch: 747/2000... Training loss: 0.5691\n",
      "Epoch: 747/2000... Training loss: 0.7320\n",
      "Epoch: 747/2000... Training loss: 0.7614\n",
      "Epoch: 747/2000... Training loss: 0.8851\n",
      "Epoch: 747/2000... Training loss: 0.8292\n",
      "Epoch: 747/2000... Training loss: 0.8890\n",
      "Epoch: 747/2000... Training loss: 0.6296\n",
      "Epoch: 747/2000... Training loss: 0.6295\n",
      "Epoch: 747/2000... Training loss: 0.8731\n",
      "Epoch: 747/2000... Training loss: 0.6381\n",
      "Epoch: 747/2000... Training loss: 0.8081\n",
      "Epoch: 747/2000... Training loss: 0.7295\n",
      "Epoch: 747/2000... Training loss: 0.6031\n",
      "Epoch: 747/2000... Training loss: 0.7366\n",
      "Epoch: 747/2000... Training loss: 0.6994\n",
      "Epoch: 747/2000... Training loss: 0.8079\n",
      "Epoch: 747/2000... Training loss: 0.4831\n",
      "Epoch: 747/2000... Training loss: 0.7336\n",
      "Epoch: 747/2000... Training loss: 0.5725\n",
      "Epoch: 748/2000... Training loss: 0.7043\n",
      "Epoch: 748/2000... Training loss: 0.8057\n",
      "Epoch: 748/2000... Training loss: 0.6368\n",
      "Epoch: 748/2000... Training loss: 0.8063\n",
      "Epoch: 748/2000... Training loss: 0.7177\n",
      "Epoch: 748/2000... Training loss: 0.7349\n",
      "Epoch: 748/2000... Training loss: 0.6170\n",
      "Epoch: 748/2000... Training loss: 0.8583\n",
      "Epoch: 748/2000... Training loss: 0.6518\n",
      "Epoch: 748/2000... Training loss: 0.6939\n",
      "Epoch: 748/2000... Training loss: 0.6443\n",
      "Epoch: 748/2000... Training loss: 0.7827\n",
      "Epoch: 748/2000... Training loss: 0.7196\n",
      "Epoch: 748/2000... Training loss: 0.9103\n",
      "Epoch: 748/2000... Training loss: 0.7479\n",
      "Epoch: 748/2000... Training loss: 0.6910\n",
      "Epoch: 748/2000... Training loss: 0.7470\n",
      "Epoch: 748/2000... Training loss: 0.7577\n",
      "Epoch: 748/2000... Training loss: 0.5136\n",
      "Epoch: 748/2000... Training loss: 0.7603\n",
      "Epoch: 748/2000... Training loss: 0.6811\n",
      "Epoch: 748/2000... Training loss: 0.7192\n",
      "Epoch: 748/2000... Training loss: 0.7216\n",
      "Epoch: 748/2000... Training loss: 0.6532\n",
      "Epoch: 748/2000... Training loss: 0.8361\n",
      "Epoch: 748/2000... Training loss: 0.6616\n",
      "Epoch: 748/2000... Training loss: 0.9002\n",
      "Epoch: 748/2000... Training loss: 0.7368\n",
      "Epoch: 748/2000... Training loss: 0.7120\n",
      "Epoch: 748/2000... Training loss: 0.6872\n",
      "Epoch: 748/2000... Training loss: 0.5432\n",
      "Epoch: 749/2000... Training loss: 0.8513\n",
      "Epoch: 749/2000... Training loss: 0.8965\n",
      "Epoch: 749/2000... Training loss: 0.7970\n",
      "Epoch: 749/2000... Training loss: 0.7397\n",
      "Epoch: 749/2000... Training loss: 0.7514\n",
      "Epoch: 749/2000... Training loss: 0.7325\n",
      "Epoch: 749/2000... Training loss: 0.6033\n",
      "Epoch: 749/2000... Training loss: 0.8508\n",
      "Epoch: 749/2000... Training loss: 0.8001\n",
      "Epoch: 749/2000... Training loss: 0.8348\n",
      "Epoch: 749/2000... Training loss: 0.6502\n",
      "Epoch: 749/2000... Training loss: 0.7160\n",
      "Epoch: 749/2000... Training loss: 0.8026\n",
      "Epoch: 749/2000... Training loss: 0.6246\n",
      "Epoch: 749/2000... Training loss: 0.7745\n",
      "Epoch: 749/2000... Training loss: 0.5463\n",
      "Epoch: 749/2000... Training loss: 0.5504\n",
      "Epoch: 749/2000... Training loss: 0.5127\n",
      "Epoch: 749/2000... Training loss: 0.5222\n",
      "Epoch: 749/2000... Training loss: 0.7322\n",
      "Epoch: 749/2000... Training loss: 0.6533\n",
      "Epoch: 749/2000... Training loss: 0.5870\n",
      "Epoch: 749/2000... Training loss: 0.9281\n",
      "Epoch: 749/2000... Training loss: 0.8738\n",
      "Epoch: 749/2000... Training loss: 0.6817\n",
      "Epoch: 749/2000... Training loss: 0.7596\n",
      "Epoch: 749/2000... Training loss: 0.7431\n",
      "Epoch: 749/2000... Training loss: 0.8551\n",
      "Epoch: 749/2000... Training loss: 0.7501\n",
      "Epoch: 749/2000... Training loss: 0.9561\n",
      "Epoch: 749/2000... Training loss: 0.7125\n",
      "Epoch: 750/2000... Training loss: 0.6826\n",
      "Epoch: 750/2000... Training loss: 0.3999\n",
      "Epoch: 750/2000... Training loss: 0.7116\n",
      "Epoch: 750/2000... Training loss: 0.8551\n",
      "Epoch: 750/2000... Training loss: 0.7589\n",
      "Epoch: 750/2000... Training loss: 0.6306\n",
      "Epoch: 750/2000... Training loss: 0.6304\n",
      "Epoch: 750/2000... Training loss: 0.6995\n",
      "Epoch: 750/2000... Training loss: 0.7168\n",
      "Epoch: 750/2000... Training loss: 0.7418\n",
      "Epoch: 750/2000... Training loss: 0.7054\n",
      "Epoch: 750/2000... Training loss: 0.5634\n",
      "Epoch: 750/2000... Training loss: 0.6646\n",
      "Epoch: 750/2000... Training loss: 0.7219\n",
      "Epoch: 750/2000... Training loss: 0.7767\n",
      "Epoch: 750/2000... Training loss: 0.7351\n",
      "Epoch: 750/2000... Training loss: 0.5363\n",
      "Epoch: 750/2000... Training loss: 0.6583\n",
      "Epoch: 750/2000... Training loss: 0.6839\n",
      "Epoch: 750/2000... Training loss: 0.7590\n",
      "Epoch: 750/2000... Training loss: 0.8246\n",
      "Epoch: 750/2000... Training loss: 0.6910\n",
      "Epoch: 750/2000... Training loss: 0.8439\n",
      "Epoch: 750/2000... Training loss: 0.4817\n",
      "Epoch: 750/2000... Training loss: 0.7046\n",
      "Epoch: 750/2000... Training loss: 0.6748\n",
      "Epoch: 750/2000... Training loss: 0.6577\n",
      "Epoch: 750/2000... Training loss: 0.7411\n",
      "Epoch: 750/2000... Training loss: 0.6193\n",
      "Epoch: 750/2000... Training loss: 0.6642\n",
      "Epoch: 750/2000... Training loss: 0.8056\n",
      "Epoch: 751/2000... Training loss: 0.7436\n",
      "Epoch: 751/2000... Training loss: 0.6303\n",
      "Epoch: 751/2000... Training loss: 0.7674\n",
      "Epoch: 751/2000... Training loss: 0.5161\n",
      "Epoch: 751/2000... Training loss: 0.8445\n",
      "Epoch: 751/2000... Training loss: 0.7998\n",
      "Epoch: 751/2000... Training loss: 0.5536\n",
      "Epoch: 751/2000... Training loss: 0.7201\n",
      "Epoch: 751/2000... Training loss: 0.7927\n",
      "Epoch: 751/2000... Training loss: 0.6022\n",
      "Epoch: 751/2000... Training loss: 0.6922\n",
      "Epoch: 751/2000... Training loss: 0.6264\n",
      "Epoch: 751/2000... Training loss: 0.6179\n",
      "Epoch: 751/2000... Training loss: 0.5583\n",
      "Epoch: 751/2000... Training loss: 0.7753\n",
      "Epoch: 751/2000... Training loss: 0.7179\n",
      "Epoch: 751/2000... Training loss: 0.7728\n",
      "Epoch: 751/2000... Training loss: 0.6807\n",
      "Epoch: 751/2000... Training loss: 0.7588\n",
      "Epoch: 751/2000... Training loss: 0.7824\n",
      "Epoch: 751/2000... Training loss: 0.8529\n",
      "Epoch: 751/2000... Training loss: 0.5962\n",
      "Epoch: 751/2000... Training loss: 0.6209\n",
      "Epoch: 751/2000... Training loss: 0.8133\n",
      "Epoch: 751/2000... Training loss: 0.7025\n",
      "Epoch: 751/2000... Training loss: 0.5445\n",
      "Epoch: 751/2000... Training loss: 0.6824\n",
      "Epoch: 751/2000... Training loss: 0.7187\n",
      "Epoch: 751/2000... Training loss: 0.5312\n",
      "Epoch: 751/2000... Training loss: 0.6760\n",
      "Epoch: 751/2000... Training loss: 0.5949\n",
      "Epoch: 752/2000... Training loss: 0.9810\n",
      "Epoch: 752/2000... Training loss: 0.5097\n",
      "Epoch: 752/2000... Training loss: 0.7541\n",
      "Epoch: 752/2000... Training loss: 0.7325\n",
      "Epoch: 752/2000... Training loss: 0.6635\n",
      "Epoch: 752/2000... Training loss: 0.6655\n",
      "Epoch: 752/2000... Training loss: 0.6572\n",
      "Epoch: 752/2000... Training loss: 0.7459\n",
      "Epoch: 752/2000... Training loss: 0.6582\n",
      "Epoch: 752/2000... Training loss: 0.7496\n",
      "Epoch: 752/2000... Training loss: 0.8288\n",
      "Epoch: 752/2000... Training loss: 0.7942\n",
      "Epoch: 752/2000... Training loss: 0.7702\n",
      "Epoch: 752/2000... Training loss: 0.7324\n",
      "Epoch: 752/2000... Training loss: 0.7609\n",
      "Epoch: 752/2000... Training loss: 0.7706\n",
      "Epoch: 752/2000... Training loss: 0.6983\n",
      "Epoch: 752/2000... Training loss: 0.7493\n",
      "Epoch: 752/2000... Training loss: 0.6731\n",
      "Epoch: 752/2000... Training loss: 0.8282\n",
      "Epoch: 752/2000... Training loss: 0.6303\n",
      "Epoch: 752/2000... Training loss: 0.6894\n",
      "Epoch: 752/2000... Training loss: 0.7360\n",
      "Epoch: 752/2000... Training loss: 0.6571\n",
      "Epoch: 752/2000... Training loss: 0.7962\n",
      "Epoch: 752/2000... Training loss: 0.5997\n",
      "Epoch: 752/2000... Training loss: 0.6815\n",
      "Epoch: 752/2000... Training loss: 0.5296\n",
      "Epoch: 752/2000... Training loss: 0.6259\n",
      "Epoch: 752/2000... Training loss: 0.6594\n",
      "Epoch: 752/2000... Training loss: 0.6543\n",
      "Epoch: 753/2000... Training loss: 0.6538\n",
      "Epoch: 753/2000... Training loss: 0.7237\n",
      "Epoch: 753/2000... Training loss: 0.4667\n",
      "Epoch: 753/2000... Training loss: 0.8841\n",
      "Epoch: 753/2000... Training loss: 0.6004\n",
      "Epoch: 753/2000... Training loss: 0.7344\n",
      "Epoch: 753/2000... Training loss: 0.7715\n",
      "Epoch: 753/2000... Training loss: 0.8229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 753/2000... Training loss: 0.7287\n",
      "Epoch: 753/2000... Training loss: 0.6482\n",
      "Epoch: 753/2000... Training loss: 0.7046\n",
      "Epoch: 753/2000... Training loss: 0.5783\n",
      "Epoch: 753/2000... Training loss: 0.6663\n",
      "Epoch: 753/2000... Training loss: 0.5986\n",
      "Epoch: 753/2000... Training loss: 0.5512\n",
      "Epoch: 753/2000... Training loss: 0.8100\n",
      "Epoch: 753/2000... Training loss: 0.7947\n",
      "Epoch: 753/2000... Training loss: 0.6682\n",
      "Epoch: 753/2000... Training loss: 0.5919\n",
      "Epoch: 753/2000... Training loss: 0.6056\n",
      "Epoch: 753/2000... Training loss: 0.6976\n",
      "Epoch: 753/2000... Training loss: 0.7394\n",
      "Epoch: 753/2000... Training loss: 0.5567\n",
      "Epoch: 753/2000... Training loss: 0.8840\n",
      "Epoch: 753/2000... Training loss: 0.5959\n",
      "Epoch: 753/2000... Training loss: 0.4862\n",
      "Epoch: 753/2000... Training loss: 0.4084\n",
      "Epoch: 753/2000... Training loss: 0.6780\n",
      "Epoch: 753/2000... Training loss: 0.8015\n",
      "Epoch: 753/2000... Training loss: 0.4913\n",
      "Epoch: 753/2000... Training loss: 0.6899\n",
      "Epoch: 754/2000... Training loss: 0.6185\n",
      "Epoch: 754/2000... Training loss: 0.5251\n",
      "Epoch: 754/2000... Training loss: 0.7324\n",
      "Epoch: 754/2000... Training loss: 0.8525\n",
      "Epoch: 754/2000... Training loss: 0.6132\n",
      "Epoch: 754/2000... Training loss: 0.6100\n",
      "Epoch: 754/2000... Training loss: 0.6902\n",
      "Epoch: 754/2000... Training loss: 0.6030\n",
      "Epoch: 754/2000... Training loss: 0.7671\n",
      "Epoch: 754/2000... Training loss: 0.7360\n",
      "Epoch: 754/2000... Training loss: 0.9450\n",
      "Epoch: 754/2000... Training loss: 0.6604\n",
      "Epoch: 754/2000... Training loss: 0.7469\n",
      "Epoch: 754/2000... Training loss: 0.7567\n",
      "Epoch: 754/2000... Training loss: 0.7653\n",
      "Epoch: 754/2000... Training loss: 0.6180\n",
      "Epoch: 754/2000... Training loss: 0.7144\n",
      "Epoch: 754/2000... Training loss: 0.4203\n",
      "Epoch: 754/2000... Training loss: 0.7627\n",
      "Epoch: 754/2000... Training loss: 0.6502\n",
      "Epoch: 754/2000... Training loss: 0.5037\n",
      "Epoch: 754/2000... Training loss: 0.6695\n",
      "Epoch: 754/2000... Training loss: 0.7225\n",
      "Epoch: 754/2000... Training loss: 0.7154\n",
      "Epoch: 754/2000... Training loss: 0.6137\n",
      "Epoch: 754/2000... Training loss: 0.6178\n",
      "Epoch: 754/2000... Training loss: 0.8052\n",
      "Epoch: 754/2000... Training loss: 0.7656\n",
      "Epoch: 754/2000... Training loss: 0.6473\n",
      "Epoch: 754/2000... Training loss: 0.4394\n",
      "Epoch: 754/2000... Training loss: 0.6707\n",
      "Epoch: 755/2000... Training loss: 0.7692\n",
      "Epoch: 755/2000... Training loss: 0.8240\n",
      "Epoch: 755/2000... Training loss: 0.8107\n",
      "Epoch: 755/2000... Training loss: 0.6892\n",
      "Epoch: 755/2000... Training loss: 0.7934\n",
      "Epoch: 755/2000... Training loss: 0.6646\n",
      "Epoch: 755/2000... Training loss: 0.8373\n",
      "Epoch: 755/2000... Training loss: 0.9044\n",
      "Epoch: 755/2000... Training loss: 0.6356\n",
      "Epoch: 755/2000... Training loss: 0.6416\n",
      "Epoch: 755/2000... Training loss: 0.5479\n",
      "Epoch: 755/2000... Training loss: 0.7289\n",
      "Epoch: 755/2000... Training loss: 0.7391\n",
      "Epoch: 755/2000... Training loss: 0.7545\n",
      "Epoch: 755/2000... Training loss: 0.8644\n",
      "Epoch: 755/2000... Training loss: 0.5764\n",
      "Epoch: 755/2000... Training loss: 0.8236\n",
      "Epoch: 755/2000... Training loss: 1.0116\n",
      "Epoch: 755/2000... Training loss: 0.5944\n",
      "Epoch: 755/2000... Training loss: 0.7945\n",
      "Epoch: 755/2000... Training loss: 0.6700\n",
      "Epoch: 755/2000... Training loss: 0.7118\n",
      "Epoch: 755/2000... Training loss: 0.8631\n",
      "Epoch: 755/2000... Training loss: 0.6631\n",
      "Epoch: 755/2000... Training loss: 0.8358\n",
      "Epoch: 755/2000... Training loss: 0.6933\n",
      "Epoch: 755/2000... Training loss: 0.6250\n",
      "Epoch: 755/2000... Training loss: 0.6159\n",
      "Epoch: 755/2000... Training loss: 0.7244\n",
      "Epoch: 755/2000... Training loss: 0.7618\n",
      "Epoch: 755/2000... Training loss: 0.8478\n",
      "Epoch: 756/2000... Training loss: 0.7196\n",
      "Epoch: 756/2000... Training loss: 0.8249\n",
      "Epoch: 756/2000... Training loss: 0.6377\n",
      "Epoch: 756/2000... Training loss: 0.5486\n",
      "Epoch: 756/2000... Training loss: 0.8021\n",
      "Epoch: 756/2000... Training loss: 0.6056\n",
      "Epoch: 756/2000... Training loss: 0.4980\n",
      "Epoch: 756/2000... Training loss: 0.9193\n",
      "Epoch: 756/2000... Training loss: 0.6734\n",
      "Epoch: 756/2000... Training loss: 0.6205\n",
      "Epoch: 756/2000... Training loss: 0.6068\n",
      "Epoch: 756/2000... Training loss: 0.8757\n",
      "Epoch: 756/2000... Training loss: 0.5569\n",
      "Epoch: 756/2000... Training loss: 0.7021\n",
      "Epoch: 756/2000... Training loss: 0.7314\n",
      "Epoch: 756/2000... Training loss: 0.6292\n",
      "Epoch: 756/2000... Training loss: 0.5517\n",
      "Epoch: 756/2000... Training loss: 0.6374\n",
      "Epoch: 756/2000... Training loss: 0.7347\n",
      "Epoch: 756/2000... Training loss: 0.6705\n",
      "Epoch: 756/2000... Training loss: 0.8809\n",
      "Epoch: 756/2000... Training loss: 0.6710\n",
      "Epoch: 756/2000... Training loss: 0.6982\n",
      "Epoch: 756/2000... Training loss: 0.8538\n",
      "Epoch: 756/2000... Training loss: 0.6995\n",
      "Epoch: 756/2000... Training loss: 0.8052\n",
      "Epoch: 756/2000... Training loss: 0.5034\n",
      "Epoch: 756/2000... Training loss: 0.6992\n",
      "Epoch: 756/2000... Training loss: 0.6124\n",
      "Epoch: 756/2000... Training loss: 0.5647\n",
      "Epoch: 756/2000... Training loss: 0.8092\n",
      "Epoch: 757/2000... Training loss: 0.5786\n",
      "Epoch: 757/2000... Training loss: 0.6200\n",
      "Epoch: 757/2000... Training loss: 0.7328\n",
      "Epoch: 757/2000... Training loss: 0.7378\n",
      "Epoch: 757/2000... Training loss: 0.5890\n",
      "Epoch: 757/2000... Training loss: 0.5973\n",
      "Epoch: 757/2000... Training loss: 0.6074\n",
      "Epoch: 757/2000... Training loss: 0.6278\n",
      "Epoch: 757/2000... Training loss: 0.7052\n",
      "Epoch: 757/2000... Training loss: 0.5519\n",
      "Epoch: 757/2000... Training loss: 0.8157\n",
      "Epoch: 757/2000... Training loss: 0.5725\n",
      "Epoch: 757/2000... Training loss: 0.8788\n",
      "Epoch: 757/2000... Training loss: 0.6602\n",
      "Epoch: 757/2000... Training loss: 0.8297\n",
      "Epoch: 757/2000... Training loss: 0.8064\n",
      "Epoch: 757/2000... Training loss: 0.6933\n",
      "Epoch: 757/2000... Training loss: 0.6543\n",
      "Epoch: 757/2000... Training loss: 0.6198\n",
      "Epoch: 757/2000... Training loss: 0.7858\n",
      "Epoch: 757/2000... Training loss: 0.7449\n",
      "Epoch: 757/2000... Training loss: 0.9501\n",
      "Epoch: 757/2000... Training loss: 0.8544\n",
      "Epoch: 757/2000... Training loss: 0.7177\n",
      "Epoch: 757/2000... Training loss: 0.7837\n",
      "Epoch: 757/2000... Training loss: 0.6635\n",
      "Epoch: 757/2000... Training loss: 0.5652\n",
      "Epoch: 757/2000... Training loss: 0.8929\n",
      "Epoch: 757/2000... Training loss: 0.7164\n",
      "Epoch: 757/2000... Training loss: 0.5746\n",
      "Epoch: 757/2000... Training loss: 0.6604\n",
      "Epoch: 758/2000... Training loss: 0.8885\n",
      "Epoch: 758/2000... Training loss: 0.7366\n",
      "Epoch: 758/2000... Training loss: 0.7078\n",
      "Epoch: 758/2000... Training loss: 0.6671\n",
      "Epoch: 758/2000... Training loss: 0.6389\n",
      "Epoch: 758/2000... Training loss: 0.6803\n",
      "Epoch: 758/2000... Training loss: 0.7120\n",
      "Epoch: 758/2000... Training loss: 0.4148\n",
      "Epoch: 758/2000... Training loss: 0.7723\n",
      "Epoch: 758/2000... Training loss: 0.7132\n",
      "Epoch: 758/2000... Training loss: 0.7009\n",
      "Epoch: 758/2000... Training loss: 0.5768\n",
      "Epoch: 758/2000... Training loss: 0.4926\n",
      "Epoch: 758/2000... Training loss: 0.8700\n",
      "Epoch: 758/2000... Training loss: 0.7246\n",
      "Epoch: 758/2000... Training loss: 0.6347\n",
      "Epoch: 758/2000... Training loss: 0.7684\n",
      "Epoch: 758/2000... Training loss: 0.9076\n",
      "Epoch: 758/2000... Training loss: 0.6539\n",
      "Epoch: 758/2000... Training loss: 0.6216\n",
      "Epoch: 758/2000... Training loss: 0.6867\n",
      "Epoch: 758/2000... Training loss: 0.7106\n",
      "Epoch: 758/2000... Training loss: 0.6966\n",
      "Epoch: 758/2000... Training loss: 0.7334\n",
      "Epoch: 758/2000... Training loss: 0.6264\n",
      "Epoch: 758/2000... Training loss: 0.9675\n",
      "Epoch: 758/2000... Training loss: 0.4911\n",
      "Epoch: 758/2000... Training loss: 0.6442\n",
      "Epoch: 758/2000... Training loss: 0.6786\n",
      "Epoch: 758/2000... Training loss: 0.6923\n",
      "Epoch: 758/2000... Training loss: 0.7905\n",
      "Epoch: 759/2000... Training loss: 0.5792\n",
      "Epoch: 759/2000... Training loss: 0.7553\n",
      "Epoch: 759/2000... Training loss: 0.7056\n",
      "Epoch: 759/2000... Training loss: 0.5976\n",
      "Epoch: 759/2000... Training loss: 0.7450\n",
      "Epoch: 759/2000... Training loss: 0.6488\n",
      "Epoch: 759/2000... Training loss: 0.5538\n",
      "Epoch: 759/2000... Training loss: 0.6535\n",
      "Epoch: 759/2000... Training loss: 0.8102\n",
      "Epoch: 759/2000... Training loss: 0.6159\n",
      "Epoch: 759/2000... Training loss: 0.6429\n",
      "Epoch: 759/2000... Training loss: 0.6470\n",
      "Epoch: 759/2000... Training loss: 0.4928\n",
      "Epoch: 759/2000... Training loss: 0.5463\n",
      "Epoch: 759/2000... Training loss: 0.8399\n",
      "Epoch: 759/2000... Training loss: 0.6816\n",
      "Epoch: 759/2000... Training loss: 0.7367\n",
      "Epoch: 759/2000... Training loss: 0.6241\n",
      "Epoch: 759/2000... Training loss: 0.7374\n",
      "Epoch: 759/2000... Training loss: 0.6964\n",
      "Epoch: 759/2000... Training loss: 0.8390\n",
      "Epoch: 759/2000... Training loss: 0.4536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 759/2000... Training loss: 0.5613\n",
      "Epoch: 759/2000... Training loss: 0.7548\n",
      "Epoch: 759/2000... Training loss: 0.6230\n",
      "Epoch: 759/2000... Training loss: 0.7506\n",
      "Epoch: 759/2000... Training loss: 0.7594\n",
      "Epoch: 759/2000... Training loss: 0.6524\n",
      "Epoch: 759/2000... Training loss: 0.6363\n",
      "Epoch: 759/2000... Training loss: 0.6961\n",
      "Epoch: 759/2000... Training loss: 0.5758\n",
      "Epoch: 760/2000... Training loss: 0.7152\n",
      "Epoch: 760/2000... Training loss: 0.4975\n",
      "Epoch: 760/2000... Training loss: 0.5828\n",
      "Epoch: 760/2000... Training loss: 0.6969\n",
      "Epoch: 760/2000... Training loss: 0.7576\n",
      "Epoch: 760/2000... Training loss: 0.7388\n",
      "Epoch: 760/2000... Training loss: 0.8009\n",
      "Epoch: 760/2000... Training loss: 0.6960\n",
      "Epoch: 760/2000... Training loss: 0.7049\n",
      "Epoch: 760/2000... Training loss: 0.6432\n",
      "Epoch: 760/2000... Training loss: 0.5712\n",
      "Epoch: 760/2000... Training loss: 0.7089\n",
      "Epoch: 760/2000... Training loss: 0.7675\n",
      "Epoch: 760/2000... Training loss: 0.8118\n",
      "Epoch: 760/2000... Training loss: 0.7503\n",
      "Epoch: 760/2000... Training loss: 0.6838\n",
      "Epoch: 760/2000... Training loss: 0.5122\n",
      "Epoch: 760/2000... Training loss: 0.8228\n",
      "Epoch: 760/2000... Training loss: 0.5423\n",
      "Epoch: 760/2000... Training loss: 0.6458\n",
      "Epoch: 760/2000... Training loss: 0.4734\n",
      "Epoch: 760/2000... Training loss: 0.5971\n",
      "Epoch: 760/2000... Training loss: 0.7910\n",
      "Epoch: 760/2000... Training loss: 0.5904\n",
      "Epoch: 760/2000... Training loss: 0.7760\n",
      "Epoch: 760/2000... Training loss: 0.5686\n",
      "Epoch: 760/2000... Training loss: 0.5218\n",
      "Epoch: 760/2000... Training loss: 0.8637\n",
      "Epoch: 760/2000... Training loss: 0.7088\n",
      "Epoch: 760/2000... Training loss: 0.5735\n",
      "Epoch: 760/2000... Training loss: 0.9024\n",
      "Epoch: 761/2000... Training loss: 0.7178\n",
      "Epoch: 761/2000... Training loss: 0.7255\n",
      "Epoch: 761/2000... Training loss: 0.8295\n",
      "Epoch: 761/2000... Training loss: 0.6979\n",
      "Epoch: 761/2000... Training loss: 0.8013\n",
      "Epoch: 761/2000... Training loss: 0.6336\n",
      "Epoch: 761/2000... Training loss: 0.4732\n",
      "Epoch: 761/2000... Training loss: 0.7808\n",
      "Epoch: 761/2000... Training loss: 0.4657\n",
      "Epoch: 761/2000... Training loss: 0.9815\n",
      "Epoch: 761/2000... Training loss: 0.6713\n",
      "Epoch: 761/2000... Training loss: 0.6246\n",
      "Epoch: 761/2000... Training loss: 0.5514\n",
      "Epoch: 761/2000... Training loss: 0.6102\n",
      "Epoch: 761/2000... Training loss: 0.6542\n",
      "Epoch: 761/2000... Training loss: 0.6844\n",
      "Epoch: 761/2000... Training loss: 0.8053\n",
      "Epoch: 761/2000... Training loss: 0.5956\n",
      "Epoch: 761/2000... Training loss: 0.7462\n",
      "Epoch: 761/2000... Training loss: 0.6670\n",
      "Epoch: 761/2000... Training loss: 0.7474\n",
      "Epoch: 761/2000... Training loss: 0.6864\n",
      "Epoch: 761/2000... Training loss: 0.6243\n",
      "Epoch: 761/2000... Training loss: 0.9104\n",
      "Epoch: 761/2000... Training loss: 0.5751\n",
      "Epoch: 761/2000... Training loss: 0.6706\n",
      "Epoch: 761/2000... Training loss: 0.5568\n",
      "Epoch: 761/2000... Training loss: 0.6859\n",
      "Epoch: 761/2000... Training loss: 0.7004\n",
      "Epoch: 761/2000... Training loss: 0.7883\n",
      "Epoch: 761/2000... Training loss: 0.8611\n",
      "Epoch: 762/2000... Training loss: 0.6339\n",
      "Epoch: 762/2000... Training loss: 0.5771\n",
      "Epoch: 762/2000... Training loss: 0.7379\n",
      "Epoch: 762/2000... Training loss: 0.5919\n",
      "Epoch: 762/2000... Training loss: 0.7265\n",
      "Epoch: 762/2000... Training loss: 0.7206\n",
      "Epoch: 762/2000... Training loss: 0.5713\n",
      "Epoch: 762/2000... Training loss: 0.6357\n",
      "Epoch: 762/2000... Training loss: 0.7737\n",
      "Epoch: 762/2000... Training loss: 0.6844\n",
      "Epoch: 762/2000... Training loss: 0.8121\n",
      "Epoch: 762/2000... Training loss: 0.6182\n",
      "Epoch: 762/2000... Training loss: 0.5421\n",
      "Epoch: 762/2000... Training loss: 0.7907\n",
      "Epoch: 762/2000... Training loss: 0.7540\n",
      "Epoch: 762/2000... Training loss: 0.6038\n",
      "Epoch: 762/2000... Training loss: 0.6625\n",
      "Epoch: 762/2000... Training loss: 0.7668\n",
      "Epoch: 762/2000... Training loss: 0.8711\n",
      "Epoch: 762/2000... Training loss: 0.7659\n",
      "Epoch: 762/2000... Training loss: 0.8083\n",
      "Epoch: 762/2000... Training loss: 0.7397\n",
      "Epoch: 762/2000... Training loss: 0.6964\n",
      "Epoch: 762/2000... Training loss: 0.5373\n",
      "Epoch: 762/2000... Training loss: 0.6988\n",
      "Epoch: 762/2000... Training loss: 0.5331\n",
      "Epoch: 762/2000... Training loss: 0.6105\n",
      "Epoch: 762/2000... Training loss: 0.7535\n",
      "Epoch: 762/2000... Training loss: 0.7765\n",
      "Epoch: 762/2000... Training loss: 0.6377\n",
      "Epoch: 762/2000... Training loss: 0.6164\n",
      "Epoch: 763/2000... Training loss: 0.6376\n",
      "Epoch: 763/2000... Training loss: 0.7380\n",
      "Epoch: 763/2000... Training loss: 0.6743\n",
      "Epoch: 763/2000... Training loss: 0.7900\n",
      "Epoch: 763/2000... Training loss: 0.6256\n",
      "Epoch: 763/2000... Training loss: 0.6250\n",
      "Epoch: 763/2000... Training loss: 0.6555\n",
      "Epoch: 763/2000... Training loss: 0.7129\n",
      "Epoch: 763/2000... Training loss: 0.8262\n",
      "Epoch: 763/2000... Training loss: 0.5502\n",
      "Epoch: 763/2000... Training loss: 0.7271\n",
      "Epoch: 763/2000... Training loss: 0.6276\n",
      "Epoch: 763/2000... Training loss: 0.7246\n",
      "Epoch: 763/2000... Training loss: 0.7798\n",
      "Epoch: 763/2000... Training loss: 0.7245\n",
      "Epoch: 763/2000... Training loss: 0.6578\n",
      "Epoch: 763/2000... Training loss: 0.6679\n",
      "Epoch: 763/2000... Training loss: 0.8853\n",
      "Epoch: 763/2000... Training loss: 0.6914\n",
      "Epoch: 763/2000... Training loss: 0.6821\n",
      "Epoch: 763/2000... Training loss: 0.6726\n",
      "Epoch: 763/2000... Training loss: 0.5899\n",
      "Epoch: 763/2000... Training loss: 0.6043\n",
      "Epoch: 763/2000... Training loss: 0.7792\n",
      "Epoch: 763/2000... Training loss: 0.6224\n",
      "Epoch: 763/2000... Training loss: 0.5461\n",
      "Epoch: 763/2000... Training loss: 0.7448\n",
      "Epoch: 763/2000... Training loss: 0.7706\n",
      "Epoch: 763/2000... Training loss: 0.5706\n",
      "Epoch: 763/2000... Training loss: 0.7269\n",
      "Epoch: 763/2000... Training loss: 0.7826\n",
      "Epoch: 764/2000... Training loss: 0.7165\n",
      "Epoch: 764/2000... Training loss: 0.6856\n",
      "Epoch: 764/2000... Training loss: 0.7335\n",
      "Epoch: 764/2000... Training loss: 0.5669\n",
      "Epoch: 764/2000... Training loss: 0.6446\n",
      "Epoch: 764/2000... Training loss: 0.7110\n",
      "Epoch: 764/2000... Training loss: 0.7509\n",
      "Epoch: 764/2000... Training loss: 0.7276\n",
      "Epoch: 764/2000... Training loss: 0.5593\n",
      "Epoch: 764/2000... Training loss: 0.8543\n",
      "Epoch: 764/2000... Training loss: 0.7468\n",
      "Epoch: 764/2000... Training loss: 0.7014\n",
      "Epoch: 764/2000... Training loss: 0.7149\n",
      "Epoch: 764/2000... Training loss: 0.8961\n",
      "Epoch: 764/2000... Training loss: 0.7571\n",
      "Epoch: 764/2000... Training loss: 0.5492\n",
      "Epoch: 764/2000... Training loss: 0.6841\n",
      "Epoch: 764/2000... Training loss: 0.7018\n",
      "Epoch: 764/2000... Training loss: 0.6281\n",
      "Epoch: 764/2000... Training loss: 0.6777\n",
      "Epoch: 764/2000... Training loss: 0.6011\n",
      "Epoch: 764/2000... Training loss: 0.5654\n",
      "Epoch: 764/2000... Training loss: 0.7344\n",
      "Epoch: 764/2000... Training loss: 0.6071\n",
      "Epoch: 764/2000... Training loss: 0.5385\n",
      "Epoch: 764/2000... Training loss: 0.7454\n",
      "Epoch: 764/2000... Training loss: 0.6952\n",
      "Epoch: 764/2000... Training loss: 0.5314\n",
      "Epoch: 764/2000... Training loss: 0.4551\n",
      "Epoch: 764/2000... Training loss: 0.7946\n",
      "Epoch: 764/2000... Training loss: 0.7414\n",
      "Epoch: 765/2000... Training loss: 0.5958\n",
      "Epoch: 765/2000... Training loss: 0.6792\n",
      "Epoch: 765/2000... Training loss: 0.7280\n",
      "Epoch: 765/2000... Training loss: 0.7076\n",
      "Epoch: 765/2000... Training loss: 0.7291\n",
      "Epoch: 765/2000... Training loss: 0.7320\n",
      "Epoch: 765/2000... Training loss: 0.5053\n",
      "Epoch: 765/2000... Training loss: 0.7250\n",
      "Epoch: 765/2000... Training loss: 0.6819\n",
      "Epoch: 765/2000... Training loss: 0.6850\n",
      "Epoch: 765/2000... Training loss: 0.5760\n",
      "Epoch: 765/2000... Training loss: 0.6922\n",
      "Epoch: 765/2000... Training loss: 0.6062\n",
      "Epoch: 765/2000... Training loss: 0.5413\n",
      "Epoch: 765/2000... Training loss: 0.6715\n",
      "Epoch: 765/2000... Training loss: 0.8146\n",
      "Epoch: 765/2000... Training loss: 0.6930\n",
      "Epoch: 765/2000... Training loss: 0.7301\n",
      "Epoch: 765/2000... Training loss: 0.5450\n",
      "Epoch: 765/2000... Training loss: 0.6130\n",
      "Epoch: 765/2000... Training loss: 0.5859\n",
      "Epoch: 765/2000... Training loss: 0.9661\n",
      "Epoch: 765/2000... Training loss: 0.7676\n",
      "Epoch: 765/2000... Training loss: 0.6379\n",
      "Epoch: 765/2000... Training loss: 0.8517\n",
      "Epoch: 765/2000... Training loss: 0.9340\n",
      "Epoch: 765/2000... Training loss: 0.7656\n",
      "Epoch: 765/2000... Training loss: 0.8281\n",
      "Epoch: 765/2000... Training loss: 0.7579\n",
      "Epoch: 765/2000... Training loss: 0.4609\n",
      "Epoch: 765/2000... Training loss: 0.6914\n",
      "Epoch: 766/2000... Training loss: 0.5865\n",
      "Epoch: 766/2000... Training loss: 0.5959\n",
      "Epoch: 766/2000... Training loss: 0.8996\n",
      "Epoch: 766/2000... Training loss: 0.9071\n",
      "Epoch: 766/2000... Training loss: 0.7935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 766/2000... Training loss: 0.6236\n",
      "Epoch: 766/2000... Training loss: 0.5004\n",
      "Epoch: 766/2000... Training loss: 0.7620\n",
      "Epoch: 766/2000... Training loss: 0.5787\n",
      "Epoch: 766/2000... Training loss: 0.6260\n",
      "Epoch: 766/2000... Training loss: 0.5879\n",
      "Epoch: 766/2000... Training loss: 0.9176\n",
      "Epoch: 766/2000... Training loss: 0.7870\n",
      "Epoch: 766/2000... Training loss: 0.6455\n",
      "Epoch: 766/2000... Training loss: 0.8032\n",
      "Epoch: 766/2000... Training loss: 0.6420\n",
      "Epoch: 766/2000... Training loss: 0.7666\n",
      "Epoch: 766/2000... Training loss: 0.5481\n",
      "Epoch: 766/2000... Training loss: 0.5187\n",
      "Epoch: 766/2000... Training loss: 0.6724\n",
      "Epoch: 766/2000... Training loss: 0.6526\n",
      "Epoch: 766/2000... Training loss: 0.5894\n",
      "Epoch: 766/2000... Training loss: 0.6181\n",
      "Epoch: 766/2000... Training loss: 0.5115\n",
      "Epoch: 766/2000... Training loss: 0.9394\n",
      "Epoch: 766/2000... Training loss: 0.5714\n",
      "Epoch: 766/2000... Training loss: 0.6485\n",
      "Epoch: 766/2000... Training loss: 0.8440\n",
      "Epoch: 766/2000... Training loss: 0.4651\n",
      "Epoch: 766/2000... Training loss: 0.6680\n",
      "Epoch: 766/2000... Training loss: 0.6507\n",
      "Epoch: 767/2000... Training loss: 0.6793\n",
      "Epoch: 767/2000... Training loss: 0.8013\n",
      "Epoch: 767/2000... Training loss: 0.6012\n",
      "Epoch: 767/2000... Training loss: 0.5962\n",
      "Epoch: 767/2000... Training loss: 0.5587\n",
      "Epoch: 767/2000... Training loss: 0.5988\n",
      "Epoch: 767/2000... Training loss: 0.7886\n",
      "Epoch: 767/2000... Training loss: 0.5226\n",
      "Epoch: 767/2000... Training loss: 0.7284\n",
      "Epoch: 767/2000... Training loss: 0.5833\n",
      "Epoch: 767/2000... Training loss: 0.6510\n",
      "Epoch: 767/2000... Training loss: 0.8964\n",
      "Epoch: 767/2000... Training loss: 0.7739\n",
      "Epoch: 767/2000... Training loss: 0.5787\n",
      "Epoch: 767/2000... Training loss: 0.5834\n",
      "Epoch: 767/2000... Training loss: 1.1043\n",
      "Epoch: 767/2000... Training loss: 0.6715\n",
      "Epoch: 767/2000... Training loss: 0.5430\n",
      "Epoch: 767/2000... Training loss: 0.8214\n",
      "Epoch: 767/2000... Training loss: 0.7349\n",
      "Epoch: 767/2000... Training loss: 0.5984\n",
      "Epoch: 767/2000... Training loss: 0.6254\n",
      "Epoch: 767/2000... Training loss: 0.6952\n",
      "Epoch: 767/2000... Training loss: 0.7307\n",
      "Epoch: 767/2000... Training loss: 0.5772\n",
      "Epoch: 767/2000... Training loss: 0.7140\n",
      "Epoch: 767/2000... Training loss: 0.5911\n",
      "Epoch: 767/2000... Training loss: 0.6317\n",
      "Epoch: 767/2000... Training loss: 0.9082\n",
      "Epoch: 767/2000... Training loss: 0.7717\n",
      "Epoch: 767/2000... Training loss: 0.6851\n",
      "Epoch: 768/2000... Training loss: 0.6723\n",
      "Epoch: 768/2000... Training loss: 0.7666\n",
      "Epoch: 768/2000... Training loss: 0.6134\n",
      "Epoch: 768/2000... Training loss: 0.7935\n",
      "Epoch: 768/2000... Training loss: 0.5831\n",
      "Epoch: 768/2000... Training loss: 0.8253\n",
      "Epoch: 768/2000... Training loss: 0.6839\n",
      "Epoch: 768/2000... Training loss: 0.7451\n",
      "Epoch: 768/2000... Training loss: 0.5720\n",
      "Epoch: 768/2000... Training loss: 0.6588\n",
      "Epoch: 768/2000... Training loss: 0.7628\n",
      "Epoch: 768/2000... Training loss: 0.8140\n",
      "Epoch: 768/2000... Training loss: 0.7188\n",
      "Epoch: 768/2000... Training loss: 0.5826\n",
      "Epoch: 768/2000... Training loss: 0.6680\n",
      "Epoch: 768/2000... Training loss: 0.9606\n",
      "Epoch: 768/2000... Training loss: 0.7579\n",
      "Epoch: 768/2000... Training loss: 0.7832\n",
      "Epoch: 768/2000... Training loss: 0.8190\n",
      "Epoch: 768/2000... Training loss: 0.7063\n",
      "Epoch: 768/2000... Training loss: 0.5917\n",
      "Epoch: 768/2000... Training loss: 0.6985\n",
      "Epoch: 768/2000... Training loss: 0.6091\n",
      "Epoch: 768/2000... Training loss: 0.7398\n",
      "Epoch: 768/2000... Training loss: 0.8552\n",
      "Epoch: 768/2000... Training loss: 0.5176\n",
      "Epoch: 768/2000... Training loss: 0.6159\n",
      "Epoch: 768/2000... Training loss: 0.5819\n",
      "Epoch: 768/2000... Training loss: 0.6636\n",
      "Epoch: 768/2000... Training loss: 0.7041\n",
      "Epoch: 768/2000... Training loss: 0.6073\n",
      "Epoch: 769/2000... Training loss: 0.6838\n",
      "Epoch: 769/2000... Training loss: 0.7105\n",
      "Epoch: 769/2000... Training loss: 0.6211\n",
      "Epoch: 769/2000... Training loss: 0.8001\n",
      "Epoch: 769/2000... Training loss: 0.6913\n",
      "Epoch: 769/2000... Training loss: 0.5973\n",
      "Epoch: 769/2000... Training loss: 0.7529\n",
      "Epoch: 769/2000... Training loss: 0.9344\n",
      "Epoch: 769/2000... Training loss: 0.7660\n",
      "Epoch: 769/2000... Training loss: 0.7788\n",
      "Epoch: 769/2000... Training loss: 0.6433\n",
      "Epoch: 769/2000... Training loss: 0.8597\n",
      "Epoch: 769/2000... Training loss: 0.6621\n",
      "Epoch: 769/2000... Training loss: 0.6401\n",
      "Epoch: 769/2000... Training loss: 0.7565\n",
      "Epoch: 769/2000... Training loss: 0.6390\n",
      "Epoch: 769/2000... Training loss: 0.5497\n",
      "Epoch: 769/2000... Training loss: 0.7286\n",
      "Epoch: 769/2000... Training loss: 0.5315\n",
      "Epoch: 769/2000... Training loss: 0.6423\n",
      "Epoch: 769/2000... Training loss: 0.6045\n",
      "Epoch: 769/2000... Training loss: 0.5900\n",
      "Epoch: 769/2000... Training loss: 0.6687\n",
      "Epoch: 769/2000... Training loss: 0.6964\n",
      "Epoch: 769/2000... Training loss: 0.5143\n",
      "Epoch: 769/2000... Training loss: 0.6315\n",
      "Epoch: 769/2000... Training loss: 0.6576\n",
      "Epoch: 769/2000... Training loss: 0.4696\n",
      "Epoch: 769/2000... Training loss: 0.4809\n",
      "Epoch: 769/2000... Training loss: 0.6996\n",
      "Epoch: 769/2000... Training loss: 0.5784\n",
      "Epoch: 770/2000... Training loss: 0.8089\n",
      "Epoch: 770/2000... Training loss: 0.7089\n",
      "Epoch: 770/2000... Training loss: 0.8155\n",
      "Epoch: 770/2000... Training loss: 0.7514\n",
      "Epoch: 770/2000... Training loss: 0.6493\n",
      "Epoch: 770/2000... Training loss: 0.6631\n",
      "Epoch: 770/2000... Training loss: 0.6858\n",
      "Epoch: 770/2000... Training loss: 0.6783\n",
      "Epoch: 770/2000... Training loss: 0.6543\n",
      "Epoch: 770/2000... Training loss: 0.8868\n",
      "Epoch: 770/2000... Training loss: 0.5996\n",
      "Epoch: 770/2000... Training loss: 0.5107\n",
      "Epoch: 770/2000... Training loss: 0.7598\n",
      "Epoch: 770/2000... Training loss: 0.5739\n",
      "Epoch: 770/2000... Training loss: 0.6275\n",
      "Epoch: 770/2000... Training loss: 0.6572\n",
      "Epoch: 770/2000... Training loss: 0.7232\n",
      "Epoch: 770/2000... Training loss: 0.3853\n",
      "Epoch: 770/2000... Training loss: 0.5723\n",
      "Epoch: 770/2000... Training loss: 0.7580\n",
      "Epoch: 770/2000... Training loss: 1.0028\n",
      "Epoch: 770/2000... Training loss: 0.6568\n",
      "Epoch: 770/2000... Training loss: 0.7176\n",
      "Epoch: 770/2000... Training loss: 0.5909\n",
      "Epoch: 770/2000... Training loss: 0.7124\n",
      "Epoch: 770/2000... Training loss: 0.6426\n",
      "Epoch: 770/2000... Training loss: 0.8701\n",
      "Epoch: 770/2000... Training loss: 0.6519\n",
      "Epoch: 770/2000... Training loss: 0.7465\n",
      "Epoch: 770/2000... Training loss: 0.4555\n",
      "Epoch: 770/2000... Training loss: 0.5680\n",
      "Epoch: 771/2000... Training loss: 0.5954\n",
      "Epoch: 771/2000... Training loss: 0.6149\n",
      "Epoch: 771/2000... Training loss: 0.8467\n",
      "Epoch: 771/2000... Training loss: 0.7739\n",
      "Epoch: 771/2000... Training loss: 0.6030\n",
      "Epoch: 771/2000... Training loss: 0.7867\n",
      "Epoch: 771/2000... Training loss: 0.7790\n",
      "Epoch: 771/2000... Training loss: 0.5211\n",
      "Epoch: 771/2000... Training loss: 0.7995\n",
      "Epoch: 771/2000... Training loss: 0.6796\n",
      "Epoch: 771/2000... Training loss: 0.5564\n",
      "Epoch: 771/2000... Training loss: 0.6132\n",
      "Epoch: 771/2000... Training loss: 0.7741\n",
      "Epoch: 771/2000... Training loss: 0.5678\n",
      "Epoch: 771/2000... Training loss: 0.7411\n",
      "Epoch: 771/2000... Training loss: 0.8234\n",
      "Epoch: 771/2000... Training loss: 0.6237\n",
      "Epoch: 771/2000... Training loss: 0.3682\n",
      "Epoch: 771/2000... Training loss: 0.5898\n",
      "Epoch: 771/2000... Training loss: 0.7140\n",
      "Epoch: 771/2000... Training loss: 0.6827\n",
      "Epoch: 771/2000... Training loss: 0.6086\n",
      "Epoch: 771/2000... Training loss: 0.8238\n",
      "Epoch: 771/2000... Training loss: 0.7143\n",
      "Epoch: 771/2000... Training loss: 0.8376\n",
      "Epoch: 771/2000... Training loss: 0.6012\n",
      "Epoch: 771/2000... Training loss: 0.6728\n",
      "Epoch: 771/2000... Training loss: 0.7768\n",
      "Epoch: 771/2000... Training loss: 0.7749\n",
      "Epoch: 771/2000... Training loss: 0.5607\n",
      "Epoch: 771/2000... Training loss: 0.6790\n",
      "Epoch: 772/2000... Training loss: 0.7177\n",
      "Epoch: 772/2000... Training loss: 0.6466\n",
      "Epoch: 772/2000... Training loss: 0.6958\n",
      "Epoch: 772/2000... Training loss: 0.6440\n",
      "Epoch: 772/2000... Training loss: 0.9449\n",
      "Epoch: 772/2000... Training loss: 0.4385\n",
      "Epoch: 772/2000... Training loss: 0.6754\n",
      "Epoch: 772/2000... Training loss: 0.5927\n",
      "Epoch: 772/2000... Training loss: 0.6660\n",
      "Epoch: 772/2000... Training loss: 0.4360\n",
      "Epoch: 772/2000... Training loss: 0.5583\n",
      "Epoch: 772/2000... Training loss: 0.6518\n",
      "Epoch: 772/2000... Training loss: 0.7154\n",
      "Epoch: 772/2000... Training loss: 0.7161\n",
      "Epoch: 772/2000... Training loss: 0.6745\n",
      "Epoch: 772/2000... Training loss: 0.7680\n",
      "Epoch: 772/2000... Training loss: 0.6290\n",
      "Epoch: 772/2000... Training loss: 0.6065\n",
      "Epoch: 772/2000... Training loss: 0.5434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 772/2000... Training loss: 0.5571\n",
      "Epoch: 772/2000... Training loss: 0.6779\n",
      "Epoch: 772/2000... Training loss: 0.4814\n",
      "Epoch: 772/2000... Training loss: 0.6563\n",
      "Epoch: 772/2000... Training loss: 0.6406\n",
      "Epoch: 772/2000... Training loss: 0.5709\n",
      "Epoch: 772/2000... Training loss: 0.5338\n",
      "Epoch: 772/2000... Training loss: 0.6998\n",
      "Epoch: 772/2000... Training loss: 0.7809\n",
      "Epoch: 772/2000... Training loss: 0.6254\n",
      "Epoch: 772/2000... Training loss: 0.8472\n",
      "Epoch: 772/2000... Training loss: 0.7978\n",
      "Epoch: 773/2000... Training loss: 0.5631\n",
      "Epoch: 773/2000... Training loss: 0.6795\n",
      "Epoch: 773/2000... Training loss: 0.7251\n",
      "Epoch: 773/2000... Training loss: 0.5552\n",
      "Epoch: 773/2000... Training loss: 0.6601\n",
      "Epoch: 773/2000... Training loss: 0.6871\n",
      "Epoch: 773/2000... Training loss: 0.5610\n",
      "Epoch: 773/2000... Training loss: 0.6281\n",
      "Epoch: 773/2000... Training loss: 0.5561\n",
      "Epoch: 773/2000... Training loss: 0.5911\n",
      "Epoch: 773/2000... Training loss: 0.5983\n",
      "Epoch: 773/2000... Training loss: 0.6422\n",
      "Epoch: 773/2000... Training loss: 0.8686\n",
      "Epoch: 773/2000... Training loss: 0.5337\n",
      "Epoch: 773/2000... Training loss: 0.6086\n",
      "Epoch: 773/2000... Training loss: 0.5586\n",
      "Epoch: 773/2000... Training loss: 0.6352\n",
      "Epoch: 773/2000... Training loss: 0.6377\n",
      "Epoch: 773/2000... Training loss: 0.7093\n",
      "Epoch: 773/2000... Training loss: 0.6790\n",
      "Epoch: 773/2000... Training loss: 0.6061\n",
      "Epoch: 773/2000... Training loss: 0.6062\n",
      "Epoch: 773/2000... Training loss: 0.7098\n",
      "Epoch: 773/2000... Training loss: 0.7666\n",
      "Epoch: 773/2000... Training loss: 0.5533\n",
      "Epoch: 773/2000... Training loss: 0.6330\n",
      "Epoch: 773/2000... Training loss: 0.7047\n",
      "Epoch: 773/2000... Training loss: 0.6287\n",
      "Epoch: 773/2000... Training loss: 0.5986\n",
      "Epoch: 773/2000... Training loss: 0.5646\n",
      "Epoch: 773/2000... Training loss: 0.8643\n",
      "Epoch: 774/2000... Training loss: 0.6761\n",
      "Epoch: 774/2000... Training loss: 0.8690\n",
      "Epoch: 774/2000... Training loss: 0.8461\n",
      "Epoch: 774/2000... Training loss: 0.6880\n",
      "Epoch: 774/2000... Training loss: 0.6810\n",
      "Epoch: 774/2000... Training loss: 0.6809\n",
      "Epoch: 774/2000... Training loss: 0.6648\n",
      "Epoch: 774/2000... Training loss: 0.8543\n",
      "Epoch: 774/2000... Training loss: 0.5566\n",
      "Epoch: 774/2000... Training loss: 0.5820\n",
      "Epoch: 774/2000... Training loss: 0.6544\n",
      "Epoch: 774/2000... Training loss: 0.6359\n",
      "Epoch: 774/2000... Training loss: 0.8011\n",
      "Epoch: 774/2000... Training loss: 0.5194\n",
      "Epoch: 774/2000... Training loss: 0.7357\n",
      "Epoch: 774/2000... Training loss: 0.8843\n",
      "Epoch: 774/2000... Training loss: 0.6475\n",
      "Epoch: 774/2000... Training loss: 0.5947\n",
      "Epoch: 774/2000... Training loss: 0.6165\n",
      "Epoch: 774/2000... Training loss: 0.7746\n",
      "Epoch: 774/2000... Training loss: 0.8766\n",
      "Epoch: 774/2000... Training loss: 0.7385\n",
      "Epoch: 774/2000... Training loss: 0.6831\n",
      "Epoch: 774/2000... Training loss: 0.7984\n",
      "Epoch: 774/2000... Training loss: 0.7163\n",
      "Epoch: 774/2000... Training loss: 0.4860\n",
      "Epoch: 774/2000... Training loss: 0.7127\n",
      "Epoch: 774/2000... Training loss: 0.5971\n",
      "Epoch: 774/2000... Training loss: 0.5304\n",
      "Epoch: 774/2000... Training loss: 0.5019\n",
      "Epoch: 774/2000... Training loss: 0.5800\n",
      "Epoch: 775/2000... Training loss: 0.5356\n",
      "Epoch: 775/2000... Training loss: 0.5633\n",
      "Epoch: 775/2000... Training loss: 0.9413\n",
      "Epoch: 775/2000... Training loss: 0.6386\n",
      "Epoch: 775/2000... Training loss: 0.7067\n",
      "Epoch: 775/2000... Training loss: 0.7671\n",
      "Epoch: 775/2000... Training loss: 0.7829\n",
      "Epoch: 775/2000... Training loss: 0.5563\n",
      "Epoch: 775/2000... Training loss: 0.7335\n",
      "Epoch: 775/2000... Training loss: 0.7844\n",
      "Epoch: 775/2000... Training loss: 0.7922\n",
      "Epoch: 775/2000... Training loss: 0.5247\n",
      "Epoch: 775/2000... Training loss: 0.7071\n",
      "Epoch: 775/2000... Training loss: 0.6710\n",
      "Epoch: 775/2000... Training loss: 0.7321\n",
      "Epoch: 775/2000... Training loss: 0.8122\n",
      "Epoch: 775/2000... Training loss: 0.9013\n",
      "Epoch: 775/2000... Training loss: 0.6430\n",
      "Epoch: 775/2000... Training loss: 0.7797\n",
      "Epoch: 775/2000... Training loss: 0.9269\n",
      "Epoch: 775/2000... Training loss: 0.5809\n",
      "Epoch: 775/2000... Training loss: 0.7254\n",
      "Epoch: 775/2000... Training loss: 0.7333\n",
      "Epoch: 775/2000... Training loss: 0.7032\n",
      "Epoch: 775/2000... Training loss: 0.5942\n",
      "Epoch: 775/2000... Training loss: 0.8827\n",
      "Epoch: 775/2000... Training loss: 0.6522\n",
      "Epoch: 775/2000... Training loss: 0.6576\n",
      "Epoch: 775/2000... Training loss: 0.5269\n",
      "Epoch: 775/2000... Training loss: 0.7643\n",
      "Epoch: 775/2000... Training loss: 0.6364\n",
      "Epoch: 776/2000... Training loss: 0.7070\n",
      "Epoch: 776/2000... Training loss: 0.7095\n",
      "Epoch: 776/2000... Training loss: 0.7079\n",
      "Epoch: 776/2000... Training loss: 0.6034\n",
      "Epoch: 776/2000... Training loss: 0.7950\n",
      "Epoch: 776/2000... Training loss: 0.6907\n",
      "Epoch: 776/2000... Training loss: 0.6837\n",
      "Epoch: 776/2000... Training loss: 0.7692\n",
      "Epoch: 776/2000... Training loss: 0.7136\n",
      "Epoch: 776/2000... Training loss: 0.6744\n",
      "Epoch: 776/2000... Training loss: 0.5838\n",
      "Epoch: 776/2000... Training loss: 0.7746\n",
      "Epoch: 776/2000... Training loss: 0.6377\n",
      "Epoch: 776/2000... Training loss: 0.5175\n",
      "Epoch: 776/2000... Training loss: 0.6168\n",
      "Epoch: 776/2000... Training loss: 0.6724\n",
      "Epoch: 776/2000... Training loss: 0.8093\n",
      "Epoch: 776/2000... Training loss: 0.8666\n",
      "Epoch: 776/2000... Training loss: 0.7790\n",
      "Epoch: 776/2000... Training loss: 0.5669\n",
      "Epoch: 776/2000... Training loss: 0.6505\n",
      "Epoch: 776/2000... Training loss: 0.7079\n",
      "Epoch: 776/2000... Training loss: 0.4623\n",
      "Epoch: 776/2000... Training loss: 0.4975\n",
      "Epoch: 776/2000... Training loss: 0.6008\n",
      "Epoch: 776/2000... Training loss: 0.6630\n",
      "Epoch: 776/2000... Training loss: 0.5569\n",
      "Epoch: 776/2000... Training loss: 0.5801\n",
      "Epoch: 776/2000... Training loss: 0.7557\n",
      "Epoch: 776/2000... Training loss: 0.6069\n",
      "Epoch: 776/2000... Training loss: 0.6326\n",
      "Epoch: 777/2000... Training loss: 0.6722\n",
      "Epoch: 777/2000... Training loss: 0.8554\n",
      "Epoch: 777/2000... Training loss: 0.8792\n",
      "Epoch: 777/2000... Training loss: 0.6018\n",
      "Epoch: 777/2000... Training loss: 0.8078\n",
      "Epoch: 777/2000... Training loss: 0.5767\n",
      "Epoch: 777/2000... Training loss: 0.7206\n",
      "Epoch: 777/2000... Training loss: 0.5726\n",
      "Epoch: 777/2000... Training loss: 0.5954\n",
      "Epoch: 777/2000... Training loss: 0.6670\n",
      "Epoch: 777/2000... Training loss: 0.7090\n",
      "Epoch: 777/2000... Training loss: 0.8300\n",
      "Epoch: 777/2000... Training loss: 0.7313\n",
      "Epoch: 777/2000... Training loss: 0.6176\n",
      "Epoch: 777/2000... Training loss: 0.5584\n",
      "Epoch: 777/2000... Training loss: 0.8727\n",
      "Epoch: 777/2000... Training loss: 0.5800\n",
      "Epoch: 777/2000... Training loss: 0.6261\n",
      "Epoch: 777/2000... Training loss: 0.7887\n",
      "Epoch: 777/2000... Training loss: 0.7697\n",
      "Epoch: 777/2000... Training loss: 0.6462\n",
      "Epoch: 777/2000... Training loss: 0.7022\n",
      "Epoch: 777/2000... Training loss: 0.7361\n",
      "Epoch: 777/2000... Training loss: 0.7890\n",
      "Epoch: 777/2000... Training loss: 0.6901\n",
      "Epoch: 777/2000... Training loss: 0.5858\n",
      "Epoch: 777/2000... Training loss: 0.7792\n",
      "Epoch: 777/2000... Training loss: 0.9169\n",
      "Epoch: 777/2000... Training loss: 0.8707\n",
      "Epoch: 777/2000... Training loss: 0.6529\n",
      "Epoch: 777/2000... Training loss: 0.5369\n",
      "Epoch: 778/2000... Training loss: 0.6156\n",
      "Epoch: 778/2000... Training loss: 0.6134\n",
      "Epoch: 778/2000... Training loss: 0.7359\n",
      "Epoch: 778/2000... Training loss: 0.7192\n",
      "Epoch: 778/2000... Training loss: 0.4673\n",
      "Epoch: 778/2000... Training loss: 0.6580\n",
      "Epoch: 778/2000... Training loss: 0.6868\n",
      "Epoch: 778/2000... Training loss: 0.5435\n",
      "Epoch: 778/2000... Training loss: 0.6827\n",
      "Epoch: 778/2000... Training loss: 0.5768\n",
      "Epoch: 778/2000... Training loss: 0.5484\n",
      "Epoch: 778/2000... Training loss: 0.7624\n",
      "Epoch: 778/2000... Training loss: 0.5052\n",
      "Epoch: 778/2000... Training loss: 0.5549\n",
      "Epoch: 778/2000... Training loss: 0.7100\n",
      "Epoch: 778/2000... Training loss: 0.7916\n",
      "Epoch: 778/2000... Training loss: 0.5908\n",
      "Epoch: 778/2000... Training loss: 0.7459\n",
      "Epoch: 778/2000... Training loss: 0.6941\n",
      "Epoch: 778/2000... Training loss: 0.5898\n",
      "Epoch: 778/2000... Training loss: 0.7078\n",
      "Epoch: 778/2000... Training loss: 0.5645\n",
      "Epoch: 778/2000... Training loss: 0.6655\n",
      "Epoch: 778/2000... Training loss: 0.7063\n",
      "Epoch: 778/2000... Training loss: 0.7776\n",
      "Epoch: 778/2000... Training loss: 0.6852\n",
      "Epoch: 778/2000... Training loss: 0.4606\n",
      "Epoch: 778/2000... Training loss: 0.7009\n",
      "Epoch: 778/2000... Training loss: 0.7658\n",
      "Epoch: 778/2000... Training loss: 0.6223\n",
      "Epoch: 778/2000... Training loss: 0.7092\n",
      "Epoch: 779/2000... Training loss: 0.6459\n",
      "Epoch: 779/2000... Training loss: 0.5534\n",
      "Epoch: 779/2000... Training loss: 0.7072\n",
      "Epoch: 779/2000... Training loss: 0.3203\n",
      "Epoch: 779/2000... Training loss: 0.7074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 779/2000... Training loss: 0.6196\n",
      "Epoch: 779/2000... Training loss: 0.8292\n",
      "Epoch: 779/2000... Training loss: 0.6511\n",
      "Epoch: 779/2000... Training loss: 0.6606\n",
      "Epoch: 779/2000... Training loss: 0.5522\n",
      "Epoch: 779/2000... Training loss: 0.8345\n",
      "Epoch: 779/2000... Training loss: 0.5751\n",
      "Epoch: 779/2000... Training loss: 0.4433\n",
      "Epoch: 779/2000... Training loss: 0.7181\n",
      "Epoch: 779/2000... Training loss: 0.6666\n",
      "Epoch: 779/2000... Training loss: 0.5318\n",
      "Epoch: 779/2000... Training loss: 0.6817\n",
      "Epoch: 779/2000... Training loss: 0.6483\n",
      "Epoch: 779/2000... Training loss: 0.5802\n",
      "Epoch: 779/2000... Training loss: 0.5643\n",
      "Epoch: 779/2000... Training loss: 0.6832\n",
      "Epoch: 779/2000... Training loss: 0.7114\n",
      "Epoch: 779/2000... Training loss: 0.9171\n",
      "Epoch: 779/2000... Training loss: 0.6660\n",
      "Epoch: 779/2000... Training loss: 0.8468\n",
      "Epoch: 779/2000... Training loss: 0.4870\n",
      "Epoch: 779/2000... Training loss: 0.5544\n",
      "Epoch: 779/2000... Training loss: 0.5532\n",
      "Epoch: 779/2000... Training loss: 0.6237\n",
      "Epoch: 779/2000... Training loss: 0.6576\n",
      "Epoch: 779/2000... Training loss: 0.5995\n",
      "Epoch: 780/2000... Training loss: 0.4103\n",
      "Epoch: 780/2000... Training loss: 0.7682\n",
      "Epoch: 780/2000... Training loss: 0.6017\n",
      "Epoch: 780/2000... Training loss: 0.6208\n",
      "Epoch: 780/2000... Training loss: 0.7498\n",
      "Epoch: 780/2000... Training loss: 0.8178\n",
      "Epoch: 780/2000... Training loss: 0.4782\n",
      "Epoch: 780/2000... Training loss: 0.7773\n",
      "Epoch: 780/2000... Training loss: 0.5523\n",
      "Epoch: 780/2000... Training loss: 0.8335\n",
      "Epoch: 780/2000... Training loss: 0.7045\n",
      "Epoch: 780/2000... Training loss: 0.7602\n",
      "Epoch: 780/2000... Training loss: 0.6598\n",
      "Epoch: 780/2000... Training loss: 0.5766\n",
      "Epoch: 780/2000... Training loss: 0.6021\n",
      "Epoch: 780/2000... Training loss: 0.6670\n",
      "Epoch: 780/2000... Training loss: 0.7955\n",
      "Epoch: 780/2000... Training loss: 0.5296\n",
      "Epoch: 780/2000... Training loss: 0.6073\n",
      "Epoch: 780/2000... Training loss: 0.5396\n",
      "Epoch: 780/2000... Training loss: 0.8370\n",
      "Epoch: 780/2000... Training loss: 0.5673\n",
      "Epoch: 780/2000... Training loss: 0.5814\n",
      "Epoch: 780/2000... Training loss: 0.6537\n",
      "Epoch: 780/2000... Training loss: 0.8583\n",
      "Epoch: 780/2000... Training loss: 0.7382\n",
      "Epoch: 780/2000... Training loss: 0.7492\n",
      "Epoch: 780/2000... Training loss: 0.8173\n",
      "Epoch: 780/2000... Training loss: 0.4389\n",
      "Epoch: 780/2000... Training loss: 0.7376\n",
      "Epoch: 780/2000... Training loss: 0.5350\n",
      "Epoch: 781/2000... Training loss: 0.6745\n",
      "Epoch: 781/2000... Training loss: 0.5361\n",
      "Epoch: 781/2000... Training loss: 0.7987\n",
      "Epoch: 781/2000... Training loss: 0.7909\n",
      "Epoch: 781/2000... Training loss: 0.8334\n",
      "Epoch: 781/2000... Training loss: 0.5154\n",
      "Epoch: 781/2000... Training loss: 0.8631\n",
      "Epoch: 781/2000... Training loss: 0.6292\n",
      "Epoch: 781/2000... Training loss: 0.6013\n",
      "Epoch: 781/2000... Training loss: 0.8209\n",
      "Epoch: 781/2000... Training loss: 0.7428\n",
      "Epoch: 781/2000... Training loss: 0.5638\n",
      "Epoch: 781/2000... Training loss: 0.5111\n",
      "Epoch: 781/2000... Training loss: 0.6589\n",
      "Epoch: 781/2000... Training loss: 0.4903\n",
      "Epoch: 781/2000... Training loss: 0.7655\n",
      "Epoch: 781/2000... Training loss: 0.5774\n",
      "Epoch: 781/2000... Training loss: 0.6051\n",
      "Epoch: 781/2000... Training loss: 0.7301\n",
      "Epoch: 781/2000... Training loss: 0.7390\n",
      "Epoch: 781/2000... Training loss: 0.6671\n",
      "Epoch: 781/2000... Training loss: 0.6740\n",
      "Epoch: 781/2000... Training loss: 0.4503\n",
      "Epoch: 781/2000... Training loss: 0.6655\n",
      "Epoch: 781/2000... Training loss: 0.5655\n",
      "Epoch: 781/2000... Training loss: 0.8193\n",
      "Epoch: 781/2000... Training loss: 0.8089\n",
      "Epoch: 781/2000... Training loss: 0.6166\n",
      "Epoch: 781/2000... Training loss: 0.6831\n",
      "Epoch: 781/2000... Training loss: 0.6622\n",
      "Epoch: 781/2000... Training loss: 0.5995\n",
      "Epoch: 782/2000... Training loss: 0.6421\n",
      "Epoch: 782/2000... Training loss: 0.6600\n",
      "Epoch: 782/2000... Training loss: 0.6440\n",
      "Epoch: 782/2000... Training loss: 0.5518\n",
      "Epoch: 782/2000... Training loss: 0.5522\n",
      "Epoch: 782/2000... Training loss: 0.7533\n",
      "Epoch: 782/2000... Training loss: 0.6685\n",
      "Epoch: 782/2000... Training loss: 0.6242\n",
      "Epoch: 782/2000... Training loss: 0.6815\n",
      "Epoch: 782/2000... Training loss: 0.5888\n",
      "Epoch: 782/2000... Training loss: 0.5938\n",
      "Epoch: 782/2000... Training loss: 0.8301\n",
      "Epoch: 782/2000... Training loss: 0.7460\n",
      "Epoch: 782/2000... Training loss: 0.6555\n",
      "Epoch: 782/2000... Training loss: 0.6926\n",
      "Epoch: 782/2000... Training loss: 0.7670\n",
      "Epoch: 782/2000... Training loss: 0.7843\n",
      "Epoch: 782/2000... Training loss: 0.5678\n",
      "Epoch: 782/2000... Training loss: 0.8919\n",
      "Epoch: 782/2000... Training loss: 0.7573\n",
      "Epoch: 782/2000... Training loss: 0.6363\n",
      "Epoch: 782/2000... Training loss: 0.7094\n",
      "Epoch: 782/2000... Training loss: 0.5176\n",
      "Epoch: 782/2000... Training loss: 0.6633\n",
      "Epoch: 782/2000... Training loss: 0.7221\n",
      "Epoch: 782/2000... Training loss: 0.7209\n",
      "Epoch: 782/2000... Training loss: 0.6892\n",
      "Epoch: 782/2000... Training loss: 0.6206\n",
      "Epoch: 782/2000... Training loss: 0.6768\n",
      "Epoch: 782/2000... Training loss: 0.6631\n",
      "Epoch: 782/2000... Training loss: 0.6452\n",
      "Epoch: 783/2000... Training loss: 0.7219\n",
      "Epoch: 783/2000... Training loss: 0.6738\n",
      "Epoch: 783/2000... Training loss: 0.5967\n",
      "Epoch: 783/2000... Training loss: 0.6398\n",
      "Epoch: 783/2000... Training loss: 0.6546\n",
      "Epoch: 783/2000... Training loss: 0.6323\n",
      "Epoch: 783/2000... Training loss: 0.5692\n",
      "Epoch: 783/2000... Training loss: 0.6179\n",
      "Epoch: 783/2000... Training loss: 0.8090\n",
      "Epoch: 783/2000... Training loss: 0.7058\n",
      "Epoch: 783/2000... Training loss: 0.6279\n",
      "Epoch: 783/2000... Training loss: 0.5272\n",
      "Epoch: 783/2000... Training loss: 0.6438\n",
      "Epoch: 783/2000... Training loss: 0.6779\n",
      "Epoch: 783/2000... Training loss: 0.8955\n",
      "Epoch: 783/2000... Training loss: 0.6476\n",
      "Epoch: 783/2000... Training loss: 0.5659\n",
      "Epoch: 783/2000... Training loss: 0.6394\n",
      "Epoch: 783/2000... Training loss: 0.7212\n",
      "Epoch: 783/2000... Training loss: 0.6920\n",
      "Epoch: 783/2000... Training loss: 0.7119\n",
      "Epoch: 783/2000... Training loss: 0.6162\n",
      "Epoch: 783/2000... Training loss: 0.4861\n",
      "Epoch: 783/2000... Training loss: 0.6182\n",
      "Epoch: 783/2000... Training loss: 0.5779\n",
      "Epoch: 783/2000... Training loss: 0.5691\n",
      "Epoch: 783/2000... Training loss: 0.5702\n",
      "Epoch: 783/2000... Training loss: 0.7372\n",
      "Epoch: 783/2000... Training loss: 0.6296\n",
      "Epoch: 783/2000... Training loss: 0.6054\n",
      "Epoch: 783/2000... Training loss: 0.7168\n",
      "Epoch: 784/2000... Training loss: 0.5838\n",
      "Epoch: 784/2000... Training loss: 0.5642\n",
      "Epoch: 784/2000... Training loss: 0.8458\n",
      "Epoch: 784/2000... Training loss: 0.7555\n",
      "Epoch: 784/2000... Training loss: 0.8427\n",
      "Epoch: 784/2000... Training loss: 0.8391\n",
      "Epoch: 784/2000... Training loss: 0.4534\n",
      "Epoch: 784/2000... Training loss: 0.5924\n",
      "Epoch: 784/2000... Training loss: 0.6732\n",
      "Epoch: 784/2000... Training loss: 0.5467\n",
      "Epoch: 784/2000... Training loss: 0.9190\n",
      "Epoch: 784/2000... Training loss: 0.7816\n",
      "Epoch: 784/2000... Training loss: 0.6007\n",
      "Epoch: 784/2000... Training loss: 0.6005\n",
      "Epoch: 784/2000... Training loss: 0.8193\n",
      "Epoch: 784/2000... Training loss: 0.6216\n",
      "Epoch: 784/2000... Training loss: 0.7069\n",
      "Epoch: 784/2000... Training loss: 0.6970\n",
      "Epoch: 784/2000... Training loss: 0.7352\n",
      "Epoch: 784/2000... Training loss: 0.7351\n",
      "Epoch: 784/2000... Training loss: 0.7061\n",
      "Epoch: 784/2000... Training loss: 0.7582\n",
      "Epoch: 784/2000... Training loss: 0.7634\n",
      "Epoch: 784/2000... Training loss: 0.6403\n",
      "Epoch: 784/2000... Training loss: 0.8965\n",
      "Epoch: 784/2000... Training loss: 0.7277\n",
      "Epoch: 784/2000... Training loss: 0.5982\n",
      "Epoch: 784/2000... Training loss: 0.7817\n",
      "Epoch: 784/2000... Training loss: 0.5736\n",
      "Epoch: 784/2000... Training loss: 0.5599\n",
      "Epoch: 784/2000... Training loss: 0.3758\n",
      "Epoch: 785/2000... Training loss: 0.4593\n",
      "Epoch: 785/2000... Training loss: 0.7022\n",
      "Epoch: 785/2000... Training loss: 0.7953\n",
      "Epoch: 785/2000... Training loss: 0.6816\n",
      "Epoch: 785/2000... Training loss: 0.7237\n",
      "Epoch: 785/2000... Training loss: 0.5517\n",
      "Epoch: 785/2000... Training loss: 0.7320\n",
      "Epoch: 785/2000... Training loss: 0.7731\n",
      "Epoch: 785/2000... Training loss: 0.9023\n",
      "Epoch: 785/2000... Training loss: 0.6403\n",
      "Epoch: 785/2000... Training loss: 0.8586\n",
      "Epoch: 785/2000... Training loss: 0.7244\n",
      "Epoch: 785/2000... Training loss: 0.7473\n",
      "Epoch: 785/2000... Training loss: 0.5706\n",
      "Epoch: 785/2000... Training loss: 0.8057\n",
      "Epoch: 785/2000... Training loss: 0.5108\n",
      "Epoch: 785/2000... Training loss: 0.7658\n",
      "Epoch: 785/2000... Training loss: 0.5505\n",
      "Epoch: 785/2000... Training loss: 0.7546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 785/2000... Training loss: 0.8013\n",
      "Epoch: 785/2000... Training loss: 0.6350\n",
      "Epoch: 785/2000... Training loss: 0.6095\n",
      "Epoch: 785/2000... Training loss: 0.6626\n",
      "Epoch: 785/2000... Training loss: 0.7471\n",
      "Epoch: 785/2000... Training loss: 0.6041\n",
      "Epoch: 785/2000... Training loss: 0.7335\n",
      "Epoch: 785/2000... Training loss: 0.6685\n",
      "Epoch: 785/2000... Training loss: 0.4998\n",
      "Epoch: 785/2000... Training loss: 0.7559\n",
      "Epoch: 785/2000... Training loss: 0.7950\n",
      "Epoch: 785/2000... Training loss: 0.7152\n",
      "Epoch: 786/2000... Training loss: 0.5191\n",
      "Epoch: 786/2000... Training loss: 0.5988\n",
      "Epoch: 786/2000... Training loss: 0.5861\n",
      "Epoch: 786/2000... Training loss: 0.6370\n",
      "Epoch: 786/2000... Training loss: 0.6060\n",
      "Epoch: 786/2000... Training loss: 0.7937\n",
      "Epoch: 786/2000... Training loss: 0.8983\n",
      "Epoch: 786/2000... Training loss: 0.7593\n",
      "Epoch: 786/2000... Training loss: 0.7062\n",
      "Epoch: 786/2000... Training loss: 0.7584\n",
      "Epoch: 786/2000... Training loss: 0.6448\n",
      "Epoch: 786/2000... Training loss: 0.7859\n",
      "Epoch: 786/2000... Training loss: 0.7120\n",
      "Epoch: 786/2000... Training loss: 0.8420\n",
      "Epoch: 786/2000... Training loss: 0.5748\n",
      "Epoch: 786/2000... Training loss: 0.9126\n",
      "Epoch: 786/2000... Training loss: 0.5223\n",
      "Epoch: 786/2000... Training loss: 0.6137\n",
      "Epoch: 786/2000... Training loss: 0.6997\n",
      "Epoch: 786/2000... Training loss: 0.8314\n",
      "Epoch: 786/2000... Training loss: 0.6745\n",
      "Epoch: 786/2000... Training loss: 0.6503\n",
      "Epoch: 786/2000... Training loss: 0.6280\n",
      "Epoch: 786/2000... Training loss: 0.8792\n",
      "Epoch: 786/2000... Training loss: 0.8085\n",
      "Epoch: 786/2000... Training loss: 0.7836\n",
      "Epoch: 786/2000... Training loss: 0.6042\n",
      "Epoch: 786/2000... Training loss: 0.5562\n",
      "Epoch: 786/2000... Training loss: 0.5031\n",
      "Epoch: 786/2000... Training loss: 0.6655\n",
      "Epoch: 786/2000... Training loss: 0.7758\n",
      "Epoch: 787/2000... Training loss: 0.8146\n",
      "Epoch: 787/2000... Training loss: 0.6240\n",
      "Epoch: 787/2000... Training loss: 0.6789\n",
      "Epoch: 787/2000... Training loss: 0.5513\n",
      "Epoch: 787/2000... Training loss: 0.7792\n",
      "Epoch: 787/2000... Training loss: 0.6316\n",
      "Epoch: 787/2000... Training loss: 0.6677\n",
      "Epoch: 787/2000... Training loss: 0.6377\n",
      "Epoch: 787/2000... Training loss: 0.5767\n",
      "Epoch: 787/2000... Training loss: 0.7595\n",
      "Epoch: 787/2000... Training loss: 0.8475\n",
      "Epoch: 787/2000... Training loss: 0.8505\n",
      "Epoch: 787/2000... Training loss: 0.5565\n",
      "Epoch: 787/2000... Training loss: 0.6883\n",
      "Epoch: 787/2000... Training loss: 0.6294\n",
      "Epoch: 787/2000... Training loss: 0.7380\n",
      "Epoch: 787/2000... Training loss: 0.4493\n",
      "Epoch: 787/2000... Training loss: 0.6382\n",
      "Epoch: 787/2000... Training loss: 0.6927\n",
      "Epoch: 787/2000... Training loss: 0.7063\n",
      "Epoch: 787/2000... Training loss: 0.5235\n",
      "Epoch: 787/2000... Training loss: 0.7427\n",
      "Epoch: 787/2000... Training loss: 0.7088\n",
      "Epoch: 787/2000... Training loss: 0.8127\n",
      "Epoch: 787/2000... Training loss: 0.5687\n",
      "Epoch: 787/2000... Training loss: 0.5835\n",
      "Epoch: 787/2000... Training loss: 0.5981\n",
      "Epoch: 787/2000... Training loss: 0.5739\n",
      "Epoch: 787/2000... Training loss: 0.6442\n",
      "Epoch: 787/2000... Training loss: 0.4842\n",
      "Epoch: 787/2000... Training loss: 0.8348\n",
      "Epoch: 788/2000... Training loss: 0.7064\n",
      "Epoch: 788/2000... Training loss: 0.6312\n",
      "Epoch: 788/2000... Training loss: 1.0358\n",
      "Epoch: 788/2000... Training loss: 0.6376\n",
      "Epoch: 788/2000... Training loss: 0.6426\n",
      "Epoch: 788/2000... Training loss: 0.8439\n",
      "Epoch: 788/2000... Training loss: 0.9500\n",
      "Epoch: 788/2000... Training loss: 0.5488\n",
      "Epoch: 788/2000... Training loss: 0.4169\n",
      "Epoch: 788/2000... Training loss: 0.6767\n",
      "Epoch: 788/2000... Training loss: 0.7791\n",
      "Epoch: 788/2000... Training loss: 0.6029\n",
      "Epoch: 788/2000... Training loss: 0.8266\n",
      "Epoch: 788/2000... Training loss: 0.6730\n",
      "Epoch: 788/2000... Training loss: 0.8227\n",
      "Epoch: 788/2000... Training loss: 0.6101\n",
      "Epoch: 788/2000... Training loss: 0.7675\n",
      "Epoch: 788/2000... Training loss: 0.6548\n",
      "Epoch: 788/2000... Training loss: 0.6336\n",
      "Epoch: 788/2000... Training loss: 0.6156\n",
      "Epoch: 788/2000... Training loss: 0.7584\n",
      "Epoch: 788/2000... Training loss: 0.6010\n",
      "Epoch: 788/2000... Training loss: 0.5466\n",
      "Epoch: 788/2000... Training loss: 0.5240\n",
      "Epoch: 788/2000... Training loss: 0.7669\n",
      "Epoch: 788/2000... Training loss: 0.7720\n",
      "Epoch: 788/2000... Training loss: 0.8118\n",
      "Epoch: 788/2000... Training loss: 0.8382\n",
      "Epoch: 788/2000... Training loss: 0.5403\n",
      "Epoch: 788/2000... Training loss: 0.6672\n",
      "Epoch: 788/2000... Training loss: 0.6753\n",
      "Epoch: 789/2000... Training loss: 0.6683\n",
      "Epoch: 789/2000... Training loss: 0.5824\n",
      "Epoch: 789/2000... Training loss: 0.7171\n",
      "Epoch: 789/2000... Training loss: 0.7663\n",
      "Epoch: 789/2000... Training loss: 0.5635\n",
      "Epoch: 789/2000... Training loss: 0.6079\n",
      "Epoch: 789/2000... Training loss: 0.6158\n",
      "Epoch: 789/2000... Training loss: 0.6502\n",
      "Epoch: 789/2000... Training loss: 0.5614\n",
      "Epoch: 789/2000... Training loss: 0.5416\n",
      "Epoch: 789/2000... Training loss: 0.7671\n",
      "Epoch: 789/2000... Training loss: 0.7879\n",
      "Epoch: 789/2000... Training loss: 0.6835\n",
      "Epoch: 789/2000... Training loss: 0.6600\n",
      "Epoch: 789/2000... Training loss: 0.6319\n",
      "Epoch: 789/2000... Training loss: 0.5268\n",
      "Epoch: 789/2000... Training loss: 0.7350\n",
      "Epoch: 789/2000... Training loss: 0.7843\n",
      "Epoch: 789/2000... Training loss: 0.6009\n",
      "Epoch: 789/2000... Training loss: 0.6808\n",
      "Epoch: 789/2000... Training loss: 0.6944\n",
      "Epoch: 789/2000... Training loss: 0.5538\n",
      "Epoch: 789/2000... Training loss: 0.7531\n",
      "Epoch: 789/2000... Training loss: 0.5553\n",
      "Epoch: 789/2000... Training loss: 0.5591\n",
      "Epoch: 789/2000... Training loss: 0.8703\n",
      "Epoch: 789/2000... Training loss: 0.5820\n",
      "Epoch: 789/2000... Training loss: 0.4957\n",
      "Epoch: 789/2000... Training loss: 0.5805\n",
      "Epoch: 789/2000... Training loss: 0.6248\n",
      "Epoch: 789/2000... Training loss: 0.6927\n",
      "Epoch: 790/2000... Training loss: 0.6086\n",
      "Epoch: 790/2000... Training loss: 0.4666\n",
      "Epoch: 790/2000... Training loss: 0.5140\n",
      "Epoch: 790/2000... Training loss: 0.8472\n",
      "Epoch: 790/2000... Training loss: 0.6618\n",
      "Epoch: 790/2000... Training loss: 0.5682\n",
      "Epoch: 790/2000... Training loss: 0.9236\n",
      "Epoch: 790/2000... Training loss: 0.6075\n",
      "Epoch: 790/2000... Training loss: 0.6398\n",
      "Epoch: 790/2000... Training loss: 0.5199\n",
      "Epoch: 790/2000... Training loss: 0.6256\n",
      "Epoch: 790/2000... Training loss: 0.7069\n",
      "Epoch: 790/2000... Training loss: 0.4871\n",
      "Epoch: 790/2000... Training loss: 0.7349\n",
      "Epoch: 790/2000... Training loss: 0.7967\n",
      "Epoch: 790/2000... Training loss: 0.5943\n",
      "Epoch: 790/2000... Training loss: 0.5779\n",
      "Epoch: 790/2000... Training loss: 0.7473\n",
      "Epoch: 790/2000... Training loss: 0.7635\n",
      "Epoch: 790/2000... Training loss: 0.7836\n",
      "Epoch: 790/2000... Training loss: 0.7911\n",
      "Epoch: 790/2000... Training loss: 0.5686\n",
      "Epoch: 790/2000... Training loss: 0.6190\n",
      "Epoch: 790/2000... Training loss: 0.5245\n",
      "Epoch: 790/2000... Training loss: 0.6386\n",
      "Epoch: 790/2000... Training loss: 0.8405\n",
      "Epoch: 790/2000... Training loss: 0.5488\n",
      "Epoch: 790/2000... Training loss: 0.6191\n",
      "Epoch: 790/2000... Training loss: 0.4698\n",
      "Epoch: 790/2000... Training loss: 0.7981\n",
      "Epoch: 790/2000... Training loss: 0.5152\n",
      "Epoch: 791/2000... Training loss: 0.6282\n",
      "Epoch: 791/2000... Training loss: 0.6109\n",
      "Epoch: 791/2000... Training loss: 0.6437\n",
      "Epoch: 791/2000... Training loss: 0.4657\n",
      "Epoch: 791/2000... Training loss: 0.6453\n",
      "Epoch: 791/2000... Training loss: 0.6748\n",
      "Epoch: 791/2000... Training loss: 0.5647\n",
      "Epoch: 791/2000... Training loss: 0.6013\n",
      "Epoch: 791/2000... Training loss: 0.8324\n",
      "Epoch: 791/2000... Training loss: 0.7132\n",
      "Epoch: 791/2000... Training loss: 0.5456\n",
      "Epoch: 791/2000... Training loss: 0.4688\n",
      "Epoch: 791/2000... Training loss: 0.7736\n",
      "Epoch: 791/2000... Training loss: 0.6095\n",
      "Epoch: 791/2000... Training loss: 0.4901\n",
      "Epoch: 791/2000... Training loss: 0.6833\n",
      "Epoch: 791/2000... Training loss: 0.4257\n",
      "Epoch: 791/2000... Training loss: 0.4411\n",
      "Epoch: 791/2000... Training loss: 0.5224\n",
      "Epoch: 791/2000... Training loss: 0.8563\n",
      "Epoch: 791/2000... Training loss: 0.5820\n",
      "Epoch: 791/2000... Training loss: 0.6580\n",
      "Epoch: 791/2000... Training loss: 0.5726\n",
      "Epoch: 791/2000... Training loss: 0.6043\n",
      "Epoch: 791/2000... Training loss: 0.7622\n",
      "Epoch: 791/2000... Training loss: 0.6633\n",
      "Epoch: 791/2000... Training loss: 0.8006\n",
      "Epoch: 791/2000... Training loss: 0.4419\n",
      "Epoch: 791/2000... Training loss: 0.3902\n",
      "Epoch: 791/2000... Training loss: 0.5651\n",
      "Epoch: 791/2000... Training loss: 0.7183\n",
      "Epoch: 792/2000... Training loss: 0.7975\n",
      "Epoch: 792/2000... Training loss: 0.7078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 792/2000... Training loss: 0.6315\n",
      "Epoch: 792/2000... Training loss: 0.6974\n",
      "Epoch: 792/2000... Training loss: 0.6065\n",
      "Epoch: 792/2000... Training loss: 0.6669\n",
      "Epoch: 792/2000... Training loss: 0.8581\n",
      "Epoch: 792/2000... Training loss: 0.9645\n",
      "Epoch: 792/2000... Training loss: 0.7215\n",
      "Epoch: 792/2000... Training loss: 0.7876\n",
      "Epoch: 792/2000... Training loss: 0.6444\n",
      "Epoch: 792/2000... Training loss: 0.7214\n",
      "Epoch: 792/2000... Training loss: 0.5620\n",
      "Epoch: 792/2000... Training loss: 0.7073\n",
      "Epoch: 792/2000... Training loss: 0.4480\n",
      "Epoch: 792/2000... Training loss: 0.5661\n",
      "Epoch: 792/2000... Training loss: 0.6550\n",
      "Epoch: 792/2000... Training loss: 0.5823\n",
      "Epoch: 792/2000... Training loss: 0.6055\n",
      "Epoch: 792/2000... Training loss: 0.7814\n",
      "Epoch: 792/2000... Training loss: 0.6169\n",
      "Epoch: 792/2000... Training loss: 0.6769\n",
      "Epoch: 792/2000... Training loss: 0.7348\n",
      "Epoch: 792/2000... Training loss: 0.6630\n",
      "Epoch: 792/2000... Training loss: 0.7396\n",
      "Epoch: 792/2000... Training loss: 0.7471\n",
      "Epoch: 792/2000... Training loss: 0.6735\n",
      "Epoch: 792/2000... Training loss: 0.8519\n",
      "Epoch: 792/2000... Training loss: 0.7926\n",
      "Epoch: 792/2000... Training loss: 0.7242\n",
      "Epoch: 792/2000... Training loss: 0.5296\n",
      "Epoch: 793/2000... Training loss: 0.6160\n",
      "Epoch: 793/2000... Training loss: 0.7352\n",
      "Epoch: 793/2000... Training loss: 0.8088\n",
      "Epoch: 793/2000... Training loss: 0.7978\n",
      "Epoch: 793/2000... Training loss: 0.7759\n",
      "Epoch: 793/2000... Training loss: 0.6480\n",
      "Epoch: 793/2000... Training loss: 0.6851\n",
      "Epoch: 793/2000... Training loss: 0.5471\n",
      "Epoch: 793/2000... Training loss: 0.5689\n",
      "Epoch: 793/2000... Training loss: 0.6467\n",
      "Epoch: 793/2000... Training loss: 0.6446\n",
      "Epoch: 793/2000... Training loss: 0.6641\n",
      "Epoch: 793/2000... Training loss: 0.8814\n",
      "Epoch: 793/2000... Training loss: 0.4936\n",
      "Epoch: 793/2000... Training loss: 0.6650\n",
      "Epoch: 793/2000... Training loss: 0.6580\n",
      "Epoch: 793/2000... Training loss: 0.6430\n",
      "Epoch: 793/2000... Training loss: 0.6052\n",
      "Epoch: 793/2000... Training loss: 0.7616\n",
      "Epoch: 793/2000... Training loss: 0.7759\n",
      "Epoch: 793/2000... Training loss: 0.6127\n",
      "Epoch: 793/2000... Training loss: 0.5857\n",
      "Epoch: 793/2000... Training loss: 0.5561\n",
      "Epoch: 793/2000... Training loss: 0.7701\n",
      "Epoch: 793/2000... Training loss: 0.4533\n",
      "Epoch: 793/2000... Training loss: 0.7049\n",
      "Epoch: 793/2000... Training loss: 0.7244\n",
      "Epoch: 793/2000... Training loss: 0.6377\n",
      "Epoch: 793/2000... Training loss: 0.5990\n",
      "Epoch: 793/2000... Training loss: 0.5937\n",
      "Epoch: 793/2000... Training loss: 0.7895\n",
      "Epoch: 794/2000... Training loss: 0.6317\n",
      "Epoch: 794/2000... Training loss: 0.4702\n",
      "Epoch: 794/2000... Training loss: 0.6920\n",
      "Epoch: 794/2000... Training loss: 0.5439\n",
      "Epoch: 794/2000... Training loss: 0.5836\n",
      "Epoch: 794/2000... Training loss: 0.7060\n",
      "Epoch: 794/2000... Training loss: 0.4927\n",
      "Epoch: 794/2000... Training loss: 0.6235\n",
      "Epoch: 794/2000... Training loss: 0.6404\n",
      "Epoch: 794/2000... Training loss: 0.6254\n",
      "Epoch: 794/2000... Training loss: 0.5580\n",
      "Epoch: 794/2000... Training loss: 0.8959\n",
      "Epoch: 794/2000... Training loss: 0.5321\n",
      "Epoch: 794/2000... Training loss: 0.7983\n",
      "Epoch: 794/2000... Training loss: 0.6285\n",
      "Epoch: 794/2000... Training loss: 0.7236\n",
      "Epoch: 794/2000... Training loss: 0.6064\n",
      "Epoch: 794/2000... Training loss: 0.6081\n",
      "Epoch: 794/2000... Training loss: 0.5733\n",
      "Epoch: 794/2000... Training loss: 0.7143\n",
      "Epoch: 794/2000... Training loss: 0.4857\n",
      "Epoch: 794/2000... Training loss: 0.6294\n",
      "Epoch: 794/2000... Training loss: 0.7965\n",
      "Epoch: 794/2000... Training loss: 0.8025\n",
      "Epoch: 794/2000... Training loss: 0.5972\n",
      "Epoch: 794/2000... Training loss: 0.5781\n",
      "Epoch: 794/2000... Training loss: 0.7509\n",
      "Epoch: 794/2000... Training loss: 0.7899\n",
      "Epoch: 794/2000... Training loss: 0.7698\n",
      "Epoch: 794/2000... Training loss: 0.5758\n",
      "Epoch: 794/2000... Training loss: 0.7095\n",
      "Epoch: 795/2000... Training loss: 0.7731\n",
      "Epoch: 795/2000... Training loss: 0.6355\n",
      "Epoch: 795/2000... Training loss: 0.4848\n",
      "Epoch: 795/2000... Training loss: 0.7131\n",
      "Epoch: 795/2000... Training loss: 0.5749\n",
      "Epoch: 795/2000... Training loss: 0.6750\n",
      "Epoch: 795/2000... Training loss: 0.7820\n",
      "Epoch: 795/2000... Training loss: 0.5956\n",
      "Epoch: 795/2000... Training loss: 0.6495\n",
      "Epoch: 795/2000... Training loss: 0.6339\n",
      "Epoch: 795/2000... Training loss: 0.5508\n",
      "Epoch: 795/2000... Training loss: 0.6102\n",
      "Epoch: 795/2000... Training loss: 0.6728\n",
      "Epoch: 795/2000... Training loss: 0.8353\n",
      "Epoch: 795/2000... Training loss: 0.5960\n",
      "Epoch: 795/2000... Training loss: 0.7644\n",
      "Epoch: 795/2000... Training loss: 0.7346\n",
      "Epoch: 795/2000... Training loss: 0.6347\n",
      "Epoch: 795/2000... Training loss: 0.6313\n",
      "Epoch: 795/2000... Training loss: 0.6581\n",
      "Epoch: 795/2000... Training loss: 0.9045\n",
      "Epoch: 795/2000... Training loss: 0.7819\n",
      "Epoch: 795/2000... Training loss: 0.8030\n",
      "Epoch: 795/2000... Training loss: 0.7290\n",
      "Epoch: 795/2000... Training loss: 0.6540\n",
      "Epoch: 795/2000... Training loss: 0.6053\n",
      "Epoch: 795/2000... Training loss: 0.5741\n",
      "Epoch: 795/2000... Training loss: 0.5437\n",
      "Epoch: 795/2000... Training loss: 0.5444\n",
      "Epoch: 795/2000... Training loss: 0.6710\n",
      "Epoch: 795/2000... Training loss: 0.6806\n",
      "Epoch: 796/2000... Training loss: 0.7429\n",
      "Epoch: 796/2000... Training loss: 0.5552\n",
      "Epoch: 796/2000... Training loss: 0.8923\n",
      "Epoch: 796/2000... Training loss: 0.7423\n",
      "Epoch: 796/2000... Training loss: 0.7824\n",
      "Epoch: 796/2000... Training loss: 0.6817\n",
      "Epoch: 796/2000... Training loss: 0.6741\n",
      "Epoch: 796/2000... Training loss: 0.7299\n",
      "Epoch: 796/2000... Training loss: 0.5766\n",
      "Epoch: 796/2000... Training loss: 0.7321\n",
      "Epoch: 796/2000... Training loss: 0.5303\n",
      "Epoch: 796/2000... Training loss: 0.6161\n",
      "Epoch: 796/2000... Training loss: 0.5779\n",
      "Epoch: 796/2000... Training loss: 0.6196\n",
      "Epoch: 796/2000... Training loss: 0.6958\n",
      "Epoch: 796/2000... Training loss: 0.7919\n",
      "Epoch: 796/2000... Training loss: 0.6828\n",
      "Epoch: 796/2000... Training loss: 0.6982\n",
      "Epoch: 796/2000... Training loss: 0.5872\n",
      "Epoch: 796/2000... Training loss: 0.6733\n",
      "Epoch: 796/2000... Training loss: 0.6955\n",
      "Epoch: 796/2000... Training loss: 0.4588\n",
      "Epoch: 796/2000... Training loss: 0.5730\n",
      "Epoch: 796/2000... Training loss: 0.7600\n",
      "Epoch: 796/2000... Training loss: 0.7108\n",
      "Epoch: 796/2000... Training loss: 0.6276\n",
      "Epoch: 796/2000... Training loss: 0.6039\n",
      "Epoch: 796/2000... Training loss: 0.6771\n",
      "Epoch: 796/2000... Training loss: 0.5530\n",
      "Epoch: 796/2000... Training loss: 0.6290\n",
      "Epoch: 796/2000... Training loss: 0.6507\n",
      "Epoch: 797/2000... Training loss: 0.7194\n",
      "Epoch: 797/2000... Training loss: 0.6629\n",
      "Epoch: 797/2000... Training loss: 0.5947\n",
      "Epoch: 797/2000... Training loss: 0.5826\n",
      "Epoch: 797/2000... Training loss: 0.9237\n",
      "Epoch: 797/2000... Training loss: 0.6273\n",
      "Epoch: 797/2000... Training loss: 0.7776\n",
      "Epoch: 797/2000... Training loss: 0.8312\n",
      "Epoch: 797/2000... Training loss: 0.7630\n",
      "Epoch: 797/2000... Training loss: 0.7150\n",
      "Epoch: 797/2000... Training loss: 0.6916\n",
      "Epoch: 797/2000... Training loss: 0.6492\n",
      "Epoch: 797/2000... Training loss: 0.6496\n",
      "Epoch: 797/2000... Training loss: 0.5765\n",
      "Epoch: 797/2000... Training loss: 0.7231\n",
      "Epoch: 797/2000... Training loss: 0.7436\n",
      "Epoch: 797/2000... Training loss: 0.7114\n",
      "Epoch: 797/2000... Training loss: 0.8150\n",
      "Epoch: 797/2000... Training loss: 0.5872\n",
      "Epoch: 797/2000... Training loss: 0.4631\n",
      "Epoch: 797/2000... Training loss: 0.6771\n",
      "Epoch: 797/2000... Training loss: 0.9623\n",
      "Epoch: 797/2000... Training loss: 0.7971\n",
      "Epoch: 797/2000... Training loss: 0.6455\n",
      "Epoch: 797/2000... Training loss: 0.5594\n",
      "Epoch: 797/2000... Training loss: 0.7671\n",
      "Epoch: 797/2000... Training loss: 0.7219\n",
      "Epoch: 797/2000... Training loss: 0.7489\n",
      "Epoch: 797/2000... Training loss: 0.6612\n",
      "Epoch: 797/2000... Training loss: 0.6717\n",
      "Epoch: 797/2000... Training loss: 0.6213\n",
      "Epoch: 798/2000... Training loss: 0.6659\n",
      "Epoch: 798/2000... Training loss: 0.7409\n",
      "Epoch: 798/2000... Training loss: 0.9024\n",
      "Epoch: 798/2000... Training loss: 0.6511\n",
      "Epoch: 798/2000... Training loss: 0.5905\n",
      "Epoch: 798/2000... Training loss: 0.5713\n",
      "Epoch: 798/2000... Training loss: 0.9588\n",
      "Epoch: 798/2000... Training loss: 0.6428\n",
      "Epoch: 798/2000... Training loss: 0.7680\n",
      "Epoch: 798/2000... Training loss: 0.7768\n",
      "Epoch: 798/2000... Training loss: 0.8962\n",
      "Epoch: 798/2000... Training loss: 0.6725\n",
      "Epoch: 798/2000... Training loss: 0.5883\n",
      "Epoch: 798/2000... Training loss: 0.6145\n",
      "Epoch: 798/2000... Training loss: 0.4416\n",
      "Epoch: 798/2000... Training loss: 0.6930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 798/2000... Training loss: 0.3835\n",
      "Epoch: 798/2000... Training loss: 0.5202\n",
      "Epoch: 798/2000... Training loss: 0.5996\n",
      "Epoch: 798/2000... Training loss: 0.7653\n",
      "Epoch: 798/2000... Training loss: 0.4844\n",
      "Epoch: 798/2000... Training loss: 0.7250\n",
      "Epoch: 798/2000... Training loss: 0.5844\n",
      "Epoch: 798/2000... Training loss: 0.7144\n",
      "Epoch: 798/2000... Training loss: 0.7105\n",
      "Epoch: 798/2000... Training loss: 0.6679\n",
      "Epoch: 798/2000... Training loss: 0.4932\n",
      "Epoch: 798/2000... Training loss: 0.6123\n",
      "Epoch: 798/2000... Training loss: 0.4843\n",
      "Epoch: 798/2000... Training loss: 0.6925\n",
      "Epoch: 798/2000... Training loss: 0.6593\n",
      "Epoch: 799/2000... Training loss: 0.6362\n",
      "Epoch: 799/2000... Training loss: 0.7171\n",
      "Epoch: 799/2000... Training loss: 0.5979\n",
      "Epoch: 799/2000... Training loss: 0.6682\n",
      "Epoch: 799/2000... Training loss: 0.7770\n",
      "Epoch: 799/2000... Training loss: 0.5230\n",
      "Epoch: 799/2000... Training loss: 0.5743\n",
      "Epoch: 799/2000... Training loss: 0.5261\n",
      "Epoch: 799/2000... Training loss: 0.7008\n",
      "Epoch: 799/2000... Training loss: 0.8657\n",
      "Epoch: 799/2000... Training loss: 0.5793\n",
      "Epoch: 799/2000... Training loss: 0.6268\n",
      "Epoch: 799/2000... Training loss: 0.6174\n",
      "Epoch: 799/2000... Training loss: 0.7404\n",
      "Epoch: 799/2000... Training loss: 0.7020\n",
      "Epoch: 799/2000... Training loss: 0.5953\n",
      "Epoch: 799/2000... Training loss: 0.4651\n",
      "Epoch: 799/2000... Training loss: 0.5316\n",
      "Epoch: 799/2000... Training loss: 0.5837\n",
      "Epoch: 799/2000... Training loss: 0.6773\n",
      "Epoch: 799/2000... Training loss: 0.5684\n",
      "Epoch: 799/2000... Training loss: 0.6548\n",
      "Epoch: 799/2000... Training loss: 0.4457\n",
      "Epoch: 799/2000... Training loss: 0.5961\n",
      "Epoch: 799/2000... Training loss: 0.6491\n",
      "Epoch: 799/2000... Training loss: 0.6241\n",
      "Epoch: 799/2000... Training loss: 0.8596\n",
      "Epoch: 799/2000... Training loss: 0.6126\n",
      "Epoch: 799/2000... Training loss: 0.5020\n",
      "Epoch: 799/2000... Training loss: 0.7739\n",
      "Epoch: 799/2000... Training loss: 0.6271\n",
      "Epoch: 800/2000... Training loss: 0.6186\n",
      "Epoch: 800/2000... Training loss: 0.7327\n",
      "Epoch: 800/2000... Training loss: 0.7599\n",
      "Epoch: 800/2000... Training loss: 0.5703\n",
      "Epoch: 800/2000... Training loss: 0.4108\n",
      "Epoch: 800/2000... Training loss: 0.5360\n",
      "Epoch: 800/2000... Training loss: 0.5265\n",
      "Epoch: 800/2000... Training loss: 0.6497\n",
      "Epoch: 800/2000... Training loss: 0.7573\n",
      "Epoch: 800/2000... Training loss: 0.7840\n",
      "Epoch: 800/2000... Training loss: 0.6334\n",
      "Epoch: 800/2000... Training loss: 0.6515\n",
      "Epoch: 800/2000... Training loss: 0.5431\n",
      "Epoch: 800/2000... Training loss: 0.7870\n",
      "Epoch: 800/2000... Training loss: 0.5373\n",
      "Epoch: 800/2000... Training loss: 0.7836\n",
      "Epoch: 800/2000... Training loss: 0.4917\n",
      "Epoch: 800/2000... Training loss: 0.7661\n",
      "Epoch: 800/2000... Training loss: 0.5814\n",
      "Epoch: 800/2000... Training loss: 0.7575\n",
      "Epoch: 800/2000... Training loss: 0.5497\n",
      "Epoch: 800/2000... Training loss: 0.4993\n",
      "Epoch: 800/2000... Training loss: 0.6032\n",
      "Epoch: 800/2000... Training loss: 0.6607\n",
      "Epoch: 800/2000... Training loss: 0.8867\n",
      "Epoch: 800/2000... Training loss: 0.8377\n",
      "Epoch: 800/2000... Training loss: 0.5875\n",
      "Epoch: 800/2000... Training loss: 0.8017\n",
      "Epoch: 800/2000... Training loss: 0.7204\n",
      "Epoch: 800/2000... Training loss: 0.6378\n",
      "Epoch: 800/2000... Training loss: 0.7892\n",
      "Epoch: 801/2000... Training loss: 0.9032\n",
      "Epoch: 801/2000... Training loss: 0.7098\n",
      "Epoch: 801/2000... Training loss: 0.7030\n",
      "Epoch: 801/2000... Training loss: 0.5817\n",
      "Epoch: 801/2000... Training loss: 0.6164\n",
      "Epoch: 801/2000... Training loss: 0.7633\n",
      "Epoch: 801/2000... Training loss: 0.4938\n",
      "Epoch: 801/2000... Training loss: 0.6812\n",
      "Epoch: 801/2000... Training loss: 0.7893\n",
      "Epoch: 801/2000... Training loss: 0.7610\n",
      "Epoch: 801/2000... Training loss: 0.5931\n",
      "Epoch: 801/2000... Training loss: 0.5744\n",
      "Epoch: 801/2000... Training loss: 0.6117\n",
      "Epoch: 801/2000... Training loss: 0.4873\n",
      "Epoch: 801/2000... Training loss: 0.7107\n",
      "Epoch: 801/2000... Training loss: 0.6003\n",
      "Epoch: 801/2000... Training loss: 0.7508\n",
      "Epoch: 801/2000... Training loss: 0.3858\n",
      "Epoch: 801/2000... Training loss: 0.6945\n",
      "Epoch: 801/2000... Training loss: 0.5846\n",
      "Epoch: 801/2000... Training loss: 0.6156\n",
      "Epoch: 801/2000... Training loss: 0.5461\n",
      "Epoch: 801/2000... Training loss: 0.8477\n",
      "Epoch: 801/2000... Training loss: 0.5539\n",
      "Epoch: 801/2000... Training loss: 0.6636\n",
      "Epoch: 801/2000... Training loss: 0.7259\n",
      "Epoch: 801/2000... Training loss: 0.5376\n",
      "Epoch: 801/2000... Training loss: 0.6748\n",
      "Epoch: 801/2000... Training loss: 0.5935\n",
      "Epoch: 801/2000... Training loss: 0.4954\n",
      "Epoch: 801/2000... Training loss: 0.6369\n",
      "Epoch: 802/2000... Training loss: 0.5987\n",
      "Epoch: 802/2000... Training loss: 0.4183\n",
      "Epoch: 802/2000... Training loss: 0.7494\n",
      "Epoch: 802/2000... Training loss: 0.6717\n",
      "Epoch: 802/2000... Training loss: 0.7571\n",
      "Epoch: 802/2000... Training loss: 0.7019\n",
      "Epoch: 802/2000... Training loss: 0.6114\n",
      "Epoch: 802/2000... Training loss: 0.7532\n",
      "Epoch: 802/2000... Training loss: 0.6259\n",
      "Epoch: 802/2000... Training loss: 0.6362\n",
      "Epoch: 802/2000... Training loss: 0.4878\n",
      "Epoch: 802/2000... Training loss: 0.7140\n",
      "Epoch: 802/2000... Training loss: 0.5935\n",
      "Epoch: 802/2000... Training loss: 0.7365\n",
      "Epoch: 802/2000... Training loss: 0.9447\n",
      "Epoch: 802/2000... Training loss: 0.5798\n",
      "Epoch: 802/2000... Training loss: 0.6667\n",
      "Epoch: 802/2000... Training loss: 0.6569\n",
      "Epoch: 802/2000... Training loss: 0.5577\n",
      "Epoch: 802/2000... Training loss: 0.7857\n",
      "Epoch: 802/2000... Training loss: 0.3868\n",
      "Epoch: 802/2000... Training loss: 0.5721\n",
      "Epoch: 802/2000... Training loss: 0.4926\n",
      "Epoch: 802/2000... Training loss: 0.7224\n",
      "Epoch: 802/2000... Training loss: 0.6767\n",
      "Epoch: 802/2000... Training loss: 0.6043\n",
      "Epoch: 802/2000... Training loss: 0.6750\n",
      "Epoch: 802/2000... Training loss: 0.6499\n",
      "Epoch: 802/2000... Training loss: 0.7032\n",
      "Epoch: 802/2000... Training loss: 0.8424\n",
      "Epoch: 802/2000... Training loss: 0.5483\n",
      "Epoch: 803/2000... Training loss: 0.6438\n",
      "Epoch: 803/2000... Training loss: 0.6721\n",
      "Epoch: 803/2000... Training loss: 0.5735\n",
      "Epoch: 803/2000... Training loss: 0.6125\n",
      "Epoch: 803/2000... Training loss: 0.7777\n",
      "Epoch: 803/2000... Training loss: 0.7364\n",
      "Epoch: 803/2000... Training loss: 0.7431\n",
      "Epoch: 803/2000... Training loss: 0.6012\n",
      "Epoch: 803/2000... Training loss: 0.6045\n",
      "Epoch: 803/2000... Training loss: 0.8133\n",
      "Epoch: 803/2000... Training loss: 0.7272\n",
      "Epoch: 803/2000... Training loss: 0.8378\n",
      "Epoch: 803/2000... Training loss: 0.4947\n",
      "Epoch: 803/2000... Training loss: 0.4851\n",
      "Epoch: 803/2000... Training loss: 0.4666\n",
      "Epoch: 803/2000... Training loss: 0.6717\n",
      "Epoch: 803/2000... Training loss: 0.6505\n",
      "Epoch: 803/2000... Training loss: 0.6363\n",
      "Epoch: 803/2000... Training loss: 0.5849\n",
      "Epoch: 803/2000... Training loss: 0.7654\n",
      "Epoch: 803/2000... Training loss: 0.6804\n",
      "Epoch: 803/2000... Training loss: 0.5019\n",
      "Epoch: 803/2000... Training loss: 0.5594\n",
      "Epoch: 803/2000... Training loss: 0.5292\n",
      "Epoch: 803/2000... Training loss: 0.6770\n",
      "Epoch: 803/2000... Training loss: 0.5836\n",
      "Epoch: 803/2000... Training loss: 0.6739\n",
      "Epoch: 803/2000... Training loss: 0.4391\n",
      "Epoch: 803/2000... Training loss: 0.4051\n",
      "Epoch: 803/2000... Training loss: 0.6493\n",
      "Epoch: 803/2000... Training loss: 0.5936\n",
      "Epoch: 804/2000... Training loss: 0.4711\n",
      "Epoch: 804/2000... Training loss: 0.6297\n",
      "Epoch: 804/2000... Training loss: 0.5465\n",
      "Epoch: 804/2000... Training loss: 0.4200\n",
      "Epoch: 804/2000... Training loss: 0.7552\n",
      "Epoch: 804/2000... Training loss: 0.4861\n",
      "Epoch: 804/2000... Training loss: 0.5818\n",
      "Epoch: 804/2000... Training loss: 0.9292\n",
      "Epoch: 804/2000... Training loss: 0.5725\n",
      "Epoch: 804/2000... Training loss: 0.6612\n",
      "Epoch: 804/2000... Training loss: 0.6055\n",
      "Epoch: 804/2000... Training loss: 0.6699\n",
      "Epoch: 804/2000... Training loss: 0.6553\n",
      "Epoch: 804/2000... Training loss: 0.8462\n",
      "Epoch: 804/2000... Training loss: 0.8749\n",
      "Epoch: 804/2000... Training loss: 0.5532\n",
      "Epoch: 804/2000... Training loss: 0.6395\n",
      "Epoch: 804/2000... Training loss: 0.4832\n",
      "Epoch: 804/2000... Training loss: 0.6575\n",
      "Epoch: 804/2000... Training loss: 0.6447\n",
      "Epoch: 804/2000... Training loss: 0.6984\n",
      "Epoch: 804/2000... Training loss: 0.5652\n",
      "Epoch: 804/2000... Training loss: 0.8495\n",
      "Epoch: 804/2000... Training loss: 0.4895\n",
      "Epoch: 804/2000... Training loss: 0.7188\n",
      "Epoch: 804/2000... Training loss: 0.5830\n",
      "Epoch: 804/2000... Training loss: 0.7628\n",
      "Epoch: 804/2000... Training loss: 0.5323\n",
      "Epoch: 804/2000... Training loss: 0.6634\n",
      "Epoch: 804/2000... Training loss: 0.4540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 804/2000... Training loss: 0.7327\n",
      "Epoch: 805/2000... Training loss: 0.7105\n",
      "Epoch: 805/2000... Training loss: 0.6836\n",
      "Epoch: 805/2000... Training loss: 0.6649\n",
      "Epoch: 805/2000... Training loss: 0.4837\n",
      "Epoch: 805/2000... Training loss: 0.4804\n",
      "Epoch: 805/2000... Training loss: 0.7532\n",
      "Epoch: 805/2000... Training loss: 0.7537\n",
      "Epoch: 805/2000... Training loss: 0.4929\n",
      "Epoch: 805/2000... Training loss: 0.6358\n",
      "Epoch: 805/2000... Training loss: 0.5285\n",
      "Epoch: 805/2000... Training loss: 0.5709\n",
      "Epoch: 805/2000... Training loss: 0.8116\n",
      "Epoch: 805/2000... Training loss: 0.7003\n",
      "Epoch: 805/2000... Training loss: 0.5511\n",
      "Epoch: 805/2000... Training loss: 0.5129\n",
      "Epoch: 805/2000... Training loss: 0.6241\n",
      "Epoch: 805/2000... Training loss: 0.6174\n",
      "Epoch: 805/2000... Training loss: 0.6636\n",
      "Epoch: 805/2000... Training loss: 0.7522\n",
      "Epoch: 805/2000... Training loss: 0.7790\n",
      "Epoch: 805/2000... Training loss: 0.6433\n",
      "Epoch: 805/2000... Training loss: 0.5695\n",
      "Epoch: 805/2000... Training loss: 0.3426\n",
      "Epoch: 805/2000... Training loss: 0.7941\n",
      "Epoch: 805/2000... Training loss: 0.7437\n",
      "Epoch: 805/2000... Training loss: 0.7847\n",
      "Epoch: 805/2000... Training loss: 0.6624\n",
      "Epoch: 805/2000... Training loss: 0.7019\n",
      "Epoch: 805/2000... Training loss: 0.7404\n",
      "Epoch: 805/2000... Training loss: 0.7382\n",
      "Epoch: 805/2000... Training loss: 0.6876\n",
      "Epoch: 806/2000... Training loss: 0.7470\n",
      "Epoch: 806/2000... Training loss: 0.5770\n",
      "Epoch: 806/2000... Training loss: 0.5977\n",
      "Epoch: 806/2000... Training loss: 0.6829\n",
      "Epoch: 806/2000... Training loss: 0.6630\n",
      "Epoch: 806/2000... Training loss: 0.7639\n",
      "Epoch: 806/2000... Training loss: 0.7335\n",
      "Epoch: 806/2000... Training loss: 0.6042\n",
      "Epoch: 806/2000... Training loss: 0.6499\n",
      "Epoch: 806/2000... Training loss: 0.6668\n",
      "Epoch: 806/2000... Training loss: 0.7653\n",
      "Epoch: 806/2000... Training loss: 0.6006\n",
      "Epoch: 806/2000... Training loss: 0.7416\n",
      "Epoch: 806/2000... Training loss: 0.6874\n",
      "Epoch: 806/2000... Training loss: 0.5739\n",
      "Epoch: 806/2000... Training loss: 0.7403\n",
      "Epoch: 806/2000... Training loss: 0.5851\n",
      "Epoch: 806/2000... Training loss: 0.8897\n",
      "Epoch: 806/2000... Training loss: 0.6283\n",
      "Epoch: 806/2000... Training loss: 0.7599\n",
      "Epoch: 806/2000... Training loss: 0.7887\n",
      "Epoch: 806/2000... Training loss: 0.6890\n",
      "Epoch: 806/2000... Training loss: 0.7872\n",
      "Epoch: 806/2000... Training loss: 0.7025\n",
      "Epoch: 806/2000... Training loss: 0.6653\n",
      "Epoch: 806/2000... Training loss: 0.6138\n",
      "Epoch: 806/2000... Training loss: 0.6570\n",
      "Epoch: 806/2000... Training loss: 0.5459\n",
      "Epoch: 806/2000... Training loss: 0.5791\n",
      "Epoch: 806/2000... Training loss: 0.6043\n",
      "Epoch: 806/2000... Training loss: 0.6330\n",
      "Epoch: 807/2000... Training loss: 0.7313\n",
      "Epoch: 807/2000... Training loss: 0.5799\n",
      "Epoch: 807/2000... Training loss: 0.6726\n",
      "Epoch: 807/2000... Training loss: 0.6148\n",
      "Epoch: 807/2000... Training loss: 0.7269\n",
      "Epoch: 807/2000... Training loss: 0.5724\n",
      "Epoch: 807/2000... Training loss: 0.6706\n",
      "Epoch: 807/2000... Training loss: 0.5210\n",
      "Epoch: 807/2000... Training loss: 0.6007\n",
      "Epoch: 807/2000... Training loss: 0.4502\n",
      "Epoch: 807/2000... Training loss: 0.9135\n",
      "Epoch: 807/2000... Training loss: 0.6146\n",
      "Epoch: 807/2000... Training loss: 0.5708\n",
      "Epoch: 807/2000... Training loss: 0.6601\n",
      "Epoch: 807/2000... Training loss: 0.6380\n",
      "Epoch: 807/2000... Training loss: 0.5111\n",
      "Epoch: 807/2000... Training loss: 0.5292\n",
      "Epoch: 807/2000... Training loss: 0.7601\n",
      "Epoch: 807/2000... Training loss: 0.5779\n",
      "Epoch: 807/2000... Training loss: 0.6428\n",
      "Epoch: 807/2000... Training loss: 0.5883\n",
      "Epoch: 807/2000... Training loss: 0.7006\n",
      "Epoch: 807/2000... Training loss: 0.5763\n",
      "Epoch: 807/2000... Training loss: 0.6609\n",
      "Epoch: 807/2000... Training loss: 0.7229\n",
      "Epoch: 807/2000... Training loss: 0.5400\n",
      "Epoch: 807/2000... Training loss: 0.6052\n",
      "Epoch: 807/2000... Training loss: 0.5650\n",
      "Epoch: 807/2000... Training loss: 0.4412\n",
      "Epoch: 807/2000... Training loss: 0.6104\n",
      "Epoch: 807/2000... Training loss: 0.7674\n",
      "Epoch: 808/2000... Training loss: 0.6618\n",
      "Epoch: 808/2000... Training loss: 0.8084\n",
      "Epoch: 808/2000... Training loss: 0.4782\n",
      "Epoch: 808/2000... Training loss: 0.5075\n",
      "Epoch: 808/2000... Training loss: 0.4283\n",
      "Epoch: 808/2000... Training loss: 0.7732\n",
      "Epoch: 808/2000... Training loss: 0.7378\n",
      "Epoch: 808/2000... Training loss: 0.6653\n",
      "Epoch: 808/2000... Training loss: 0.6031\n",
      "Epoch: 808/2000... Training loss: 0.5188\n",
      "Epoch: 808/2000... Training loss: 0.5234\n",
      "Epoch: 808/2000... Training loss: 0.6533\n",
      "Epoch: 808/2000... Training loss: 0.7326\n",
      "Epoch: 808/2000... Training loss: 0.9305\n",
      "Epoch: 808/2000... Training loss: 0.6244\n",
      "Epoch: 808/2000... Training loss: 0.7351\n",
      "Epoch: 808/2000... Training loss: 0.6036\n",
      "Epoch: 808/2000... Training loss: 0.7532\n",
      "Epoch: 808/2000... Training loss: 0.6402\n",
      "Epoch: 808/2000... Training loss: 0.6303\n",
      "Epoch: 808/2000... Training loss: 0.5430\n",
      "Epoch: 808/2000... Training loss: 0.5583\n",
      "Epoch: 808/2000... Training loss: 0.7293\n",
      "Epoch: 808/2000... Training loss: 0.8802\n",
      "Epoch: 808/2000... Training loss: 0.6051\n",
      "Epoch: 808/2000... Training loss: 0.6481\n",
      "Epoch: 808/2000... Training loss: 0.6916\n",
      "Epoch: 808/2000... Training loss: 0.6669\n",
      "Epoch: 808/2000... Training loss: 0.6348\n",
      "Epoch: 808/2000... Training loss: 0.5240\n",
      "Epoch: 808/2000... Training loss: 0.6501\n",
      "Epoch: 809/2000... Training loss: 0.5733\n",
      "Epoch: 809/2000... Training loss: 0.6074\n",
      "Epoch: 809/2000... Training loss: 0.5790\n",
      "Epoch: 809/2000... Training loss: 0.5848\n",
      "Epoch: 809/2000... Training loss: 0.8348\n",
      "Epoch: 809/2000... Training loss: 0.7399\n",
      "Epoch: 809/2000... Training loss: 0.7924\n",
      "Epoch: 809/2000... Training loss: 0.8394\n",
      "Epoch: 809/2000... Training loss: 0.5268\n",
      "Epoch: 809/2000... Training loss: 0.8214\n",
      "Epoch: 809/2000... Training loss: 0.8647\n",
      "Epoch: 809/2000... Training loss: 0.7027\n",
      "Epoch: 809/2000... Training loss: 0.7366\n",
      "Epoch: 809/2000... Training loss: 0.5998\n",
      "Epoch: 809/2000... Training loss: 0.5590\n",
      "Epoch: 809/2000... Training loss: 0.5754\n",
      "Epoch: 809/2000... Training loss: 0.7080\n",
      "Epoch: 809/2000... Training loss: 0.7740\n",
      "Epoch: 809/2000... Training loss: 0.6799\n",
      "Epoch: 809/2000... Training loss: 1.0121\n",
      "Epoch: 809/2000... Training loss: 0.4552\n",
      "Epoch: 809/2000... Training loss: 0.6411\n",
      "Epoch: 809/2000... Training loss: 0.6693\n",
      "Epoch: 809/2000... Training loss: 0.4857\n",
      "Epoch: 809/2000... Training loss: 0.6297\n",
      "Epoch: 809/2000... Training loss: 0.7817\n",
      "Epoch: 809/2000... Training loss: 0.5082\n",
      "Epoch: 809/2000... Training loss: 0.6257\n",
      "Epoch: 809/2000... Training loss: 0.7069\n",
      "Epoch: 809/2000... Training loss: 0.6670\n",
      "Epoch: 809/2000... Training loss: 0.6879\n",
      "Epoch: 810/2000... Training loss: 0.6424\n",
      "Epoch: 810/2000... Training loss: 0.7012\n",
      "Epoch: 810/2000... Training loss: 0.7527\n",
      "Epoch: 810/2000... Training loss: 0.6330\n",
      "Epoch: 810/2000... Training loss: 0.7171\n",
      "Epoch: 810/2000... Training loss: 0.8411\n",
      "Epoch: 810/2000... Training loss: 0.5329\n",
      "Epoch: 810/2000... Training loss: 0.7084\n",
      "Epoch: 810/2000... Training loss: 0.6845\n",
      "Epoch: 810/2000... Training loss: 0.5880\n",
      "Epoch: 810/2000... Training loss: 0.5579\n",
      "Epoch: 810/2000... Training loss: 0.7175\n",
      "Epoch: 810/2000... Training loss: 0.5235\n",
      "Epoch: 810/2000... Training loss: 0.5923\n",
      "Epoch: 810/2000... Training loss: 0.5534\n",
      "Epoch: 810/2000... Training loss: 0.7420\n",
      "Epoch: 810/2000... Training loss: 0.6292\n",
      "Epoch: 810/2000... Training loss: 0.5693\n",
      "Epoch: 810/2000... Training loss: 0.4798\n",
      "Epoch: 810/2000... Training loss: 0.8548\n",
      "Epoch: 810/2000... Training loss: 0.5360\n",
      "Epoch: 810/2000... Training loss: 0.5928\n",
      "Epoch: 810/2000... Training loss: 0.6617\n",
      "Epoch: 810/2000... Training loss: 0.6435\n",
      "Epoch: 810/2000... Training loss: 0.5799\n",
      "Epoch: 810/2000... Training loss: 0.6720\n",
      "Epoch: 810/2000... Training loss: 0.7147\n",
      "Epoch: 810/2000... Training loss: 0.5351\n",
      "Epoch: 810/2000... Training loss: 0.4597\n",
      "Epoch: 810/2000... Training loss: 0.6232\n",
      "Epoch: 810/2000... Training loss: 0.5587\n",
      "Epoch: 811/2000... Training loss: 0.7429\n",
      "Epoch: 811/2000... Training loss: 0.7644\n",
      "Epoch: 811/2000... Training loss: 0.5248\n",
      "Epoch: 811/2000... Training loss: 0.6334\n",
      "Epoch: 811/2000... Training loss: 0.5481\n",
      "Epoch: 811/2000... Training loss: 0.6759\n",
      "Epoch: 811/2000... Training loss: 0.6709\n",
      "Epoch: 811/2000... Training loss: 0.5142\n",
      "Epoch: 811/2000... Training loss: 0.7908\n",
      "Epoch: 811/2000... Training loss: 0.6765\n",
      "Epoch: 811/2000... Training loss: 0.6089\n",
      "Epoch: 811/2000... Training loss: 0.4980\n",
      "Epoch: 811/2000... Training loss: 0.6744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 811/2000... Training loss: 0.6331\n",
      "Epoch: 811/2000... Training loss: 0.8598\n",
      "Epoch: 811/2000... Training loss: 0.6133\n",
      "Epoch: 811/2000... Training loss: 0.5839\n",
      "Epoch: 811/2000... Training loss: 0.4507\n",
      "Epoch: 811/2000... Training loss: 0.7327\n",
      "Epoch: 811/2000... Training loss: 0.4927\n",
      "Epoch: 811/2000... Training loss: 0.6493\n",
      "Epoch: 811/2000... Training loss: 0.5367\n",
      "Epoch: 811/2000... Training loss: 0.7418\n",
      "Epoch: 811/2000... Training loss: 0.8097\n",
      "Epoch: 811/2000... Training loss: 0.6952\n",
      "Epoch: 811/2000... Training loss: 0.7477\n",
      "Epoch: 811/2000... Training loss: 0.5200\n",
      "Epoch: 811/2000... Training loss: 0.6242\n",
      "Epoch: 811/2000... Training loss: 0.6630\n",
      "Epoch: 811/2000... Training loss: 0.5559\n",
      "Epoch: 811/2000... Training loss: 0.6639\n",
      "Epoch: 812/2000... Training loss: 0.7021\n",
      "Epoch: 812/2000... Training loss: 0.6684\n",
      "Epoch: 812/2000... Training loss: 0.4992\n",
      "Epoch: 812/2000... Training loss: 0.6516\n",
      "Epoch: 812/2000... Training loss: 0.6044\n",
      "Epoch: 812/2000... Training loss: 0.6107\n",
      "Epoch: 812/2000... Training loss: 0.7115\n",
      "Epoch: 812/2000... Training loss: 0.5217\n",
      "Epoch: 812/2000... Training loss: 0.5790\n",
      "Epoch: 812/2000... Training loss: 0.6335\n",
      "Epoch: 812/2000... Training loss: 0.4662\n",
      "Epoch: 812/2000... Training loss: 0.9844\n",
      "Epoch: 812/2000... Training loss: 0.5741\n",
      "Epoch: 812/2000... Training loss: 0.7381\n",
      "Epoch: 812/2000... Training loss: 0.4871\n",
      "Epoch: 812/2000... Training loss: 0.4824\n",
      "Epoch: 812/2000... Training loss: 0.4397\n",
      "Epoch: 812/2000... Training loss: 0.5665\n",
      "Epoch: 812/2000... Training loss: 0.7614\n",
      "Epoch: 812/2000... Training loss: 0.6599\n",
      "Epoch: 812/2000... Training loss: 0.4185\n",
      "Epoch: 812/2000... Training loss: 0.6311\n",
      "Epoch: 812/2000... Training loss: 0.5405\n",
      "Epoch: 812/2000... Training loss: 0.6093\n",
      "Epoch: 812/2000... Training loss: 0.5921\n",
      "Epoch: 812/2000... Training loss: 0.6661\n",
      "Epoch: 812/2000... Training loss: 0.5283\n",
      "Epoch: 812/2000... Training loss: 0.6060\n",
      "Epoch: 812/2000... Training loss: 0.7046\n",
      "Epoch: 812/2000... Training loss: 0.8469\n",
      "Epoch: 812/2000... Training loss: 0.5185\n",
      "Epoch: 813/2000... Training loss: 0.5047\n",
      "Epoch: 813/2000... Training loss: 0.5767\n",
      "Epoch: 813/2000... Training loss: 0.7497\n",
      "Epoch: 813/2000... Training loss: 0.8220\n",
      "Epoch: 813/2000... Training loss: 0.6374\n",
      "Epoch: 813/2000... Training loss: 0.4420\n",
      "Epoch: 813/2000... Training loss: 0.5620\n",
      "Epoch: 813/2000... Training loss: 0.7620\n",
      "Epoch: 813/2000... Training loss: 0.6181\n",
      "Epoch: 813/2000... Training loss: 0.5292\n",
      "Epoch: 813/2000... Training loss: 0.5306\n",
      "Epoch: 813/2000... Training loss: 0.8149\n",
      "Epoch: 813/2000... Training loss: 0.5254\n",
      "Epoch: 813/2000... Training loss: 0.5161\n",
      "Epoch: 813/2000... Training loss: 0.5725\n",
      "Epoch: 813/2000... Training loss: 0.6079\n",
      "Epoch: 813/2000... Training loss: 0.7722\n",
      "Epoch: 813/2000... Training loss: 0.5738\n",
      "Epoch: 813/2000... Training loss: 0.6791\n",
      "Epoch: 813/2000... Training loss: 0.5767\n",
      "Epoch: 813/2000... Training loss: 0.5628\n",
      "Epoch: 813/2000... Training loss: 0.5452\n",
      "Epoch: 813/2000... Training loss: 0.6787\n",
      "Epoch: 813/2000... Training loss: 0.5325\n",
      "Epoch: 813/2000... Training loss: 0.4922\n",
      "Epoch: 813/2000... Training loss: 0.3827\n",
      "Epoch: 813/2000... Training loss: 0.7608\n",
      "Epoch: 813/2000... Training loss: 0.6894\n",
      "Epoch: 813/2000... Training loss: 0.7649\n",
      "Epoch: 813/2000... Training loss: 0.3881\n",
      "Epoch: 813/2000... Training loss: 0.5695\n",
      "Epoch: 814/2000... Training loss: 0.6450\n",
      "Epoch: 814/2000... Training loss: 0.4516\n",
      "Epoch: 814/2000... Training loss: 0.7184\n",
      "Epoch: 814/2000... Training loss: 0.4978\n",
      "Epoch: 814/2000... Training loss: 0.6838\n",
      "Epoch: 814/2000... Training loss: 0.7467\n",
      "Epoch: 814/2000... Training loss: 0.7867\n",
      "Epoch: 814/2000... Training loss: 0.6776\n",
      "Epoch: 814/2000... Training loss: 0.5825\n",
      "Epoch: 814/2000... Training loss: 0.7520\n",
      "Epoch: 814/2000... Training loss: 0.5715\n",
      "Epoch: 814/2000... Training loss: 0.5414\n",
      "Epoch: 814/2000... Training loss: 0.5524\n",
      "Epoch: 814/2000... Training loss: 0.6254\n",
      "Epoch: 814/2000... Training loss: 0.6570\n",
      "Epoch: 814/2000... Training loss: 0.7479\n",
      "Epoch: 814/2000... Training loss: 0.6019\n",
      "Epoch: 814/2000... Training loss: 0.5943\n",
      "Epoch: 814/2000... Training loss: 0.7707\n",
      "Epoch: 814/2000... Training loss: 0.6418\n",
      "Epoch: 814/2000... Training loss: 0.6992\n",
      "Epoch: 814/2000... Training loss: 0.7631\n",
      "Epoch: 814/2000... Training loss: 0.5668\n",
      "Epoch: 814/2000... Training loss: 0.7884\n",
      "Epoch: 814/2000... Training loss: 0.7485\n",
      "Epoch: 814/2000... Training loss: 0.6746\n",
      "Epoch: 814/2000... Training loss: 0.4441\n",
      "Epoch: 814/2000... Training loss: 0.4270\n",
      "Epoch: 814/2000... Training loss: 0.7702\n",
      "Epoch: 814/2000... Training loss: 0.6555\n",
      "Epoch: 814/2000... Training loss: 0.6284\n",
      "Epoch: 815/2000... Training loss: 0.5925\n",
      "Epoch: 815/2000... Training loss: 0.6475\n",
      "Epoch: 815/2000... Training loss: 0.7533\n",
      "Epoch: 815/2000... Training loss: 0.7388\n",
      "Epoch: 815/2000... Training loss: 0.6979\n",
      "Epoch: 815/2000... Training loss: 0.6466\n",
      "Epoch: 815/2000... Training loss: 0.6081\n",
      "Epoch: 815/2000... Training loss: 0.4639\n",
      "Epoch: 815/2000... Training loss: 0.4737\n",
      "Epoch: 815/2000... Training loss: 0.6386\n",
      "Epoch: 815/2000... Training loss: 0.7461\n",
      "Epoch: 815/2000... Training loss: 0.6265\n",
      "Epoch: 815/2000... Training loss: 0.4772\n",
      "Epoch: 815/2000... Training loss: 0.4153\n",
      "Epoch: 815/2000... Training loss: 0.6012\n",
      "Epoch: 815/2000... Training loss: 0.5910\n",
      "Epoch: 815/2000... Training loss: 0.5714\n",
      "Epoch: 815/2000... Training loss: 0.7407\n",
      "Epoch: 815/2000... Training loss: 0.4553\n",
      "Epoch: 815/2000... Training loss: 0.7681\n",
      "Epoch: 815/2000... Training loss: 0.7375\n",
      "Epoch: 815/2000... Training loss: 0.5102\n",
      "Epoch: 815/2000... Training loss: 0.5475\n",
      "Epoch: 815/2000... Training loss: 0.6073\n",
      "Epoch: 815/2000... Training loss: 0.5831\n",
      "Epoch: 815/2000... Training loss: 0.6831\n",
      "Epoch: 815/2000... Training loss: 0.4546\n",
      "Epoch: 815/2000... Training loss: 0.6478\n",
      "Epoch: 815/2000... Training loss: 0.5297\n",
      "Epoch: 815/2000... Training loss: 0.6488\n",
      "Epoch: 815/2000... Training loss: 0.5875\n",
      "Epoch: 816/2000... Training loss: 0.7558\n",
      "Epoch: 816/2000... Training loss: 0.6008\n",
      "Epoch: 816/2000... Training loss: 0.3673\n",
      "Epoch: 816/2000... Training loss: 0.6259\n",
      "Epoch: 816/2000... Training loss: 0.4255\n",
      "Epoch: 816/2000... Training loss: 0.5865\n",
      "Epoch: 816/2000... Training loss: 0.7751\n",
      "Epoch: 816/2000... Training loss: 0.8691\n",
      "Epoch: 816/2000... Training loss: 0.5028\n",
      "Epoch: 816/2000... Training loss: 0.5799\n",
      "Epoch: 816/2000... Training loss: 0.7177\n",
      "Epoch: 816/2000... Training loss: 0.4914\n",
      "Epoch: 816/2000... Training loss: 0.6393\n",
      "Epoch: 816/2000... Training loss: 0.4214\n",
      "Epoch: 816/2000... Training loss: 0.4897\n",
      "Epoch: 816/2000... Training loss: 0.6257\n",
      "Epoch: 816/2000... Training loss: 0.6986\n",
      "Epoch: 816/2000... Training loss: 0.5630\n",
      "Epoch: 816/2000... Training loss: 0.6369\n",
      "Epoch: 816/2000... Training loss: 0.5565\n",
      "Epoch: 816/2000... Training loss: 0.5513\n",
      "Epoch: 816/2000... Training loss: 0.4874\n",
      "Epoch: 816/2000... Training loss: 0.6634\n",
      "Epoch: 816/2000... Training loss: 0.7679\n",
      "Epoch: 816/2000... Training loss: 0.7109\n",
      "Epoch: 816/2000... Training loss: 0.5429\n",
      "Epoch: 816/2000... Training loss: 0.6734\n",
      "Epoch: 816/2000... Training loss: 0.7939\n",
      "Epoch: 816/2000... Training loss: 0.6771\n",
      "Epoch: 816/2000... Training loss: 0.6691\n",
      "Epoch: 816/2000... Training loss: 0.5876\n",
      "Epoch: 817/2000... Training loss: 0.8132\n",
      "Epoch: 817/2000... Training loss: 0.7331\n",
      "Epoch: 817/2000... Training loss: 0.6450\n",
      "Epoch: 817/2000... Training loss: 0.5987\n",
      "Epoch: 817/2000... Training loss: 0.6035\n",
      "Epoch: 817/2000... Training loss: 0.6421\n",
      "Epoch: 817/2000... Training loss: 0.7021\n",
      "Epoch: 817/2000... Training loss: 0.6231\n",
      "Epoch: 817/2000... Training loss: 0.5358\n",
      "Epoch: 817/2000... Training loss: 0.7264\n",
      "Epoch: 817/2000... Training loss: 0.7352\n",
      "Epoch: 817/2000... Training loss: 0.4593\n",
      "Epoch: 817/2000... Training loss: 0.6038\n",
      "Epoch: 817/2000... Training loss: 0.5557\n",
      "Epoch: 817/2000... Training loss: 0.5223\n",
      "Epoch: 817/2000... Training loss: 0.8066\n",
      "Epoch: 817/2000... Training loss: 0.8290\n",
      "Epoch: 817/2000... Training loss: 0.6879\n",
      "Epoch: 817/2000... Training loss: 0.7413\n",
      "Epoch: 817/2000... Training loss: 0.8421\n",
      "Epoch: 817/2000... Training loss: 0.6925\n",
      "Epoch: 817/2000... Training loss: 0.3925\n",
      "Epoch: 817/2000... Training loss: 0.5372\n",
      "Epoch: 817/2000... Training loss: 0.5131\n",
      "Epoch: 817/2000... Training loss: 0.4868\n",
      "Epoch: 817/2000... Training loss: 0.5638\n",
      "Epoch: 817/2000... Training loss: 0.7592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 817/2000... Training loss: 0.5381\n",
      "Epoch: 817/2000... Training loss: 0.6258\n",
      "Epoch: 817/2000... Training loss: 0.6477\n",
      "Epoch: 817/2000... Training loss: 0.6107\n",
      "Epoch: 818/2000... Training loss: 0.5497\n",
      "Epoch: 818/2000... Training loss: 0.5613\n",
      "Epoch: 818/2000... Training loss: 0.4841\n",
      "Epoch: 818/2000... Training loss: 0.5443\n",
      "Epoch: 818/2000... Training loss: 0.7254\n",
      "Epoch: 818/2000... Training loss: 0.5749\n",
      "Epoch: 818/2000... Training loss: 0.8919\n",
      "Epoch: 818/2000... Training loss: 0.6586\n",
      "Epoch: 818/2000... Training loss: 0.6491\n",
      "Epoch: 818/2000... Training loss: 0.6450\n",
      "Epoch: 818/2000... Training loss: 0.6685\n",
      "Epoch: 818/2000... Training loss: 0.5453\n",
      "Epoch: 818/2000... Training loss: 0.5868\n",
      "Epoch: 818/2000... Training loss: 0.5606\n",
      "Epoch: 818/2000... Training loss: 0.5803\n",
      "Epoch: 818/2000... Training loss: 0.6440\n",
      "Epoch: 818/2000... Training loss: 0.5042\n",
      "Epoch: 818/2000... Training loss: 0.6941\n",
      "Epoch: 818/2000... Training loss: 0.6036\n",
      "Epoch: 818/2000... Training loss: 0.5306\n",
      "Epoch: 818/2000... Training loss: 0.5905\n",
      "Epoch: 818/2000... Training loss: 0.7329\n",
      "Epoch: 818/2000... Training loss: 0.6557\n",
      "Epoch: 818/2000... Training loss: 0.7551\n",
      "Epoch: 818/2000... Training loss: 0.6245\n",
      "Epoch: 818/2000... Training loss: 0.6771\n",
      "Epoch: 818/2000... Training loss: 0.7904\n",
      "Epoch: 818/2000... Training loss: 0.6257\n",
      "Epoch: 818/2000... Training loss: 0.4941\n",
      "Epoch: 818/2000... Training loss: 0.6268\n",
      "Epoch: 818/2000... Training loss: 0.5499\n",
      "Epoch: 819/2000... Training loss: 0.8056\n",
      "Epoch: 819/2000... Training loss: 0.6932\n",
      "Epoch: 819/2000... Training loss: 0.6319\n",
      "Epoch: 819/2000... Training loss: 0.7456\n",
      "Epoch: 819/2000... Training loss: 0.6518\n",
      "Epoch: 819/2000... Training loss: 0.6200\n",
      "Epoch: 819/2000... Training loss: 0.6452\n",
      "Epoch: 819/2000... Training loss: 0.6957\n",
      "Epoch: 819/2000... Training loss: 0.5474\n",
      "Epoch: 819/2000... Training loss: 0.7179\n",
      "Epoch: 819/2000... Training loss: 0.6614\n",
      "Epoch: 819/2000... Training loss: 0.7479\n",
      "Epoch: 819/2000... Training loss: 0.4271\n",
      "Epoch: 819/2000... Training loss: 0.8058\n",
      "Epoch: 819/2000... Training loss: 0.5692\n",
      "Epoch: 819/2000... Training loss: 0.5391\n",
      "Epoch: 819/2000... Training loss: 0.5006\n",
      "Epoch: 819/2000... Training loss: 0.6338\n",
      "Epoch: 819/2000... Training loss: 0.8319\n",
      "Epoch: 819/2000... Training loss: 0.8317\n",
      "Epoch: 819/2000... Training loss: 0.8674\n",
      "Epoch: 819/2000... Training loss: 0.7416\n",
      "Epoch: 819/2000... Training loss: 0.6349\n",
      "Epoch: 819/2000... Training loss: 0.9686\n",
      "Epoch: 819/2000... Training loss: 0.7680\n",
      "Epoch: 819/2000... Training loss: 0.8467\n",
      "Epoch: 819/2000... Training loss: 0.6522\n",
      "Epoch: 819/2000... Training loss: 0.7386\n",
      "Epoch: 819/2000... Training loss: 0.7101\n",
      "Epoch: 819/2000... Training loss: 0.5299\n",
      "Epoch: 819/2000... Training loss: 0.5634\n",
      "Epoch: 820/2000... Training loss: 0.6584\n",
      "Epoch: 820/2000... Training loss: 0.5723\n",
      "Epoch: 820/2000... Training loss: 0.7102\n",
      "Epoch: 820/2000... Training loss: 0.6084\n",
      "Epoch: 820/2000... Training loss: 0.5489\n",
      "Epoch: 820/2000... Training loss: 0.7742\n",
      "Epoch: 820/2000... Training loss: 0.6764\n",
      "Epoch: 820/2000... Training loss: 0.8391\n",
      "Epoch: 820/2000... Training loss: 0.6836\n",
      "Epoch: 820/2000... Training loss: 0.6862\n",
      "Epoch: 820/2000... Training loss: 0.8480\n",
      "Epoch: 820/2000... Training loss: 0.8127\n",
      "Epoch: 820/2000... Training loss: 0.5936\n",
      "Epoch: 820/2000... Training loss: 0.4704\n",
      "Epoch: 820/2000... Training loss: 0.7506\n",
      "Epoch: 820/2000... Training loss: 0.7196\n",
      "Epoch: 820/2000... Training loss: 0.6477\n",
      "Epoch: 820/2000... Training loss: 0.5740\n",
      "Epoch: 820/2000... Training loss: 0.5318\n",
      "Epoch: 820/2000... Training loss: 0.6859\n",
      "Epoch: 820/2000... Training loss: 0.6379\n",
      "Epoch: 820/2000... Training loss: 0.5331\n",
      "Epoch: 820/2000... Training loss: 0.6991\n",
      "Epoch: 820/2000... Training loss: 0.6194\n",
      "Epoch: 820/2000... Training loss: 0.6123\n",
      "Epoch: 820/2000... Training loss: 0.8393\n",
      "Epoch: 820/2000... Training loss: 0.6153\n",
      "Epoch: 820/2000... Training loss: 0.4386\n",
      "Epoch: 820/2000... Training loss: 0.5214\n",
      "Epoch: 820/2000... Training loss: 0.4865\n",
      "Epoch: 820/2000... Training loss: 0.6777\n",
      "Epoch: 821/2000... Training loss: 0.6908\n",
      "Epoch: 821/2000... Training loss: 0.9607\n",
      "Epoch: 821/2000... Training loss: 0.8540\n",
      "Epoch: 821/2000... Training loss: 0.6418\n",
      "Epoch: 821/2000... Training loss: 0.8351\n",
      "Epoch: 821/2000... Training loss: 0.5941\n",
      "Epoch: 821/2000... Training loss: 0.6035\n",
      "Epoch: 821/2000... Training loss: 0.6166\n",
      "Epoch: 821/2000... Training loss: 0.7865\n",
      "Epoch: 821/2000... Training loss: 0.6590\n",
      "Epoch: 821/2000... Training loss: 0.6394\n",
      "Epoch: 821/2000... Training loss: 0.6305\n",
      "Epoch: 821/2000... Training loss: 0.7292\n",
      "Epoch: 821/2000... Training loss: 0.7094\n",
      "Epoch: 821/2000... Training loss: 0.9156\n",
      "Epoch: 821/2000... Training loss: 0.4602\n",
      "Epoch: 821/2000... Training loss: 0.5370\n",
      "Epoch: 821/2000... Training loss: 0.6791\n",
      "Epoch: 821/2000... Training loss: 0.6892\n",
      "Epoch: 821/2000... Training loss: 0.5577\n",
      "Epoch: 821/2000... Training loss: 0.5337\n",
      "Epoch: 821/2000... Training loss: 0.4308\n",
      "Epoch: 821/2000... Training loss: 0.6375\n",
      "Epoch: 821/2000... Training loss: 0.8364\n",
      "Epoch: 821/2000... Training loss: 0.7599\n",
      "Epoch: 821/2000... Training loss: 0.5651\n",
      "Epoch: 821/2000... Training loss: 0.6933\n",
      "Epoch: 821/2000... Training loss: 0.7864\n",
      "Epoch: 821/2000... Training loss: 0.5374\n",
      "Epoch: 821/2000... Training loss: 0.7353\n",
      "Epoch: 821/2000... Training loss: 0.4681\n",
      "Epoch: 822/2000... Training loss: 0.7484\n",
      "Epoch: 822/2000... Training loss: 0.5247\n",
      "Epoch: 822/2000... Training loss: 0.7618\n",
      "Epoch: 822/2000... Training loss: 0.7428\n",
      "Epoch: 822/2000... Training loss: 0.5472\n",
      "Epoch: 822/2000... Training loss: 0.8817\n",
      "Epoch: 822/2000... Training loss: 0.8790\n",
      "Epoch: 822/2000... Training loss: 0.5718\n",
      "Epoch: 822/2000... Training loss: 0.6651\n",
      "Epoch: 822/2000... Training loss: 0.6726\n",
      "Epoch: 822/2000... Training loss: 0.7367\n",
      "Epoch: 822/2000... Training loss: 0.6473\n",
      "Epoch: 822/2000... Training loss: 0.7236\n",
      "Epoch: 822/2000... Training loss: 0.8116\n",
      "Epoch: 822/2000... Training loss: 0.6437\n",
      "Epoch: 822/2000... Training loss: 0.8089\n",
      "Epoch: 822/2000... Training loss: 0.5588\n",
      "Epoch: 822/2000... Training loss: 0.5414\n",
      "Epoch: 822/2000... Training loss: 0.5508\n",
      "Epoch: 822/2000... Training loss: 0.7889\n",
      "Epoch: 822/2000... Training loss: 0.6313\n",
      "Epoch: 822/2000... Training loss: 0.6266\n",
      "Epoch: 822/2000... Training loss: 0.5325\n",
      "Epoch: 822/2000... Training loss: 0.5398\n",
      "Epoch: 822/2000... Training loss: 0.6282\n",
      "Epoch: 822/2000... Training loss: 0.6217\n",
      "Epoch: 822/2000... Training loss: 0.6505\n",
      "Epoch: 822/2000... Training loss: 0.5563\n",
      "Epoch: 822/2000... Training loss: 0.8671\n",
      "Epoch: 822/2000... Training loss: 0.6102\n",
      "Epoch: 822/2000... Training loss: 0.6693\n",
      "Epoch: 823/2000... Training loss: 0.6003\n",
      "Epoch: 823/2000... Training loss: 0.6569\n",
      "Epoch: 823/2000... Training loss: 0.5842\n",
      "Epoch: 823/2000... Training loss: 0.7731\n",
      "Epoch: 823/2000... Training loss: 0.6035\n",
      "Epoch: 823/2000... Training loss: 0.5287\n",
      "Epoch: 823/2000... Training loss: 0.7777\n",
      "Epoch: 823/2000... Training loss: 0.5107\n",
      "Epoch: 823/2000... Training loss: 0.5024\n",
      "Epoch: 823/2000... Training loss: 0.4732\n",
      "Epoch: 823/2000... Training loss: 0.6420\n",
      "Epoch: 823/2000... Training loss: 0.7300\n",
      "Epoch: 823/2000... Training loss: 0.7551\n",
      "Epoch: 823/2000... Training loss: 0.6548\n",
      "Epoch: 823/2000... Training loss: 0.4795\n",
      "Epoch: 823/2000... Training loss: 0.6584\n",
      "Epoch: 823/2000... Training loss: 0.5963\n",
      "Epoch: 823/2000... Training loss: 0.6032\n",
      "Epoch: 823/2000... Training loss: 0.4368\n",
      "Epoch: 823/2000... Training loss: 0.7675\n",
      "Epoch: 823/2000... Training loss: 0.6127\n",
      "Epoch: 823/2000... Training loss: 0.5936\n",
      "Epoch: 823/2000... Training loss: 0.5506\n",
      "Epoch: 823/2000... Training loss: 0.6781\n",
      "Epoch: 823/2000... Training loss: 0.6802\n",
      "Epoch: 823/2000... Training loss: 0.5622\n",
      "Epoch: 823/2000... Training loss: 0.6900\n",
      "Epoch: 823/2000... Training loss: 0.8281\n",
      "Epoch: 823/2000... Training loss: 0.5263\n",
      "Epoch: 823/2000... Training loss: 0.5049\n",
      "Epoch: 823/2000... Training loss: 0.4916\n",
      "Epoch: 824/2000... Training loss: 0.5989\n",
      "Epoch: 824/2000... Training loss: 0.7724\n",
      "Epoch: 824/2000... Training loss: 0.6506\n",
      "Epoch: 824/2000... Training loss: 0.6137\n",
      "Epoch: 824/2000... Training loss: 0.5310\n",
      "Epoch: 824/2000... Training loss: 0.8800\n",
      "Epoch: 824/2000... Training loss: 0.5134\n",
      "Epoch: 824/2000... Training loss: 0.6464\n",
      "Epoch: 824/2000... Training loss: 0.5645\n",
      "Epoch: 824/2000... Training loss: 0.7355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 824/2000... Training loss: 0.5499\n",
      "Epoch: 824/2000... Training loss: 0.8957\n",
      "Epoch: 824/2000... Training loss: 0.5373\n",
      "Epoch: 824/2000... Training loss: 0.4818\n",
      "Epoch: 824/2000... Training loss: 0.7073\n",
      "Epoch: 824/2000... Training loss: 0.6340\n",
      "Epoch: 824/2000... Training loss: 0.8008\n",
      "Epoch: 824/2000... Training loss: 0.6410\n",
      "Epoch: 824/2000... Training loss: 0.6325\n",
      "Epoch: 824/2000... Training loss: 0.4582\n",
      "Epoch: 824/2000... Training loss: 0.6026\n",
      "Epoch: 824/2000... Training loss: 0.6623\n",
      "Epoch: 824/2000... Training loss: 0.7427\n",
      "Epoch: 824/2000... Training loss: 0.6352\n",
      "Epoch: 824/2000... Training loss: 0.5702\n",
      "Epoch: 824/2000... Training loss: 0.6829\n",
      "Epoch: 824/2000... Training loss: 0.7184\n",
      "Epoch: 824/2000... Training loss: 0.7144\n",
      "Epoch: 824/2000... Training loss: 0.5392\n",
      "Epoch: 824/2000... Training loss: 0.5381\n",
      "Epoch: 824/2000... Training loss: 0.4653\n",
      "Epoch: 825/2000... Training loss: 0.5989\n",
      "Epoch: 825/2000... Training loss: 0.7592\n",
      "Epoch: 825/2000... Training loss: 0.6030\n",
      "Epoch: 825/2000... Training loss: 0.5701\n",
      "Epoch: 825/2000... Training loss: 0.8550\n",
      "Epoch: 825/2000... Training loss: 0.5856\n",
      "Epoch: 825/2000... Training loss: 0.7281\n",
      "Epoch: 825/2000... Training loss: 0.7950\n",
      "Epoch: 825/2000... Training loss: 0.6127\n",
      "Epoch: 825/2000... Training loss: 0.7774\n",
      "Epoch: 825/2000... Training loss: 0.7315\n",
      "Epoch: 825/2000... Training loss: 0.5882\n",
      "Epoch: 825/2000... Training loss: 0.6665\n",
      "Epoch: 825/2000... Training loss: 0.5411\n",
      "Epoch: 825/2000... Training loss: 0.4975\n",
      "Epoch: 825/2000... Training loss: 0.5677\n",
      "Epoch: 825/2000... Training loss: 0.6736\n",
      "Epoch: 825/2000... Training loss: 0.5707\n",
      "Epoch: 825/2000... Training loss: 0.7522\n",
      "Epoch: 825/2000... Training loss: 0.6420\n",
      "Epoch: 825/2000... Training loss: 0.5562\n",
      "Epoch: 825/2000... Training loss: 0.4729\n",
      "Epoch: 825/2000... Training loss: 0.5182\n",
      "Epoch: 825/2000... Training loss: 0.8710\n",
      "Epoch: 825/2000... Training loss: 0.6939\n",
      "Epoch: 825/2000... Training loss: 0.5356\n",
      "Epoch: 825/2000... Training loss: 0.5392\n",
      "Epoch: 825/2000... Training loss: 0.6449\n",
      "Epoch: 825/2000... Training loss: 0.8002\n",
      "Epoch: 825/2000... Training loss: 0.6679\n",
      "Epoch: 825/2000... Training loss: 0.7324\n",
      "Epoch: 826/2000... Training loss: 0.6986\n",
      "Epoch: 826/2000... Training loss: 0.6313\n",
      "Epoch: 826/2000... Training loss: 0.6241\n",
      "Epoch: 826/2000... Training loss: 0.4404\n",
      "Epoch: 826/2000... Training loss: 0.7397\n",
      "Epoch: 826/2000... Training loss: 0.5627\n",
      "Epoch: 826/2000... Training loss: 0.6240\n",
      "Epoch: 826/2000... Training loss: 0.6353\n",
      "Epoch: 826/2000... Training loss: 0.6854\n",
      "Epoch: 826/2000... Training loss: 0.5814\n",
      "Epoch: 826/2000... Training loss: 0.7612\n",
      "Epoch: 826/2000... Training loss: 0.7545\n",
      "Epoch: 826/2000... Training loss: 0.6220\n",
      "Epoch: 826/2000... Training loss: 0.7295\n",
      "Epoch: 826/2000... Training loss: 0.8722\n",
      "Epoch: 826/2000... Training loss: 0.5286\n",
      "Epoch: 826/2000... Training loss: 0.7143\n",
      "Epoch: 826/2000... Training loss: 0.4885\n",
      "Epoch: 826/2000... Training loss: 0.6213\n",
      "Epoch: 826/2000... Training loss: 0.6321\n",
      "Epoch: 826/2000... Training loss: 0.6179\n",
      "Epoch: 826/2000... Training loss: 0.7329\n",
      "Epoch: 826/2000... Training loss: 0.8335\n",
      "Epoch: 826/2000... Training loss: 0.7399\n",
      "Epoch: 826/2000... Training loss: 0.4319\n",
      "Epoch: 826/2000... Training loss: 0.3792\n",
      "Epoch: 826/2000... Training loss: 0.6029\n",
      "Epoch: 826/2000... Training loss: 0.6462\n",
      "Epoch: 826/2000... Training loss: 0.4983\n",
      "Epoch: 826/2000... Training loss: 0.6381\n",
      "Epoch: 826/2000... Training loss: 0.6119\n",
      "Epoch: 827/2000... Training loss: 0.9421\n",
      "Epoch: 827/2000... Training loss: 0.4785\n",
      "Epoch: 827/2000... Training loss: 0.8756\n",
      "Epoch: 827/2000... Training loss: 0.5431\n",
      "Epoch: 827/2000... Training loss: 0.6218\n",
      "Epoch: 827/2000... Training loss: 0.5769\n",
      "Epoch: 827/2000... Training loss: 0.6530\n",
      "Epoch: 827/2000... Training loss: 0.5853\n",
      "Epoch: 827/2000... Training loss: 0.7662\n",
      "Epoch: 827/2000... Training loss: 0.6428\n",
      "Epoch: 827/2000... Training loss: 0.6066\n",
      "Epoch: 827/2000... Training loss: 0.5318\n",
      "Epoch: 827/2000... Training loss: 0.6674\n",
      "Epoch: 827/2000... Training loss: 0.7481\n",
      "Epoch: 827/2000... Training loss: 0.6244\n",
      "Epoch: 827/2000... Training loss: 0.6044\n",
      "Epoch: 827/2000... Training loss: 0.6197\n",
      "Epoch: 827/2000... Training loss: 0.7706\n",
      "Epoch: 827/2000... Training loss: 0.6480\n",
      "Epoch: 827/2000... Training loss: 0.6552\n",
      "Epoch: 827/2000... Training loss: 0.5748\n",
      "Epoch: 827/2000... Training loss: 0.6693\n",
      "Epoch: 827/2000... Training loss: 0.3875\n",
      "Epoch: 827/2000... Training loss: 0.4140\n",
      "Epoch: 827/2000... Training loss: 0.6165\n",
      "Epoch: 827/2000... Training loss: 0.6426\n",
      "Epoch: 827/2000... Training loss: 0.5314\n",
      "Epoch: 827/2000... Training loss: 0.4543\n",
      "Epoch: 827/2000... Training loss: 0.5481\n",
      "Epoch: 827/2000... Training loss: 0.5829\n",
      "Epoch: 827/2000... Training loss: 0.5323\n",
      "Epoch: 828/2000... Training loss: 0.4990\n",
      "Epoch: 828/2000... Training loss: 0.5106\n",
      "Epoch: 828/2000... Training loss: 0.6166\n",
      "Epoch: 828/2000... Training loss: 0.5228\n",
      "Epoch: 828/2000... Training loss: 0.4642\n",
      "Epoch: 828/2000... Training loss: 0.6719\n",
      "Epoch: 828/2000... Training loss: 0.8152\n",
      "Epoch: 828/2000... Training loss: 0.5798\n",
      "Epoch: 828/2000... Training loss: 0.3959\n",
      "Epoch: 828/2000... Training loss: 0.7829\n",
      "Epoch: 828/2000... Training loss: 0.5435\n",
      "Epoch: 828/2000... Training loss: 0.8684\n",
      "Epoch: 828/2000... Training loss: 0.4073\n",
      "Epoch: 828/2000... Training loss: 0.7707\n",
      "Epoch: 828/2000... Training loss: 0.5348\n",
      "Epoch: 828/2000... Training loss: 0.5848\n",
      "Epoch: 828/2000... Training loss: 0.6702\n",
      "Epoch: 828/2000... Training loss: 0.6978\n",
      "Epoch: 828/2000... Training loss: 0.6810\n",
      "Epoch: 828/2000... Training loss: 0.5195\n",
      "Epoch: 828/2000... Training loss: 0.6737\n",
      "Epoch: 828/2000... Training loss: 0.7060\n",
      "Epoch: 828/2000... Training loss: 0.7877\n",
      "Epoch: 828/2000... Training loss: 0.9606\n",
      "Epoch: 828/2000... Training loss: 0.5892\n",
      "Epoch: 828/2000... Training loss: 0.7865\n",
      "Epoch: 828/2000... Training loss: 0.6518\n",
      "Epoch: 828/2000... Training loss: 0.6170\n",
      "Epoch: 828/2000... Training loss: 0.6155\n",
      "Epoch: 828/2000... Training loss: 0.5223\n",
      "Epoch: 828/2000... Training loss: 0.5336\n",
      "Epoch: 829/2000... Training loss: 0.6121\n",
      "Epoch: 829/2000... Training loss: 0.6190\n",
      "Epoch: 829/2000... Training loss: 0.7365\n",
      "Epoch: 829/2000... Training loss: 0.5702\n",
      "Epoch: 829/2000... Training loss: 0.7237\n",
      "Epoch: 829/2000... Training loss: 0.6642\n",
      "Epoch: 829/2000... Training loss: 0.6027\n",
      "Epoch: 829/2000... Training loss: 0.6334\n",
      "Epoch: 829/2000... Training loss: 0.5615\n",
      "Epoch: 829/2000... Training loss: 0.5158\n",
      "Epoch: 829/2000... Training loss: 0.6152\n",
      "Epoch: 829/2000... Training loss: 0.6980\n",
      "Epoch: 829/2000... Training loss: 0.6272\n",
      "Epoch: 829/2000... Training loss: 0.6136\n",
      "Epoch: 829/2000... Training loss: 0.6300\n",
      "Epoch: 829/2000... Training loss: 0.5166\n",
      "Epoch: 829/2000... Training loss: 0.5495\n",
      "Epoch: 829/2000... Training loss: 0.7194\n",
      "Epoch: 829/2000... Training loss: 0.5094\n",
      "Epoch: 829/2000... Training loss: 0.6832\n",
      "Epoch: 829/2000... Training loss: 0.4978\n",
      "Epoch: 829/2000... Training loss: 0.6825\n",
      "Epoch: 829/2000... Training loss: 0.5964\n",
      "Epoch: 829/2000... Training loss: 0.5870\n",
      "Epoch: 829/2000... Training loss: 0.8489\n",
      "Epoch: 829/2000... Training loss: 0.4498\n",
      "Epoch: 829/2000... Training loss: 0.7891\n",
      "Epoch: 829/2000... Training loss: 0.5633\n",
      "Epoch: 829/2000... Training loss: 0.8270\n",
      "Epoch: 829/2000... Training loss: 0.5658\n",
      "Epoch: 829/2000... Training loss: 0.7456\n",
      "Epoch: 830/2000... Training loss: 0.5954\n",
      "Epoch: 830/2000... Training loss: 0.5757\n",
      "Epoch: 830/2000... Training loss: 0.7759\n",
      "Epoch: 830/2000... Training loss: 0.4537\n",
      "Epoch: 830/2000... Training loss: 0.6094\n",
      "Epoch: 830/2000... Training loss: 0.6819\n",
      "Epoch: 830/2000... Training loss: 0.6282\n",
      "Epoch: 830/2000... Training loss: 0.6903\n",
      "Epoch: 830/2000... Training loss: 0.6256\n",
      "Epoch: 830/2000... Training loss: 0.6502\n",
      "Epoch: 830/2000... Training loss: 0.6440\n",
      "Epoch: 830/2000... Training loss: 0.6773\n",
      "Epoch: 830/2000... Training loss: 0.4811\n",
      "Epoch: 830/2000... Training loss: 0.7380\n",
      "Epoch: 830/2000... Training loss: 0.6264\n",
      "Epoch: 830/2000... Training loss: 0.5842\n",
      "Epoch: 830/2000... Training loss: 0.4822\n",
      "Epoch: 830/2000... Training loss: 0.6916\n",
      "Epoch: 830/2000... Training loss: 0.5661\n",
      "Epoch: 830/2000... Training loss: 0.7003\n",
      "Epoch: 830/2000... Training loss: 0.5518\n",
      "Epoch: 830/2000... Training loss: 0.4269\n",
      "Epoch: 830/2000... Training loss: 0.5340\n",
      "Epoch: 830/2000... Training loss: 0.5910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 830/2000... Training loss: 0.6049\n",
      "Epoch: 830/2000... Training loss: 0.5581\n",
      "Epoch: 830/2000... Training loss: 0.6323\n",
      "Epoch: 830/2000... Training loss: 0.7308\n",
      "Epoch: 830/2000... Training loss: 0.7361\n",
      "Epoch: 830/2000... Training loss: 0.5487\n",
      "Epoch: 830/2000... Training loss: 0.7614\n",
      "Epoch: 831/2000... Training loss: 0.4566\n",
      "Epoch: 831/2000... Training loss: 0.5247\n",
      "Epoch: 831/2000... Training loss: 0.5829\n",
      "Epoch: 831/2000... Training loss: 0.7112\n",
      "Epoch: 831/2000... Training loss: 0.5839\n",
      "Epoch: 831/2000... Training loss: 0.5736\n",
      "Epoch: 831/2000... Training loss: 0.6162\n",
      "Epoch: 831/2000... Training loss: 0.8148\n",
      "Epoch: 831/2000... Training loss: 0.7357\n",
      "Epoch: 831/2000... Training loss: 0.6572\n",
      "Epoch: 831/2000... Training loss: 0.5921\n",
      "Epoch: 831/2000... Training loss: 0.6018\n",
      "Epoch: 831/2000... Training loss: 0.6868\n",
      "Epoch: 831/2000... Training loss: 0.5379\n",
      "Epoch: 831/2000... Training loss: 0.7664\n",
      "Epoch: 831/2000... Training loss: 0.6350\n",
      "Epoch: 831/2000... Training loss: 0.4673\n",
      "Epoch: 831/2000... Training loss: 0.8103\n",
      "Epoch: 831/2000... Training loss: 0.5913\n",
      "Epoch: 831/2000... Training loss: 0.6981\n",
      "Epoch: 831/2000... Training loss: 0.5627\n",
      "Epoch: 831/2000... Training loss: 0.6250\n",
      "Epoch: 831/2000... Training loss: 0.5114\n",
      "Epoch: 831/2000... Training loss: 0.8916\n",
      "Epoch: 831/2000... Training loss: 0.7516\n",
      "Epoch: 831/2000... Training loss: 0.6113\n",
      "Epoch: 831/2000... Training loss: 0.6431\n",
      "Epoch: 831/2000... Training loss: 0.8853\n",
      "Epoch: 831/2000... Training loss: 0.9450\n",
      "Epoch: 831/2000... Training loss: 0.4064\n",
      "Epoch: 831/2000... Training loss: 0.5598\n",
      "Epoch: 832/2000... Training loss: 0.5581\n",
      "Epoch: 832/2000... Training loss: 0.6710\n",
      "Epoch: 832/2000... Training loss: 0.4549\n",
      "Epoch: 832/2000... Training loss: 0.7160\n",
      "Epoch: 832/2000... Training loss: 0.5786\n",
      "Epoch: 832/2000... Training loss: 0.6257\n",
      "Epoch: 832/2000... Training loss: 0.7402\n",
      "Epoch: 832/2000... Training loss: 0.6521\n",
      "Epoch: 832/2000... Training loss: 0.6232\n",
      "Epoch: 832/2000... Training loss: 0.5834\n",
      "Epoch: 832/2000... Training loss: 0.8166\n",
      "Epoch: 832/2000... Training loss: 0.6246\n",
      "Epoch: 832/2000... Training loss: 0.9368\n",
      "Epoch: 832/2000... Training loss: 0.9116\n",
      "Epoch: 832/2000... Training loss: 0.5749\n",
      "Epoch: 832/2000... Training loss: 0.6334\n",
      "Epoch: 832/2000... Training loss: 0.7644\n",
      "Epoch: 832/2000... Training loss: 0.7530\n",
      "Epoch: 832/2000... Training loss: 0.5329\n",
      "Epoch: 832/2000... Training loss: 0.5602\n",
      "Epoch: 832/2000... Training loss: 0.5736\n",
      "Epoch: 832/2000... Training loss: 0.7460\n",
      "Epoch: 832/2000... Training loss: 0.7758\n",
      "Epoch: 832/2000... Training loss: 0.6216\n",
      "Epoch: 832/2000... Training loss: 0.6471\n",
      "Epoch: 832/2000... Training loss: 0.6858\n",
      "Epoch: 832/2000... Training loss: 0.6099\n",
      "Epoch: 832/2000... Training loss: 0.4497\n",
      "Epoch: 832/2000... Training loss: 0.6140\n",
      "Epoch: 832/2000... Training loss: 0.5585\n",
      "Epoch: 832/2000... Training loss: 0.5007\n",
      "Epoch: 833/2000... Training loss: 0.8577\n",
      "Epoch: 833/2000... Training loss: 0.7778\n",
      "Epoch: 833/2000... Training loss: 0.7785\n",
      "Epoch: 833/2000... Training loss: 0.7043\n",
      "Epoch: 833/2000... Training loss: 0.4619\n",
      "Epoch: 833/2000... Training loss: 0.5923\n",
      "Epoch: 833/2000... Training loss: 0.7247\n",
      "Epoch: 833/2000... Training loss: 0.5312\n",
      "Epoch: 833/2000... Training loss: 0.6776\n",
      "Epoch: 833/2000... Training loss: 0.4899\n",
      "Epoch: 833/2000... Training loss: 0.6700\n",
      "Epoch: 833/2000... Training loss: 0.5484\n",
      "Epoch: 833/2000... Training loss: 0.4515\n",
      "Epoch: 833/2000... Training loss: 0.4508\n",
      "Epoch: 833/2000... Training loss: 0.5747\n",
      "Epoch: 833/2000... Training loss: 0.4597\n",
      "Epoch: 833/2000... Training loss: 0.6599\n",
      "Epoch: 833/2000... Training loss: 0.6395\n",
      "Epoch: 833/2000... Training loss: 0.5123\n",
      "Epoch: 833/2000... Training loss: 0.6830\n",
      "Epoch: 833/2000... Training loss: 0.7290\n",
      "Epoch: 833/2000... Training loss: 0.7204\n",
      "Epoch: 833/2000... Training loss: 0.7191\n",
      "Epoch: 833/2000... Training loss: 0.7881\n",
      "Epoch: 833/2000... Training loss: 0.4435\n",
      "Epoch: 833/2000... Training loss: 0.7698\n",
      "Epoch: 833/2000... Training loss: 0.4673\n",
      "Epoch: 833/2000... Training loss: 0.7129\n",
      "Epoch: 833/2000... Training loss: 0.6521\n",
      "Epoch: 833/2000... Training loss: 0.6062\n",
      "Epoch: 833/2000... Training loss: 0.8036\n",
      "Epoch: 834/2000... Training loss: 0.6949\n",
      "Epoch: 834/2000... Training loss: 0.7807\n",
      "Epoch: 834/2000... Training loss: 0.7708\n",
      "Epoch: 834/2000... Training loss: 0.7867\n",
      "Epoch: 834/2000... Training loss: 0.7432\n",
      "Epoch: 834/2000... Training loss: 0.7363\n",
      "Epoch: 834/2000... Training loss: 0.5615\n",
      "Epoch: 834/2000... Training loss: 0.5077\n",
      "Epoch: 834/2000... Training loss: 0.5982\n",
      "Epoch: 834/2000... Training loss: 0.5189\n",
      "Epoch: 834/2000... Training loss: 0.5135\n",
      "Epoch: 834/2000... Training loss: 0.7729\n",
      "Epoch: 834/2000... Training loss: 0.5928\n",
      "Epoch: 834/2000... Training loss: 0.5622\n",
      "Epoch: 834/2000... Training loss: 0.6946\n",
      "Epoch: 834/2000... Training loss: 0.5664\n",
      "Epoch: 834/2000... Training loss: 0.6161\n",
      "Epoch: 834/2000... Training loss: 0.6974\n",
      "Epoch: 834/2000... Training loss: 0.5696\n",
      "Epoch: 834/2000... Training loss: 0.6506\n",
      "Epoch: 834/2000... Training loss: 0.5700\n",
      "Epoch: 834/2000... Training loss: 0.5327\n",
      "Epoch: 834/2000... Training loss: 0.6892\n",
      "Epoch: 834/2000... Training loss: 0.4987\n",
      "Epoch: 834/2000... Training loss: 0.7374\n",
      "Epoch: 834/2000... Training loss: 0.7415\n",
      "Epoch: 834/2000... Training loss: 0.7761\n",
      "Epoch: 834/2000... Training loss: 0.5007\n",
      "Epoch: 834/2000... Training loss: 0.5891\n",
      "Epoch: 834/2000... Training loss: 0.7183\n",
      "Epoch: 834/2000... Training loss: 0.8327\n",
      "Epoch: 835/2000... Training loss: 0.6419\n",
      "Epoch: 835/2000... Training loss: 0.4777\n",
      "Epoch: 835/2000... Training loss: 0.8254\n",
      "Epoch: 835/2000... Training loss: 0.5814\n",
      "Epoch: 835/2000... Training loss: 0.7103\n",
      "Epoch: 835/2000... Training loss: 0.9433\n",
      "Epoch: 835/2000... Training loss: 0.6213\n",
      "Epoch: 835/2000... Training loss: 0.4022\n",
      "Epoch: 835/2000... Training loss: 0.5932\n",
      "Epoch: 835/2000... Training loss: 0.7089\n",
      "Epoch: 835/2000... Training loss: 0.6476\n",
      "Epoch: 835/2000... Training loss: 0.6232\n",
      "Epoch: 835/2000... Training loss: 0.6899\n",
      "Epoch: 835/2000... Training loss: 0.7711\n",
      "Epoch: 835/2000... Training loss: 0.6913\n",
      "Epoch: 835/2000... Training loss: 0.6985\n",
      "Epoch: 835/2000... Training loss: 0.5514\n",
      "Epoch: 835/2000... Training loss: 0.7292\n",
      "Epoch: 835/2000... Training loss: 0.7568\n",
      "Epoch: 835/2000... Training loss: 0.5321\n",
      "Epoch: 835/2000... Training loss: 0.4893\n",
      "Epoch: 835/2000... Training loss: 0.5316\n",
      "Epoch: 835/2000... Training loss: 0.6740\n",
      "Epoch: 835/2000... Training loss: 0.5434\n",
      "Epoch: 835/2000... Training loss: 0.6944\n",
      "Epoch: 835/2000... Training loss: 0.5343\n",
      "Epoch: 835/2000... Training loss: 0.6739\n",
      "Epoch: 835/2000... Training loss: 0.5968\n",
      "Epoch: 835/2000... Training loss: 0.6449\n",
      "Epoch: 835/2000... Training loss: 0.5090\n",
      "Epoch: 835/2000... Training loss: 0.6822\n",
      "Epoch: 836/2000... Training loss: 0.5158\n",
      "Epoch: 836/2000... Training loss: 0.6150\n",
      "Epoch: 836/2000... Training loss: 0.6934\n",
      "Epoch: 836/2000... Training loss: 0.5752\n",
      "Epoch: 836/2000... Training loss: 0.7033\n",
      "Epoch: 836/2000... Training loss: 0.5856\n",
      "Epoch: 836/2000... Training loss: 0.5573\n",
      "Epoch: 836/2000... Training loss: 0.4661\n",
      "Epoch: 836/2000... Training loss: 0.5599\n",
      "Epoch: 836/2000... Training loss: 0.6088\n",
      "Epoch: 836/2000... Training loss: 0.5182\n",
      "Epoch: 836/2000... Training loss: 0.6264\n",
      "Epoch: 836/2000... Training loss: 0.5660\n",
      "Epoch: 836/2000... Training loss: 0.7839\n",
      "Epoch: 836/2000... Training loss: 0.6215\n",
      "Epoch: 836/2000... Training loss: 0.8558\n",
      "Epoch: 836/2000... Training loss: 0.6283\n",
      "Epoch: 836/2000... Training loss: 0.5091\n",
      "Epoch: 836/2000... Training loss: 0.6502\n",
      "Epoch: 836/2000... Training loss: 0.4712\n",
      "Epoch: 836/2000... Training loss: 0.5090\n",
      "Epoch: 836/2000... Training loss: 0.6334\n",
      "Epoch: 836/2000... Training loss: 0.9003\n",
      "Epoch: 836/2000... Training loss: 0.4662\n",
      "Epoch: 836/2000... Training loss: 0.9274\n",
      "Epoch: 836/2000... Training loss: 0.7055\n",
      "Epoch: 836/2000... Training loss: 0.5133\n",
      "Epoch: 836/2000... Training loss: 0.7833\n",
      "Epoch: 836/2000... Training loss: 0.7180\n",
      "Epoch: 836/2000... Training loss: 0.6776\n",
      "Epoch: 836/2000... Training loss: 0.4402\n",
      "Epoch: 837/2000... Training loss: 0.7894\n",
      "Epoch: 837/2000... Training loss: 0.5888\n",
      "Epoch: 837/2000... Training loss: 0.5507\n",
      "Epoch: 837/2000... Training loss: 0.5874\n",
      "Epoch: 837/2000... Training loss: 0.5736\n",
      "Epoch: 837/2000... Training loss: 0.6104\n",
      "Epoch: 837/2000... Training loss: 0.5993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 837/2000... Training loss: 0.6370\n",
      "Epoch: 837/2000... Training loss: 0.4798\n",
      "Epoch: 837/2000... Training loss: 0.5399\n",
      "Epoch: 837/2000... Training loss: 0.5524\n",
      "Epoch: 837/2000... Training loss: 0.6535\n",
      "Epoch: 837/2000... Training loss: 0.5956\n",
      "Epoch: 837/2000... Training loss: 0.7226\n",
      "Epoch: 837/2000... Training loss: 0.4286\n",
      "Epoch: 837/2000... Training loss: 0.5931\n",
      "Epoch: 837/2000... Training loss: 0.4545\n",
      "Epoch: 837/2000... Training loss: 0.7286\n",
      "Epoch: 837/2000... Training loss: 0.5542\n",
      "Epoch: 837/2000... Training loss: 0.7716\n",
      "Epoch: 837/2000... Training loss: 0.6146\n",
      "Epoch: 837/2000... Training loss: 0.3801\n",
      "Epoch: 837/2000... Training loss: 0.6235\n",
      "Epoch: 837/2000... Training loss: 0.5857\n",
      "Epoch: 837/2000... Training loss: 0.7316\n",
      "Epoch: 837/2000... Training loss: 0.6577\n",
      "Epoch: 837/2000... Training loss: 0.6597\n",
      "Epoch: 837/2000... Training loss: 0.4677\n",
      "Epoch: 837/2000... Training loss: 0.5224\n",
      "Epoch: 837/2000... Training loss: 0.3648\n",
      "Epoch: 837/2000... Training loss: 0.7183\n",
      "Epoch: 838/2000... Training loss: 0.5699\n",
      "Epoch: 838/2000... Training loss: 0.8569\n",
      "Epoch: 838/2000... Training loss: 0.5427\n",
      "Epoch: 838/2000... Training loss: 0.6669\n",
      "Epoch: 838/2000... Training loss: 0.7114\n",
      "Epoch: 838/2000... Training loss: 0.7245\n",
      "Epoch: 838/2000... Training loss: 0.7026\n",
      "Epoch: 838/2000... Training loss: 0.5580\n",
      "Epoch: 838/2000... Training loss: 0.5298\n",
      "Epoch: 838/2000... Training loss: 0.6285\n",
      "Epoch: 838/2000... Training loss: 0.7674\n",
      "Epoch: 838/2000... Training loss: 0.6454\n",
      "Epoch: 838/2000... Training loss: 0.8025\n",
      "Epoch: 838/2000... Training loss: 0.4568\n",
      "Epoch: 838/2000... Training loss: 0.7133\n",
      "Epoch: 838/2000... Training loss: 0.5462\n",
      "Epoch: 838/2000... Training loss: 0.5415\n",
      "Epoch: 838/2000... Training loss: 0.5920\n",
      "Epoch: 838/2000... Training loss: 0.7164\n",
      "Epoch: 838/2000... Training loss: 0.7275\n",
      "Epoch: 838/2000... Training loss: 0.4931\n",
      "Epoch: 838/2000... Training loss: 0.5549\n",
      "Epoch: 838/2000... Training loss: 0.5082\n",
      "Epoch: 838/2000... Training loss: 0.6467\n",
      "Epoch: 838/2000... Training loss: 0.6825\n",
      "Epoch: 838/2000... Training loss: 0.8109\n",
      "Epoch: 838/2000... Training loss: 0.8230\n",
      "Epoch: 838/2000... Training loss: 0.6111\n",
      "Epoch: 838/2000... Training loss: 0.7579\n",
      "Epoch: 838/2000... Training loss: 0.5491\n",
      "Epoch: 838/2000... Training loss: 0.6488\n",
      "Epoch: 839/2000... Training loss: 0.6358\n",
      "Epoch: 839/2000... Training loss: 0.4951\n",
      "Epoch: 839/2000... Training loss: 0.6390\n",
      "Epoch: 839/2000... Training loss: 0.8609\n",
      "Epoch: 839/2000... Training loss: 0.4862\n",
      "Epoch: 839/2000... Training loss: 0.8961\n",
      "Epoch: 839/2000... Training loss: 0.5490\n",
      "Epoch: 839/2000... Training loss: 0.7350\n",
      "Epoch: 839/2000... Training loss: 0.5286\n",
      "Epoch: 839/2000... Training loss: 0.5496\n",
      "Epoch: 839/2000... Training loss: 0.5825\n",
      "Epoch: 839/2000... Training loss: 0.7903\n",
      "Epoch: 839/2000... Training loss: 0.5363\n",
      "Epoch: 839/2000... Training loss: 0.6130\n",
      "Epoch: 839/2000... Training loss: 0.5354\n",
      "Epoch: 839/2000... Training loss: 0.7758\n",
      "Epoch: 839/2000... Training loss: 0.6690\n",
      "Epoch: 839/2000... Training loss: 0.5297\n",
      "Epoch: 839/2000... Training loss: 0.7411\n",
      "Epoch: 839/2000... Training loss: 0.8752\n",
      "Epoch: 839/2000... Training loss: 0.5642\n",
      "Epoch: 839/2000... Training loss: 0.7007\n",
      "Epoch: 839/2000... Training loss: 0.5808\n",
      "Epoch: 839/2000... Training loss: 0.6929\n",
      "Epoch: 839/2000... Training loss: 0.6599\n",
      "Epoch: 839/2000... Training loss: 0.4637\n",
      "Epoch: 839/2000... Training loss: 0.5194\n",
      "Epoch: 839/2000... Training loss: 0.6423\n",
      "Epoch: 839/2000... Training loss: 0.4621\n",
      "Epoch: 839/2000... Training loss: 0.5824\n",
      "Epoch: 839/2000... Training loss: 0.5201\n",
      "Epoch: 840/2000... Training loss: 0.7450\n",
      "Epoch: 840/2000... Training loss: 0.7423\n",
      "Epoch: 840/2000... Training loss: 0.5749\n",
      "Epoch: 840/2000... Training loss: 0.6267\n",
      "Epoch: 840/2000... Training loss: 0.6326\n",
      "Epoch: 840/2000... Training loss: 0.5528\n",
      "Epoch: 840/2000... Training loss: 0.5289\n",
      "Epoch: 840/2000... Training loss: 0.5980\n",
      "Epoch: 840/2000... Training loss: 0.6037\n",
      "Epoch: 840/2000... Training loss: 0.5634\n",
      "Epoch: 840/2000... Training loss: 0.8347\n",
      "Epoch: 840/2000... Training loss: 0.7527\n",
      "Epoch: 840/2000... Training loss: 0.6768\n",
      "Epoch: 840/2000... Training loss: 0.4876\n",
      "Epoch: 840/2000... Training loss: 0.7356\n",
      "Epoch: 840/2000... Training loss: 0.8961\n",
      "Epoch: 840/2000... Training loss: 0.6574\n",
      "Epoch: 840/2000... Training loss: 0.6553\n",
      "Epoch: 840/2000... Training loss: 0.5849\n",
      "Epoch: 840/2000... Training loss: 0.5625\n",
      "Epoch: 840/2000... Training loss: 0.4442\n",
      "Epoch: 840/2000... Training loss: 0.7247\n",
      "Epoch: 840/2000... Training loss: 0.7572\n",
      "Epoch: 840/2000... Training loss: 0.6805\n",
      "Epoch: 840/2000... Training loss: 0.4634\n",
      "Epoch: 840/2000... Training loss: 0.5882\n",
      "Epoch: 840/2000... Training loss: 0.5025\n",
      "Epoch: 840/2000... Training loss: 0.7357\n",
      "Epoch: 840/2000... Training loss: 0.7998\n",
      "Epoch: 840/2000... Training loss: 0.6186\n",
      "Epoch: 840/2000... Training loss: 0.6485\n",
      "Epoch: 841/2000... Training loss: 0.6210\n",
      "Epoch: 841/2000... Training loss: 0.3925\n",
      "Epoch: 841/2000... Training loss: 0.5863\n",
      "Epoch: 841/2000... Training loss: 0.4901\n",
      "Epoch: 841/2000... Training loss: 0.7149\n",
      "Epoch: 841/2000... Training loss: 0.5099\n",
      "Epoch: 841/2000... Training loss: 0.7477\n",
      "Epoch: 841/2000... Training loss: 0.7715\n",
      "Epoch: 841/2000... Training loss: 0.5305\n",
      "Epoch: 841/2000... Training loss: 0.5659\n",
      "Epoch: 841/2000... Training loss: 0.5771\n",
      "Epoch: 841/2000... Training loss: 0.6583\n",
      "Epoch: 841/2000... Training loss: 0.5173\n",
      "Epoch: 841/2000... Training loss: 0.6663\n",
      "Epoch: 841/2000... Training loss: 0.5404\n",
      "Epoch: 841/2000... Training loss: 0.5080\n",
      "Epoch: 841/2000... Training loss: 0.5194\n",
      "Epoch: 841/2000... Training loss: 0.6618\n",
      "Epoch: 841/2000... Training loss: 0.5820\n",
      "Epoch: 841/2000... Training loss: 0.6428\n",
      "Epoch: 841/2000... Training loss: 0.4465\n",
      "Epoch: 841/2000... Training loss: 0.4849\n",
      "Epoch: 841/2000... Training loss: 0.6881\n",
      "Epoch: 841/2000... Training loss: 0.5976\n",
      "Epoch: 841/2000... Training loss: 0.4611\n",
      "Epoch: 841/2000... Training loss: 0.5587\n",
      "Epoch: 841/2000... Training loss: 0.6301\n",
      "Epoch: 841/2000... Training loss: 0.7774\n",
      "Epoch: 841/2000... Training loss: 0.4596\n",
      "Epoch: 841/2000... Training loss: 0.5955\n",
      "Epoch: 841/2000... Training loss: 0.7671\n",
      "Epoch: 842/2000... Training loss: 0.6714\n",
      "Epoch: 842/2000... Training loss: 0.5060\n",
      "Epoch: 842/2000... Training loss: 0.5188\n",
      "Epoch: 842/2000... Training loss: 0.4299\n",
      "Epoch: 842/2000... Training loss: 0.9508\n",
      "Epoch: 842/2000... Training loss: 0.4202\n",
      "Epoch: 842/2000... Training loss: 0.4970\n",
      "Epoch: 842/2000... Training loss: 0.8845\n",
      "Epoch: 842/2000... Training loss: 0.4553\n",
      "Epoch: 842/2000... Training loss: 0.5993\n",
      "Epoch: 842/2000... Training loss: 0.6338\n",
      "Epoch: 842/2000... Training loss: 0.6006\n",
      "Epoch: 842/2000... Training loss: 0.6800\n",
      "Epoch: 842/2000... Training loss: 0.6313\n",
      "Epoch: 842/2000... Training loss: 0.6096\n",
      "Epoch: 842/2000... Training loss: 0.9290\n",
      "Epoch: 842/2000... Training loss: 0.5445\n",
      "Epoch: 842/2000... Training loss: 0.4906\n",
      "Epoch: 842/2000... Training loss: 0.8328\n",
      "Epoch: 842/2000... Training loss: 0.6888\n",
      "Epoch: 842/2000... Training loss: 0.5448\n",
      "Epoch: 842/2000... Training loss: 0.5843\n",
      "Epoch: 842/2000... Training loss: 0.6849\n",
      "Epoch: 842/2000... Training loss: 0.4999\n",
      "Epoch: 842/2000... Training loss: 0.8084\n",
      "Epoch: 842/2000... Training loss: 0.5586\n",
      "Epoch: 842/2000... Training loss: 0.5855\n",
      "Epoch: 842/2000... Training loss: 0.5080\n",
      "Epoch: 842/2000... Training loss: 0.7138\n",
      "Epoch: 842/2000... Training loss: 0.7866\n",
      "Epoch: 842/2000... Training loss: 0.4998\n",
      "Epoch: 843/2000... Training loss: 0.7423\n",
      "Epoch: 843/2000... Training loss: 0.5582\n",
      "Epoch: 843/2000... Training loss: 0.6084\n",
      "Epoch: 843/2000... Training loss: 0.5886\n",
      "Epoch: 843/2000... Training loss: 0.5330\n",
      "Epoch: 843/2000... Training loss: 0.6011\n",
      "Epoch: 843/2000... Training loss: 0.6258\n",
      "Epoch: 843/2000... Training loss: 0.6198\n",
      "Epoch: 843/2000... Training loss: 0.6369\n",
      "Epoch: 843/2000... Training loss: 0.6090\n",
      "Epoch: 843/2000... Training loss: 0.5457\n",
      "Epoch: 843/2000... Training loss: 0.6454\n",
      "Epoch: 843/2000... Training loss: 0.4946\n",
      "Epoch: 843/2000... Training loss: 0.4771\n",
      "Epoch: 843/2000... Training loss: 0.5398\n",
      "Epoch: 843/2000... Training loss: 0.6505\n",
      "Epoch: 843/2000... Training loss: 0.6448\n",
      "Epoch: 843/2000... Training loss: 0.7708\n",
      "Epoch: 843/2000... Training loss: 0.6096\n",
      "Epoch: 843/2000... Training loss: 0.6583\n",
      "Epoch: 843/2000... Training loss: 0.6184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 843/2000... Training loss: 0.5380\n",
      "Epoch: 843/2000... Training loss: 0.3264\n",
      "Epoch: 843/2000... Training loss: 0.5421\n",
      "Epoch: 843/2000... Training loss: 0.6514\n",
      "Epoch: 843/2000... Training loss: 0.5028\n",
      "Epoch: 843/2000... Training loss: 0.8103\n",
      "Epoch: 843/2000... Training loss: 0.5694\n",
      "Epoch: 843/2000... Training loss: 0.5643\n",
      "Epoch: 843/2000... Training loss: 0.4859\n",
      "Epoch: 843/2000... Training loss: 0.5123\n",
      "Epoch: 844/2000... Training loss: 0.6761\n",
      "Epoch: 844/2000... Training loss: 0.3905\n",
      "Epoch: 844/2000... Training loss: 0.7534\n",
      "Epoch: 844/2000... Training loss: 0.5209\n",
      "Epoch: 844/2000... Training loss: 0.6393\n",
      "Epoch: 844/2000... Training loss: 0.8868\n",
      "Epoch: 844/2000... Training loss: 0.5955\n",
      "Epoch: 844/2000... Training loss: 0.6841\n",
      "Epoch: 844/2000... Training loss: 0.7517\n",
      "Epoch: 844/2000... Training loss: 0.7529\n",
      "Epoch: 844/2000... Training loss: 0.5793\n",
      "Epoch: 844/2000... Training loss: 0.5635\n",
      "Epoch: 844/2000... Training loss: 0.4803\n",
      "Epoch: 844/2000... Training loss: 0.6277\n",
      "Epoch: 844/2000... Training loss: 0.6042\n",
      "Epoch: 844/2000... Training loss: 0.4724\n",
      "Epoch: 844/2000... Training loss: 0.5151\n",
      "Epoch: 844/2000... Training loss: 0.8920\n",
      "Epoch: 844/2000... Training loss: 0.6724\n",
      "Epoch: 844/2000... Training loss: 0.6756\n",
      "Epoch: 844/2000... Training loss: 0.4654\n",
      "Epoch: 844/2000... Training loss: 0.7373\n",
      "Epoch: 844/2000... Training loss: 0.5751\n",
      "Epoch: 844/2000... Training loss: 0.5808\n",
      "Epoch: 844/2000... Training loss: 0.6914\n",
      "Epoch: 844/2000... Training loss: 0.6435\n",
      "Epoch: 844/2000... Training loss: 0.7660\n",
      "Epoch: 844/2000... Training loss: 0.6671\n",
      "Epoch: 844/2000... Training loss: 0.6018\n",
      "Epoch: 844/2000... Training loss: 0.5784\n",
      "Epoch: 844/2000... Training loss: 0.6096\n",
      "Epoch: 845/2000... Training loss: 0.5758\n",
      "Epoch: 845/2000... Training loss: 0.6855\n",
      "Epoch: 845/2000... Training loss: 0.5449\n",
      "Epoch: 845/2000... Training loss: 0.5371\n",
      "Epoch: 845/2000... Training loss: 0.5455\n",
      "Epoch: 845/2000... Training loss: 0.6623\n",
      "Epoch: 845/2000... Training loss: 0.7039\n",
      "Epoch: 845/2000... Training loss: 0.6149\n",
      "Epoch: 845/2000... Training loss: 0.3944\n",
      "Epoch: 845/2000... Training loss: 0.6185\n",
      "Epoch: 845/2000... Training loss: 0.4794\n",
      "Epoch: 845/2000... Training loss: 0.7261\n",
      "Epoch: 845/2000... Training loss: 0.6569\n",
      "Epoch: 845/2000... Training loss: 0.6090\n",
      "Epoch: 845/2000... Training loss: 0.6025\n",
      "Epoch: 845/2000... Training loss: 0.5631\n",
      "Epoch: 845/2000... Training loss: 0.3614\n",
      "Epoch: 845/2000... Training loss: 0.4823\n",
      "Epoch: 845/2000... Training loss: 0.5288\n",
      "Epoch: 845/2000... Training loss: 0.8077\n",
      "Epoch: 845/2000... Training loss: 0.5854\n",
      "Epoch: 845/2000... Training loss: 0.5451\n",
      "Epoch: 845/2000... Training loss: 0.7736\n",
      "Epoch: 845/2000... Training loss: 0.5866\n",
      "Epoch: 845/2000... Training loss: 0.6235\n",
      "Epoch: 845/2000... Training loss: 0.8476\n",
      "Epoch: 845/2000... Training loss: 0.6429\n",
      "Epoch: 845/2000... Training loss: 0.4626\n",
      "Epoch: 845/2000... Training loss: 0.5176\n",
      "Epoch: 845/2000... Training loss: 0.7386\n",
      "Epoch: 845/2000... Training loss: 0.5382\n",
      "Epoch: 846/2000... Training loss: 0.8159\n",
      "Epoch: 846/2000... Training loss: 0.6438\n",
      "Epoch: 846/2000... Training loss: 0.7022\n",
      "Epoch: 846/2000... Training loss: 0.5038\n",
      "Epoch: 846/2000... Training loss: 0.6375\n",
      "Epoch: 846/2000... Training loss: 0.7862\n",
      "Epoch: 846/2000... Training loss: 0.6001\n",
      "Epoch: 846/2000... Training loss: 0.5380\n",
      "Epoch: 846/2000... Training loss: 0.7521\n",
      "Epoch: 846/2000... Training loss: 0.6080\n",
      "Epoch: 846/2000... Training loss: 0.6371\n",
      "Epoch: 846/2000... Training loss: 0.7046\n",
      "Epoch: 846/2000... Training loss: 0.5987\n",
      "Epoch: 846/2000... Training loss: 0.8024\n",
      "Epoch: 846/2000... Training loss: 0.5148\n",
      "Epoch: 846/2000... Training loss: 0.7621\n",
      "Epoch: 846/2000... Training loss: 0.4444\n",
      "Epoch: 846/2000... Training loss: 0.5289\n",
      "Epoch: 846/2000... Training loss: 0.5693\n",
      "Epoch: 846/2000... Training loss: 0.4995\n",
      "Epoch: 846/2000... Training loss: 0.4685\n",
      "Epoch: 846/2000... Training loss: 0.9496\n",
      "Epoch: 846/2000... Training loss: 0.6371\n",
      "Epoch: 846/2000... Training loss: 0.5005\n",
      "Epoch: 846/2000... Training loss: 0.8726\n",
      "Epoch: 846/2000... Training loss: 0.6612\n",
      "Epoch: 846/2000... Training loss: 0.6568\n",
      "Epoch: 846/2000... Training loss: 0.5479\n",
      "Epoch: 846/2000... Training loss: 0.8017\n",
      "Epoch: 846/2000... Training loss: 0.6175\n",
      "Epoch: 846/2000... Training loss: 0.8279\n",
      "Epoch: 847/2000... Training loss: 0.5067\n",
      "Epoch: 847/2000... Training loss: 0.5175\n",
      "Epoch: 847/2000... Training loss: 0.5396\n",
      "Epoch: 847/2000... Training loss: 0.7286\n",
      "Epoch: 847/2000... Training loss: 0.5327\n",
      "Epoch: 847/2000... Training loss: 0.6383\n",
      "Epoch: 847/2000... Training loss: 0.5402\n",
      "Epoch: 847/2000... Training loss: 0.7268\n",
      "Epoch: 847/2000... Training loss: 0.7583\n",
      "Epoch: 847/2000... Training loss: 0.6268\n",
      "Epoch: 847/2000... Training loss: 0.8561\n",
      "Epoch: 847/2000... Training loss: 0.8342\n",
      "Epoch: 847/2000... Training loss: 0.6288\n",
      "Epoch: 847/2000... Training loss: 0.5154\n",
      "Epoch: 847/2000... Training loss: 0.6521\n",
      "Epoch: 847/2000... Training loss: 0.5297\n",
      "Epoch: 847/2000... Training loss: 0.6018\n",
      "Epoch: 847/2000... Training loss: 0.8292\n",
      "Epoch: 847/2000... Training loss: 0.7165\n",
      "Epoch: 847/2000... Training loss: 0.5846\n",
      "Epoch: 847/2000... Training loss: 0.5572\n",
      "Epoch: 847/2000... Training loss: 0.6276\n",
      "Epoch: 847/2000... Training loss: 0.7200\n",
      "Epoch: 847/2000... Training loss: 0.5872\n",
      "Epoch: 847/2000... Training loss: 0.6643\n",
      "Epoch: 847/2000... Training loss: 0.6667\n",
      "Epoch: 847/2000... Training loss: 0.4758\n",
      "Epoch: 847/2000... Training loss: 0.5647\n",
      "Epoch: 847/2000... Training loss: 0.4396\n",
      "Epoch: 847/2000... Training loss: 0.5184\n",
      "Epoch: 847/2000... Training loss: 0.6800\n",
      "Epoch: 848/2000... Training loss: 0.8081\n",
      "Epoch: 848/2000... Training loss: 0.5034\n",
      "Epoch: 848/2000... Training loss: 0.4664\n",
      "Epoch: 848/2000... Training loss: 0.5471\n",
      "Epoch: 848/2000... Training loss: 0.6909\n",
      "Epoch: 848/2000... Training loss: 0.6828\n",
      "Epoch: 848/2000... Training loss: 0.6123\n",
      "Epoch: 848/2000... Training loss: 0.7531\n",
      "Epoch: 848/2000... Training loss: 0.6214\n",
      "Epoch: 848/2000... Training loss: 0.7385\n",
      "Epoch: 848/2000... Training loss: 0.6555\n",
      "Epoch: 848/2000... Training loss: 0.8865\n",
      "Epoch: 848/2000... Training loss: 0.4169\n",
      "Epoch: 848/2000... Training loss: 0.5483\n",
      "Epoch: 848/2000... Training loss: 0.5727\n",
      "Epoch: 848/2000... Training loss: 0.6370\n",
      "Epoch: 848/2000... Training loss: 0.8556\n",
      "Epoch: 848/2000... Training loss: 0.6220\n",
      "Epoch: 848/2000... Training loss: 0.5723\n",
      "Epoch: 848/2000... Training loss: 0.6073\n",
      "Epoch: 848/2000... Training loss: 0.6164\n",
      "Epoch: 848/2000... Training loss: 0.5802\n",
      "Epoch: 848/2000... Training loss: 0.7480\n",
      "Epoch: 848/2000... Training loss: 0.5109\n",
      "Epoch: 848/2000... Training loss: 0.5745\n",
      "Epoch: 848/2000... Training loss: 0.4814\n",
      "Epoch: 848/2000... Training loss: 0.5520\n",
      "Epoch: 848/2000... Training loss: 0.8103\n",
      "Epoch: 848/2000... Training loss: 0.6830\n",
      "Epoch: 848/2000... Training loss: 0.6707\n",
      "Epoch: 848/2000... Training loss: 0.7601\n",
      "Epoch: 849/2000... Training loss: 0.6668\n",
      "Epoch: 849/2000... Training loss: 0.7437\n",
      "Epoch: 849/2000... Training loss: 0.6438\n",
      "Epoch: 849/2000... Training loss: 0.3929\n",
      "Epoch: 849/2000... Training loss: 0.5622\n",
      "Epoch: 849/2000... Training loss: 0.5799\n",
      "Epoch: 849/2000... Training loss: 0.6377\n",
      "Epoch: 849/2000... Training loss: 0.5003\n",
      "Epoch: 849/2000... Training loss: 0.5358\n",
      "Epoch: 849/2000... Training loss: 0.6199\n",
      "Epoch: 849/2000... Training loss: 0.5907\n",
      "Epoch: 849/2000... Training loss: 0.6258\n",
      "Epoch: 849/2000... Training loss: 0.5485\n",
      "Epoch: 849/2000... Training loss: 0.6878\n",
      "Epoch: 849/2000... Training loss: 0.7415\n",
      "Epoch: 849/2000... Training loss: 0.4604\n",
      "Epoch: 849/2000... Training loss: 0.4374\n",
      "Epoch: 849/2000... Training loss: 0.6818\n",
      "Epoch: 849/2000... Training loss: 0.6024\n",
      "Epoch: 849/2000... Training loss: 0.6811\n",
      "Epoch: 849/2000... Training loss: 0.5727\n",
      "Epoch: 849/2000... Training loss: 0.5398\n",
      "Epoch: 849/2000... Training loss: 0.4633\n",
      "Epoch: 849/2000... Training loss: 0.6482\n",
      "Epoch: 849/2000... Training loss: 0.6072\n",
      "Epoch: 849/2000... Training loss: 0.5262\n",
      "Epoch: 849/2000... Training loss: 0.6904\n",
      "Epoch: 849/2000... Training loss: 0.6593\n",
      "Epoch: 849/2000... Training loss: 0.6680\n",
      "Epoch: 849/2000... Training loss: 0.6757\n",
      "Epoch: 849/2000... Training loss: 0.6124\n",
      "Epoch: 850/2000... Training loss: 0.4715\n",
      "Epoch: 850/2000... Training loss: 0.5995\n",
      "Epoch: 850/2000... Training loss: 0.5528\n",
      "Epoch: 850/2000... Training loss: 0.6132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 850/2000... Training loss: 0.6348\n",
      "Epoch: 850/2000... Training loss: 0.4676\n",
      "Epoch: 850/2000... Training loss: 0.5723\n",
      "Epoch: 850/2000... Training loss: 0.6211\n",
      "Epoch: 850/2000... Training loss: 0.5137\n",
      "Epoch: 850/2000... Training loss: 0.6950\n",
      "Epoch: 850/2000... Training loss: 0.6338\n",
      "Epoch: 850/2000... Training loss: 0.5315\n",
      "Epoch: 850/2000... Training loss: 0.5588\n",
      "Epoch: 850/2000... Training loss: 0.6753\n",
      "Epoch: 850/2000... Training loss: 0.5663\n",
      "Epoch: 850/2000... Training loss: 0.6641\n",
      "Epoch: 850/2000... Training loss: 0.5431\n",
      "Epoch: 850/2000... Training loss: 0.6211\n",
      "Epoch: 850/2000... Training loss: 0.6967\n",
      "Epoch: 850/2000... Training loss: 0.4490\n",
      "Epoch: 850/2000... Training loss: 0.7178\n",
      "Epoch: 850/2000... Training loss: 0.6694\n",
      "Epoch: 850/2000... Training loss: 0.5132\n",
      "Epoch: 850/2000... Training loss: 0.6010\n",
      "Epoch: 850/2000... Training loss: 0.5483\n",
      "Epoch: 850/2000... Training loss: 0.6790\n",
      "Epoch: 850/2000... Training loss: 0.7254\n",
      "Epoch: 850/2000... Training loss: 0.5964\n",
      "Epoch: 850/2000... Training loss: 0.7013\n",
      "Epoch: 850/2000... Training loss: 0.6556\n",
      "Epoch: 850/2000... Training loss: 0.5009\n",
      "Epoch: 851/2000... Training loss: 0.6977\n",
      "Epoch: 851/2000... Training loss: 0.6874\n",
      "Epoch: 851/2000... Training loss: 0.6901\n",
      "Epoch: 851/2000... Training loss: 0.5405\n",
      "Epoch: 851/2000... Training loss: 0.3017\n",
      "Epoch: 851/2000... Training loss: 0.8885\n",
      "Epoch: 851/2000... Training loss: 0.6618\n",
      "Epoch: 851/2000... Training loss: 0.5900\n",
      "Epoch: 851/2000... Training loss: 0.4000\n",
      "Epoch: 851/2000... Training loss: 0.3281\n",
      "Epoch: 851/2000... Training loss: 0.7000\n",
      "Epoch: 851/2000... Training loss: 0.5906\n",
      "Epoch: 851/2000... Training loss: 0.6916\n",
      "Epoch: 851/2000... Training loss: 0.5877\n",
      "Epoch: 851/2000... Training loss: 0.6180\n",
      "Epoch: 851/2000... Training loss: 0.6993\n",
      "Epoch: 851/2000... Training loss: 0.7755\n",
      "Epoch: 851/2000... Training loss: 0.6458\n",
      "Epoch: 851/2000... Training loss: 0.6598\n",
      "Epoch: 851/2000... Training loss: 0.6524\n",
      "Epoch: 851/2000... Training loss: 0.6492\n",
      "Epoch: 851/2000... Training loss: 0.6289\n",
      "Epoch: 851/2000... Training loss: 0.5867\n",
      "Epoch: 851/2000... Training loss: 0.6327\n",
      "Epoch: 851/2000... Training loss: 0.6036\n",
      "Epoch: 851/2000... Training loss: 0.6012\n",
      "Epoch: 851/2000... Training loss: 0.5044\n",
      "Epoch: 851/2000... Training loss: 0.5022\n",
      "Epoch: 851/2000... Training loss: 0.5520\n",
      "Epoch: 851/2000... Training loss: 0.6722\n",
      "Epoch: 851/2000... Training loss: 0.4997\n",
      "Epoch: 852/2000... Training loss: 0.5923\n",
      "Epoch: 852/2000... Training loss: 0.5450\n",
      "Epoch: 852/2000... Training loss: 0.5181\n",
      "Epoch: 852/2000... Training loss: 0.6031\n",
      "Epoch: 852/2000... Training loss: 0.7398\n",
      "Epoch: 852/2000... Training loss: 0.5784\n",
      "Epoch: 852/2000... Training loss: 0.5401\n",
      "Epoch: 852/2000... Training loss: 0.4176\n",
      "Epoch: 852/2000... Training loss: 0.5803\n",
      "Epoch: 852/2000... Training loss: 0.7498\n",
      "Epoch: 852/2000... Training loss: 0.5829\n",
      "Epoch: 852/2000... Training loss: 0.5885\n",
      "Epoch: 852/2000... Training loss: 0.6415\n",
      "Epoch: 852/2000... Training loss: 0.6277\n",
      "Epoch: 852/2000... Training loss: 0.4944\n",
      "Epoch: 852/2000... Training loss: 0.6472\n",
      "Epoch: 852/2000... Training loss: 0.5612\n",
      "Epoch: 852/2000... Training loss: 0.6728\n",
      "Epoch: 852/2000... Training loss: 0.6080\n",
      "Epoch: 852/2000... Training loss: 0.7346\n",
      "Epoch: 852/2000... Training loss: 0.4516\n",
      "Epoch: 852/2000... Training loss: 0.5912\n",
      "Epoch: 852/2000... Training loss: 0.7707\n",
      "Epoch: 852/2000... Training loss: 0.6030\n",
      "Epoch: 852/2000... Training loss: 0.5935\n",
      "Epoch: 852/2000... Training loss: 0.5982\n",
      "Epoch: 852/2000... Training loss: 0.7146\n",
      "Epoch: 852/2000... Training loss: 0.8536\n",
      "Epoch: 852/2000... Training loss: 0.5742\n",
      "Epoch: 852/2000... Training loss: 0.4659\n",
      "Epoch: 852/2000... Training loss: 0.5750\n",
      "Epoch: 853/2000... Training loss: 0.7549\n",
      "Epoch: 853/2000... Training loss: 0.7373\n",
      "Epoch: 853/2000... Training loss: 0.5969\n",
      "Epoch: 853/2000... Training loss: 0.6048\n",
      "Epoch: 853/2000... Training loss: 0.6063\n",
      "Epoch: 853/2000... Training loss: 0.5603\n",
      "Epoch: 853/2000... Training loss: 0.4494\n",
      "Epoch: 853/2000... Training loss: 0.7697\n",
      "Epoch: 853/2000... Training loss: 0.6782\n",
      "Epoch: 853/2000... Training loss: 0.5202\n",
      "Epoch: 853/2000... Training loss: 0.7382\n",
      "Epoch: 853/2000... Training loss: 0.5120\n",
      "Epoch: 853/2000... Training loss: 0.7150\n",
      "Epoch: 853/2000... Training loss: 0.6616\n",
      "Epoch: 853/2000... Training loss: 0.7957\n",
      "Epoch: 853/2000... Training loss: 0.5409\n",
      "Epoch: 853/2000... Training loss: 0.5428\n",
      "Epoch: 853/2000... Training loss: 0.5438\n",
      "Epoch: 853/2000... Training loss: 0.5178\n",
      "Epoch: 853/2000... Training loss: 0.4246\n",
      "Epoch: 853/2000... Training loss: 0.5894\n",
      "Epoch: 853/2000... Training loss: 0.5399\n",
      "Epoch: 853/2000... Training loss: 0.6167\n",
      "Epoch: 853/2000... Training loss: 0.6030\n",
      "Epoch: 853/2000... Training loss: 0.6288\n",
      "Epoch: 853/2000... Training loss: 0.5590\n",
      "Epoch: 853/2000... Training loss: 0.4836\n",
      "Epoch: 853/2000... Training loss: 0.5391\n",
      "Epoch: 853/2000... Training loss: 0.6294\n",
      "Epoch: 853/2000... Training loss: 0.7449\n",
      "Epoch: 853/2000... Training loss: 0.4846\n",
      "Epoch: 854/2000... Training loss: 0.5525\n",
      "Epoch: 854/2000... Training loss: 0.6273\n",
      "Epoch: 854/2000... Training loss: 0.7167\n",
      "Epoch: 854/2000... Training loss: 0.4660\n",
      "Epoch: 854/2000... Training loss: 0.7113\n",
      "Epoch: 854/2000... Training loss: 0.4660\n",
      "Epoch: 854/2000... Training loss: 0.5610\n",
      "Epoch: 854/2000... Training loss: 0.4853\n",
      "Epoch: 854/2000... Training loss: 0.4665\n",
      "Epoch: 854/2000... Training loss: 0.5470\n",
      "Epoch: 854/2000... Training loss: 0.6206\n",
      "Epoch: 854/2000... Training loss: 0.7258\n",
      "Epoch: 854/2000... Training loss: 0.6958\n",
      "Epoch: 854/2000... Training loss: 0.5768\n",
      "Epoch: 854/2000... Training loss: 0.6798\n",
      "Epoch: 854/2000... Training loss: 0.6456\n",
      "Epoch: 854/2000... Training loss: 0.4791\n",
      "Epoch: 854/2000... Training loss: 0.6165\n",
      "Epoch: 854/2000... Training loss: 0.5182\n",
      "Epoch: 854/2000... Training loss: 0.5036\n",
      "Epoch: 854/2000... Training loss: 0.4932\n",
      "Epoch: 854/2000... Training loss: 0.5763\n",
      "Epoch: 854/2000... Training loss: 0.5357\n",
      "Epoch: 854/2000... Training loss: 0.5222\n",
      "Epoch: 854/2000... Training loss: 0.7157\n",
      "Epoch: 854/2000... Training loss: 0.5846\n",
      "Epoch: 854/2000... Training loss: 0.6419\n",
      "Epoch: 854/2000... Training loss: 0.5782\n",
      "Epoch: 854/2000... Training loss: 0.5408\n",
      "Epoch: 854/2000... Training loss: 0.6569\n",
      "Epoch: 854/2000... Training loss: 0.4743\n",
      "Epoch: 855/2000... Training loss: 0.5363\n",
      "Epoch: 855/2000... Training loss: 0.7846\n",
      "Epoch: 855/2000... Training loss: 0.6062\n",
      "Epoch: 855/2000... Training loss: 0.5761\n",
      "Epoch: 855/2000... Training loss: 0.6619\n",
      "Epoch: 855/2000... Training loss: 0.5750\n",
      "Epoch: 855/2000... Training loss: 0.6324\n",
      "Epoch: 855/2000... Training loss: 0.7437\n",
      "Epoch: 855/2000... Training loss: 0.5411\n",
      "Epoch: 855/2000... Training loss: 0.5415\n",
      "Epoch: 855/2000... Training loss: 0.6596\n",
      "Epoch: 855/2000... Training loss: 0.6201\n",
      "Epoch: 855/2000... Training loss: 0.5657\n",
      "Epoch: 855/2000... Training loss: 0.7298\n",
      "Epoch: 855/2000... Training loss: 0.8075\n",
      "Epoch: 855/2000... Training loss: 0.4663\n",
      "Epoch: 855/2000... Training loss: 0.5316\n",
      "Epoch: 855/2000... Training loss: 0.5268\n",
      "Epoch: 855/2000... Training loss: 0.5004\n",
      "Epoch: 855/2000... Training loss: 0.8270\n",
      "Epoch: 855/2000... Training loss: 0.6099\n",
      "Epoch: 855/2000... Training loss: 0.5864\n",
      "Epoch: 855/2000... Training loss: 0.5683\n",
      "Epoch: 855/2000... Training loss: 0.7927\n",
      "Epoch: 855/2000... Training loss: 0.6410\n",
      "Epoch: 855/2000... Training loss: 0.7396\n",
      "Epoch: 855/2000... Training loss: 0.7211\n",
      "Epoch: 855/2000... Training loss: 0.5270\n",
      "Epoch: 855/2000... Training loss: 0.4999\n",
      "Epoch: 855/2000... Training loss: 0.5069\n",
      "Epoch: 855/2000... Training loss: 0.5905\n",
      "Epoch: 856/2000... Training loss: 0.5497\n",
      "Epoch: 856/2000... Training loss: 0.7128\n",
      "Epoch: 856/2000... Training loss: 0.5584\n",
      "Epoch: 856/2000... Training loss: 0.4436\n",
      "Epoch: 856/2000... Training loss: 0.5557\n",
      "Epoch: 856/2000... Training loss: 0.5363\n",
      "Epoch: 856/2000... Training loss: 0.6214\n",
      "Epoch: 856/2000... Training loss: 0.6877\n",
      "Epoch: 856/2000... Training loss: 0.5491\n",
      "Epoch: 856/2000... Training loss: 0.6486\n",
      "Epoch: 856/2000... Training loss: 0.5153\n",
      "Epoch: 856/2000... Training loss: 0.5334\n",
      "Epoch: 856/2000... Training loss: 0.3768\n",
      "Epoch: 856/2000... Training loss: 0.5075\n",
      "Epoch: 856/2000... Training loss: 0.7489\n",
      "Epoch: 856/2000... Training loss: 0.6198\n",
      "Epoch: 856/2000... Training loss: 0.4591\n",
      "Epoch: 856/2000... Training loss: 0.7060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 856/2000... Training loss: 0.6086\n",
      "Epoch: 856/2000... Training loss: 0.6226\n",
      "Epoch: 856/2000... Training loss: 0.3536\n",
      "Epoch: 856/2000... Training loss: 0.5364\n",
      "Epoch: 856/2000... Training loss: 0.6141\n",
      "Epoch: 856/2000... Training loss: 0.7252\n",
      "Epoch: 856/2000... Training loss: 0.4466\n",
      "Epoch: 856/2000... Training loss: 0.5654\n",
      "Epoch: 856/2000... Training loss: 0.4102\n",
      "Epoch: 856/2000... Training loss: 0.6012\n",
      "Epoch: 856/2000... Training loss: 0.4482\n",
      "Epoch: 856/2000... Training loss: 0.3812\n",
      "Epoch: 856/2000... Training loss: 0.6909\n",
      "Epoch: 857/2000... Training loss: 0.7654\n",
      "Epoch: 857/2000... Training loss: 0.4538\n",
      "Epoch: 857/2000... Training loss: 0.6793\n",
      "Epoch: 857/2000... Training loss: 0.3788\n",
      "Epoch: 857/2000... Training loss: 0.4818\n",
      "Epoch: 857/2000... Training loss: 0.4945\n",
      "Epoch: 857/2000... Training loss: 0.7489\n",
      "Epoch: 857/2000... Training loss: 0.6863\n",
      "Epoch: 857/2000... Training loss: 0.6101\n",
      "Epoch: 857/2000... Training loss: 0.4218\n",
      "Epoch: 857/2000... Training loss: 0.5615\n",
      "Epoch: 857/2000... Training loss: 0.6590\n",
      "Epoch: 857/2000... Training loss: 0.4656\n",
      "Epoch: 857/2000... Training loss: 0.6204\n",
      "Epoch: 857/2000... Training loss: 0.4062\n",
      "Epoch: 857/2000... Training loss: 0.5857\n",
      "Epoch: 857/2000... Training loss: 0.5545\n",
      "Epoch: 857/2000... Training loss: 0.5473\n",
      "Epoch: 857/2000... Training loss: 0.6112\n",
      "Epoch: 857/2000... Training loss: 0.5284\n",
      "Epoch: 857/2000... Training loss: 0.5853\n",
      "Epoch: 857/2000... Training loss: 0.4666\n",
      "Epoch: 857/2000... Training loss: 0.5968\n",
      "Epoch: 857/2000... Training loss: 0.6510\n",
      "Epoch: 857/2000... Training loss: 0.4775\n",
      "Epoch: 857/2000... Training loss: 0.5756\n",
      "Epoch: 857/2000... Training loss: 0.6768\n",
      "Epoch: 857/2000... Training loss: 0.6935\n",
      "Epoch: 857/2000... Training loss: 0.5047\n",
      "Epoch: 857/2000... Training loss: 0.5562\n",
      "Epoch: 857/2000... Training loss: 0.5820\n",
      "Epoch: 858/2000... Training loss: 0.5557\n",
      "Epoch: 858/2000... Training loss: 0.3962\n",
      "Epoch: 858/2000... Training loss: 0.5779\n",
      "Epoch: 858/2000... Training loss: 0.5490\n",
      "Epoch: 858/2000... Training loss: 0.4588\n",
      "Epoch: 858/2000... Training loss: 0.5497\n",
      "Epoch: 858/2000... Training loss: 0.4433\n",
      "Epoch: 858/2000... Training loss: 0.3948\n",
      "Epoch: 858/2000... Training loss: 0.6561\n",
      "Epoch: 858/2000... Training loss: 0.4514\n",
      "Epoch: 858/2000... Training loss: 0.3498\n",
      "Epoch: 858/2000... Training loss: 0.5209\n",
      "Epoch: 858/2000... Training loss: 0.5716\n",
      "Epoch: 858/2000... Training loss: 0.5569\n",
      "Epoch: 858/2000... Training loss: 0.4165\n",
      "Epoch: 858/2000... Training loss: 0.6028\n",
      "Epoch: 858/2000... Training loss: 0.4796\n",
      "Epoch: 858/2000... Training loss: 0.6169\n",
      "Epoch: 858/2000... Training loss: 0.8286\n",
      "Epoch: 858/2000... Training loss: 0.7241\n",
      "Epoch: 858/2000... Training loss: 0.5281\n",
      "Epoch: 858/2000... Training loss: 0.6036\n",
      "Epoch: 858/2000... Training loss: 0.4846\n",
      "Epoch: 858/2000... Training loss: 0.6468\n",
      "Epoch: 858/2000... Training loss: 0.6056\n",
      "Epoch: 858/2000... Training loss: 0.5706\n",
      "Epoch: 858/2000... Training loss: 0.7472\n",
      "Epoch: 858/2000... Training loss: 0.6053\n",
      "Epoch: 858/2000... Training loss: 0.5003\n",
      "Epoch: 858/2000... Training loss: 0.6209\n",
      "Epoch: 858/2000... Training loss: 0.6227\n",
      "Epoch: 859/2000... Training loss: 0.5212\n",
      "Epoch: 859/2000... Training loss: 0.7093\n",
      "Epoch: 859/2000... Training loss: 0.6186\n",
      "Epoch: 859/2000... Training loss: 0.7182\n",
      "Epoch: 859/2000... Training loss: 0.6669\n",
      "Epoch: 859/2000... Training loss: 0.4643\n",
      "Epoch: 859/2000... Training loss: 0.5077\n",
      "Epoch: 859/2000... Training loss: 0.4178\n",
      "Epoch: 859/2000... Training loss: 0.4486\n",
      "Epoch: 859/2000... Training loss: 0.6430\n",
      "Epoch: 859/2000... Training loss: 0.5088\n",
      "Epoch: 859/2000... Training loss: 0.4510\n",
      "Epoch: 859/2000... Training loss: 0.5846\n",
      "Epoch: 859/2000... Training loss: 0.7277\n",
      "Epoch: 859/2000... Training loss: 0.7577\n",
      "Epoch: 859/2000... Training loss: 0.8440\n",
      "Epoch: 859/2000... Training loss: 0.5655\n",
      "Epoch: 859/2000... Training loss: 0.5137\n",
      "Epoch: 859/2000... Training loss: 0.5366\n",
      "Epoch: 859/2000... Training loss: 0.6569\n",
      "Epoch: 859/2000... Training loss: 0.4520\n",
      "Epoch: 859/2000... Training loss: 0.6707\n",
      "Epoch: 859/2000... Training loss: 0.5718\n",
      "Epoch: 859/2000... Training loss: 0.4944\n",
      "Epoch: 859/2000... Training loss: 0.6575\n",
      "Epoch: 859/2000... Training loss: 0.7884\n",
      "Epoch: 859/2000... Training loss: 0.6035\n",
      "Epoch: 859/2000... Training loss: 0.8219\n",
      "Epoch: 859/2000... Training loss: 0.5063\n",
      "Epoch: 859/2000... Training loss: 0.5595\n",
      "Epoch: 859/2000... Training loss: 0.7012\n",
      "Epoch: 860/2000... Training loss: 0.6452\n",
      "Epoch: 860/2000... Training loss: 0.5084\n",
      "Epoch: 860/2000... Training loss: 0.6419\n",
      "Epoch: 860/2000... Training loss: 0.6465\n",
      "Epoch: 860/2000... Training loss: 0.7154\n",
      "Epoch: 860/2000... Training loss: 0.4337\n",
      "Epoch: 860/2000... Training loss: 0.5429\n",
      "Epoch: 860/2000... Training loss: 0.6449\n",
      "Epoch: 860/2000... Training loss: 0.5755\n",
      "Epoch: 860/2000... Training loss: 0.5890\n",
      "Epoch: 860/2000... Training loss: 0.7135\n",
      "Epoch: 860/2000... Training loss: 0.6282\n",
      "Epoch: 860/2000... Training loss: 0.3778\n",
      "Epoch: 860/2000... Training loss: 0.6471\n",
      "Epoch: 860/2000... Training loss: 0.8071\n",
      "Epoch: 860/2000... Training loss: 0.5176\n",
      "Epoch: 860/2000... Training loss: 0.6728\n",
      "Epoch: 860/2000... Training loss: 0.4926\n",
      "Epoch: 860/2000... Training loss: 0.7003\n",
      "Epoch: 860/2000... Training loss: 0.5849\n",
      "Epoch: 860/2000... Training loss: 0.4570\n",
      "Epoch: 860/2000... Training loss: 0.5643\n",
      "Epoch: 860/2000... Training loss: 0.6536\n",
      "Epoch: 860/2000... Training loss: 0.5405\n",
      "Epoch: 860/2000... Training loss: 0.6283\n",
      "Epoch: 860/2000... Training loss: 0.4065\n",
      "Epoch: 860/2000... Training loss: 0.5324\n",
      "Epoch: 860/2000... Training loss: 0.5363\n",
      "Epoch: 860/2000... Training loss: 0.4313\n",
      "Epoch: 860/2000... Training loss: 0.6635\n",
      "Epoch: 860/2000... Training loss: 0.5919\n",
      "Epoch: 861/2000... Training loss: 0.5077\n",
      "Epoch: 861/2000... Training loss: 0.5840\n",
      "Epoch: 861/2000... Training loss: 0.4998\n",
      "Epoch: 861/2000... Training loss: 0.4451\n",
      "Epoch: 861/2000... Training loss: 0.5699\n",
      "Epoch: 861/2000... Training loss: 0.4493\n",
      "Epoch: 861/2000... Training loss: 0.4379\n",
      "Epoch: 861/2000... Training loss: 0.5891\n",
      "Epoch: 861/2000... Training loss: 0.4757\n",
      "Epoch: 861/2000... Training loss: 0.5841\n",
      "Epoch: 861/2000... Training loss: 0.6589\n",
      "Epoch: 861/2000... Training loss: 0.7671\n",
      "Epoch: 861/2000... Training loss: 0.7843\n",
      "Epoch: 861/2000... Training loss: 0.6152\n",
      "Epoch: 861/2000... Training loss: 0.4695\n",
      "Epoch: 861/2000... Training loss: 0.6905\n",
      "Epoch: 861/2000... Training loss: 0.6610\n",
      "Epoch: 861/2000... Training loss: 0.5554\n",
      "Epoch: 861/2000... Training loss: 0.6846\n",
      "Epoch: 861/2000... Training loss: 0.3452\n",
      "Epoch: 861/2000... Training loss: 0.5491\n",
      "Epoch: 861/2000... Training loss: 0.6021\n",
      "Epoch: 861/2000... Training loss: 0.5832\n",
      "Epoch: 861/2000... Training loss: 0.6040\n",
      "Epoch: 861/2000... Training loss: 0.4964\n",
      "Epoch: 861/2000... Training loss: 0.7552\n",
      "Epoch: 861/2000... Training loss: 0.7407\n",
      "Epoch: 861/2000... Training loss: 0.8497\n",
      "Epoch: 861/2000... Training loss: 0.7478\n",
      "Epoch: 861/2000... Training loss: 0.7801\n",
      "Epoch: 861/2000... Training loss: 0.7687\n",
      "Epoch: 862/2000... Training loss: 0.7781\n",
      "Epoch: 862/2000... Training loss: 0.5528\n",
      "Epoch: 862/2000... Training loss: 0.3847\n",
      "Epoch: 862/2000... Training loss: 0.5272\n",
      "Epoch: 862/2000... Training loss: 0.6166\n",
      "Epoch: 862/2000... Training loss: 0.7523\n",
      "Epoch: 862/2000... Training loss: 0.7205\n",
      "Epoch: 862/2000... Training loss: 0.6197\n",
      "Epoch: 862/2000... Training loss: 0.4846\n",
      "Epoch: 862/2000... Training loss: 0.6009\n",
      "Epoch: 862/2000... Training loss: 0.5180\n",
      "Epoch: 862/2000... Training loss: 0.7023\n",
      "Epoch: 862/2000... Training loss: 0.7287\n",
      "Epoch: 862/2000... Training loss: 0.5582\n",
      "Epoch: 862/2000... Training loss: 0.6479\n",
      "Epoch: 862/2000... Training loss: 0.4430\n",
      "Epoch: 862/2000... Training loss: 0.4491\n",
      "Epoch: 862/2000... Training loss: 0.6146\n",
      "Epoch: 862/2000... Training loss: 0.5793\n",
      "Epoch: 862/2000... Training loss: 0.7339\n",
      "Epoch: 862/2000... Training loss: 0.5504\n",
      "Epoch: 862/2000... Training loss: 0.6869\n",
      "Epoch: 862/2000... Training loss: 0.6736\n",
      "Epoch: 862/2000... Training loss: 0.7301\n",
      "Epoch: 862/2000... Training loss: 0.6274\n",
      "Epoch: 862/2000... Training loss: 0.4145\n",
      "Epoch: 862/2000... Training loss: 0.6003\n",
      "Epoch: 862/2000... Training loss: 0.6387\n",
      "Epoch: 862/2000... Training loss: 0.6185\n",
      "Epoch: 862/2000... Training loss: 0.6343\n",
      "Epoch: 862/2000... Training loss: 0.5149\n",
      "Epoch: 863/2000... Training loss: 0.5180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 863/2000... Training loss: 0.5819\n",
      "Epoch: 863/2000... Training loss: 0.5903\n",
      "Epoch: 863/2000... Training loss: 0.3384\n",
      "Epoch: 863/2000... Training loss: 0.7213\n",
      "Epoch: 863/2000... Training loss: 0.6388\n",
      "Epoch: 863/2000... Training loss: 0.6774\n",
      "Epoch: 863/2000... Training loss: 0.3987\n",
      "Epoch: 863/2000... Training loss: 0.4720\n",
      "Epoch: 863/2000... Training loss: 0.6619\n",
      "Epoch: 863/2000... Training loss: 0.5490\n",
      "Epoch: 863/2000... Training loss: 0.6576\n",
      "Epoch: 863/2000... Training loss: 0.4275\n",
      "Epoch: 863/2000... Training loss: 0.6126\n",
      "Epoch: 863/2000... Training loss: 0.4980\n",
      "Epoch: 863/2000... Training loss: 0.5420\n",
      "Epoch: 863/2000... Training loss: 0.5885\n",
      "Epoch: 863/2000... Training loss: 0.7604\n",
      "Epoch: 863/2000... Training loss: 0.8139\n",
      "Epoch: 863/2000... Training loss: 0.6731\n",
      "Epoch: 863/2000... Training loss: 0.6002\n",
      "Epoch: 863/2000... Training loss: 0.4665\n",
      "Epoch: 863/2000... Training loss: 0.5786\n",
      "Epoch: 863/2000... Training loss: 0.4596\n",
      "Epoch: 863/2000... Training loss: 0.5214\n",
      "Epoch: 863/2000... Training loss: 0.6800\n",
      "Epoch: 863/2000... Training loss: 0.4205\n",
      "Epoch: 863/2000... Training loss: 0.4265\n",
      "Epoch: 863/2000... Training loss: 0.5851\n",
      "Epoch: 863/2000... Training loss: 0.7077\n",
      "Epoch: 863/2000... Training loss: 0.5162\n",
      "Epoch: 864/2000... Training loss: 0.6362\n",
      "Epoch: 864/2000... Training loss: 0.8149\n",
      "Epoch: 864/2000... Training loss: 0.8783\n",
      "Epoch: 864/2000... Training loss: 0.4345\n",
      "Epoch: 864/2000... Training loss: 0.6023\n",
      "Epoch: 864/2000... Training loss: 0.5399\n",
      "Epoch: 864/2000... Training loss: 0.7923\n",
      "Epoch: 864/2000... Training loss: 0.5114\n",
      "Epoch: 864/2000... Training loss: 0.5153\n",
      "Epoch: 864/2000... Training loss: 0.6402\n",
      "Epoch: 864/2000... Training loss: 0.5582\n",
      "Epoch: 864/2000... Training loss: 0.5542\n",
      "Epoch: 864/2000... Training loss: 0.6199\n",
      "Epoch: 864/2000... Training loss: 0.5524\n",
      "Epoch: 864/2000... Training loss: 0.6294\n",
      "Epoch: 864/2000... Training loss: 0.7161\n",
      "Epoch: 864/2000... Training loss: 0.5785\n",
      "Epoch: 864/2000... Training loss: 0.4109\n",
      "Epoch: 864/2000... Training loss: 0.5068\n",
      "Epoch: 864/2000... Training loss: 0.4630\n",
      "Epoch: 864/2000... Training loss: 0.5632\n",
      "Epoch: 864/2000... Training loss: 0.5106\n",
      "Epoch: 864/2000... Training loss: 0.6461\n",
      "Epoch: 864/2000... Training loss: 0.6968\n",
      "Epoch: 864/2000... Training loss: 0.6181\n",
      "Epoch: 864/2000... Training loss: 0.5185\n",
      "Epoch: 864/2000... Training loss: 0.5268\n",
      "Epoch: 864/2000... Training loss: 0.6653\n",
      "Epoch: 864/2000... Training loss: 0.5127\n",
      "Epoch: 864/2000... Training loss: 0.5529\n",
      "Epoch: 864/2000... Training loss: 0.6082\n",
      "Epoch: 865/2000... Training loss: 0.6895\n",
      "Epoch: 865/2000... Training loss: 0.6986\n",
      "Epoch: 865/2000... Training loss: 0.5088\n",
      "Epoch: 865/2000... Training loss: 0.5625\n",
      "Epoch: 865/2000... Training loss: 0.6068\n",
      "Epoch: 865/2000... Training loss: 0.5744\n",
      "Epoch: 865/2000... Training loss: 0.5694\n",
      "Epoch: 865/2000... Training loss: 0.5307\n",
      "Epoch: 865/2000... Training loss: 0.5211\n",
      "Epoch: 865/2000... Training loss: 0.6653\n",
      "Epoch: 865/2000... Training loss: 0.7412\n",
      "Epoch: 865/2000... Training loss: 0.7957\n",
      "Epoch: 865/2000... Training loss: 0.5715\n",
      "Epoch: 865/2000... Training loss: 0.5140\n",
      "Epoch: 865/2000... Training loss: 0.5300\n",
      "Epoch: 865/2000... Training loss: 0.7812\n",
      "Epoch: 865/2000... Training loss: 0.5703\n",
      "Epoch: 865/2000... Training loss: 0.5440\n",
      "Epoch: 865/2000... Training loss: 0.5786\n",
      "Epoch: 865/2000... Training loss: 0.6441\n",
      "Epoch: 865/2000... Training loss: 0.6867\n",
      "Epoch: 865/2000... Training loss: 0.5642\n",
      "Epoch: 865/2000... Training loss: 0.7705\n",
      "Epoch: 865/2000... Training loss: 0.6464\n",
      "Epoch: 865/2000... Training loss: 0.5638\n",
      "Epoch: 865/2000... Training loss: 0.5362\n",
      "Epoch: 865/2000... Training loss: 0.5219\n",
      "Epoch: 865/2000... Training loss: 0.5914\n",
      "Epoch: 865/2000... Training loss: 0.6474\n",
      "Epoch: 865/2000... Training loss: 0.7620\n",
      "Epoch: 865/2000... Training loss: 0.8032\n",
      "Epoch: 866/2000... Training loss: 0.8414\n",
      "Epoch: 866/2000... Training loss: 0.5759\n",
      "Epoch: 866/2000... Training loss: 0.6046\n",
      "Epoch: 866/2000... Training loss: 0.4698\n",
      "Epoch: 866/2000... Training loss: 0.4174\n",
      "Epoch: 866/2000... Training loss: 0.7471\n",
      "Epoch: 866/2000... Training loss: 0.3101\n",
      "Epoch: 866/2000... Training loss: 0.6436\n",
      "Epoch: 866/2000... Training loss: 0.6306\n",
      "Epoch: 866/2000... Training loss: 0.5537\n",
      "Epoch: 866/2000... Training loss: 0.6764\n",
      "Epoch: 866/2000... Training loss: 0.4215\n",
      "Epoch: 866/2000... Training loss: 0.9289\n",
      "Epoch: 866/2000... Training loss: 0.4971\n",
      "Epoch: 866/2000... Training loss: 0.6004\n",
      "Epoch: 866/2000... Training loss: 0.7844\n",
      "Epoch: 866/2000... Training loss: 0.5243\n",
      "Epoch: 866/2000... Training loss: 0.5315\n",
      "Epoch: 866/2000... Training loss: 0.8456\n",
      "Epoch: 866/2000... Training loss: 0.6886\n",
      "Epoch: 866/2000... Training loss: 0.6239\n",
      "Epoch: 866/2000... Training loss: 0.5806\n",
      "Epoch: 866/2000... Training loss: 0.3907\n",
      "Epoch: 866/2000... Training loss: 0.4642\n",
      "Epoch: 866/2000... Training loss: 0.4545\n",
      "Epoch: 866/2000... Training loss: 0.4759\n",
      "Epoch: 866/2000... Training loss: 0.4960\n",
      "Epoch: 866/2000... Training loss: 0.7192\n",
      "Epoch: 866/2000... Training loss: 0.4421\n",
      "Epoch: 866/2000... Training loss: 0.7445\n",
      "Epoch: 866/2000... Training loss: 0.4877\n",
      "Epoch: 867/2000... Training loss: 0.5438\n",
      "Epoch: 867/2000... Training loss: 0.5553\n",
      "Epoch: 867/2000... Training loss: 0.6832\n",
      "Epoch: 867/2000... Training loss: 0.5513\n",
      "Epoch: 867/2000... Training loss: 0.5617\n",
      "Epoch: 867/2000... Training loss: 0.6395\n",
      "Epoch: 867/2000... Training loss: 0.5667\n",
      "Epoch: 867/2000... Training loss: 0.5515\n",
      "Epoch: 867/2000... Training loss: 0.7683\n",
      "Epoch: 867/2000... Training loss: 0.7858\n",
      "Epoch: 867/2000... Training loss: 0.6910\n",
      "Epoch: 867/2000... Training loss: 0.5186\n",
      "Epoch: 867/2000... Training loss: 0.4712\n",
      "Epoch: 867/2000... Training loss: 0.6343\n",
      "Epoch: 867/2000... Training loss: 0.6058\n",
      "Epoch: 867/2000... Training loss: 0.7355\n",
      "Epoch: 867/2000... Training loss: 0.4962\n",
      "Epoch: 867/2000... Training loss: 0.5439\n",
      "Epoch: 867/2000... Training loss: 0.5958\n",
      "Epoch: 867/2000... Training loss: 0.6509\n",
      "Epoch: 867/2000... Training loss: 0.4447\n",
      "Epoch: 867/2000... Training loss: 0.4723\n",
      "Epoch: 867/2000... Training loss: 0.4605\n",
      "Epoch: 867/2000... Training loss: 0.8742\n",
      "Epoch: 867/2000... Training loss: 0.6197\n",
      "Epoch: 867/2000... Training loss: 0.6664\n",
      "Epoch: 867/2000... Training loss: 0.6548\n",
      "Epoch: 867/2000... Training loss: 0.4254\n",
      "Epoch: 867/2000... Training loss: 0.6395\n",
      "Epoch: 867/2000... Training loss: 0.6124\n",
      "Epoch: 867/2000... Training loss: 0.6594\n",
      "Epoch: 868/2000... Training loss: 0.5763\n",
      "Epoch: 868/2000... Training loss: 0.7771\n",
      "Epoch: 868/2000... Training loss: 0.6212\n",
      "Epoch: 868/2000... Training loss: 0.5892\n",
      "Epoch: 868/2000... Training loss: 0.7114\n",
      "Epoch: 868/2000... Training loss: 0.5618\n",
      "Epoch: 868/2000... Training loss: 0.6267\n",
      "Epoch: 868/2000... Training loss: 0.5069\n",
      "Epoch: 868/2000... Training loss: 0.7764\n",
      "Epoch: 868/2000... Training loss: 0.5376\n",
      "Epoch: 868/2000... Training loss: 0.4583\n",
      "Epoch: 868/2000... Training loss: 0.5385\n",
      "Epoch: 868/2000... Training loss: 0.6052\n",
      "Epoch: 868/2000... Training loss: 0.6111\n",
      "Epoch: 868/2000... Training loss: 0.4844\n",
      "Epoch: 868/2000... Training loss: 0.6813\n",
      "Epoch: 868/2000... Training loss: 0.5522\n",
      "Epoch: 868/2000... Training loss: 0.5488\n",
      "Epoch: 868/2000... Training loss: 0.4443\n",
      "Epoch: 868/2000... Training loss: 0.5553\n",
      "Epoch: 868/2000... Training loss: 0.6728\n",
      "Epoch: 868/2000... Training loss: 0.5827\n",
      "Epoch: 868/2000... Training loss: 0.6003\n",
      "Epoch: 868/2000... Training loss: 0.7562\n",
      "Epoch: 868/2000... Training loss: 0.8322\n",
      "Epoch: 868/2000... Training loss: 0.4454\n",
      "Epoch: 868/2000... Training loss: 0.4643\n",
      "Epoch: 868/2000... Training loss: 0.6125\n",
      "Epoch: 868/2000... Training loss: 0.6632\n",
      "Epoch: 868/2000... Training loss: 0.5707\n",
      "Epoch: 868/2000... Training loss: 0.6672\n",
      "Epoch: 869/2000... Training loss: 0.4648\n",
      "Epoch: 869/2000... Training loss: 0.7973\n",
      "Epoch: 869/2000... Training loss: 0.5139\n",
      "Epoch: 869/2000... Training loss: 0.6188\n",
      "Epoch: 869/2000... Training loss: 0.5010\n",
      "Epoch: 869/2000... Training loss: 0.4151\n",
      "Epoch: 869/2000... Training loss: 0.5550\n",
      "Epoch: 869/2000... Training loss: 0.6214\n",
      "Epoch: 869/2000... Training loss: 0.4732\n",
      "Epoch: 869/2000... Training loss: 0.5180\n",
      "Epoch: 869/2000... Training loss: 0.7012\n",
      "Epoch: 869/2000... Training loss: 0.5989\n",
      "Epoch: 869/2000... Training loss: 0.5355\n",
      "Epoch: 869/2000... Training loss: 0.5125\n",
      "Epoch: 869/2000... Training loss: 0.6827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 869/2000... Training loss: 0.5981\n",
      "Epoch: 869/2000... Training loss: 0.5643\n",
      "Epoch: 869/2000... Training loss: 0.7721\n",
      "Epoch: 869/2000... Training loss: 0.8007\n",
      "Epoch: 869/2000... Training loss: 0.6717\n",
      "Epoch: 869/2000... Training loss: 0.6129\n",
      "Epoch: 869/2000... Training loss: 0.5083\n",
      "Epoch: 869/2000... Training loss: 0.6505\n",
      "Epoch: 869/2000... Training loss: 0.5109\n",
      "Epoch: 869/2000... Training loss: 0.6279\n",
      "Epoch: 869/2000... Training loss: 0.6490\n",
      "Epoch: 869/2000... Training loss: 0.5269\n",
      "Epoch: 869/2000... Training loss: 0.5992\n",
      "Epoch: 869/2000... Training loss: 0.8551\n",
      "Epoch: 869/2000... Training loss: 0.5671\n",
      "Epoch: 869/2000... Training loss: 0.5508\n",
      "Epoch: 870/2000... Training loss: 0.6644\n",
      "Epoch: 870/2000... Training loss: 0.5776\n",
      "Epoch: 870/2000... Training loss: 0.5123\n",
      "Epoch: 870/2000... Training loss: 0.5960\n",
      "Epoch: 870/2000... Training loss: 0.6088\n",
      "Epoch: 870/2000... Training loss: 0.5429\n",
      "Epoch: 870/2000... Training loss: 0.6597\n",
      "Epoch: 870/2000... Training loss: 0.6200\n",
      "Epoch: 870/2000... Training loss: 0.6632\n",
      "Epoch: 870/2000... Training loss: 0.6748\n",
      "Epoch: 870/2000... Training loss: 0.6561\n",
      "Epoch: 870/2000... Training loss: 0.6462\n",
      "Epoch: 870/2000... Training loss: 0.6516\n",
      "Epoch: 870/2000... Training loss: 0.6211\n",
      "Epoch: 870/2000... Training loss: 0.5306\n",
      "Epoch: 870/2000... Training loss: 0.5507\n",
      "Epoch: 870/2000... Training loss: 0.5102\n",
      "Epoch: 870/2000... Training loss: 0.9191\n",
      "Epoch: 870/2000... Training loss: 0.6316\n",
      "Epoch: 870/2000... Training loss: 0.6534\n",
      "Epoch: 870/2000... Training loss: 0.8001\n",
      "Epoch: 870/2000... Training loss: 0.6244\n",
      "Epoch: 870/2000... Training loss: 0.4974\n",
      "Epoch: 870/2000... Training loss: 0.5905\n",
      "Epoch: 870/2000... Training loss: 0.5509\n",
      "Epoch: 870/2000... Training loss: 0.6050\n",
      "Epoch: 870/2000... Training loss: 0.5587\n",
      "Epoch: 870/2000... Training loss: 0.4492\n",
      "Epoch: 870/2000... Training loss: 0.5620\n",
      "Epoch: 870/2000... Training loss: 0.5429\n",
      "Epoch: 870/2000... Training loss: 0.6607\n",
      "Epoch: 871/2000... Training loss: 0.5012\n",
      "Epoch: 871/2000... Training loss: 0.4908\n",
      "Epoch: 871/2000... Training loss: 0.7229\n",
      "Epoch: 871/2000... Training loss: 0.4873\n",
      "Epoch: 871/2000... Training loss: 0.6080\n",
      "Epoch: 871/2000... Training loss: 0.8431\n",
      "Epoch: 871/2000... Training loss: 0.6516\n",
      "Epoch: 871/2000... Training loss: 0.6867\n",
      "Epoch: 871/2000... Training loss: 0.7411\n",
      "Epoch: 871/2000... Training loss: 0.5844\n",
      "Epoch: 871/2000... Training loss: 0.5569\n",
      "Epoch: 871/2000... Training loss: 0.6315\n",
      "Epoch: 871/2000... Training loss: 0.4423\n",
      "Epoch: 871/2000... Training loss: 0.7741\n",
      "Epoch: 871/2000... Training loss: 0.4953\n",
      "Epoch: 871/2000... Training loss: 0.6026\n",
      "Epoch: 871/2000... Training loss: 0.4447\n",
      "Epoch: 871/2000... Training loss: 0.4484\n",
      "Epoch: 871/2000... Training loss: 0.5550\n",
      "Epoch: 871/2000... Training loss: 0.6442\n",
      "Epoch: 871/2000... Training loss: 0.6712\n",
      "Epoch: 871/2000... Training loss: 0.4958\n",
      "Epoch: 871/2000... Training loss: 0.7233\n",
      "Epoch: 871/2000... Training loss: 0.4156\n",
      "Epoch: 871/2000... Training loss: 0.5782\n",
      "Epoch: 871/2000... Training loss: 0.6081\n",
      "Epoch: 871/2000... Training loss: 0.6256\n",
      "Epoch: 871/2000... Training loss: 0.5836\n",
      "Epoch: 871/2000... Training loss: 0.5587\n",
      "Epoch: 871/2000... Training loss: 0.6590\n",
      "Epoch: 871/2000... Training loss: 0.4594\n",
      "Epoch: 872/2000... Training loss: 0.7248\n",
      "Epoch: 872/2000... Training loss: 0.6309\n",
      "Epoch: 872/2000... Training loss: 0.7749\n",
      "Epoch: 872/2000... Training loss: 0.3797\n",
      "Epoch: 872/2000... Training loss: 0.5569\n",
      "Epoch: 872/2000... Training loss: 0.4289\n",
      "Epoch: 872/2000... Training loss: 0.4913\n",
      "Epoch: 872/2000... Training loss: 0.5184\n",
      "Epoch: 872/2000... Training loss: 0.7470\n",
      "Epoch: 872/2000... Training loss: 0.4318\n",
      "Epoch: 872/2000... Training loss: 0.6955\n",
      "Epoch: 872/2000... Training loss: 0.6231\n",
      "Epoch: 872/2000... Training loss: 0.8204\n",
      "Epoch: 872/2000... Training loss: 0.4800\n",
      "Epoch: 872/2000... Training loss: 0.5100\n",
      "Epoch: 872/2000... Training loss: 0.5159\n",
      "Epoch: 872/2000... Training loss: 0.6173\n",
      "Epoch: 872/2000... Training loss: 0.5020\n",
      "Epoch: 872/2000... Training loss: 0.6202\n",
      "Epoch: 872/2000... Training loss: 0.6964\n",
      "Epoch: 872/2000... Training loss: 0.5912\n",
      "Epoch: 872/2000... Training loss: 0.7587\n",
      "Epoch: 872/2000... Training loss: 0.5250\n",
      "Epoch: 872/2000... Training loss: 0.5475\n",
      "Epoch: 872/2000... Training loss: 0.6386\n",
      "Epoch: 872/2000... Training loss: 0.5611\n",
      "Epoch: 872/2000... Training loss: 0.4685\n",
      "Epoch: 872/2000... Training loss: 0.6539\n",
      "Epoch: 872/2000... Training loss: 0.5700\n",
      "Epoch: 872/2000... Training loss: 0.6787\n",
      "Epoch: 872/2000... Training loss: 0.7103\n",
      "Epoch: 873/2000... Training loss: 0.6695\n",
      "Epoch: 873/2000... Training loss: 0.4903\n",
      "Epoch: 873/2000... Training loss: 0.5908\n",
      "Epoch: 873/2000... Training loss: 0.6271\n",
      "Epoch: 873/2000... Training loss: 0.6134\n",
      "Epoch: 873/2000... Training loss: 0.5194\n",
      "Epoch: 873/2000... Training loss: 0.5797\n",
      "Epoch: 873/2000... Training loss: 0.4540\n",
      "Epoch: 873/2000... Training loss: 0.5968\n",
      "Epoch: 873/2000... Training loss: 0.5727\n",
      "Epoch: 873/2000... Training loss: 0.4998\n",
      "Epoch: 873/2000... Training loss: 0.7627\n",
      "Epoch: 873/2000... Training loss: 0.8285\n",
      "Epoch: 873/2000... Training loss: 0.8638\n",
      "Epoch: 873/2000... Training loss: 0.5284\n",
      "Epoch: 873/2000... Training loss: 0.4959\n",
      "Epoch: 873/2000... Training loss: 0.6668\n",
      "Epoch: 873/2000... Training loss: 0.6485\n",
      "Epoch: 873/2000... Training loss: 0.6232\n",
      "Epoch: 873/2000... Training loss: 0.6486\n",
      "Epoch: 873/2000... Training loss: 0.4693\n",
      "Epoch: 873/2000... Training loss: 0.4368\n",
      "Epoch: 873/2000... Training loss: 0.5234\n",
      "Epoch: 873/2000... Training loss: 0.5625\n",
      "Epoch: 873/2000... Training loss: 0.4204\n",
      "Epoch: 873/2000... Training loss: 0.6063\n",
      "Epoch: 873/2000... Training loss: 0.6371\n",
      "Epoch: 873/2000... Training loss: 0.4966\n",
      "Epoch: 873/2000... Training loss: 0.5672\n",
      "Epoch: 873/2000... Training loss: 0.4823\n",
      "Epoch: 873/2000... Training loss: 0.6125\n",
      "Epoch: 874/2000... Training loss: 0.6386\n",
      "Epoch: 874/2000... Training loss: 0.5949\n",
      "Epoch: 874/2000... Training loss: 0.6664\n",
      "Epoch: 874/2000... Training loss: 0.5026\n",
      "Epoch: 874/2000... Training loss: 0.6928\n",
      "Epoch: 874/2000... Training loss: 0.4048\n",
      "Epoch: 874/2000... Training loss: 0.5678\n",
      "Epoch: 874/2000... Training loss: 0.5790\n",
      "Epoch: 874/2000... Training loss: 0.6123\n",
      "Epoch: 874/2000... Training loss: 0.3806\n",
      "Epoch: 874/2000... Training loss: 0.8373\n",
      "Epoch: 874/2000... Training loss: 0.5584\n",
      "Epoch: 874/2000... Training loss: 0.5751\n",
      "Epoch: 874/2000... Training loss: 0.6406\n",
      "Epoch: 874/2000... Training loss: 0.5283\n",
      "Epoch: 874/2000... Training loss: 0.5499\n",
      "Epoch: 874/2000... Training loss: 0.4207\n",
      "Epoch: 874/2000... Training loss: 0.6587\n",
      "Epoch: 874/2000... Training loss: 0.6695\n",
      "Epoch: 874/2000... Training loss: 0.6088\n",
      "Epoch: 874/2000... Training loss: 0.5067\n",
      "Epoch: 874/2000... Training loss: 0.7944\n",
      "Epoch: 874/2000... Training loss: 0.7769\n",
      "Epoch: 874/2000... Training loss: 0.7084\n",
      "Epoch: 874/2000... Training loss: 0.5066\n",
      "Epoch: 874/2000... Training loss: 0.6503\n",
      "Epoch: 874/2000... Training loss: 0.7515\n",
      "Epoch: 874/2000... Training loss: 0.6433\n",
      "Epoch: 874/2000... Training loss: 0.5273\n",
      "Epoch: 874/2000... Training loss: 0.6650\n",
      "Epoch: 874/2000... Training loss: 0.4374\n",
      "Epoch: 875/2000... Training loss: 0.4682\n",
      "Epoch: 875/2000... Training loss: 0.6882\n",
      "Epoch: 875/2000... Training loss: 0.6728\n",
      "Epoch: 875/2000... Training loss: 0.7364\n",
      "Epoch: 875/2000... Training loss: 0.6493\n",
      "Epoch: 875/2000... Training loss: 0.5206\n",
      "Epoch: 875/2000... Training loss: 0.4784\n",
      "Epoch: 875/2000... Training loss: 0.3878\n",
      "Epoch: 875/2000... Training loss: 0.6348\n",
      "Epoch: 875/2000... Training loss: 0.4462\n",
      "Epoch: 875/2000... Training loss: 0.4423\n",
      "Epoch: 875/2000... Training loss: 0.7580\n",
      "Epoch: 875/2000... Training loss: 0.4139\n",
      "Epoch: 875/2000... Training loss: 0.4903\n",
      "Epoch: 875/2000... Training loss: 0.7444\n",
      "Epoch: 875/2000... Training loss: 0.6333\n",
      "Epoch: 875/2000... Training loss: 0.4875\n",
      "Epoch: 875/2000... Training loss: 0.5103\n",
      "Epoch: 875/2000... Training loss: 0.7634\n",
      "Epoch: 875/2000... Training loss: 0.6756\n",
      "Epoch: 875/2000... Training loss: 0.5087\n",
      "Epoch: 875/2000... Training loss: 0.6746\n",
      "Epoch: 875/2000... Training loss: 0.6779\n",
      "Epoch: 875/2000... Training loss: 0.4782\n",
      "Epoch: 875/2000... Training loss: 0.8158\n",
      "Epoch: 875/2000... Training loss: 0.5800\n",
      "Epoch: 875/2000... Training loss: 0.6895\n",
      "Epoch: 875/2000... Training loss: 0.6273\n",
      "Epoch: 875/2000... Training loss: 0.7336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 875/2000... Training loss: 0.5170\n",
      "Epoch: 875/2000... Training loss: 0.5606\n",
      "Epoch: 876/2000... Training loss: 0.6187\n",
      "Epoch: 876/2000... Training loss: 0.5519\n",
      "Epoch: 876/2000... Training loss: 0.5927\n",
      "Epoch: 876/2000... Training loss: 0.4978\n",
      "Epoch: 876/2000... Training loss: 0.4459\n",
      "Epoch: 876/2000... Training loss: 0.6257\n",
      "Epoch: 876/2000... Training loss: 0.7754\n",
      "Epoch: 876/2000... Training loss: 0.6886\n",
      "Epoch: 876/2000... Training loss: 0.5307\n",
      "Epoch: 876/2000... Training loss: 0.8538\n",
      "Epoch: 876/2000... Training loss: 0.7862\n",
      "Epoch: 876/2000... Training loss: 0.6133\n",
      "Epoch: 876/2000... Training loss: 0.6731\n",
      "Epoch: 876/2000... Training loss: 0.5746\n",
      "Epoch: 876/2000... Training loss: 0.4598\n",
      "Epoch: 876/2000... Training loss: 0.5397\n",
      "Epoch: 876/2000... Training loss: 0.5826\n",
      "Epoch: 876/2000... Training loss: 0.6241\n",
      "Epoch: 876/2000... Training loss: 0.4496\n",
      "Epoch: 876/2000... Training loss: 0.4934\n",
      "Epoch: 876/2000... Training loss: 0.5586\n",
      "Epoch: 876/2000... Training loss: 0.6032\n",
      "Epoch: 876/2000... Training loss: 0.6051\n",
      "Epoch: 876/2000... Training loss: 0.6560\n",
      "Epoch: 876/2000... Training loss: 0.4736\n",
      "Epoch: 876/2000... Training loss: 0.4888\n",
      "Epoch: 876/2000... Training loss: 0.4924\n",
      "Epoch: 876/2000... Training loss: 0.6856\n",
      "Epoch: 876/2000... Training loss: 0.6212\n",
      "Epoch: 876/2000... Training loss: 0.6349\n",
      "Epoch: 876/2000... Training loss: 0.7735\n",
      "Epoch: 877/2000... Training loss: 0.9013\n",
      "Epoch: 877/2000... Training loss: 0.7716\n",
      "Epoch: 877/2000... Training loss: 0.6397\n",
      "Epoch: 877/2000... Training loss: 0.4549\n",
      "Epoch: 877/2000... Training loss: 0.4827\n",
      "Epoch: 877/2000... Training loss: 0.4355\n",
      "Epoch: 877/2000... Training loss: 0.8855\n",
      "Epoch: 877/2000... Training loss: 0.7384\n",
      "Epoch: 877/2000... Training loss: 0.6615\n",
      "Epoch: 877/2000... Training loss: 0.6354\n",
      "Epoch: 877/2000... Training loss: 0.6652\n",
      "Epoch: 877/2000... Training loss: 0.5566\n",
      "Epoch: 877/2000... Training loss: 0.4428\n",
      "Epoch: 877/2000... Training loss: 0.7242\n",
      "Epoch: 877/2000... Training loss: 0.5468\n",
      "Epoch: 877/2000... Training loss: 0.5281\n",
      "Epoch: 877/2000... Training loss: 0.6020\n",
      "Epoch: 877/2000... Training loss: 0.5717\n",
      "Epoch: 877/2000... Training loss: 0.5840\n",
      "Epoch: 877/2000... Training loss: 0.7212\n",
      "Epoch: 877/2000... Training loss: 0.6520\n",
      "Epoch: 877/2000... Training loss: 0.5635\n",
      "Epoch: 877/2000... Training loss: 0.8646\n",
      "Epoch: 877/2000... Training loss: 0.5748\n",
      "Epoch: 877/2000... Training loss: 0.7196\n",
      "Epoch: 877/2000... Training loss: 0.6970\n",
      "Epoch: 877/2000... Training loss: 0.5916\n",
      "Epoch: 877/2000... Training loss: 0.4524\n",
      "Epoch: 877/2000... Training loss: 0.5174\n",
      "Epoch: 877/2000... Training loss: 0.5545\n",
      "Epoch: 877/2000... Training loss: 0.4927\n",
      "Epoch: 878/2000... Training loss: 0.4108\n",
      "Epoch: 878/2000... Training loss: 0.4506\n",
      "Epoch: 878/2000... Training loss: 0.6355\n",
      "Epoch: 878/2000... Training loss: 0.5225\n",
      "Epoch: 878/2000... Training loss: 0.8792\n",
      "Epoch: 878/2000... Training loss: 0.6921\n",
      "Epoch: 878/2000... Training loss: 0.4417\n",
      "Epoch: 878/2000... Training loss: 0.4999\n",
      "Epoch: 878/2000... Training loss: 0.6053\n",
      "Epoch: 878/2000... Training loss: 0.4806\n",
      "Epoch: 878/2000... Training loss: 0.7005\n",
      "Epoch: 878/2000... Training loss: 0.5525\n",
      "Epoch: 878/2000... Training loss: 0.7919\n",
      "Epoch: 878/2000... Training loss: 0.5122\n",
      "Epoch: 878/2000... Training loss: 0.8400\n",
      "Epoch: 878/2000... Training loss: 0.5076\n",
      "Epoch: 878/2000... Training loss: 0.5345\n",
      "Epoch: 878/2000... Training loss: 0.5796\n",
      "Epoch: 878/2000... Training loss: 0.3761\n",
      "Epoch: 878/2000... Training loss: 0.5443\n",
      "Epoch: 878/2000... Training loss: 0.5800\n",
      "Epoch: 878/2000... Training loss: 0.5817\n",
      "Epoch: 878/2000... Training loss: 0.3890\n",
      "Epoch: 878/2000... Training loss: 0.6038\n",
      "Epoch: 878/2000... Training loss: 0.4826\n",
      "Epoch: 878/2000... Training loss: 0.4808\n",
      "Epoch: 878/2000... Training loss: 0.5854\n",
      "Epoch: 878/2000... Training loss: 0.6159\n",
      "Epoch: 878/2000... Training loss: 0.5270\n",
      "Epoch: 878/2000... Training loss: 0.6040\n",
      "Epoch: 878/2000... Training loss: 0.6900\n",
      "Epoch: 879/2000... Training loss: 0.6175\n",
      "Epoch: 879/2000... Training loss: 0.4311\n",
      "Epoch: 879/2000... Training loss: 0.8315\n",
      "Epoch: 879/2000... Training loss: 0.5832\n",
      "Epoch: 879/2000... Training loss: 0.4606\n",
      "Epoch: 879/2000... Training loss: 0.6450\n",
      "Epoch: 879/2000... Training loss: 0.5637\n",
      "Epoch: 879/2000... Training loss: 0.6207\n",
      "Epoch: 879/2000... Training loss: 0.5030\n",
      "Epoch: 879/2000... Training loss: 0.5019\n",
      "Epoch: 879/2000... Training loss: 0.4704\n",
      "Epoch: 879/2000... Training loss: 0.5214\n",
      "Epoch: 879/2000... Training loss: 0.4111\n",
      "Epoch: 879/2000... Training loss: 0.5412\n",
      "Epoch: 879/2000... Training loss: 0.5561\n",
      "Epoch: 879/2000... Training loss: 0.6318\n",
      "Epoch: 879/2000... Training loss: 0.6170\n",
      "Epoch: 879/2000... Training loss: 0.4482\n",
      "Epoch: 879/2000... Training loss: 0.6225\n",
      "Epoch: 879/2000... Training loss: 0.3915\n",
      "Epoch: 879/2000... Training loss: 0.6977\n",
      "Epoch: 879/2000... Training loss: 0.6093\n",
      "Epoch: 879/2000... Training loss: 0.5389\n",
      "Epoch: 879/2000... Training loss: 0.5187\n",
      "Epoch: 879/2000... Training loss: 0.7104\n",
      "Epoch: 879/2000... Training loss: 0.4745\n",
      "Epoch: 879/2000... Training loss: 0.5227\n",
      "Epoch: 879/2000... Training loss: 0.4746\n",
      "Epoch: 879/2000... Training loss: 0.5689\n",
      "Epoch: 879/2000... Training loss: 0.4240\n",
      "Epoch: 879/2000... Training loss: 0.5691\n",
      "Epoch: 880/2000... Training loss: 0.6168\n",
      "Epoch: 880/2000... Training loss: 0.4193\n",
      "Epoch: 880/2000... Training loss: 0.5552\n",
      "Epoch: 880/2000... Training loss: 0.4449\n",
      "Epoch: 880/2000... Training loss: 0.7280\n",
      "Epoch: 880/2000... Training loss: 0.5355\n",
      "Epoch: 880/2000... Training loss: 0.5127\n",
      "Epoch: 880/2000... Training loss: 0.4544\n",
      "Epoch: 880/2000... Training loss: 0.6244\n",
      "Epoch: 880/2000... Training loss: 0.5046\n",
      "Epoch: 880/2000... Training loss: 0.5753\n",
      "Epoch: 880/2000... Training loss: 0.7186\n",
      "Epoch: 880/2000... Training loss: 0.5856\n",
      "Epoch: 880/2000... Training loss: 0.5793\n",
      "Epoch: 880/2000... Training loss: 0.4668\n",
      "Epoch: 880/2000... Training loss: 0.5350\n",
      "Epoch: 880/2000... Training loss: 0.8244\n",
      "Epoch: 880/2000... Training loss: 0.6712\n",
      "Epoch: 880/2000... Training loss: 0.6004\n",
      "Epoch: 880/2000... Training loss: 0.5570\n",
      "Epoch: 880/2000... Training loss: 0.4935\n",
      "Epoch: 880/2000... Training loss: 0.3953\n",
      "Epoch: 880/2000... Training loss: 0.6207\n",
      "Epoch: 880/2000... Training loss: 0.5864\n",
      "Epoch: 880/2000... Training loss: 0.5458\n",
      "Epoch: 880/2000... Training loss: 0.5811\n",
      "Epoch: 880/2000... Training loss: 0.6781\n",
      "Epoch: 880/2000... Training loss: 0.7453\n",
      "Epoch: 880/2000... Training loss: 0.6878\n",
      "Epoch: 880/2000... Training loss: 0.6385\n",
      "Epoch: 880/2000... Training loss: 0.7070\n",
      "Epoch: 881/2000... Training loss: 0.6880\n",
      "Epoch: 881/2000... Training loss: 0.4894\n",
      "Epoch: 881/2000... Training loss: 0.7354\n",
      "Epoch: 881/2000... Training loss: 0.7230\n",
      "Epoch: 881/2000... Training loss: 0.6228\n",
      "Epoch: 881/2000... Training loss: 0.4984\n",
      "Epoch: 881/2000... Training loss: 0.4863\n",
      "Epoch: 881/2000... Training loss: 0.5897\n",
      "Epoch: 881/2000... Training loss: 0.4850\n",
      "Epoch: 881/2000... Training loss: 0.6598\n",
      "Epoch: 881/2000... Training loss: 0.4837\n",
      "Epoch: 881/2000... Training loss: 0.5770\n",
      "Epoch: 881/2000... Training loss: 0.5356\n",
      "Epoch: 881/2000... Training loss: 0.6550\n",
      "Epoch: 881/2000... Training loss: 0.6355\n",
      "Epoch: 881/2000... Training loss: 0.5568\n",
      "Epoch: 881/2000... Training loss: 0.6927\n",
      "Epoch: 881/2000... Training loss: 0.5699\n",
      "Epoch: 881/2000... Training loss: 0.3987\n",
      "Epoch: 881/2000... Training loss: 0.7405\n",
      "Epoch: 881/2000... Training loss: 1.0132\n",
      "Epoch: 881/2000... Training loss: 0.5553\n",
      "Epoch: 881/2000... Training loss: 0.4153\n",
      "Epoch: 881/2000... Training loss: 0.6005\n",
      "Epoch: 881/2000... Training loss: 0.5492\n",
      "Epoch: 881/2000... Training loss: 0.5236\n",
      "Epoch: 881/2000... Training loss: 0.7417\n",
      "Epoch: 881/2000... Training loss: 0.5852\n",
      "Epoch: 881/2000... Training loss: 0.8690\n",
      "Epoch: 881/2000... Training loss: 0.3769\n",
      "Epoch: 881/2000... Training loss: 0.5401\n",
      "Epoch: 882/2000... Training loss: 0.6128\n",
      "Epoch: 882/2000... Training loss: 0.3440\n",
      "Epoch: 882/2000... Training loss: 0.4090\n",
      "Epoch: 882/2000... Training loss: 0.5917\n",
      "Epoch: 882/2000... Training loss: 0.8111\n",
      "Epoch: 882/2000... Training loss: 0.5470\n",
      "Epoch: 882/2000... Training loss: 0.6401\n",
      "Epoch: 882/2000... Training loss: 0.5213\n",
      "Epoch: 882/2000... Training loss: 0.5862\n",
      "Epoch: 882/2000... Training loss: 0.5669\n",
      "Epoch: 882/2000... Training loss: 0.6200\n",
      "Epoch: 882/2000... Training loss: 0.5004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 882/2000... Training loss: 0.6970\n",
      "Epoch: 882/2000... Training loss: 0.5442\n",
      "Epoch: 882/2000... Training loss: 0.4402\n",
      "Epoch: 882/2000... Training loss: 0.5848\n",
      "Epoch: 882/2000... Training loss: 0.3117\n",
      "Epoch: 882/2000... Training loss: 0.5288\n",
      "Epoch: 882/2000... Training loss: 0.6029\n",
      "Epoch: 882/2000... Training loss: 0.7177\n",
      "Epoch: 882/2000... Training loss: 0.6555\n",
      "Epoch: 882/2000... Training loss: 0.6963\n",
      "Epoch: 882/2000... Training loss: 0.4077\n",
      "Epoch: 882/2000... Training loss: 0.4595\n",
      "Epoch: 882/2000... Training loss: 0.5913\n",
      "Epoch: 882/2000... Training loss: 0.8562\n",
      "Epoch: 882/2000... Training loss: 0.4543\n",
      "Epoch: 882/2000... Training loss: 0.4888\n",
      "Epoch: 882/2000... Training loss: 0.7155\n",
      "Epoch: 882/2000... Training loss: 0.5409\n",
      "Epoch: 882/2000... Training loss: 0.4794\n",
      "Epoch: 883/2000... Training loss: 0.5018\n",
      "Epoch: 883/2000... Training loss: 0.6413\n",
      "Epoch: 883/2000... Training loss: 0.5759\n",
      "Epoch: 883/2000... Training loss: 0.6670\n",
      "Epoch: 883/2000... Training loss: 0.5484\n",
      "Epoch: 883/2000... Training loss: 0.7372\n",
      "Epoch: 883/2000... Training loss: 0.5389\n",
      "Epoch: 883/2000... Training loss: 0.5354\n",
      "Epoch: 883/2000... Training loss: 0.6078\n",
      "Epoch: 883/2000... Training loss: 0.4852\n",
      "Epoch: 883/2000... Training loss: 0.5529\n",
      "Epoch: 883/2000... Training loss: 0.5547\n",
      "Epoch: 883/2000... Training loss: 0.5989\n",
      "Epoch: 883/2000... Training loss: 0.6248\n",
      "Epoch: 883/2000... Training loss: 0.3885\n",
      "Epoch: 883/2000... Training loss: 0.6533\n",
      "Epoch: 883/2000... Training loss: 0.5478\n",
      "Epoch: 883/2000... Training loss: 0.4772\n",
      "Epoch: 883/2000... Training loss: 0.6674\n",
      "Epoch: 883/2000... Training loss: 0.6488\n",
      "Epoch: 883/2000... Training loss: 0.4465\n",
      "Epoch: 883/2000... Training loss: 0.5298\n",
      "Epoch: 883/2000... Training loss: 0.5253\n",
      "Epoch: 883/2000... Training loss: 0.5862\n",
      "Epoch: 883/2000... Training loss: 0.5452\n",
      "Epoch: 883/2000... Training loss: 0.6193\n",
      "Epoch: 883/2000... Training loss: 0.7160\n",
      "Epoch: 883/2000... Training loss: 0.4689\n",
      "Epoch: 883/2000... Training loss: 0.6713\n",
      "Epoch: 883/2000... Training loss: 0.6327\n",
      "Epoch: 883/2000... Training loss: 0.5308\n",
      "Epoch: 884/2000... Training loss: 0.6055\n",
      "Epoch: 884/2000... Training loss: 0.5062\n",
      "Epoch: 884/2000... Training loss: 0.7791\n",
      "Epoch: 884/2000... Training loss: 0.5718\n",
      "Epoch: 884/2000... Training loss: 0.7425\n",
      "Epoch: 884/2000... Training loss: 0.6116\n",
      "Epoch: 884/2000... Training loss: 0.5688\n",
      "Epoch: 884/2000... Training loss: 0.6280\n",
      "Epoch: 884/2000... Training loss: 0.5811\n",
      "Epoch: 884/2000... Training loss: 0.6677\n",
      "Epoch: 884/2000... Training loss: 0.5792\n",
      "Epoch: 884/2000... Training loss: 0.8253\n",
      "Epoch: 884/2000... Training loss: 0.6253\n",
      "Epoch: 884/2000... Training loss: 0.7049\n",
      "Epoch: 884/2000... Training loss: 0.6755\n",
      "Epoch: 884/2000... Training loss: 0.4277\n",
      "Epoch: 884/2000... Training loss: 0.4760\n",
      "Epoch: 884/2000... Training loss: 0.5116\n",
      "Epoch: 884/2000... Training loss: 0.8384\n",
      "Epoch: 884/2000... Training loss: 0.4998\n",
      "Epoch: 884/2000... Training loss: 0.5179\n",
      "Epoch: 884/2000... Training loss: 0.4871\n",
      "Epoch: 884/2000... Training loss: 0.6915\n",
      "Epoch: 884/2000... Training loss: 0.5744\n",
      "Epoch: 884/2000... Training loss: 0.3735\n",
      "Epoch: 884/2000... Training loss: 0.3708\n",
      "Epoch: 884/2000... Training loss: 0.7860\n",
      "Epoch: 884/2000... Training loss: 0.5608\n",
      "Epoch: 884/2000... Training loss: 0.5300\n",
      "Epoch: 884/2000... Training loss: 0.6294\n",
      "Epoch: 884/2000... Training loss: 0.3937\n",
      "Epoch: 885/2000... Training loss: 0.3658\n",
      "Epoch: 885/2000... Training loss: 0.5086\n",
      "Epoch: 885/2000... Training loss: 0.8081\n",
      "Epoch: 885/2000... Training loss: 0.5078\n",
      "Epoch: 885/2000... Training loss: 0.6488\n",
      "Epoch: 885/2000... Training loss: 0.5842\n",
      "Epoch: 885/2000... Training loss: 0.6461\n",
      "Epoch: 885/2000... Training loss: 0.6192\n",
      "Epoch: 885/2000... Training loss: 0.7367\n",
      "Epoch: 885/2000... Training loss: 0.5299\n",
      "Epoch: 885/2000... Training loss: 0.6665\n",
      "Epoch: 885/2000... Training loss: 0.6410\n",
      "Epoch: 885/2000... Training loss: 0.5582\n",
      "Epoch: 885/2000... Training loss: 0.7014\n",
      "Epoch: 885/2000... Training loss: 0.5663\n",
      "Epoch: 885/2000... Training loss: 0.5221\n",
      "Epoch: 885/2000... Training loss: 0.3946\n",
      "Epoch: 885/2000... Training loss: 0.6356\n",
      "Epoch: 885/2000... Training loss: 0.5502\n",
      "Epoch: 885/2000... Training loss: 0.4940\n",
      "Epoch: 885/2000... Training loss: 0.8106\n",
      "Epoch: 885/2000... Training loss: 0.5382\n",
      "Epoch: 885/2000... Training loss: 0.8663\n",
      "Epoch: 885/2000... Training loss: 0.4785\n",
      "Epoch: 885/2000... Training loss: 0.4245\n",
      "Epoch: 885/2000... Training loss: 0.4871\n",
      "Epoch: 885/2000... Training loss: 0.4555\n",
      "Epoch: 885/2000... Training loss: 0.5489\n",
      "Epoch: 885/2000... Training loss: 0.6455\n",
      "Epoch: 885/2000... Training loss: 0.6481\n",
      "Epoch: 885/2000... Training loss: 0.5348\n",
      "Epoch: 886/2000... Training loss: 0.5410\n",
      "Epoch: 886/2000... Training loss: 0.3907\n",
      "Epoch: 886/2000... Training loss: 0.5634\n",
      "Epoch: 886/2000... Training loss: 0.5414\n",
      "Epoch: 886/2000... Training loss: 0.5299\n",
      "Epoch: 886/2000... Training loss: 0.5207\n",
      "Epoch: 886/2000... Training loss: 0.4810\n",
      "Epoch: 886/2000... Training loss: 0.5581\n",
      "Epoch: 886/2000... Training loss: 0.6077\n",
      "Epoch: 886/2000... Training loss: 0.6746\n",
      "Epoch: 886/2000... Training loss: 0.4976\n",
      "Epoch: 886/2000... Training loss: 0.5853\n",
      "Epoch: 886/2000... Training loss: 0.7069\n",
      "Epoch: 886/2000... Training loss: 0.5100\n",
      "Epoch: 886/2000... Training loss: 0.4773\n",
      "Epoch: 886/2000... Training loss: 0.5227\n",
      "Epoch: 886/2000... Training loss: 0.4777\n",
      "Epoch: 886/2000... Training loss: 0.4060\n",
      "Epoch: 886/2000... Training loss: 0.5306\n",
      "Epoch: 886/2000... Training loss: 0.5771\n",
      "Epoch: 886/2000... Training loss: 0.4296\n",
      "Epoch: 886/2000... Training loss: 0.6567\n",
      "Epoch: 886/2000... Training loss: 0.6250\n",
      "Epoch: 886/2000... Training loss: 0.4662\n",
      "Epoch: 886/2000... Training loss: 0.6187\n",
      "Epoch: 886/2000... Training loss: 0.6549\n",
      "Epoch: 886/2000... Training loss: 0.6051\n",
      "Epoch: 886/2000... Training loss: 0.5951\n",
      "Epoch: 886/2000... Training loss: 0.8288\n",
      "Epoch: 886/2000... Training loss: 0.5234\n",
      "Epoch: 886/2000... Training loss: 0.6626\n",
      "Epoch: 887/2000... Training loss: 0.4968\n",
      "Epoch: 887/2000... Training loss: 0.5056\n",
      "Epoch: 887/2000... Training loss: 0.2736\n",
      "Epoch: 887/2000... Training loss: 0.4446\n",
      "Epoch: 887/2000... Training loss: 0.6113\n",
      "Epoch: 887/2000... Training loss: 0.5868\n",
      "Epoch: 887/2000... Training loss: 0.6017\n",
      "Epoch: 887/2000... Training loss: 0.5836\n",
      "Epoch: 887/2000... Training loss: 0.6327\n",
      "Epoch: 887/2000... Training loss: 0.5969\n",
      "Epoch: 887/2000... Training loss: 0.7012\n",
      "Epoch: 887/2000... Training loss: 0.3156\n",
      "Epoch: 887/2000... Training loss: 0.5371\n",
      "Epoch: 887/2000... Training loss: 0.7429\n",
      "Epoch: 887/2000... Training loss: 0.6293\n",
      "Epoch: 887/2000... Training loss: 0.5798\n",
      "Epoch: 887/2000... Training loss: 0.5025\n",
      "Epoch: 887/2000... Training loss: 0.5574\n",
      "Epoch: 887/2000... Training loss: 0.6871\n",
      "Epoch: 887/2000... Training loss: 0.5599\n",
      "Epoch: 887/2000... Training loss: 0.4771\n",
      "Epoch: 887/2000... Training loss: 0.7662\n",
      "Epoch: 887/2000... Training loss: 0.4405\n",
      "Epoch: 887/2000... Training loss: 0.4137\n",
      "Epoch: 887/2000... Training loss: 0.5868\n",
      "Epoch: 887/2000... Training loss: 0.5322\n",
      "Epoch: 887/2000... Training loss: 0.5558\n",
      "Epoch: 887/2000... Training loss: 0.9139\n",
      "Epoch: 887/2000... Training loss: 0.4672\n",
      "Epoch: 887/2000... Training loss: 0.5254\n",
      "Epoch: 887/2000... Training loss: 0.7190\n",
      "Epoch: 888/2000... Training loss: 0.5437\n",
      "Epoch: 888/2000... Training loss: 0.5276\n",
      "Epoch: 888/2000... Training loss: 0.4734\n",
      "Epoch: 888/2000... Training loss: 0.5246\n",
      "Epoch: 888/2000... Training loss: 0.5897\n",
      "Epoch: 888/2000... Training loss: 0.6176\n",
      "Epoch: 888/2000... Training loss: 0.5581\n",
      "Epoch: 888/2000... Training loss: 0.4930\n",
      "Epoch: 888/2000... Training loss: 0.7512\n",
      "Epoch: 888/2000... Training loss: 0.5465\n",
      "Epoch: 888/2000... Training loss: 0.5776\n",
      "Epoch: 888/2000... Training loss: 0.5911\n",
      "Epoch: 888/2000... Training loss: 0.5318\n",
      "Epoch: 888/2000... Training loss: 0.5943\n",
      "Epoch: 888/2000... Training loss: 0.7437\n",
      "Epoch: 888/2000... Training loss: 0.5805\n",
      "Epoch: 888/2000... Training loss: 0.5364\n",
      "Epoch: 888/2000... Training loss: 0.5992\n",
      "Epoch: 888/2000... Training loss: 0.4603\n",
      "Epoch: 888/2000... Training loss: 0.6498\n",
      "Epoch: 888/2000... Training loss: 0.5217\n",
      "Epoch: 888/2000... Training loss: 0.4745\n",
      "Epoch: 888/2000... Training loss: 0.8622\n",
      "Epoch: 888/2000... Training loss: 0.7599\n",
      "Epoch: 888/2000... Training loss: 0.8319\n",
      "Epoch: 888/2000... Training loss: 0.4494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 888/2000... Training loss: 0.5167\n",
      "Epoch: 888/2000... Training loss: 0.4783\n",
      "Epoch: 888/2000... Training loss: 0.6350\n",
      "Epoch: 888/2000... Training loss: 0.4863\n",
      "Epoch: 888/2000... Training loss: 0.4785\n",
      "Epoch: 889/2000... Training loss: 0.5415\n",
      "Epoch: 889/2000... Training loss: 0.5139\n",
      "Epoch: 889/2000... Training loss: 0.4566\n",
      "Epoch: 889/2000... Training loss: 0.4092\n",
      "Epoch: 889/2000... Training loss: 0.4660\n",
      "Epoch: 889/2000... Training loss: 0.3925\n",
      "Epoch: 889/2000... Training loss: 0.7511\n",
      "Epoch: 889/2000... Training loss: 0.5101\n",
      "Epoch: 889/2000... Training loss: 0.6197\n",
      "Epoch: 889/2000... Training loss: 0.5759\n",
      "Epoch: 889/2000... Training loss: 0.6689\n",
      "Epoch: 889/2000... Training loss: 0.5485\n",
      "Epoch: 889/2000... Training loss: 0.6046\n",
      "Epoch: 889/2000... Training loss: 0.6628\n",
      "Epoch: 889/2000... Training loss: 0.6152\n",
      "Epoch: 889/2000... Training loss: 0.4794\n",
      "Epoch: 889/2000... Training loss: 0.5437\n",
      "Epoch: 889/2000... Training loss: 0.6290\n",
      "Epoch: 889/2000... Training loss: 0.4880\n",
      "Epoch: 889/2000... Training loss: 0.5860\n",
      "Epoch: 889/2000... Training loss: 0.5289\n",
      "Epoch: 889/2000... Training loss: 0.5758\n",
      "Epoch: 889/2000... Training loss: 0.5769\n",
      "Epoch: 889/2000... Training loss: 0.3466\n",
      "Epoch: 889/2000... Training loss: 0.5320\n",
      "Epoch: 889/2000... Training loss: 0.6174\n",
      "Epoch: 889/2000... Training loss: 0.7345\n",
      "Epoch: 889/2000... Training loss: 0.7608\n",
      "Epoch: 889/2000... Training loss: 0.5262\n",
      "Epoch: 889/2000... Training loss: 0.5774\n",
      "Epoch: 889/2000... Training loss: 0.6938\n",
      "Epoch: 890/2000... Training loss: 0.6962\n",
      "Epoch: 890/2000... Training loss: 0.4466\n",
      "Epoch: 890/2000... Training loss: 0.5353\n",
      "Epoch: 890/2000... Training loss: 0.6833\n",
      "Epoch: 890/2000... Training loss: 0.5433\n",
      "Epoch: 890/2000... Training loss: 0.5510\n",
      "Epoch: 890/2000... Training loss: 0.6502\n",
      "Epoch: 890/2000... Training loss: 0.5257\n",
      "Epoch: 890/2000... Training loss: 0.7251\n",
      "Epoch: 890/2000... Training loss: 0.7762\n",
      "Epoch: 890/2000... Training loss: 0.7355\n",
      "Epoch: 890/2000... Training loss: 0.7487\n",
      "Epoch: 890/2000... Training loss: 0.5103\n",
      "Epoch: 890/2000... Training loss: 0.7326\n",
      "Epoch: 890/2000... Training loss: 0.4833\n",
      "Epoch: 890/2000... Training loss: 0.4813\n",
      "Epoch: 890/2000... Training loss: 0.4354\n",
      "Epoch: 890/2000... Training loss: 0.7564\n",
      "Epoch: 890/2000... Training loss: 0.6509\n",
      "Epoch: 890/2000... Training loss: 0.7847\n",
      "Epoch: 890/2000... Training loss: 0.5802\n",
      "Epoch: 890/2000... Training loss: 0.5039\n",
      "Epoch: 890/2000... Training loss: 0.5754\n",
      "Epoch: 890/2000... Training loss: 0.5998\n",
      "Epoch: 890/2000... Training loss: 0.6973\n",
      "Epoch: 890/2000... Training loss: 0.6362\n",
      "Epoch: 890/2000... Training loss: 0.5569\n",
      "Epoch: 890/2000... Training loss: 0.3162\n",
      "Epoch: 890/2000... Training loss: 0.6013\n",
      "Epoch: 890/2000... Training loss: 0.7739\n",
      "Epoch: 890/2000... Training loss: 0.5434\n",
      "Epoch: 891/2000... Training loss: 0.6753\n",
      "Epoch: 891/2000... Training loss: 0.6732\n",
      "Epoch: 891/2000... Training loss: 0.5956\n",
      "Epoch: 891/2000... Training loss: 0.5961\n",
      "Epoch: 891/2000... Training loss: 0.5573\n",
      "Epoch: 891/2000... Training loss: 0.5294\n",
      "Epoch: 891/2000... Training loss: 0.5888\n",
      "Epoch: 891/2000... Training loss: 0.5606\n",
      "Epoch: 891/2000... Training loss: 0.5490\n",
      "Epoch: 891/2000... Training loss: 0.5178\n",
      "Epoch: 891/2000... Training loss: 0.5115\n",
      "Epoch: 891/2000... Training loss: 0.3972\n",
      "Epoch: 891/2000... Training loss: 0.5384\n",
      "Epoch: 891/2000... Training loss: 0.4298\n",
      "Epoch: 891/2000... Training loss: 0.3021\n",
      "Epoch: 891/2000... Training loss: 0.5737\n",
      "Epoch: 891/2000... Training loss: 0.6918\n",
      "Epoch: 891/2000... Training loss: 0.5654\n",
      "Epoch: 891/2000... Training loss: 0.6157\n",
      "Epoch: 891/2000... Training loss: 0.6385\n",
      "Epoch: 891/2000... Training loss: 0.5842\n",
      "Epoch: 891/2000... Training loss: 0.6068\n",
      "Epoch: 891/2000... Training loss: 0.7456\n",
      "Epoch: 891/2000... Training loss: 0.4923\n",
      "Epoch: 891/2000... Training loss: 0.7490\n",
      "Epoch: 891/2000... Training loss: 0.4899\n",
      "Epoch: 891/2000... Training loss: 0.5545\n",
      "Epoch: 891/2000... Training loss: 0.7854\n",
      "Epoch: 891/2000... Training loss: 0.6682\n",
      "Epoch: 891/2000... Training loss: 0.5716\n",
      "Epoch: 891/2000... Training loss: 0.5074\n",
      "Epoch: 892/2000... Training loss: 0.6934\n",
      "Epoch: 892/2000... Training loss: 0.7496\n",
      "Epoch: 892/2000... Training loss: 0.4446\n",
      "Epoch: 892/2000... Training loss: 0.6505\n",
      "Epoch: 892/2000... Training loss: 0.7139\n",
      "Epoch: 892/2000... Training loss: 0.4406\n",
      "Epoch: 892/2000... Training loss: 0.6640\n",
      "Epoch: 892/2000... Training loss: 0.4074\n",
      "Epoch: 892/2000... Training loss: 0.8222\n",
      "Epoch: 892/2000... Training loss: 0.4444\n",
      "Epoch: 892/2000... Training loss: 0.4791\n",
      "Epoch: 892/2000... Training loss: 0.7014\n",
      "Epoch: 892/2000... Training loss: 0.5716\n",
      "Epoch: 892/2000... Training loss: 0.5763\n",
      "Epoch: 892/2000... Training loss: 0.5101\n",
      "Epoch: 892/2000... Training loss: 0.6846\n",
      "Epoch: 892/2000... Training loss: 0.5727\n",
      "Epoch: 892/2000... Training loss: 0.6726\n",
      "Epoch: 892/2000... Training loss: 0.6263\n",
      "Epoch: 892/2000... Training loss: 0.4993\n",
      "Epoch: 892/2000... Training loss: 0.4987\n",
      "Epoch: 892/2000... Training loss: 0.7650\n",
      "Epoch: 892/2000... Training loss: 0.7918\n",
      "Epoch: 892/2000... Training loss: 0.6274\n",
      "Epoch: 892/2000... Training loss: 0.4632\n",
      "Epoch: 892/2000... Training loss: 0.6061\n",
      "Epoch: 892/2000... Training loss: 0.5286\n",
      "Epoch: 892/2000... Training loss: 0.6336\n",
      "Epoch: 892/2000... Training loss: 0.5062\n",
      "Epoch: 892/2000... Training loss: 0.8299\n",
      "Epoch: 892/2000... Training loss: 0.6375\n",
      "Epoch: 893/2000... Training loss: 0.5047\n",
      "Epoch: 893/2000... Training loss: 0.5600\n",
      "Epoch: 893/2000... Training loss: 0.5792\n",
      "Epoch: 893/2000... Training loss: 0.7873\n",
      "Epoch: 893/2000... Training loss: 0.4826\n",
      "Epoch: 893/2000... Training loss: 0.4660\n",
      "Epoch: 893/2000... Training loss: 0.5265\n",
      "Epoch: 893/2000... Training loss: 0.3423\n",
      "Epoch: 893/2000... Training loss: 0.6057\n",
      "Epoch: 893/2000... Training loss: 0.4982\n",
      "Epoch: 893/2000... Training loss: 0.5241\n",
      "Epoch: 893/2000... Training loss: 0.7126\n",
      "Epoch: 893/2000... Training loss: 0.6280\n",
      "Epoch: 893/2000... Training loss: 0.6737\n",
      "Epoch: 893/2000... Training loss: 0.5006\n",
      "Epoch: 893/2000... Training loss: 0.5947\n",
      "Epoch: 893/2000... Training loss: 0.6007\n",
      "Epoch: 893/2000... Training loss: 0.4533\n",
      "Epoch: 893/2000... Training loss: 0.5249\n",
      "Epoch: 893/2000... Training loss: 0.7191\n",
      "Epoch: 893/2000... Training loss: 0.6754\n",
      "Epoch: 893/2000... Training loss: 0.5220\n",
      "Epoch: 893/2000... Training loss: 0.6370\n",
      "Epoch: 893/2000... Training loss: 0.6869\n",
      "Epoch: 893/2000... Training loss: 0.5868\n",
      "Epoch: 893/2000... Training loss: 0.7622\n",
      "Epoch: 893/2000... Training loss: 0.6402\n",
      "Epoch: 893/2000... Training loss: 0.5623\n",
      "Epoch: 893/2000... Training loss: 0.5439\n",
      "Epoch: 893/2000... Training loss: 0.5361\n",
      "Epoch: 893/2000... Training loss: 0.8501\n",
      "Epoch: 894/2000... Training loss: 0.4370\n",
      "Epoch: 894/2000... Training loss: 0.5015\n",
      "Epoch: 894/2000... Training loss: 0.5555\n",
      "Epoch: 894/2000... Training loss: 0.5846\n",
      "Epoch: 894/2000... Training loss: 0.6461\n",
      "Epoch: 894/2000... Training loss: 0.5749\n",
      "Epoch: 894/2000... Training loss: 0.7201\n",
      "Epoch: 894/2000... Training loss: 0.6081\n",
      "Epoch: 894/2000... Training loss: 0.6155\n",
      "Epoch: 894/2000... Training loss: 0.6220\n",
      "Epoch: 894/2000... Training loss: 0.5621\n",
      "Epoch: 894/2000... Training loss: 0.7055\n",
      "Epoch: 894/2000... Training loss: 0.4920\n",
      "Epoch: 894/2000... Training loss: 0.4447\n",
      "Epoch: 894/2000... Training loss: 0.4349\n",
      "Epoch: 894/2000... Training loss: 0.5961\n",
      "Epoch: 894/2000... Training loss: 0.4587\n",
      "Epoch: 894/2000... Training loss: 0.4896\n",
      "Epoch: 894/2000... Training loss: 0.6819\n",
      "Epoch: 894/2000... Training loss: 0.6272\n",
      "Epoch: 894/2000... Training loss: 0.5577\n",
      "Epoch: 894/2000... Training loss: 0.7608\n",
      "Epoch: 894/2000... Training loss: 0.6118\n",
      "Epoch: 894/2000... Training loss: 0.6312\n",
      "Epoch: 894/2000... Training loss: 0.7250\n",
      "Epoch: 894/2000... Training loss: 0.8129\n",
      "Epoch: 894/2000... Training loss: 0.4373\n",
      "Epoch: 894/2000... Training loss: 0.5716\n",
      "Epoch: 894/2000... Training loss: 0.6410\n",
      "Epoch: 894/2000... Training loss: 0.4871\n",
      "Epoch: 894/2000... Training loss: 0.5869\n",
      "Epoch: 895/2000... Training loss: 0.5637\n",
      "Epoch: 895/2000... Training loss: 0.8117\n",
      "Epoch: 895/2000... Training loss: 0.7491\n",
      "Epoch: 895/2000... Training loss: 0.4504\n",
      "Epoch: 895/2000... Training loss: 0.5778\n",
      "Epoch: 895/2000... Training loss: 0.5964\n",
      "Epoch: 895/2000... Training loss: 0.4820\n",
      "Epoch: 895/2000... Training loss: 0.5261\n",
      "Epoch: 895/2000... Training loss: 0.5691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 895/2000... Training loss: 0.5685\n",
      "Epoch: 895/2000... Training loss: 0.4552\n",
      "Epoch: 895/2000... Training loss: 0.5621\n",
      "Epoch: 895/2000... Training loss: 0.4264\n",
      "Epoch: 895/2000... Training loss: 0.4832\n",
      "Epoch: 895/2000... Training loss: 0.8184\n",
      "Epoch: 895/2000... Training loss: 0.5739\n",
      "Epoch: 895/2000... Training loss: 0.6297\n",
      "Epoch: 895/2000... Training loss: 0.6308\n",
      "Epoch: 895/2000... Training loss: 0.7793\n",
      "Epoch: 895/2000... Training loss: 0.6579\n",
      "Epoch: 895/2000... Training loss: 0.3455\n",
      "Epoch: 895/2000... Training loss: 0.5477\n",
      "Epoch: 895/2000... Training loss: 0.6433\n",
      "Epoch: 895/2000... Training loss: 0.4862\n",
      "Epoch: 895/2000... Training loss: 0.5252\n",
      "Epoch: 895/2000... Training loss: 0.5463\n",
      "Epoch: 895/2000... Training loss: 0.5573\n",
      "Epoch: 895/2000... Training loss: 0.6110\n",
      "Epoch: 895/2000... Training loss: 0.5751\n",
      "Epoch: 895/2000... Training loss: 0.6458\n",
      "Epoch: 895/2000... Training loss: 0.7518\n",
      "Epoch: 896/2000... Training loss: 0.5079\n",
      "Epoch: 896/2000... Training loss: 0.5406\n",
      "Epoch: 896/2000... Training loss: 0.4145\n",
      "Epoch: 896/2000... Training loss: 0.3625\n",
      "Epoch: 896/2000... Training loss: 0.4963\n",
      "Epoch: 896/2000... Training loss: 0.5335\n",
      "Epoch: 896/2000... Training loss: 0.5687\n",
      "Epoch: 896/2000... Training loss: 0.4441\n",
      "Epoch: 896/2000... Training loss: 0.4387\n",
      "Epoch: 896/2000... Training loss: 0.5464\n",
      "Epoch: 896/2000... Training loss: 0.7536\n",
      "Epoch: 896/2000... Training loss: 0.3837\n",
      "Epoch: 896/2000... Training loss: 0.5821\n",
      "Epoch: 896/2000... Training loss: 0.6102\n",
      "Epoch: 896/2000... Training loss: 0.5306\n",
      "Epoch: 896/2000... Training loss: 0.5924\n",
      "Epoch: 896/2000... Training loss: 0.4840\n",
      "Epoch: 896/2000... Training loss: 0.5610\n",
      "Epoch: 896/2000... Training loss: 0.6514\n",
      "Epoch: 896/2000... Training loss: 0.5397\n",
      "Epoch: 896/2000... Training loss: 0.6584\n",
      "Epoch: 896/2000... Training loss: 0.6624\n",
      "Epoch: 896/2000... Training loss: 0.6846\n",
      "Epoch: 896/2000... Training loss: 0.5504\n",
      "Epoch: 896/2000... Training loss: 0.6684\n",
      "Epoch: 896/2000... Training loss: 0.7045\n",
      "Epoch: 896/2000... Training loss: 0.7151\n",
      "Epoch: 896/2000... Training loss: 0.6826\n",
      "Epoch: 896/2000... Training loss: 0.6971\n",
      "Epoch: 896/2000... Training loss: 0.6895\n",
      "Epoch: 896/2000... Training loss: 0.5867\n",
      "Epoch: 897/2000... Training loss: 0.6194\n",
      "Epoch: 897/2000... Training loss: 0.5427\n",
      "Epoch: 897/2000... Training loss: 0.6787\n",
      "Epoch: 897/2000... Training loss: 0.6524\n",
      "Epoch: 897/2000... Training loss: 0.5600\n",
      "Epoch: 897/2000... Training loss: 0.4050\n",
      "Epoch: 897/2000... Training loss: 0.6690\n",
      "Epoch: 897/2000... Training loss: 0.7856\n",
      "Epoch: 897/2000... Training loss: 0.6768\n",
      "Epoch: 897/2000... Training loss: 0.5093\n",
      "Epoch: 897/2000... Training loss: 0.5164\n",
      "Epoch: 897/2000... Training loss: 0.5067\n",
      "Epoch: 897/2000... Training loss: 0.6392\n",
      "Epoch: 897/2000... Training loss: 0.5855\n",
      "Epoch: 897/2000... Training loss: 0.4915\n",
      "Epoch: 897/2000... Training loss: 0.8431\n",
      "Epoch: 897/2000... Training loss: 0.6709\n",
      "Epoch: 897/2000... Training loss: 0.6215\n",
      "Epoch: 897/2000... Training loss: 0.6291\n",
      "Epoch: 897/2000... Training loss: 0.7650\n",
      "Epoch: 897/2000... Training loss: 0.8385\n",
      "Epoch: 897/2000... Training loss: 0.7580\n",
      "Epoch: 897/2000... Training loss: 0.4559\n",
      "Epoch: 897/2000... Training loss: 0.5445\n",
      "Epoch: 897/2000... Training loss: 0.5050\n",
      "Epoch: 897/2000... Training loss: 0.6536\n",
      "Epoch: 897/2000... Training loss: 0.5430\n",
      "Epoch: 897/2000... Training loss: 0.5267\n",
      "Epoch: 897/2000... Training loss: 0.6327\n",
      "Epoch: 897/2000... Training loss: 0.7104\n",
      "Epoch: 897/2000... Training loss: 0.5358\n",
      "Epoch: 898/2000... Training loss: 0.8279\n",
      "Epoch: 898/2000... Training loss: 0.6419\n",
      "Epoch: 898/2000... Training loss: 0.5307\n",
      "Epoch: 898/2000... Training loss: 0.4955\n",
      "Epoch: 898/2000... Training loss: 0.5751\n",
      "Epoch: 898/2000... Training loss: 0.5771\n",
      "Epoch: 898/2000... Training loss: 0.4997\n",
      "Epoch: 898/2000... Training loss: 0.5523\n",
      "Epoch: 898/2000... Training loss: 0.4890\n",
      "Epoch: 898/2000... Training loss: 0.6085\n",
      "Epoch: 898/2000... Training loss: 0.6825\n",
      "Epoch: 898/2000... Training loss: 0.6341\n",
      "Epoch: 898/2000... Training loss: 0.4374\n",
      "Epoch: 898/2000... Training loss: 0.5108\n",
      "Epoch: 898/2000... Training loss: 0.5101\n",
      "Epoch: 898/2000... Training loss: 0.5630\n",
      "Epoch: 898/2000... Training loss: 0.4401\n",
      "Epoch: 898/2000... Training loss: 0.5601\n",
      "Epoch: 898/2000... Training loss: 0.6069\n",
      "Epoch: 898/2000... Training loss: 0.8181\n",
      "Epoch: 898/2000... Training loss: 0.7942\n",
      "Epoch: 898/2000... Training loss: 0.3798\n",
      "Epoch: 898/2000... Training loss: 0.5470\n",
      "Epoch: 898/2000... Training loss: 0.5524\n",
      "Epoch: 898/2000... Training loss: 0.5215\n",
      "Epoch: 898/2000... Training loss: 0.3337\n",
      "Epoch: 898/2000... Training loss: 0.7611\n",
      "Epoch: 898/2000... Training loss: 0.5724\n",
      "Epoch: 898/2000... Training loss: 0.5408\n",
      "Epoch: 898/2000... Training loss: 0.6377\n",
      "Epoch: 898/2000... Training loss: 0.4666\n",
      "Epoch: 899/2000... Training loss: 0.6394\n",
      "Epoch: 899/2000... Training loss: 0.6374\n",
      "Epoch: 899/2000... Training loss: 0.5188\n",
      "Epoch: 899/2000... Training loss: 0.6209\n",
      "Epoch: 899/2000... Training loss: 0.5731\n",
      "Epoch: 899/2000... Training loss: 0.4396\n",
      "Epoch: 899/2000... Training loss: 0.7275\n",
      "Epoch: 899/2000... Training loss: 0.4649\n",
      "Epoch: 899/2000... Training loss: 0.7501\n",
      "Epoch: 899/2000... Training loss: 0.6307\n",
      "Epoch: 899/2000... Training loss: 0.4434\n",
      "Epoch: 899/2000... Training loss: 0.7314\n",
      "Epoch: 899/2000... Training loss: 0.5834\n",
      "Epoch: 899/2000... Training loss: 0.7796\n",
      "Epoch: 899/2000... Training loss: 0.4889\n",
      "Epoch: 899/2000... Training loss: 0.5573\n",
      "Epoch: 899/2000... Training loss: 0.5563\n",
      "Epoch: 899/2000... Training loss: 0.5733\n",
      "Epoch: 899/2000... Training loss: 0.5073\n",
      "Epoch: 899/2000... Training loss: 0.5207\n",
      "Epoch: 899/2000... Training loss: 0.4331\n",
      "Epoch: 899/2000... Training loss: 0.5442\n",
      "Epoch: 899/2000... Training loss: 0.6790\n",
      "Epoch: 899/2000... Training loss: 0.6181\n",
      "Epoch: 899/2000... Training loss: 0.5306\n",
      "Epoch: 899/2000... Training loss: 0.5317\n",
      "Epoch: 899/2000... Training loss: 0.5285\n",
      "Epoch: 899/2000... Training loss: 0.5650\n",
      "Epoch: 899/2000... Training loss: 0.5481\n",
      "Epoch: 899/2000... Training loss: 0.3822\n",
      "Epoch: 899/2000... Training loss: 0.6123\n",
      "Epoch: 900/2000... Training loss: 0.5557\n",
      "Epoch: 900/2000... Training loss: 0.4993\n",
      "Epoch: 900/2000... Training loss: 0.6551\n",
      "Epoch: 900/2000... Training loss: 0.4907\n",
      "Epoch: 900/2000... Training loss: 0.6722\n",
      "Epoch: 900/2000... Training loss: 0.5846\n",
      "Epoch: 900/2000... Training loss: 0.3722\n",
      "Epoch: 900/2000... Training loss: 0.4917\n",
      "Epoch: 900/2000... Training loss: 0.6494\n",
      "Epoch: 900/2000... Training loss: 0.5919\n",
      "Epoch: 900/2000... Training loss: 0.5332\n",
      "Epoch: 900/2000... Training loss: 0.6175\n",
      "Epoch: 900/2000... Training loss: 0.5023\n",
      "Epoch: 900/2000... Training loss: 0.6821\n",
      "Epoch: 900/2000... Training loss: 0.6065\n",
      "Epoch: 900/2000... Training loss: 0.4500\n",
      "Epoch: 900/2000... Training loss: 0.5287\n",
      "Epoch: 900/2000... Training loss: 0.4756\n",
      "Epoch: 900/2000... Training loss: 0.5991\n",
      "Epoch: 900/2000... Training loss: 0.6151\n",
      "Epoch: 900/2000... Training loss: 0.6453\n",
      "Epoch: 900/2000... Training loss: 0.4921\n",
      "Epoch: 900/2000... Training loss: 0.6885\n",
      "Epoch: 900/2000... Training loss: 0.4325\n",
      "Epoch: 900/2000... Training loss: 0.4053\n",
      "Epoch: 900/2000... Training loss: 0.5657\n",
      "Epoch: 900/2000... Training loss: 0.6907\n",
      "Epoch: 900/2000... Training loss: 0.5603\n",
      "Epoch: 900/2000... Training loss: 0.5021\n",
      "Epoch: 900/2000... Training loss: 0.5792\n",
      "Epoch: 900/2000... Training loss: 0.5868\n",
      "Epoch: 901/2000... Training loss: 0.6594\n",
      "Epoch: 901/2000... Training loss: 0.8400\n",
      "Epoch: 901/2000... Training loss: 0.5603\n",
      "Epoch: 901/2000... Training loss: 0.6561\n",
      "Epoch: 901/2000... Training loss: 0.8047\n",
      "Epoch: 901/2000... Training loss: 0.4856\n",
      "Epoch: 901/2000... Training loss: 0.4310\n",
      "Epoch: 901/2000... Training loss: 0.5744\n",
      "Epoch: 901/2000... Training loss: 0.6916\n",
      "Epoch: 901/2000... Training loss: 0.5883\n",
      "Epoch: 901/2000... Training loss: 0.8383\n",
      "Epoch: 901/2000... Training loss: 0.7810\n",
      "Epoch: 901/2000... Training loss: 0.4479\n",
      "Epoch: 901/2000... Training loss: 0.5861\n",
      "Epoch: 901/2000... Training loss: 0.4122\n",
      "Epoch: 901/2000... Training loss: 0.3450\n",
      "Epoch: 901/2000... Training loss: 0.4950\n",
      "Epoch: 901/2000... Training loss: 0.6758\n",
      "Epoch: 901/2000... Training loss: 0.6165\n",
      "Epoch: 901/2000... Training loss: 0.6616\n",
      "Epoch: 901/2000... Training loss: 0.5364\n",
      "Epoch: 901/2000... Training loss: 0.6186\n",
      "Epoch: 901/2000... Training loss: 0.5275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 901/2000... Training loss: 0.4860\n",
      "Epoch: 901/2000... Training loss: 0.5631\n",
      "Epoch: 901/2000... Training loss: 0.5088\n",
      "Epoch: 901/2000... Training loss: 0.4650\n",
      "Epoch: 901/2000... Training loss: 0.4067\n",
      "Epoch: 901/2000... Training loss: 0.5871\n",
      "Epoch: 901/2000... Training loss: 0.5842\n",
      "Epoch: 901/2000... Training loss: 0.5713\n",
      "Epoch: 902/2000... Training loss: 0.4612\n",
      "Epoch: 902/2000... Training loss: 0.4884\n",
      "Epoch: 902/2000... Training loss: 0.6375\n",
      "Epoch: 902/2000... Training loss: 0.5452\n",
      "Epoch: 902/2000... Training loss: 0.7415\n",
      "Epoch: 902/2000... Training loss: 0.5590\n",
      "Epoch: 902/2000... Training loss: 0.4533\n",
      "Epoch: 902/2000... Training loss: 0.4661\n",
      "Epoch: 902/2000... Training loss: 0.5782\n",
      "Epoch: 902/2000... Training loss: 0.6186\n",
      "Epoch: 902/2000... Training loss: 0.6628\n",
      "Epoch: 902/2000... Training loss: 0.5199\n",
      "Epoch: 902/2000... Training loss: 0.6148\n",
      "Epoch: 902/2000... Training loss: 0.4613\n",
      "Epoch: 902/2000... Training loss: 0.5168\n",
      "Epoch: 902/2000... Training loss: 0.6351\n",
      "Epoch: 902/2000... Training loss: 0.4967\n",
      "Epoch: 902/2000... Training loss: 0.5771\n",
      "Epoch: 902/2000... Training loss: 0.6073\n",
      "Epoch: 902/2000... Training loss: 0.5944\n",
      "Epoch: 902/2000... Training loss: 0.5510\n",
      "Epoch: 902/2000... Training loss: 0.4827\n",
      "Epoch: 902/2000... Training loss: 0.8859\n",
      "Epoch: 902/2000... Training loss: 0.7285\n",
      "Epoch: 902/2000... Training loss: 0.5802\n",
      "Epoch: 902/2000... Training loss: 0.6688\n",
      "Epoch: 902/2000... Training loss: 0.5072\n",
      "Epoch: 902/2000... Training loss: 0.6541\n",
      "Epoch: 902/2000... Training loss: 0.3716\n",
      "Epoch: 902/2000... Training loss: 0.5550\n",
      "Epoch: 902/2000... Training loss: 0.6097\n",
      "Epoch: 903/2000... Training loss: 0.4522\n",
      "Epoch: 903/2000... Training loss: 0.7082\n",
      "Epoch: 903/2000... Training loss: 0.5128\n",
      "Epoch: 903/2000... Training loss: 0.5655\n",
      "Epoch: 903/2000... Training loss: 0.5207\n",
      "Epoch: 903/2000... Training loss: 0.5445\n",
      "Epoch: 903/2000... Training loss: 0.5559\n",
      "Epoch: 903/2000... Training loss: 0.6069\n",
      "Epoch: 903/2000... Training loss: 0.5063\n",
      "Epoch: 903/2000... Training loss: 0.4017\n",
      "Epoch: 903/2000... Training loss: 0.7565\n",
      "Epoch: 903/2000... Training loss: 0.7990\n",
      "Epoch: 903/2000... Training loss: 0.4822\n",
      "Epoch: 903/2000... Training loss: 0.6276\n",
      "Epoch: 903/2000... Training loss: 0.5349\n",
      "Epoch: 903/2000... Training loss: 0.5878\n",
      "Epoch: 903/2000... Training loss: 0.6342\n",
      "Epoch: 903/2000... Training loss: 0.4874\n",
      "Epoch: 903/2000... Training loss: 0.7168\n",
      "Epoch: 903/2000... Training loss: 0.6152\n",
      "Epoch: 903/2000... Training loss: 0.7318\n",
      "Epoch: 903/2000... Training loss: 0.6096\n",
      "Epoch: 903/2000... Training loss: 0.7714\n",
      "Epoch: 903/2000... Training loss: 0.4645\n",
      "Epoch: 903/2000... Training loss: 0.3163\n",
      "Epoch: 903/2000... Training loss: 0.5457\n",
      "Epoch: 903/2000... Training loss: 0.5253\n",
      "Epoch: 903/2000... Training loss: 0.8496\n",
      "Epoch: 903/2000... Training loss: 0.5816\n",
      "Epoch: 903/2000... Training loss: 0.6253\n",
      "Epoch: 903/2000... Training loss: 0.4917\n",
      "Epoch: 904/2000... Training loss: 0.5224\n",
      "Epoch: 904/2000... Training loss: 0.3879\n",
      "Epoch: 904/2000... Training loss: 0.5467\n",
      "Epoch: 904/2000... Training loss: 0.5303\n",
      "Epoch: 904/2000... Training loss: 0.5717\n",
      "Epoch: 904/2000... Training loss: 0.5189\n",
      "Epoch: 904/2000... Training loss: 0.6557\n",
      "Epoch: 904/2000... Training loss: 0.6384\n",
      "Epoch: 904/2000... Training loss: 0.7457\n",
      "Epoch: 904/2000... Training loss: 0.6945\n",
      "Epoch: 904/2000... Training loss: 0.6208\n",
      "Epoch: 904/2000... Training loss: 0.7563\n",
      "Epoch: 904/2000... Training loss: 0.5926\n",
      "Epoch: 904/2000... Training loss: 0.4429\n",
      "Epoch: 904/2000... Training loss: 0.4317\n",
      "Epoch: 904/2000... Training loss: 0.4030\n",
      "Epoch: 904/2000... Training loss: 0.4583\n",
      "Epoch: 904/2000... Training loss: 0.6135\n",
      "Epoch: 904/2000... Training loss: 0.6580\n",
      "Epoch: 904/2000... Training loss: 0.5817\n",
      "Epoch: 904/2000... Training loss: 0.7776\n",
      "Epoch: 904/2000... Training loss: 0.4261\n",
      "Epoch: 904/2000... Training loss: 0.5031\n",
      "Epoch: 904/2000... Training loss: 0.6045\n",
      "Epoch: 904/2000... Training loss: 0.6497\n",
      "Epoch: 904/2000... Training loss: 0.7243\n",
      "Epoch: 904/2000... Training loss: 0.5214\n",
      "Epoch: 904/2000... Training loss: 0.6166\n",
      "Epoch: 904/2000... Training loss: 0.6263\n",
      "Epoch: 904/2000... Training loss: 0.5773\n",
      "Epoch: 904/2000... Training loss: 0.4512\n",
      "Epoch: 905/2000... Training loss: 0.6485\n",
      "Epoch: 905/2000... Training loss: 0.6509\n",
      "Epoch: 905/2000... Training loss: 0.5726\n",
      "Epoch: 905/2000... Training loss: 0.6148\n",
      "Epoch: 905/2000... Training loss: 0.5497\n",
      "Epoch: 905/2000... Training loss: 0.5531\n",
      "Epoch: 905/2000... Training loss: 0.8486\n",
      "Epoch: 905/2000... Training loss: 0.3242\n",
      "Epoch: 905/2000... Training loss: 0.8323\n",
      "Epoch: 905/2000... Training loss: 0.5076\n",
      "Epoch: 905/2000... Training loss: 0.7234\n",
      "Epoch: 905/2000... Training loss: 0.3933\n",
      "Epoch: 905/2000... Training loss: 0.5101\n",
      "Epoch: 905/2000... Training loss: 0.4681\n",
      "Epoch: 905/2000... Training loss: 0.6722\n",
      "Epoch: 905/2000... Training loss: 0.7564\n",
      "Epoch: 905/2000... Training loss: 0.4477\n",
      "Epoch: 905/2000... Training loss: 0.6009\n",
      "Epoch: 905/2000... Training loss: 0.6488\n",
      "Epoch: 905/2000... Training loss: 0.5821\n",
      "Epoch: 905/2000... Training loss: 0.5121\n",
      "Epoch: 905/2000... Training loss: 0.6042\n",
      "Epoch: 905/2000... Training loss: 0.6064\n",
      "Epoch: 905/2000... Training loss: 0.6245\n",
      "Epoch: 905/2000... Training loss: 0.6939\n",
      "Epoch: 905/2000... Training loss: 0.5749\n",
      "Epoch: 905/2000... Training loss: 0.5183\n",
      "Epoch: 905/2000... Training loss: 0.5564\n",
      "Epoch: 905/2000... Training loss: 0.6840\n",
      "Epoch: 905/2000... Training loss: 0.3030\n",
      "Epoch: 905/2000... Training loss: 0.6910\n",
      "Epoch: 906/2000... Training loss: 0.6025\n",
      "Epoch: 906/2000... Training loss: 0.6450\n",
      "Epoch: 906/2000... Training loss: 0.4169\n",
      "Epoch: 906/2000... Training loss: 0.5610\n",
      "Epoch: 906/2000... Training loss: 0.5512\n",
      "Epoch: 906/2000... Training loss: 0.5494\n",
      "Epoch: 906/2000... Training loss: 0.6300\n",
      "Epoch: 906/2000... Training loss: 0.6681\n",
      "Epoch: 906/2000... Training loss: 0.6276\n",
      "Epoch: 906/2000... Training loss: 0.4924\n",
      "Epoch: 906/2000... Training loss: 0.6142\n",
      "Epoch: 906/2000... Training loss: 0.4347\n",
      "Epoch: 906/2000... Training loss: 0.6318\n",
      "Epoch: 906/2000... Training loss: 0.5293\n",
      "Epoch: 906/2000... Training loss: 0.5651\n",
      "Epoch: 906/2000... Training loss: 0.5606\n",
      "Epoch: 906/2000... Training loss: 0.5359\n",
      "Epoch: 906/2000... Training loss: 0.7404\n",
      "Epoch: 906/2000... Training loss: 0.5788\n",
      "Epoch: 906/2000... Training loss: 0.7511\n",
      "Epoch: 906/2000... Training loss: 0.3885\n",
      "Epoch: 906/2000... Training loss: 0.5965\n",
      "Epoch: 906/2000... Training loss: 0.7985\n",
      "Epoch: 906/2000... Training loss: 0.5266\n",
      "Epoch: 906/2000... Training loss: 0.8005\n",
      "Epoch: 906/2000... Training loss: 0.7284\n",
      "Epoch: 906/2000... Training loss: 0.6726\n",
      "Epoch: 906/2000... Training loss: 0.4980\n",
      "Epoch: 906/2000... Training loss: 0.4951\n",
      "Epoch: 906/2000... Training loss: 0.5554\n",
      "Epoch: 906/2000... Training loss: 0.4759\n",
      "Epoch: 907/2000... Training loss: 0.5627\n",
      "Epoch: 907/2000... Training loss: 0.7256\n",
      "Epoch: 907/2000... Training loss: 0.7393\n",
      "Epoch: 907/2000... Training loss: 0.5193\n",
      "Epoch: 907/2000... Training loss: 0.6618\n",
      "Epoch: 907/2000... Training loss: 0.4554\n",
      "Epoch: 907/2000... Training loss: 0.4458\n",
      "Epoch: 907/2000... Training loss: 0.6491\n",
      "Epoch: 907/2000... Training loss: 0.4984\n",
      "Epoch: 907/2000... Training loss: 0.7451\n",
      "Epoch: 907/2000... Training loss: 0.5796\n",
      "Epoch: 907/2000... Training loss: 0.7572\n",
      "Epoch: 907/2000... Training loss: 0.4607\n",
      "Epoch: 907/2000... Training loss: 0.7214\n",
      "Epoch: 907/2000... Training loss: 0.5463\n",
      "Epoch: 907/2000... Training loss: 0.8075\n",
      "Epoch: 907/2000... Training loss: 0.5630\n",
      "Epoch: 907/2000... Training loss: 0.5050\n",
      "Epoch: 907/2000... Training loss: 0.6105\n",
      "Epoch: 907/2000... Training loss: 0.5844\n",
      "Epoch: 907/2000... Training loss: 0.7114\n",
      "Epoch: 907/2000... Training loss: 0.7758\n",
      "Epoch: 907/2000... Training loss: 0.4998\n",
      "Epoch: 907/2000... Training loss: 0.7067\n",
      "Epoch: 907/2000... Training loss: 0.4889\n",
      "Epoch: 907/2000... Training loss: 0.5511\n",
      "Epoch: 907/2000... Training loss: 0.4176\n",
      "Epoch: 907/2000... Training loss: 0.5974\n",
      "Epoch: 907/2000... Training loss: 0.2944\n",
      "Epoch: 907/2000... Training loss: 0.5651\n",
      "Epoch: 907/2000... Training loss: 0.5811\n",
      "Epoch: 908/2000... Training loss: 0.4733\n",
      "Epoch: 908/2000... Training loss: 0.6979\n",
      "Epoch: 908/2000... Training loss: 0.4151\n",
      "Epoch: 908/2000... Training loss: 0.7400\n",
      "Epoch: 908/2000... Training loss: 0.6529\n",
      "Epoch: 908/2000... Training loss: 0.7372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 908/2000... Training loss: 0.6297\n",
      "Epoch: 908/2000... Training loss: 0.6464\n",
      "Epoch: 908/2000... Training loss: 0.5049\n",
      "Epoch: 908/2000... Training loss: 0.5559\n",
      "Epoch: 908/2000... Training loss: 0.4386\n",
      "Epoch: 908/2000... Training loss: 0.5270\n",
      "Epoch: 908/2000... Training loss: 0.6077\n",
      "Epoch: 908/2000... Training loss: 0.7043\n",
      "Epoch: 908/2000... Training loss: 0.5212\n",
      "Epoch: 908/2000... Training loss: 0.5025\n",
      "Epoch: 908/2000... Training loss: 0.4256\n",
      "Epoch: 908/2000... Training loss: 0.6402\n",
      "Epoch: 908/2000... Training loss: 0.6170\n",
      "Epoch: 908/2000... Training loss: 0.5770\n",
      "Epoch: 908/2000... Training loss: 0.6176\n",
      "Epoch: 908/2000... Training loss: 0.5025\n",
      "Epoch: 908/2000... Training loss: 0.4871\n",
      "Epoch: 908/2000... Training loss: 0.5399\n",
      "Epoch: 908/2000... Training loss: 0.6390\n",
      "Epoch: 908/2000... Training loss: 0.4431\n",
      "Epoch: 908/2000... Training loss: 0.5302\n",
      "Epoch: 908/2000... Training loss: 0.5126\n",
      "Epoch: 908/2000... Training loss: 0.5148\n",
      "Epoch: 908/2000... Training loss: 0.4934\n",
      "Epoch: 908/2000... Training loss: 0.3151\n",
      "Epoch: 909/2000... Training loss: 0.7465\n",
      "Epoch: 909/2000... Training loss: 0.4624\n",
      "Epoch: 909/2000... Training loss: 0.5920\n",
      "Epoch: 909/2000... Training loss: 0.4897\n",
      "Epoch: 909/2000... Training loss: 0.6566\n",
      "Epoch: 909/2000... Training loss: 0.5612\n",
      "Epoch: 909/2000... Training loss: 0.6999\n",
      "Epoch: 909/2000... Training loss: 0.4854\n",
      "Epoch: 909/2000... Training loss: 0.5937\n",
      "Epoch: 909/2000... Training loss: 0.4738\n",
      "Epoch: 909/2000... Training loss: 0.6200\n",
      "Epoch: 909/2000... Training loss: 0.7078\n",
      "Epoch: 909/2000... Training loss: 0.5891\n",
      "Epoch: 909/2000... Training loss: 0.5996\n",
      "Epoch: 909/2000... Training loss: 0.4677\n",
      "Epoch: 909/2000... Training loss: 0.6040\n",
      "Epoch: 909/2000... Training loss: 0.5704\n",
      "Epoch: 909/2000... Training loss: 0.6178\n",
      "Epoch: 909/2000... Training loss: 0.5968\n",
      "Epoch: 909/2000... Training loss: 0.4975\n",
      "Epoch: 909/2000... Training loss: 0.4801\n",
      "Epoch: 909/2000... Training loss: 0.5515\n",
      "Epoch: 909/2000... Training loss: 0.4718\n",
      "Epoch: 909/2000... Training loss: 0.5197\n",
      "Epoch: 909/2000... Training loss: 0.6479\n",
      "Epoch: 909/2000... Training loss: 0.4892\n",
      "Epoch: 909/2000... Training loss: 0.4111\n",
      "Epoch: 909/2000... Training loss: 0.6230\n",
      "Epoch: 909/2000... Training loss: 0.4978\n",
      "Epoch: 909/2000... Training loss: 0.2774\n",
      "Epoch: 909/2000... Training loss: 0.5156\n",
      "Epoch: 910/2000... Training loss: 0.6040\n",
      "Epoch: 910/2000... Training loss: 0.7306\n",
      "Epoch: 910/2000... Training loss: 0.5007\n",
      "Epoch: 910/2000... Training loss: 0.6303\n",
      "Epoch: 910/2000... Training loss: 0.7266\n",
      "Epoch: 910/2000... Training loss: 0.6463\n",
      "Epoch: 910/2000... Training loss: 0.4585\n",
      "Epoch: 910/2000... Training loss: 0.6159\n",
      "Epoch: 910/2000... Training loss: 0.5887\n",
      "Epoch: 910/2000... Training loss: 0.3840\n",
      "Epoch: 910/2000... Training loss: 0.6742\n",
      "Epoch: 910/2000... Training loss: 0.6140\n",
      "Epoch: 910/2000... Training loss: 0.6106\n",
      "Epoch: 910/2000... Training loss: 0.6958\n",
      "Epoch: 910/2000... Training loss: 0.5029\n",
      "Epoch: 910/2000... Training loss: 0.4269\n",
      "Epoch: 910/2000... Training loss: 0.3242\n",
      "Epoch: 910/2000... Training loss: 0.4504\n",
      "Epoch: 910/2000... Training loss: 0.6092\n",
      "Epoch: 910/2000... Training loss: 0.5292\n",
      "Epoch: 910/2000... Training loss: 0.4459\n",
      "Epoch: 910/2000... Training loss: 0.4750\n",
      "Epoch: 910/2000... Training loss: 0.6961\n",
      "Epoch: 910/2000... Training loss: 0.8269\n",
      "Epoch: 910/2000... Training loss: 0.7821\n",
      "Epoch: 910/2000... Training loss: 0.5584\n",
      "Epoch: 910/2000... Training loss: 0.5477\n",
      "Epoch: 910/2000... Training loss: 0.6431\n",
      "Epoch: 910/2000... Training loss: 0.6105\n",
      "Epoch: 910/2000... Training loss: 0.4626\n",
      "Epoch: 910/2000... Training loss: 0.6150\n",
      "Epoch: 911/2000... Training loss: 0.4953\n",
      "Epoch: 911/2000... Training loss: 0.5158\n",
      "Epoch: 911/2000... Training loss: 0.8329\n",
      "Epoch: 911/2000... Training loss: 0.5928\n",
      "Epoch: 911/2000... Training loss: 0.6090\n",
      "Epoch: 911/2000... Training loss: 0.7303\n",
      "Epoch: 911/2000... Training loss: 0.7047\n",
      "Epoch: 911/2000... Training loss: 0.6053\n",
      "Epoch: 911/2000... Training loss: 0.6384\n",
      "Epoch: 911/2000... Training loss: 0.3972\n",
      "Epoch: 911/2000... Training loss: 0.4100\n",
      "Epoch: 911/2000... Training loss: 0.4429\n",
      "Epoch: 911/2000... Training loss: 0.7099\n",
      "Epoch: 911/2000... Training loss: 0.5473\n",
      "Epoch: 911/2000... Training loss: 0.5708\n",
      "Epoch: 911/2000... Training loss: 0.7245\n",
      "Epoch: 911/2000... Training loss: 0.3937\n",
      "Epoch: 911/2000... Training loss: 0.5622\n",
      "Epoch: 911/2000... Training loss: 0.4413\n",
      "Epoch: 911/2000... Training loss: 0.5095\n",
      "Epoch: 911/2000... Training loss: 0.5349\n",
      "Epoch: 911/2000... Training loss: 0.6696\n",
      "Epoch: 911/2000... Training loss: 0.6177\n",
      "Epoch: 911/2000... Training loss: 0.5827\n",
      "Epoch: 911/2000... Training loss: 0.6899\n",
      "Epoch: 911/2000... Training loss: 0.7599\n",
      "Epoch: 911/2000... Training loss: 0.5594\n",
      "Epoch: 911/2000... Training loss: 0.5214\n",
      "Epoch: 911/2000... Training loss: 0.5672\n",
      "Epoch: 911/2000... Training loss: 0.4742\n",
      "Epoch: 911/2000... Training loss: 0.4390\n",
      "Epoch: 912/2000... Training loss: 0.6767\n",
      "Epoch: 912/2000... Training loss: 0.6153\n",
      "Epoch: 912/2000... Training loss: 0.6201\n",
      "Epoch: 912/2000... Training loss: 0.4793\n",
      "Epoch: 912/2000... Training loss: 0.6681\n",
      "Epoch: 912/2000... Training loss: 0.4990\n",
      "Epoch: 912/2000... Training loss: 0.6553\n",
      "Epoch: 912/2000... Training loss: 0.4588\n",
      "Epoch: 912/2000... Training loss: 0.9348\n",
      "Epoch: 912/2000... Training loss: 0.7338\n",
      "Epoch: 912/2000... Training loss: 0.6049\n",
      "Epoch: 912/2000... Training loss: 0.4896\n",
      "Epoch: 912/2000... Training loss: 0.6663\n",
      "Epoch: 912/2000... Training loss: 0.5150\n",
      "Epoch: 912/2000... Training loss: 0.4639\n",
      "Epoch: 912/2000... Training loss: 0.5870\n",
      "Epoch: 912/2000... Training loss: 0.4535\n",
      "Epoch: 912/2000... Training loss: 0.5812\n",
      "Epoch: 912/2000... Training loss: 0.5112\n",
      "Epoch: 912/2000... Training loss: 0.7046\n",
      "Epoch: 912/2000... Training loss: 0.6691\n",
      "Epoch: 912/2000... Training loss: 0.8460\n",
      "Epoch: 912/2000... Training loss: 0.5044\n",
      "Epoch: 912/2000... Training loss: 0.6323\n",
      "Epoch: 912/2000... Training loss: 0.9138\n",
      "Epoch: 912/2000... Training loss: 0.6297\n",
      "Epoch: 912/2000... Training loss: 0.6571\n",
      "Epoch: 912/2000... Training loss: 0.4117\n",
      "Epoch: 912/2000... Training loss: 0.7782\n",
      "Epoch: 912/2000... Training loss: 0.6004\n",
      "Epoch: 912/2000... Training loss: 0.5074\n",
      "Epoch: 913/2000... Training loss: 0.7431\n",
      "Epoch: 913/2000... Training loss: 0.5487\n",
      "Epoch: 913/2000... Training loss: 0.6249\n",
      "Epoch: 913/2000... Training loss: 0.4725\n",
      "Epoch: 913/2000... Training loss: 0.5640\n",
      "Epoch: 913/2000... Training loss: 0.3865\n",
      "Epoch: 913/2000... Training loss: 0.4886\n",
      "Epoch: 913/2000... Training loss: 0.6741\n",
      "Epoch: 913/2000... Training loss: 0.5906\n",
      "Epoch: 913/2000... Training loss: 0.4688\n",
      "Epoch: 913/2000... Training loss: 0.7394\n",
      "Epoch: 913/2000... Training loss: 0.4738\n",
      "Epoch: 913/2000... Training loss: 0.5456\n",
      "Epoch: 913/2000... Training loss: 0.6019\n",
      "Epoch: 913/2000... Training loss: 0.4726\n",
      "Epoch: 913/2000... Training loss: 0.7227\n",
      "Epoch: 913/2000... Training loss: 0.4471\n",
      "Epoch: 913/2000... Training loss: 0.7544\n",
      "Epoch: 913/2000... Training loss: 0.4648\n",
      "Epoch: 913/2000... Training loss: 0.5723\n",
      "Epoch: 913/2000... Training loss: 0.6552\n",
      "Epoch: 913/2000... Training loss: 0.6880\n",
      "Epoch: 913/2000... Training loss: 0.8193\n",
      "Epoch: 913/2000... Training loss: 0.5014\n",
      "Epoch: 913/2000... Training loss: 0.4521\n",
      "Epoch: 913/2000... Training loss: 0.6542\n",
      "Epoch: 913/2000... Training loss: 0.4794\n",
      "Epoch: 913/2000... Training loss: 0.4971\n",
      "Epoch: 913/2000... Training loss: 0.5157\n",
      "Epoch: 913/2000... Training loss: 0.3841\n",
      "Epoch: 913/2000... Training loss: 0.5497\n",
      "Epoch: 914/2000... Training loss: 0.5041\n",
      "Epoch: 914/2000... Training loss: 0.5001\n",
      "Epoch: 914/2000... Training loss: 0.5682\n",
      "Epoch: 914/2000... Training loss: 0.5121\n",
      "Epoch: 914/2000... Training loss: 0.6994\n",
      "Epoch: 914/2000... Training loss: 0.3878\n",
      "Epoch: 914/2000... Training loss: 0.4511\n",
      "Epoch: 914/2000... Training loss: 0.5048\n",
      "Epoch: 914/2000... Training loss: 0.4682\n",
      "Epoch: 914/2000... Training loss: 0.4749\n",
      "Epoch: 914/2000... Training loss: 0.6071\n",
      "Epoch: 914/2000... Training loss: 0.6063\n",
      "Epoch: 914/2000... Training loss: 0.6041\n",
      "Epoch: 914/2000... Training loss: 0.5103\n",
      "Epoch: 914/2000... Training loss: 0.6189\n",
      "Epoch: 914/2000... Training loss: 0.6188\n",
      "Epoch: 914/2000... Training loss: 0.4584\n",
      "Epoch: 914/2000... Training loss: 0.4458\n",
      "Epoch: 914/2000... Training loss: 0.3413\n",
      "Epoch: 914/2000... Training loss: 0.5384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 914/2000... Training loss: 0.6277\n",
      "Epoch: 914/2000... Training loss: 0.6775\n",
      "Epoch: 914/2000... Training loss: 0.8096\n",
      "Epoch: 914/2000... Training loss: 0.8327\n",
      "Epoch: 914/2000... Training loss: 0.4641\n",
      "Epoch: 914/2000... Training loss: 0.5279\n",
      "Epoch: 914/2000... Training loss: 0.4341\n",
      "Epoch: 914/2000... Training loss: 0.5603\n",
      "Epoch: 914/2000... Training loss: 0.5314\n",
      "Epoch: 914/2000... Training loss: 0.5268\n",
      "Epoch: 914/2000... Training loss: 0.5967\n",
      "Epoch: 915/2000... Training loss: 0.6478\n",
      "Epoch: 915/2000... Training loss: 0.6076\n",
      "Epoch: 915/2000... Training loss: 0.5274\n",
      "Epoch: 915/2000... Training loss: 0.4838\n",
      "Epoch: 915/2000... Training loss: 0.5143\n",
      "Epoch: 915/2000... Training loss: 0.5855\n",
      "Epoch: 915/2000... Training loss: 0.5032\n",
      "Epoch: 915/2000... Training loss: 0.4191\n",
      "Epoch: 915/2000... Training loss: 0.3669\n",
      "Epoch: 915/2000... Training loss: 0.4954\n",
      "Epoch: 915/2000... Training loss: 0.5035\n",
      "Epoch: 915/2000... Training loss: 0.6829\n",
      "Epoch: 915/2000... Training loss: 0.5147\n",
      "Epoch: 915/2000... Training loss: 0.7054\n",
      "Epoch: 915/2000... Training loss: 0.4529\n",
      "Epoch: 915/2000... Training loss: 0.5583\n",
      "Epoch: 915/2000... Training loss: 0.5938\n",
      "Epoch: 915/2000... Training loss: 0.3586\n",
      "Epoch: 915/2000... Training loss: 0.3768\n",
      "Epoch: 915/2000... Training loss: 0.7428\n",
      "Epoch: 915/2000... Training loss: 0.5798\n",
      "Epoch: 915/2000... Training loss: 0.5224\n",
      "Epoch: 915/2000... Training loss: 0.6988\n",
      "Epoch: 915/2000... Training loss: 0.5411\n",
      "Epoch: 915/2000... Training loss: 0.4701\n",
      "Epoch: 915/2000... Training loss: 0.5137\n",
      "Epoch: 915/2000... Training loss: 0.4219\n",
      "Epoch: 915/2000... Training loss: 0.4613\n",
      "Epoch: 915/2000... Training loss: 0.7206\n",
      "Epoch: 915/2000... Training loss: 0.6795\n",
      "Epoch: 915/2000... Training loss: 0.4901\n",
      "Epoch: 916/2000... Training loss: 0.4378\n",
      "Epoch: 916/2000... Training loss: 0.3910\n",
      "Epoch: 916/2000... Training loss: 0.6000\n",
      "Epoch: 916/2000... Training loss: 0.6317\n",
      "Epoch: 916/2000... Training loss: 0.4974\n",
      "Epoch: 916/2000... Training loss: 0.3179\n",
      "Epoch: 916/2000... Training loss: 0.9810\n",
      "Epoch: 916/2000... Training loss: 0.5317\n",
      "Epoch: 916/2000... Training loss: 0.6270\n",
      "Epoch: 916/2000... Training loss: 0.7486\n",
      "Epoch: 916/2000... Training loss: 0.5121\n",
      "Epoch: 916/2000... Training loss: 0.7379\n",
      "Epoch: 916/2000... Training loss: 0.5258\n",
      "Epoch: 916/2000... Training loss: 0.6678\n",
      "Epoch: 916/2000... Training loss: 0.3877\n",
      "Epoch: 916/2000... Training loss: 0.6072\n",
      "Epoch: 916/2000... Training loss: 0.5170\n",
      "Epoch: 916/2000... Training loss: 0.4896\n",
      "Epoch: 916/2000... Training loss: 0.5175\n",
      "Epoch: 916/2000... Training loss: 0.5097\n",
      "Epoch: 916/2000... Training loss: 0.4838\n",
      "Epoch: 916/2000... Training loss: 0.3796\n",
      "Epoch: 916/2000... Training loss: 0.6138\n",
      "Epoch: 916/2000... Training loss: 0.7386\n",
      "Epoch: 916/2000... Training loss: 0.6572\n",
      "Epoch: 916/2000... Training loss: 0.5332\n",
      "Epoch: 916/2000... Training loss: 0.6760\n",
      "Epoch: 916/2000... Training loss: 0.6255\n",
      "Epoch: 916/2000... Training loss: 0.6892\n",
      "Epoch: 916/2000... Training loss: 0.5801\n",
      "Epoch: 916/2000... Training loss: 0.5483\n",
      "Epoch: 917/2000... Training loss: 0.6326\n",
      "Epoch: 917/2000... Training loss: 0.5422\n",
      "Epoch: 917/2000... Training loss: 0.5382\n",
      "Epoch: 917/2000... Training loss: 0.5721\n",
      "Epoch: 917/2000... Training loss: 0.7150\n",
      "Epoch: 917/2000... Training loss: 0.3853\n",
      "Epoch: 917/2000... Training loss: 0.5846\n",
      "Epoch: 917/2000... Training loss: 0.4506\n",
      "Epoch: 917/2000... Training loss: 0.6152\n",
      "Epoch: 917/2000... Training loss: 0.6176\n",
      "Epoch: 917/2000... Training loss: 0.4932\n",
      "Epoch: 917/2000... Training loss: 0.5824\n",
      "Epoch: 917/2000... Training loss: 0.5619\n",
      "Epoch: 917/2000... Training loss: 0.6603\n",
      "Epoch: 917/2000... Training loss: 0.6464\n",
      "Epoch: 917/2000... Training loss: 0.4953\n",
      "Epoch: 917/2000... Training loss: 0.3925\n",
      "Epoch: 917/2000... Training loss: 0.3863\n",
      "Epoch: 917/2000... Training loss: 0.9676\n",
      "Epoch: 917/2000... Training loss: 0.6436\n",
      "Epoch: 917/2000... Training loss: 0.5284\n",
      "Epoch: 917/2000... Training loss: 0.5903\n",
      "Epoch: 917/2000... Training loss: 0.5973\n",
      "Epoch: 917/2000... Training loss: 0.4127\n",
      "Epoch: 917/2000... Training loss: 0.4641\n",
      "Epoch: 917/2000... Training loss: 0.7061\n",
      "Epoch: 917/2000... Training loss: 0.6008\n",
      "Epoch: 917/2000... Training loss: 0.7568\n",
      "Epoch: 917/2000... Training loss: 0.5908\n",
      "Epoch: 917/2000... Training loss: 0.6997\n",
      "Epoch: 917/2000... Training loss: 0.5042\n",
      "Epoch: 918/2000... Training loss: 0.6464\n",
      "Epoch: 918/2000... Training loss: 0.4439\n",
      "Epoch: 918/2000... Training loss: 0.5795\n",
      "Epoch: 918/2000... Training loss: 0.6934\n",
      "Epoch: 918/2000... Training loss: 0.4939\n",
      "Epoch: 918/2000... Training loss: 0.7007\n",
      "Epoch: 918/2000... Training loss: 0.5530\n",
      "Epoch: 918/2000... Training loss: 0.5721\n",
      "Epoch: 918/2000... Training loss: 0.5401\n",
      "Epoch: 918/2000... Training loss: 0.5849\n",
      "Epoch: 918/2000... Training loss: 0.6439\n",
      "Epoch: 918/2000... Training loss: 0.4554\n",
      "Epoch: 918/2000... Training loss: 0.5488\n",
      "Epoch: 918/2000... Training loss: 0.7140\n",
      "Epoch: 918/2000... Training loss: 0.4196\n",
      "Epoch: 918/2000... Training loss: 0.5053\n",
      "Epoch: 918/2000... Training loss: 0.5560\n",
      "Epoch: 918/2000... Training loss: 0.4258\n",
      "Epoch: 918/2000... Training loss: 0.5737\n",
      "Epoch: 918/2000... Training loss: 0.5115\n",
      "Epoch: 918/2000... Training loss: 0.4956\n",
      "Epoch: 918/2000... Training loss: 0.6029\n",
      "Epoch: 918/2000... Training loss: 0.6746\n",
      "Epoch: 918/2000... Training loss: 0.4556\n",
      "Epoch: 918/2000... Training loss: 0.5428\n",
      "Epoch: 918/2000... Training loss: 0.4094\n",
      "Epoch: 918/2000... Training loss: 0.5526\n",
      "Epoch: 918/2000... Training loss: 0.5917\n",
      "Epoch: 918/2000... Training loss: 0.5150\n",
      "Epoch: 918/2000... Training loss: 0.6115\n",
      "Epoch: 918/2000... Training loss: 0.6251\n",
      "Epoch: 919/2000... Training loss: 0.6014\n",
      "Epoch: 919/2000... Training loss: 0.4337\n",
      "Epoch: 919/2000... Training loss: 0.5861\n",
      "Epoch: 919/2000... Training loss: 0.6564\n",
      "Epoch: 919/2000... Training loss: 0.4531\n",
      "Epoch: 919/2000... Training loss: 0.6484\n",
      "Epoch: 919/2000... Training loss: 0.5281\n",
      "Epoch: 919/2000... Training loss: 0.3637\n",
      "Epoch: 919/2000... Training loss: 0.7057\n",
      "Epoch: 919/2000... Training loss: 0.5474\n",
      "Epoch: 919/2000... Training loss: 0.5604\n",
      "Epoch: 919/2000... Training loss: 0.6463\n",
      "Epoch: 919/2000... Training loss: 0.4894\n",
      "Epoch: 919/2000... Training loss: 0.4769\n",
      "Epoch: 919/2000... Training loss: 0.4915\n",
      "Epoch: 919/2000... Training loss: 0.3436\n",
      "Epoch: 919/2000... Training loss: 0.4461\n",
      "Epoch: 919/2000... Training loss: 0.4082\n",
      "Epoch: 919/2000... Training loss: 0.5927\n",
      "Epoch: 919/2000... Training loss: 0.4540\n",
      "Epoch: 919/2000... Training loss: 0.5038\n",
      "Epoch: 919/2000... Training loss: 0.4656\n",
      "Epoch: 919/2000... Training loss: 0.5954\n",
      "Epoch: 919/2000... Training loss: 0.5121\n",
      "Epoch: 919/2000... Training loss: 0.6985\n",
      "Epoch: 919/2000... Training loss: 0.5346\n",
      "Epoch: 919/2000... Training loss: 0.5262\n",
      "Epoch: 919/2000... Training loss: 0.7017\n",
      "Epoch: 919/2000... Training loss: 0.4747\n",
      "Epoch: 919/2000... Training loss: 0.3878\n",
      "Epoch: 919/2000... Training loss: 0.6052\n",
      "Epoch: 920/2000... Training loss: 0.5534\n",
      "Epoch: 920/2000... Training loss: 0.5626\n",
      "Epoch: 920/2000... Training loss: 0.5883\n",
      "Epoch: 920/2000... Training loss: 0.5874\n",
      "Epoch: 920/2000... Training loss: 0.5793\n",
      "Epoch: 920/2000... Training loss: 0.5100\n",
      "Epoch: 920/2000... Training loss: 0.5750\n",
      "Epoch: 920/2000... Training loss: 0.5597\n",
      "Epoch: 920/2000... Training loss: 0.4198\n",
      "Epoch: 920/2000... Training loss: 0.6016\n",
      "Epoch: 920/2000... Training loss: 0.5684\n",
      "Epoch: 920/2000... Training loss: 0.4548\n",
      "Epoch: 920/2000... Training loss: 0.5167\n",
      "Epoch: 920/2000... Training loss: 0.4068\n",
      "Epoch: 920/2000... Training loss: 0.5704\n",
      "Epoch: 920/2000... Training loss: 0.7719\n",
      "Epoch: 920/2000... Training loss: 0.5623\n",
      "Epoch: 920/2000... Training loss: 0.6600\n",
      "Epoch: 920/2000... Training loss: 0.5777\n",
      "Epoch: 920/2000... Training loss: 0.4632\n",
      "Epoch: 920/2000... Training loss: 0.6539\n",
      "Epoch: 920/2000... Training loss: 0.5230\n",
      "Epoch: 920/2000... Training loss: 0.7668\n",
      "Epoch: 920/2000... Training loss: 0.5083\n",
      "Epoch: 920/2000... Training loss: 0.4061\n",
      "Epoch: 920/2000... Training loss: 0.3649\n",
      "Epoch: 920/2000... Training loss: 0.7228\n",
      "Epoch: 920/2000... Training loss: 0.6596\n",
      "Epoch: 920/2000... Training loss: 0.5958\n",
      "Epoch: 920/2000... Training loss: 0.5732\n",
      "Epoch: 920/2000... Training loss: 0.5100\n",
      "Epoch: 921/2000... Training loss: 0.4482\n",
      "Epoch: 921/2000... Training loss: 0.7785\n",
      "Epoch: 921/2000... Training loss: 0.5176\n",
      "Epoch: 921/2000... Training loss: 0.5028\n",
      "Epoch: 921/2000... Training loss: 0.5413\n",
      "Epoch: 921/2000... Training loss: 0.6446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 921/2000... Training loss: 0.5781\n",
      "Epoch: 921/2000... Training loss: 0.3802\n",
      "Epoch: 921/2000... Training loss: 0.6500\n",
      "Epoch: 921/2000... Training loss: 0.6997\n",
      "Epoch: 921/2000... Training loss: 0.5988\n",
      "Epoch: 921/2000... Training loss: 0.5703\n",
      "Epoch: 921/2000... Training loss: 0.4311\n",
      "Epoch: 921/2000... Training loss: 0.4465\n",
      "Epoch: 921/2000... Training loss: 0.6562\n",
      "Epoch: 921/2000... Training loss: 0.5025\n",
      "Epoch: 921/2000... Training loss: 0.5423\n",
      "Epoch: 921/2000... Training loss: 0.4438\n",
      "Epoch: 921/2000... Training loss: 0.5654\n",
      "Epoch: 921/2000... Training loss: 0.5744\n",
      "Epoch: 921/2000... Training loss: 0.4621\n",
      "Epoch: 921/2000... Training loss: 0.3824\n",
      "Epoch: 921/2000... Training loss: 0.5990\n",
      "Epoch: 921/2000... Training loss: 0.3799\n",
      "Epoch: 921/2000... Training loss: 0.4407\n",
      "Epoch: 921/2000... Training loss: 0.5226\n",
      "Epoch: 921/2000... Training loss: 0.6561\n",
      "Epoch: 921/2000... Training loss: 0.6414\n",
      "Epoch: 921/2000... Training loss: 0.4146\n",
      "Epoch: 921/2000... Training loss: 0.5456\n",
      "Epoch: 921/2000... Training loss: 0.5709\n",
      "Epoch: 922/2000... Training loss: 0.5950\n",
      "Epoch: 922/2000... Training loss: 0.5205\n",
      "Epoch: 922/2000... Training loss: 0.6087\n",
      "Epoch: 922/2000... Training loss: 0.5799\n",
      "Epoch: 922/2000... Training loss: 0.3317\n",
      "Epoch: 922/2000... Training loss: 0.6951\n",
      "Epoch: 922/2000... Training loss: 0.6168\n",
      "Epoch: 922/2000... Training loss: 0.4696\n",
      "Epoch: 922/2000... Training loss: 0.5119\n",
      "Epoch: 922/2000... Training loss: 0.5500\n",
      "Epoch: 922/2000... Training loss: 0.5776\n",
      "Epoch: 922/2000... Training loss: 0.4981\n",
      "Epoch: 922/2000... Training loss: 0.6480\n",
      "Epoch: 922/2000... Training loss: 0.5474\n",
      "Epoch: 922/2000... Training loss: 0.4014\n",
      "Epoch: 922/2000... Training loss: 0.5914\n",
      "Epoch: 922/2000... Training loss: 0.3528\n",
      "Epoch: 922/2000... Training loss: 0.4576\n",
      "Epoch: 922/2000... Training loss: 0.5778\n",
      "Epoch: 922/2000... Training loss: 0.6403\n",
      "Epoch: 922/2000... Training loss: 0.7425\n",
      "Epoch: 922/2000... Training loss: 0.8468\n",
      "Epoch: 922/2000... Training loss: 0.6541\n",
      "Epoch: 922/2000... Training loss: 0.4090\n",
      "Epoch: 922/2000... Training loss: 0.5248\n",
      "Epoch: 922/2000... Training loss: 0.5285\n",
      "Epoch: 922/2000... Training loss: 0.4916\n",
      "Epoch: 922/2000... Training loss: 0.6682\n",
      "Epoch: 922/2000... Training loss: 0.6831\n",
      "Epoch: 922/2000... Training loss: 0.6330\n",
      "Epoch: 922/2000... Training loss: 0.5260\n",
      "Epoch: 923/2000... Training loss: 0.4918\n",
      "Epoch: 923/2000... Training loss: 0.5720\n",
      "Epoch: 923/2000... Training loss: 0.6126\n",
      "Epoch: 923/2000... Training loss: 0.4529\n",
      "Epoch: 923/2000... Training loss: 0.4722\n",
      "Epoch: 923/2000... Training loss: 0.4222\n",
      "Epoch: 923/2000... Training loss: 0.4265\n",
      "Epoch: 923/2000... Training loss: 0.5305\n",
      "Epoch: 923/2000... Training loss: 0.5340\n",
      "Epoch: 923/2000... Training loss: 0.5311\n",
      "Epoch: 923/2000... Training loss: 0.3878\n",
      "Epoch: 923/2000... Training loss: 0.5136\n",
      "Epoch: 923/2000... Training loss: 0.4774\n",
      "Epoch: 923/2000... Training loss: 0.6245\n",
      "Epoch: 923/2000... Training loss: 0.6195\n",
      "Epoch: 923/2000... Training loss: 0.5404\n",
      "Epoch: 923/2000... Training loss: 0.3650\n",
      "Epoch: 923/2000... Training loss: 0.6111\n",
      "Epoch: 923/2000... Training loss: 0.5177\n",
      "Epoch: 923/2000... Training loss: 0.5129\n",
      "Epoch: 923/2000... Training loss: 0.5992\n",
      "Epoch: 923/2000... Training loss: 0.6135\n",
      "Epoch: 923/2000... Training loss: 0.7110\n",
      "Epoch: 923/2000... Training loss: 0.8182\n",
      "Epoch: 923/2000... Training loss: 0.6912\n",
      "Epoch: 923/2000... Training loss: 0.6131\n",
      "Epoch: 923/2000... Training loss: 0.5357\n",
      "Epoch: 923/2000... Training loss: 0.5410\n",
      "Epoch: 923/2000... Training loss: 0.6622\n",
      "Epoch: 923/2000... Training loss: 0.5194\n",
      "Epoch: 923/2000... Training loss: 0.3422\n",
      "Epoch: 924/2000... Training loss: 0.5356\n",
      "Epoch: 924/2000... Training loss: 0.5484\n",
      "Epoch: 924/2000... Training loss: 0.4498\n",
      "Epoch: 924/2000... Training loss: 0.6495\n",
      "Epoch: 924/2000... Training loss: 0.7250\n",
      "Epoch: 924/2000... Training loss: 0.4527\n",
      "Epoch: 924/2000... Training loss: 0.4329\n",
      "Epoch: 924/2000... Training loss: 0.5281\n",
      "Epoch: 924/2000... Training loss: 0.4203\n",
      "Epoch: 924/2000... Training loss: 0.5161\n",
      "Epoch: 924/2000... Training loss: 0.7227\n",
      "Epoch: 924/2000... Training loss: 0.5339\n",
      "Epoch: 924/2000... Training loss: 0.5524\n",
      "Epoch: 924/2000... Training loss: 0.4892\n",
      "Epoch: 924/2000... Training loss: 0.4825\n",
      "Epoch: 924/2000... Training loss: 0.5533\n",
      "Epoch: 924/2000... Training loss: 0.5481\n",
      "Epoch: 924/2000... Training loss: 0.4000\n",
      "Epoch: 924/2000... Training loss: 0.4329\n",
      "Epoch: 924/2000... Training loss: 0.5178\n",
      "Epoch: 924/2000... Training loss: 0.5366\n",
      "Epoch: 924/2000... Training loss: 0.5019\n",
      "Epoch: 924/2000... Training loss: 0.7165\n",
      "Epoch: 924/2000... Training loss: 0.6331\n",
      "Epoch: 924/2000... Training loss: 0.6721\n",
      "Epoch: 924/2000... Training loss: 0.5424\n",
      "Epoch: 924/2000... Training loss: 0.6533\n",
      "Epoch: 924/2000... Training loss: 0.6113\n",
      "Epoch: 924/2000... Training loss: 0.6560\n",
      "Epoch: 924/2000... Training loss: 0.7584\n",
      "Epoch: 924/2000... Training loss: 0.4236\n",
      "Epoch: 925/2000... Training loss: 0.5219\n",
      "Epoch: 925/2000... Training loss: 0.5710\n",
      "Epoch: 925/2000... Training loss: 0.5850\n",
      "Epoch: 925/2000... Training loss: 0.5993\n",
      "Epoch: 925/2000... Training loss: 0.5965\n",
      "Epoch: 925/2000... Training loss: 0.4140\n",
      "Epoch: 925/2000... Training loss: 0.5110\n",
      "Epoch: 925/2000... Training loss: 0.8836\n",
      "Epoch: 925/2000... Training loss: 0.4514\n",
      "Epoch: 925/2000... Training loss: 0.3888\n",
      "Epoch: 925/2000... Training loss: 0.7171\n",
      "Epoch: 925/2000... Training loss: 0.6640\n",
      "Epoch: 925/2000... Training loss: 0.5253\n",
      "Epoch: 925/2000... Training loss: 0.4465\n",
      "Epoch: 925/2000... Training loss: 0.6176\n",
      "Epoch: 925/2000... Training loss: 0.3971\n",
      "Epoch: 925/2000... Training loss: 0.4615\n",
      "Epoch: 925/2000... Training loss: 0.6115\n",
      "Epoch: 925/2000... Training loss: 0.6037\n",
      "Epoch: 925/2000... Training loss: 0.7232\n",
      "Epoch: 925/2000... Training loss: 0.5703\n",
      "Epoch: 925/2000... Training loss: 0.5621\n",
      "Epoch: 925/2000... Training loss: 0.5587\n",
      "Epoch: 925/2000... Training loss: 0.3986\n",
      "Epoch: 925/2000... Training loss: 0.5771\n",
      "Epoch: 925/2000... Training loss: 0.4180\n",
      "Epoch: 925/2000... Training loss: 0.6020\n",
      "Epoch: 925/2000... Training loss: 0.4470\n",
      "Epoch: 925/2000... Training loss: 0.5180\n",
      "Epoch: 925/2000... Training loss: 0.6205\n",
      "Epoch: 925/2000... Training loss: 0.8010\n",
      "Epoch: 926/2000... Training loss: 0.4178\n",
      "Epoch: 926/2000... Training loss: 0.5895\n",
      "Epoch: 926/2000... Training loss: 0.7918\n",
      "Epoch: 926/2000... Training loss: 0.5305\n",
      "Epoch: 926/2000... Training loss: 0.5362\n",
      "Epoch: 926/2000... Training loss: 0.4002\n",
      "Epoch: 926/2000... Training loss: 0.5738\n",
      "Epoch: 926/2000... Training loss: 0.5343\n",
      "Epoch: 926/2000... Training loss: 0.6584\n",
      "Epoch: 926/2000... Training loss: 0.6211\n",
      "Epoch: 926/2000... Training loss: 0.6091\n",
      "Epoch: 926/2000... Training loss: 0.4752\n",
      "Epoch: 926/2000... Training loss: 0.6138\n",
      "Epoch: 926/2000... Training loss: 0.3631\n",
      "Epoch: 926/2000... Training loss: 0.4886\n",
      "Epoch: 926/2000... Training loss: 0.5525\n",
      "Epoch: 926/2000... Training loss: 0.6083\n",
      "Epoch: 926/2000... Training loss: 0.5692\n",
      "Epoch: 926/2000... Training loss: 0.3794\n",
      "Epoch: 926/2000... Training loss: 0.5710\n",
      "Epoch: 926/2000... Training loss: 0.5726\n",
      "Epoch: 926/2000... Training loss: 0.4298\n",
      "Epoch: 926/2000... Training loss: 0.7030\n",
      "Epoch: 926/2000... Training loss: 0.5003\n",
      "Epoch: 926/2000... Training loss: 0.5082\n",
      "Epoch: 926/2000... Training loss: 0.6484\n",
      "Epoch: 926/2000... Training loss: 0.4636\n",
      "Epoch: 926/2000... Training loss: 0.4073\n",
      "Epoch: 926/2000... Training loss: 0.5337\n",
      "Epoch: 926/2000... Training loss: 0.4871\n",
      "Epoch: 926/2000... Training loss: 0.4814\n",
      "Epoch: 927/2000... Training loss: 0.5718\n",
      "Epoch: 927/2000... Training loss: 0.5893\n",
      "Epoch: 927/2000... Training loss: 0.3223\n",
      "Epoch: 927/2000... Training loss: 0.4799\n",
      "Epoch: 927/2000... Training loss: 0.6198\n",
      "Epoch: 927/2000... Training loss: 0.5420\n",
      "Epoch: 927/2000... Training loss: 0.7337\n",
      "Epoch: 927/2000... Training loss: 0.5582\n",
      "Epoch: 927/2000... Training loss: 0.6101\n",
      "Epoch: 927/2000... Training loss: 0.6945\n",
      "Epoch: 927/2000... Training loss: 0.3619\n",
      "Epoch: 927/2000... Training loss: 0.6268\n",
      "Epoch: 927/2000... Training loss: 0.5211\n",
      "Epoch: 927/2000... Training loss: 0.6044\n",
      "Epoch: 927/2000... Training loss: 0.3581\n",
      "Epoch: 927/2000... Training loss: 0.5159\n",
      "Epoch: 927/2000... Training loss: 0.3142\n",
      "Epoch: 927/2000... Training loss: 0.5588\n",
      "Epoch: 927/2000... Training loss: 0.5603\n",
      "Epoch: 927/2000... Training loss: 0.6869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 927/2000... Training loss: 0.4883\n",
      "Epoch: 927/2000... Training loss: 0.6141\n",
      "Epoch: 927/2000... Training loss: 0.4147\n",
      "Epoch: 927/2000... Training loss: 0.5859\n",
      "Epoch: 927/2000... Training loss: 0.6221\n",
      "Epoch: 927/2000... Training loss: 0.4942\n",
      "Epoch: 927/2000... Training loss: 0.4877\n",
      "Epoch: 927/2000... Training loss: 0.5616\n",
      "Epoch: 927/2000... Training loss: 0.5934\n",
      "Epoch: 927/2000... Training loss: 0.4899\n",
      "Epoch: 927/2000... Training loss: 0.4754\n",
      "Epoch: 928/2000... Training loss: 0.7979\n",
      "Epoch: 928/2000... Training loss: 0.4147\n",
      "Epoch: 928/2000... Training loss: 0.5606\n",
      "Epoch: 928/2000... Training loss: 0.5860\n",
      "Epoch: 928/2000... Training loss: 0.3689\n",
      "Epoch: 928/2000... Training loss: 0.4301\n",
      "Epoch: 928/2000... Training loss: 0.5364\n",
      "Epoch: 928/2000... Training loss: 0.4471\n",
      "Epoch: 928/2000... Training loss: 0.5075\n",
      "Epoch: 928/2000... Training loss: 0.5362\n",
      "Epoch: 928/2000... Training loss: 0.4028\n",
      "Epoch: 928/2000... Training loss: 0.5006\n",
      "Epoch: 928/2000... Training loss: 0.5965\n",
      "Epoch: 928/2000... Training loss: 0.4877\n",
      "Epoch: 928/2000... Training loss: 0.4726\n",
      "Epoch: 928/2000... Training loss: 0.9014\n",
      "Epoch: 928/2000... Training loss: 0.4639\n",
      "Epoch: 928/2000... Training loss: 0.6584\n",
      "Epoch: 928/2000... Training loss: 0.4596\n",
      "Epoch: 928/2000... Training loss: 0.5520\n",
      "Epoch: 928/2000... Training loss: 0.3754\n",
      "Epoch: 928/2000... Training loss: 0.4433\n",
      "Epoch: 928/2000... Training loss: 0.5642\n",
      "Epoch: 928/2000... Training loss: 0.5642\n",
      "Epoch: 928/2000... Training loss: 0.6009\n",
      "Epoch: 928/2000... Training loss: 0.5152\n",
      "Epoch: 928/2000... Training loss: 0.3835\n",
      "Epoch: 928/2000... Training loss: 0.5529\n",
      "Epoch: 928/2000... Training loss: 0.6387\n",
      "Epoch: 928/2000... Training loss: 0.6223\n",
      "Epoch: 928/2000... Training loss: 0.6992\n",
      "Epoch: 929/2000... Training loss: 0.6099\n",
      "Epoch: 929/2000... Training loss: 0.6184\n",
      "Epoch: 929/2000... Training loss: 0.4459\n",
      "Epoch: 929/2000... Training loss: 0.6068\n",
      "Epoch: 929/2000... Training loss: 0.7424\n",
      "Epoch: 929/2000... Training loss: 0.7406\n",
      "Epoch: 929/2000... Training loss: 0.5768\n",
      "Epoch: 929/2000... Training loss: 0.5504\n",
      "Epoch: 929/2000... Training loss: 0.5508\n",
      "Epoch: 929/2000... Training loss: 0.4798\n",
      "Epoch: 929/2000... Training loss: 0.5475\n",
      "Epoch: 929/2000... Training loss: 0.4693\n",
      "Epoch: 929/2000... Training loss: 0.4826\n",
      "Epoch: 929/2000... Training loss: 0.5899\n",
      "Epoch: 929/2000... Training loss: 0.5960\n",
      "Epoch: 929/2000... Training loss: 0.5171\n",
      "Epoch: 929/2000... Training loss: 0.7171\n",
      "Epoch: 929/2000... Training loss: 0.6492\n",
      "Epoch: 929/2000... Training loss: 0.5649\n",
      "Epoch: 929/2000... Training loss: 0.9370\n",
      "Epoch: 929/2000... Training loss: 0.5408\n",
      "Epoch: 929/2000... Training loss: 0.5629\n",
      "Epoch: 929/2000... Training loss: 0.5289\n",
      "Epoch: 929/2000... Training loss: 0.5292\n",
      "Epoch: 929/2000... Training loss: 0.4886\n",
      "Epoch: 929/2000... Training loss: 0.3818\n",
      "Epoch: 929/2000... Training loss: 0.7395\n",
      "Epoch: 929/2000... Training loss: 0.5398\n",
      "Epoch: 929/2000... Training loss: 0.5133\n",
      "Epoch: 929/2000... Training loss: 0.5055\n",
      "Epoch: 929/2000... Training loss: 0.4274\n",
      "Epoch: 930/2000... Training loss: 0.5477\n",
      "Epoch: 930/2000... Training loss: 0.4468\n",
      "Epoch: 930/2000... Training loss: 0.5795\n",
      "Epoch: 930/2000... Training loss: 0.4465\n",
      "Epoch: 930/2000... Training loss: 0.7921\n",
      "Epoch: 930/2000... Training loss: 0.5850\n",
      "Epoch: 930/2000... Training loss: 0.6878\n",
      "Epoch: 930/2000... Training loss: 0.3568\n",
      "Epoch: 930/2000... Training loss: 0.5034\n",
      "Epoch: 930/2000... Training loss: 0.4372\n",
      "Epoch: 930/2000... Training loss: 0.5682\n",
      "Epoch: 930/2000... Training loss: 0.7405\n",
      "Epoch: 930/2000... Training loss: 0.3910\n",
      "Epoch: 930/2000... Training loss: 0.4574\n",
      "Epoch: 930/2000... Training loss: 0.6390\n",
      "Epoch: 930/2000... Training loss: 0.5253\n",
      "Epoch: 930/2000... Training loss: 0.4352\n",
      "Epoch: 930/2000... Training loss: 0.5063\n",
      "Epoch: 930/2000... Training loss: 0.7357\n",
      "Epoch: 930/2000... Training loss: 0.5777\n",
      "Epoch: 930/2000... Training loss: 0.3499\n",
      "Epoch: 930/2000... Training loss: 0.5577\n",
      "Epoch: 930/2000... Training loss: 0.6182\n",
      "Epoch: 930/2000... Training loss: 0.4178\n",
      "Epoch: 930/2000... Training loss: 0.3653\n",
      "Epoch: 930/2000... Training loss: 0.6456\n",
      "Epoch: 930/2000... Training loss: 0.4623\n",
      "Epoch: 930/2000... Training loss: 0.4520\n",
      "Epoch: 930/2000... Training loss: 0.5572\n",
      "Epoch: 930/2000... Training loss: 0.7201\n",
      "Epoch: 930/2000... Training loss: 0.6528\n",
      "Epoch: 931/2000... Training loss: 0.6608\n",
      "Epoch: 931/2000... Training loss: 0.7037\n",
      "Epoch: 931/2000... Training loss: 0.6549\n",
      "Epoch: 931/2000... Training loss: 0.3800\n",
      "Epoch: 931/2000... Training loss: 0.3997\n",
      "Epoch: 931/2000... Training loss: 0.4062\n",
      "Epoch: 931/2000... Training loss: 0.3697\n",
      "Epoch: 931/2000... Training loss: 0.4501\n",
      "Epoch: 931/2000... Training loss: 0.7388\n",
      "Epoch: 931/2000... Training loss: 0.7388\n",
      "Epoch: 931/2000... Training loss: 0.6278\n",
      "Epoch: 931/2000... Training loss: 0.5717\n",
      "Epoch: 931/2000... Training loss: 0.7304\n",
      "Epoch: 931/2000... Training loss: 0.5072\n",
      "Epoch: 931/2000... Training loss: 0.4393\n",
      "Epoch: 931/2000... Training loss: 0.5683\n",
      "Epoch: 931/2000... Training loss: 0.5521\n",
      "Epoch: 931/2000... Training loss: 0.4338\n",
      "Epoch: 931/2000... Training loss: 0.6836\n",
      "Epoch: 931/2000... Training loss: 0.5566\n",
      "Epoch: 931/2000... Training loss: 0.3319\n",
      "Epoch: 931/2000... Training loss: 0.3615\n",
      "Epoch: 931/2000... Training loss: 0.6830\n",
      "Epoch: 931/2000... Training loss: 0.6722\n",
      "Epoch: 931/2000... Training loss: 0.4261\n",
      "Epoch: 931/2000... Training loss: 0.3857\n",
      "Epoch: 931/2000... Training loss: 0.7758\n",
      "Epoch: 931/2000... Training loss: 0.6263\n",
      "Epoch: 931/2000... Training loss: 0.7088\n",
      "Epoch: 931/2000... Training loss: 0.4264\n",
      "Epoch: 931/2000... Training loss: 0.7056\n",
      "Epoch: 932/2000... Training loss: 0.4977\n",
      "Epoch: 932/2000... Training loss: 0.5913\n",
      "Epoch: 932/2000... Training loss: 0.5141\n",
      "Epoch: 932/2000... Training loss: 0.7086\n",
      "Epoch: 932/2000... Training loss: 0.4111\n",
      "Epoch: 932/2000... Training loss: 0.9139\n",
      "Epoch: 932/2000... Training loss: 0.4884\n",
      "Epoch: 932/2000... Training loss: 0.4503\n",
      "Epoch: 932/2000... Training loss: 0.5153\n",
      "Epoch: 932/2000... Training loss: 0.5698\n",
      "Epoch: 932/2000... Training loss: 0.4664\n",
      "Epoch: 932/2000... Training loss: 0.5909\n",
      "Epoch: 932/2000... Training loss: 0.5448\n",
      "Epoch: 932/2000... Training loss: 0.4945\n",
      "Epoch: 932/2000... Training loss: 0.7160\n",
      "Epoch: 932/2000... Training loss: 0.6370\n",
      "Epoch: 932/2000... Training loss: 0.4633\n",
      "Epoch: 932/2000... Training loss: 0.4927\n",
      "Epoch: 932/2000... Training loss: 0.7062\n",
      "Epoch: 932/2000... Training loss: 0.4565\n",
      "Epoch: 932/2000... Training loss: 0.5066\n",
      "Epoch: 932/2000... Training loss: 0.7476\n",
      "Epoch: 932/2000... Training loss: 0.4682\n",
      "Epoch: 932/2000... Training loss: 0.5481\n",
      "Epoch: 932/2000... Training loss: 0.5610\n",
      "Epoch: 932/2000... Training loss: 0.5690\n",
      "Epoch: 932/2000... Training loss: 0.5886\n",
      "Epoch: 932/2000... Training loss: 0.7128\n",
      "Epoch: 932/2000... Training loss: 0.5916\n",
      "Epoch: 932/2000... Training loss: 0.2385\n",
      "Epoch: 932/2000... Training loss: 0.3180\n",
      "Epoch: 933/2000... Training loss: 0.6056\n",
      "Epoch: 933/2000... Training loss: 0.5485\n",
      "Epoch: 933/2000... Training loss: 0.5304\n",
      "Epoch: 933/2000... Training loss: 0.4146\n",
      "Epoch: 933/2000... Training loss: 0.5293\n",
      "Epoch: 933/2000... Training loss: 0.6733\n",
      "Epoch: 933/2000... Training loss: 0.6686\n",
      "Epoch: 933/2000... Training loss: 0.4414\n",
      "Epoch: 933/2000... Training loss: 0.5670\n",
      "Epoch: 933/2000... Training loss: 0.4770\n",
      "Epoch: 933/2000... Training loss: 0.7207\n",
      "Epoch: 933/2000... Training loss: 0.5486\n",
      "Epoch: 933/2000... Training loss: 0.5310\n",
      "Epoch: 933/2000... Training loss: 0.5519\n",
      "Epoch: 933/2000... Training loss: 0.6132\n",
      "Epoch: 933/2000... Training loss: 0.4361\n",
      "Epoch: 933/2000... Training loss: 0.5975\n",
      "Epoch: 933/2000... Training loss: 0.3519\n",
      "Epoch: 933/2000... Training loss: 0.3722\n",
      "Epoch: 933/2000... Training loss: 0.5515\n",
      "Epoch: 933/2000... Training loss: 0.6044\n",
      "Epoch: 933/2000... Training loss: 0.7599\n",
      "Epoch: 933/2000... Training loss: 0.5857\n",
      "Epoch: 933/2000... Training loss: 0.5153\n",
      "Epoch: 933/2000... Training loss: 0.8080\n",
      "Epoch: 933/2000... Training loss: 0.9197\n",
      "Epoch: 933/2000... Training loss: 0.6772\n",
      "Epoch: 933/2000... Training loss: 0.7173\n",
      "Epoch: 933/2000... Training loss: 0.6103\n",
      "Epoch: 933/2000... Training loss: 0.5908\n",
      "Epoch: 933/2000... Training loss: 0.5306\n",
      "Epoch: 934/2000... Training loss: 0.4588\n",
      "Epoch: 934/2000... Training loss: 0.5851\n",
      "Epoch: 934/2000... Training loss: 0.6006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 934/2000... Training loss: 0.3076\n",
      "Epoch: 934/2000... Training loss: 0.6583\n",
      "Epoch: 934/2000... Training loss: 0.6826\n",
      "Epoch: 934/2000... Training loss: 0.5620\n",
      "Epoch: 934/2000... Training loss: 0.5362\n",
      "Epoch: 934/2000... Training loss: 0.5068\n",
      "Epoch: 934/2000... Training loss: 0.7464\n",
      "Epoch: 934/2000... Training loss: 0.3960\n",
      "Epoch: 934/2000... Training loss: 0.7305\n",
      "Epoch: 934/2000... Training loss: 0.4491\n",
      "Epoch: 934/2000... Training loss: 0.3684\n",
      "Epoch: 934/2000... Training loss: 0.6892\n",
      "Epoch: 934/2000... Training loss: 0.5668\n",
      "Epoch: 934/2000... Training loss: 0.6083\n",
      "Epoch: 934/2000... Training loss: 0.5097\n",
      "Epoch: 934/2000... Training loss: 0.5518\n",
      "Epoch: 934/2000... Training loss: 0.5812\n",
      "Epoch: 934/2000... Training loss: 0.4328\n",
      "Epoch: 934/2000... Training loss: 0.3349\n",
      "Epoch: 934/2000... Training loss: 0.5996\n",
      "Epoch: 934/2000... Training loss: 0.5280\n",
      "Epoch: 934/2000... Training loss: 0.4928\n",
      "Epoch: 934/2000... Training loss: 0.6097\n",
      "Epoch: 934/2000... Training loss: 0.4755\n",
      "Epoch: 934/2000... Training loss: 0.4877\n",
      "Epoch: 934/2000... Training loss: 0.4186\n",
      "Epoch: 934/2000... Training loss: 0.5173\n",
      "Epoch: 934/2000... Training loss: 0.7577\n",
      "Epoch: 935/2000... Training loss: 0.4562\n",
      "Epoch: 935/2000... Training loss: 0.4049\n",
      "Epoch: 935/2000... Training loss: 0.4664\n",
      "Epoch: 935/2000... Training loss: 0.5852\n",
      "Epoch: 935/2000... Training loss: 0.6422\n",
      "Epoch: 935/2000... Training loss: 0.4429\n",
      "Epoch: 935/2000... Training loss: 0.5927\n",
      "Epoch: 935/2000... Training loss: 0.5677\n",
      "Epoch: 935/2000... Training loss: 0.5075\n",
      "Epoch: 935/2000... Training loss: 0.4132\n",
      "Epoch: 935/2000... Training loss: 0.5764\n",
      "Epoch: 935/2000... Training loss: 0.4985\n",
      "Epoch: 935/2000... Training loss: 0.3934\n",
      "Epoch: 935/2000... Training loss: 0.4472\n",
      "Epoch: 935/2000... Training loss: 0.4445\n",
      "Epoch: 935/2000... Training loss: 0.6142\n",
      "Epoch: 935/2000... Training loss: 0.7045\n",
      "Epoch: 935/2000... Training loss: 0.5577\n",
      "Epoch: 935/2000... Training loss: 0.7257\n",
      "Epoch: 935/2000... Training loss: 0.7207\n",
      "Epoch: 935/2000... Training loss: 0.5370\n",
      "Epoch: 935/2000... Training loss: 0.4596\n",
      "Epoch: 935/2000... Training loss: 0.4784\n",
      "Epoch: 935/2000... Training loss: 0.5494\n",
      "Epoch: 935/2000... Training loss: 0.5872\n",
      "Epoch: 935/2000... Training loss: 0.5429\n",
      "Epoch: 935/2000... Training loss: 0.4050\n",
      "Epoch: 935/2000... Training loss: 0.6195\n",
      "Epoch: 935/2000... Training loss: 0.4368\n",
      "Epoch: 935/2000... Training loss: 0.5173\n",
      "Epoch: 935/2000... Training loss: 0.4459\n",
      "Epoch: 936/2000... Training loss: 0.6064\n",
      "Epoch: 936/2000... Training loss: 0.4685\n",
      "Epoch: 936/2000... Training loss: 0.4447\n",
      "Epoch: 936/2000... Training loss: 0.4928\n",
      "Epoch: 936/2000... Training loss: 0.6280\n",
      "Epoch: 936/2000... Training loss: 0.4944\n",
      "Epoch: 936/2000... Training loss: 0.5767\n",
      "Epoch: 936/2000... Training loss: 0.4713\n",
      "Epoch: 936/2000... Training loss: 0.6530\n",
      "Epoch: 936/2000... Training loss: 0.5619\n",
      "Epoch: 936/2000... Training loss: 0.5769\n",
      "Epoch: 936/2000... Training loss: 0.5275\n",
      "Epoch: 936/2000... Training loss: 0.4284\n",
      "Epoch: 936/2000... Training loss: 0.6875\n",
      "Epoch: 936/2000... Training loss: 0.6769\n",
      "Epoch: 936/2000... Training loss: 0.7358\n",
      "Epoch: 936/2000... Training loss: 0.4254\n",
      "Epoch: 936/2000... Training loss: 0.5587\n",
      "Epoch: 936/2000... Training loss: 0.4540\n",
      "Epoch: 936/2000... Training loss: 0.4198\n",
      "Epoch: 936/2000... Training loss: 0.5856\n",
      "Epoch: 936/2000... Training loss: 0.5179\n",
      "Epoch: 936/2000... Training loss: 0.5411\n",
      "Epoch: 936/2000... Training loss: 0.5226\n",
      "Epoch: 936/2000... Training loss: 0.4373\n",
      "Epoch: 936/2000... Training loss: 0.5201\n",
      "Epoch: 936/2000... Training loss: 0.7068\n",
      "Epoch: 936/2000... Training loss: 0.5343\n",
      "Epoch: 936/2000... Training loss: 0.5575\n",
      "Epoch: 936/2000... Training loss: 0.4775\n",
      "Epoch: 936/2000... Training loss: 0.5063\n",
      "Epoch: 937/2000... Training loss: 0.6031\n",
      "Epoch: 937/2000... Training loss: 0.5661\n",
      "Epoch: 937/2000... Training loss: 0.4804\n",
      "Epoch: 937/2000... Training loss: 0.5798\n",
      "Epoch: 937/2000... Training loss: 0.5458\n",
      "Epoch: 937/2000... Training loss: 0.6100\n",
      "Epoch: 937/2000... Training loss: 0.4979\n",
      "Epoch: 937/2000... Training loss: 0.5227\n",
      "Epoch: 937/2000... Training loss: 0.6309\n",
      "Epoch: 937/2000... Training loss: 0.6764\n",
      "Epoch: 937/2000... Training loss: 0.6730\n",
      "Epoch: 937/2000... Training loss: 0.5280\n",
      "Epoch: 937/2000... Training loss: 0.4001\n",
      "Epoch: 937/2000... Training loss: 0.4840\n",
      "Epoch: 937/2000... Training loss: 0.5092\n",
      "Epoch: 937/2000... Training loss: 0.2771\n",
      "Epoch: 937/2000... Training loss: 0.6563\n",
      "Epoch: 937/2000... Training loss: 0.7254\n",
      "Epoch: 937/2000... Training loss: 0.5369\n",
      "Epoch: 937/2000... Training loss: 0.6622\n",
      "Epoch: 937/2000... Training loss: 0.3966\n",
      "Epoch: 937/2000... Training loss: 0.4769\n",
      "Epoch: 937/2000... Training loss: 0.3497\n",
      "Epoch: 937/2000... Training loss: 0.9642\n",
      "Epoch: 937/2000... Training loss: 0.5421\n",
      "Epoch: 937/2000... Training loss: 0.3768\n",
      "Epoch: 937/2000... Training loss: 0.3767\n",
      "Epoch: 937/2000... Training loss: 0.6258\n",
      "Epoch: 937/2000... Training loss: 0.6466\n",
      "Epoch: 937/2000... Training loss: 0.4049\n",
      "Epoch: 937/2000... Training loss: 0.4412\n",
      "Epoch: 938/2000... Training loss: 0.6169\n",
      "Epoch: 938/2000... Training loss: 0.5943\n",
      "Epoch: 938/2000... Training loss: 0.4524\n",
      "Epoch: 938/2000... Training loss: 0.7371\n",
      "Epoch: 938/2000... Training loss: 0.6522\n",
      "Epoch: 938/2000... Training loss: 0.5396\n",
      "Epoch: 938/2000... Training loss: 0.5399\n",
      "Epoch: 938/2000... Training loss: 0.3639\n",
      "Epoch: 938/2000... Training loss: 0.6391\n",
      "Epoch: 938/2000... Training loss: 0.4348\n",
      "Epoch: 938/2000... Training loss: 0.6043\n",
      "Epoch: 938/2000... Training loss: 0.5869\n",
      "Epoch: 938/2000... Training loss: 0.4101\n",
      "Epoch: 938/2000... Training loss: 0.5658\n",
      "Epoch: 938/2000... Training loss: 0.5697\n",
      "Epoch: 938/2000... Training loss: 0.4603\n",
      "Epoch: 938/2000... Training loss: 0.5203\n",
      "Epoch: 938/2000... Training loss: 0.4884\n",
      "Epoch: 938/2000... Training loss: 0.6248\n",
      "Epoch: 938/2000... Training loss: 0.3957\n",
      "Epoch: 938/2000... Training loss: 0.5897\n",
      "Epoch: 938/2000... Training loss: 0.4325\n",
      "Epoch: 938/2000... Training loss: 0.5369\n",
      "Epoch: 938/2000... Training loss: 0.4715\n",
      "Epoch: 938/2000... Training loss: 0.7545\n",
      "Epoch: 938/2000... Training loss: 0.5443\n",
      "Epoch: 938/2000... Training loss: 0.5113\n",
      "Epoch: 938/2000... Training loss: 0.3984\n",
      "Epoch: 938/2000... Training loss: 0.6673\n",
      "Epoch: 938/2000... Training loss: 0.5059\n",
      "Epoch: 938/2000... Training loss: 0.5822\n",
      "Epoch: 939/2000... Training loss: 0.4560\n",
      "Epoch: 939/2000... Training loss: 0.4988\n",
      "Epoch: 939/2000... Training loss: 0.5096\n",
      "Epoch: 939/2000... Training loss: 0.6440\n",
      "Epoch: 939/2000... Training loss: 0.4771\n",
      "Epoch: 939/2000... Training loss: 0.5422\n",
      "Epoch: 939/2000... Training loss: 0.3951\n",
      "Epoch: 939/2000... Training loss: 0.4966\n",
      "Epoch: 939/2000... Training loss: 0.4572\n",
      "Epoch: 939/2000... Training loss: 0.4474\n",
      "Epoch: 939/2000... Training loss: 0.5750\n",
      "Epoch: 939/2000... Training loss: 0.6939\n",
      "Epoch: 939/2000... Training loss: 0.4434\n",
      "Epoch: 939/2000... Training loss: 0.8861\n",
      "Epoch: 939/2000... Training loss: 0.5269\n",
      "Epoch: 939/2000... Training loss: 0.5988\n",
      "Epoch: 939/2000... Training loss: 0.5855\n",
      "Epoch: 939/2000... Training loss: 0.5068\n",
      "Epoch: 939/2000... Training loss: 0.5004\n",
      "Epoch: 939/2000... Training loss: 0.6232\n",
      "Epoch: 939/2000... Training loss: 0.4193\n",
      "Epoch: 939/2000... Training loss: 0.4974\n",
      "Epoch: 939/2000... Training loss: 0.5364\n",
      "Epoch: 939/2000... Training loss: 0.4373\n",
      "Epoch: 939/2000... Training loss: 0.6978\n",
      "Epoch: 939/2000... Training loss: 0.5258\n",
      "Epoch: 939/2000... Training loss: 0.5913\n",
      "Epoch: 939/2000... Training loss: 0.5764\n",
      "Epoch: 939/2000... Training loss: 0.6701\n",
      "Epoch: 939/2000... Training loss: 0.4950\n",
      "Epoch: 939/2000... Training loss: 0.6559\n",
      "Epoch: 940/2000... Training loss: 0.5285\n",
      "Epoch: 940/2000... Training loss: 0.6644\n",
      "Epoch: 940/2000... Training loss: 0.5305\n",
      "Epoch: 940/2000... Training loss: 0.4013\n",
      "Epoch: 940/2000... Training loss: 0.4578\n",
      "Epoch: 940/2000... Training loss: 0.5350\n",
      "Epoch: 940/2000... Training loss: 0.4590\n",
      "Epoch: 940/2000... Training loss: 0.4688\n",
      "Epoch: 940/2000... Training loss: 0.7143\n",
      "Epoch: 940/2000... Training loss: 0.6858\n",
      "Epoch: 940/2000... Training loss: 0.5353\n",
      "Epoch: 940/2000... Training loss: 0.4278\n",
      "Epoch: 940/2000... Training loss: 0.8338\n",
      "Epoch: 940/2000... Training loss: 0.5425\n",
      "Epoch: 940/2000... Training loss: 0.5483\n",
      "Epoch: 940/2000... Training loss: 0.6682\n",
      "Epoch: 940/2000... Training loss: 0.6702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 940/2000... Training loss: 0.6881\n",
      "Epoch: 940/2000... Training loss: 0.5913\n",
      "Epoch: 940/2000... Training loss: 0.5327\n",
      "Epoch: 940/2000... Training loss: 0.7564\n",
      "Epoch: 940/2000... Training loss: 0.5830\n",
      "Epoch: 940/2000... Training loss: 0.6571\n",
      "Epoch: 940/2000... Training loss: 0.5520\n",
      "Epoch: 940/2000... Training loss: 0.5111\n",
      "Epoch: 940/2000... Training loss: 0.4369\n",
      "Epoch: 940/2000... Training loss: 0.7801\n",
      "Epoch: 940/2000... Training loss: 0.6741\n",
      "Epoch: 940/2000... Training loss: 0.8322\n",
      "Epoch: 940/2000... Training loss: 0.5676\n",
      "Epoch: 940/2000... Training loss: 0.6111\n",
      "Epoch: 941/2000... Training loss: 0.6841\n",
      "Epoch: 941/2000... Training loss: 0.5095\n",
      "Epoch: 941/2000... Training loss: 0.4553\n",
      "Epoch: 941/2000... Training loss: 0.3605\n",
      "Epoch: 941/2000... Training loss: 0.5388\n",
      "Epoch: 941/2000... Training loss: 0.6279\n",
      "Epoch: 941/2000... Training loss: 0.4605\n",
      "Epoch: 941/2000... Training loss: 0.4799\n",
      "Epoch: 941/2000... Training loss: 0.6067\n",
      "Epoch: 941/2000... Training loss: 0.6711\n",
      "Epoch: 941/2000... Training loss: 0.3866\n",
      "Epoch: 941/2000... Training loss: 0.5073\n",
      "Epoch: 941/2000... Training loss: 0.4287\n",
      "Epoch: 941/2000... Training loss: 0.3541\n",
      "Epoch: 941/2000... Training loss: 0.5826\n",
      "Epoch: 941/2000... Training loss: 0.4827\n",
      "Epoch: 941/2000... Training loss: 0.5690\n",
      "Epoch: 941/2000... Training loss: 0.6005\n",
      "Epoch: 941/2000... Training loss: 0.5401\n",
      "Epoch: 941/2000... Training loss: 0.5049\n",
      "Epoch: 941/2000... Training loss: 0.5360\n",
      "Epoch: 941/2000... Training loss: 0.6632\n",
      "Epoch: 941/2000... Training loss: 0.5857\n",
      "Epoch: 941/2000... Training loss: 0.4989\n",
      "Epoch: 941/2000... Training loss: 0.5037\n",
      "Epoch: 941/2000... Training loss: 0.5815\n",
      "Epoch: 941/2000... Training loss: 0.7154\n",
      "Epoch: 941/2000... Training loss: 0.5353\n",
      "Epoch: 941/2000... Training loss: 0.4509\n",
      "Epoch: 941/2000... Training loss: 0.4985\n",
      "Epoch: 941/2000... Training loss: 0.7140\n",
      "Epoch: 942/2000... Training loss: 0.4530\n",
      "Epoch: 942/2000... Training loss: 0.6982\n",
      "Epoch: 942/2000... Training loss: 0.6739\n",
      "Epoch: 942/2000... Training loss: 0.4640\n",
      "Epoch: 942/2000... Training loss: 0.6640\n",
      "Epoch: 942/2000... Training loss: 0.4449\n",
      "Epoch: 942/2000... Training loss: 0.8444\n",
      "Epoch: 942/2000... Training loss: 0.5359\n",
      "Epoch: 942/2000... Training loss: 0.4064\n",
      "Epoch: 942/2000... Training loss: 0.5065\n",
      "Epoch: 942/2000... Training loss: 0.6223\n",
      "Epoch: 942/2000... Training loss: 0.5748\n",
      "Epoch: 942/2000... Training loss: 0.5784\n",
      "Epoch: 942/2000... Training loss: 0.7880\n",
      "Epoch: 942/2000... Training loss: 0.5668\n",
      "Epoch: 942/2000... Training loss: 0.5582\n",
      "Epoch: 942/2000... Training loss: 0.5798\n",
      "Epoch: 942/2000... Training loss: 0.3922\n",
      "Epoch: 942/2000... Training loss: 0.6542\n",
      "Epoch: 942/2000... Training loss: 0.7848\n",
      "Epoch: 942/2000... Training loss: 0.4966\n",
      "Epoch: 942/2000... Training loss: 0.3510\n",
      "Epoch: 942/2000... Training loss: 0.4984\n",
      "Epoch: 942/2000... Training loss: 0.5393\n",
      "Epoch: 942/2000... Training loss: 0.5602\n",
      "Epoch: 942/2000... Training loss: 0.4452\n",
      "Epoch: 942/2000... Training loss: 0.7278\n",
      "Epoch: 942/2000... Training loss: 0.6056\n",
      "Epoch: 942/2000... Training loss: 0.4339\n",
      "Epoch: 942/2000... Training loss: 0.6220\n",
      "Epoch: 942/2000... Training loss: 0.4123\n",
      "Epoch: 943/2000... Training loss: 0.4141\n",
      "Epoch: 943/2000... Training loss: 0.4820\n",
      "Epoch: 943/2000... Training loss: 0.4181\n",
      "Epoch: 943/2000... Training loss: 0.3755\n",
      "Epoch: 943/2000... Training loss: 0.5916\n",
      "Epoch: 943/2000... Training loss: 0.5558\n",
      "Epoch: 943/2000... Training loss: 0.5374\n",
      "Epoch: 943/2000... Training loss: 0.5947\n",
      "Epoch: 943/2000... Training loss: 0.8480\n",
      "Epoch: 943/2000... Training loss: 0.5304\n",
      "Epoch: 943/2000... Training loss: 0.4836\n",
      "Epoch: 943/2000... Training loss: 0.3955\n",
      "Epoch: 943/2000... Training loss: 0.4554\n",
      "Epoch: 943/2000... Training loss: 0.5629\n",
      "Epoch: 943/2000... Training loss: 0.5378\n",
      "Epoch: 943/2000... Training loss: 0.6410\n",
      "Epoch: 943/2000... Training loss: 0.6204\n",
      "Epoch: 943/2000... Training loss: 0.5001\n",
      "Epoch: 943/2000... Training loss: 0.4110\n",
      "Epoch: 943/2000... Training loss: 0.4807\n",
      "Epoch: 943/2000... Training loss: 0.7079\n",
      "Epoch: 943/2000... Training loss: 0.3548\n",
      "Epoch: 943/2000... Training loss: 0.6572\n",
      "Epoch: 943/2000... Training loss: 0.4530\n",
      "Epoch: 943/2000... Training loss: 0.5800\n",
      "Epoch: 943/2000... Training loss: 0.6886\n",
      "Epoch: 943/2000... Training loss: 0.7724\n",
      "Epoch: 943/2000... Training loss: 0.5157\n",
      "Epoch: 943/2000... Training loss: 0.5774\n",
      "Epoch: 943/2000... Training loss: 0.5443\n",
      "Epoch: 943/2000... Training loss: 0.4745\n",
      "Epoch: 944/2000... Training loss: 0.5080\n",
      "Epoch: 944/2000... Training loss: 0.3192\n",
      "Epoch: 944/2000... Training loss: 0.6515\n",
      "Epoch: 944/2000... Training loss: 0.3204\n",
      "Epoch: 944/2000... Training loss: 0.6103\n",
      "Epoch: 944/2000... Training loss: 0.5372\n",
      "Epoch: 944/2000... Training loss: 0.5072\n",
      "Epoch: 944/2000... Training loss: 0.7522\n",
      "Epoch: 944/2000... Training loss: 0.6222\n",
      "Epoch: 944/2000... Training loss: 0.6074\n",
      "Epoch: 944/2000... Training loss: 0.8419\n",
      "Epoch: 944/2000... Training loss: 0.7257\n",
      "Epoch: 944/2000... Training loss: 0.5692\n",
      "Epoch: 944/2000... Training loss: 0.4602\n",
      "Epoch: 944/2000... Training loss: 0.5085\n",
      "Epoch: 944/2000... Training loss: 0.4356\n",
      "Epoch: 944/2000... Training loss: 0.5263\n",
      "Epoch: 944/2000... Training loss: 0.5594\n",
      "Epoch: 944/2000... Training loss: 0.7201\n",
      "Epoch: 944/2000... Training loss: 0.5908\n",
      "Epoch: 944/2000... Training loss: 0.4840\n",
      "Epoch: 944/2000... Training loss: 0.4473\n",
      "Epoch: 944/2000... Training loss: 0.6144\n",
      "Epoch: 944/2000... Training loss: 0.3064\n",
      "Epoch: 944/2000... Training loss: 0.8051\n",
      "Epoch: 944/2000... Training loss: 0.4269\n",
      "Epoch: 944/2000... Training loss: 0.5331\n",
      "Epoch: 944/2000... Training loss: 0.7553\n",
      "Epoch: 944/2000... Training loss: 0.7435\n",
      "Epoch: 944/2000... Training loss: 0.4713\n",
      "Epoch: 944/2000... Training loss: 0.5618\n",
      "Epoch: 945/2000... Training loss: 0.6079\n",
      "Epoch: 945/2000... Training loss: 0.6163\n",
      "Epoch: 945/2000... Training loss: 0.6114\n",
      "Epoch: 945/2000... Training loss: 0.4912\n",
      "Epoch: 945/2000... Training loss: 0.4312\n",
      "Epoch: 945/2000... Training loss: 0.4833\n",
      "Epoch: 945/2000... Training loss: 0.5685\n",
      "Epoch: 945/2000... Training loss: 0.4674\n",
      "Epoch: 945/2000... Training loss: 0.5539\n",
      "Epoch: 945/2000... Training loss: 0.4185\n",
      "Epoch: 945/2000... Training loss: 0.5857\n",
      "Epoch: 945/2000... Training loss: 0.5535\n",
      "Epoch: 945/2000... Training loss: 0.5287\n",
      "Epoch: 945/2000... Training loss: 0.3709\n",
      "Epoch: 945/2000... Training loss: 0.4788\n",
      "Epoch: 945/2000... Training loss: 0.7806\n",
      "Epoch: 945/2000... Training loss: 0.4177\n",
      "Epoch: 945/2000... Training loss: 0.4928\n",
      "Epoch: 945/2000... Training loss: 0.5479\n",
      "Epoch: 945/2000... Training loss: 0.6339\n",
      "Epoch: 945/2000... Training loss: 0.4930\n",
      "Epoch: 945/2000... Training loss: 0.3952\n",
      "Epoch: 945/2000... Training loss: 0.4487\n",
      "Epoch: 945/2000... Training loss: 0.4705\n",
      "Epoch: 945/2000... Training loss: 0.5427\n",
      "Epoch: 945/2000... Training loss: 0.6314\n",
      "Epoch: 945/2000... Training loss: 0.5165\n",
      "Epoch: 945/2000... Training loss: 0.5419\n",
      "Epoch: 945/2000... Training loss: 0.4058\n",
      "Epoch: 945/2000... Training loss: 0.6605\n",
      "Epoch: 945/2000... Training loss: 0.3460\n",
      "Epoch: 946/2000... Training loss: 0.4314\n",
      "Epoch: 946/2000... Training loss: 0.6363\n",
      "Epoch: 946/2000... Training loss: 0.6163\n",
      "Epoch: 946/2000... Training loss: 0.3307\n",
      "Epoch: 946/2000... Training loss: 0.5249\n",
      "Epoch: 946/2000... Training loss: 0.5322\n",
      "Epoch: 946/2000... Training loss: 0.6222\n",
      "Epoch: 946/2000... Training loss: 0.5867\n",
      "Epoch: 946/2000... Training loss: 0.4533\n",
      "Epoch: 946/2000... Training loss: 0.5189\n",
      "Epoch: 946/2000... Training loss: 0.5811\n",
      "Epoch: 946/2000... Training loss: 0.4788\n",
      "Epoch: 946/2000... Training loss: 0.7049\n",
      "Epoch: 946/2000... Training loss: 0.3881\n",
      "Epoch: 946/2000... Training loss: 0.5140\n",
      "Epoch: 946/2000... Training loss: 0.5021\n",
      "Epoch: 946/2000... Training loss: 0.5040\n",
      "Epoch: 946/2000... Training loss: 0.5777\n",
      "Epoch: 946/2000... Training loss: 0.6404\n",
      "Epoch: 946/2000... Training loss: 0.6500\n",
      "Epoch: 946/2000... Training loss: 0.5678\n",
      "Epoch: 946/2000... Training loss: 0.4118\n",
      "Epoch: 946/2000... Training loss: 0.5436\n",
      "Epoch: 946/2000... Training loss: 0.5776\n",
      "Epoch: 946/2000... Training loss: 0.5258\n",
      "Epoch: 946/2000... Training loss: 0.6936\n",
      "Epoch: 946/2000... Training loss: 0.5487\n",
      "Epoch: 946/2000... Training loss: 0.4970\n",
      "Epoch: 946/2000... Training loss: 0.4398\n",
      "Epoch: 946/2000... Training loss: 0.5735\n",
      "Epoch: 946/2000... Training loss: 0.6050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 947/2000... Training loss: 0.3092\n",
      "Epoch: 947/2000... Training loss: 0.4043\n",
      "Epoch: 947/2000... Training loss: 0.4032\n",
      "Epoch: 947/2000... Training loss: 0.6460\n",
      "Epoch: 947/2000... Training loss: 0.5465\n",
      "Epoch: 947/2000... Training loss: 0.5913\n",
      "Epoch: 947/2000... Training loss: 0.5775\n",
      "Epoch: 947/2000... Training loss: 0.4524\n",
      "Epoch: 947/2000... Training loss: 0.6171\n",
      "Epoch: 947/2000... Training loss: 0.4974\n",
      "Epoch: 947/2000... Training loss: 0.6289\n",
      "Epoch: 947/2000... Training loss: 0.3976\n",
      "Epoch: 947/2000... Training loss: 0.4024\n",
      "Epoch: 947/2000... Training loss: 0.6125\n",
      "Epoch: 947/2000... Training loss: 0.4430\n",
      "Epoch: 947/2000... Training loss: 0.4800\n",
      "Epoch: 947/2000... Training loss: 0.4655\n",
      "Epoch: 947/2000... Training loss: 0.4675\n",
      "Epoch: 947/2000... Training loss: 0.4546\n",
      "Epoch: 947/2000... Training loss: 0.5505\n",
      "Epoch: 947/2000... Training loss: 0.5325\n",
      "Epoch: 947/2000... Training loss: 0.5194\n",
      "Epoch: 947/2000... Training loss: 0.5124\n",
      "Epoch: 947/2000... Training loss: 0.7291\n",
      "Epoch: 947/2000... Training loss: 0.7770\n",
      "Epoch: 947/2000... Training loss: 0.3865\n",
      "Epoch: 947/2000... Training loss: 0.5319\n",
      "Epoch: 947/2000... Training loss: 0.5382\n",
      "Epoch: 947/2000... Training loss: 0.6985\n",
      "Epoch: 947/2000... Training loss: 0.4510\n",
      "Epoch: 947/2000... Training loss: 0.5486\n",
      "Epoch: 948/2000... Training loss: 0.5937\n",
      "Epoch: 948/2000... Training loss: 0.6554\n",
      "Epoch: 948/2000... Training loss: 0.5928\n",
      "Epoch: 948/2000... Training loss: 0.4155\n",
      "Epoch: 948/2000... Training loss: 0.5924\n",
      "Epoch: 948/2000... Training loss: 0.5629\n",
      "Epoch: 948/2000... Training loss: 0.6553\n",
      "Epoch: 948/2000... Training loss: 0.5457\n",
      "Epoch: 948/2000... Training loss: 0.6749\n",
      "Epoch: 948/2000... Training loss: 0.5811\n",
      "Epoch: 948/2000... Training loss: 0.6424\n",
      "Epoch: 948/2000... Training loss: 0.4135\n",
      "Epoch: 948/2000... Training loss: 0.4846\n",
      "Epoch: 948/2000... Training loss: 0.6686\n",
      "Epoch: 948/2000... Training loss: 0.5273\n",
      "Epoch: 948/2000... Training loss: 0.3865\n",
      "Epoch: 948/2000... Training loss: 0.6794\n",
      "Epoch: 948/2000... Training loss: 0.5145\n",
      "Epoch: 948/2000... Training loss: 0.4467\n",
      "Epoch: 948/2000... Training loss: 0.3996\n",
      "Epoch: 948/2000... Training loss: 0.5738\n",
      "Epoch: 948/2000... Training loss: 0.4679\n",
      "Epoch: 948/2000... Training loss: 0.6217\n",
      "Epoch: 948/2000... Training loss: 0.5412\n",
      "Epoch: 948/2000... Training loss: 0.6805\n",
      "Epoch: 948/2000... Training loss: 0.6244\n",
      "Epoch: 948/2000... Training loss: 0.7019\n",
      "Epoch: 948/2000... Training loss: 0.4402\n",
      "Epoch: 948/2000... Training loss: 0.5130\n",
      "Epoch: 948/2000... Training loss: 0.4712\n",
      "Epoch: 948/2000... Training loss: 0.5142\n",
      "Epoch: 949/2000... Training loss: 0.5745\n",
      "Epoch: 949/2000... Training loss: 0.6775\n",
      "Epoch: 949/2000... Training loss: 0.3940\n",
      "Epoch: 949/2000... Training loss: 0.3816\n",
      "Epoch: 949/2000... Training loss: 0.4195\n",
      "Epoch: 949/2000... Training loss: 0.3529\n",
      "Epoch: 949/2000... Training loss: 0.4779\n",
      "Epoch: 949/2000... Training loss: 0.8299\n",
      "Epoch: 949/2000... Training loss: 0.3636\n",
      "Epoch: 949/2000... Training loss: 0.4344\n",
      "Epoch: 949/2000... Training loss: 0.4016\n",
      "Epoch: 949/2000... Training loss: 0.7554\n",
      "Epoch: 949/2000... Training loss: 0.4594\n",
      "Epoch: 949/2000... Training loss: 0.3170\n",
      "Epoch: 949/2000... Training loss: 0.4700\n",
      "Epoch: 949/2000... Training loss: 0.5933\n",
      "Epoch: 949/2000... Training loss: 0.4806\n",
      "Epoch: 949/2000... Training loss: 0.4655\n",
      "Epoch: 949/2000... Training loss: 0.6868\n",
      "Epoch: 949/2000... Training loss: 0.5701\n",
      "Epoch: 949/2000... Training loss: 0.6279\n",
      "Epoch: 949/2000... Training loss: 0.4996\n",
      "Epoch: 949/2000... Training loss: 0.5542\n",
      "Epoch: 949/2000... Training loss: 0.6127\n",
      "Epoch: 949/2000... Training loss: 0.6418\n",
      "Epoch: 949/2000... Training loss: 0.4383\n",
      "Epoch: 949/2000... Training loss: 0.5041\n",
      "Epoch: 949/2000... Training loss: 0.5576\n",
      "Epoch: 949/2000... Training loss: 0.5798\n",
      "Epoch: 949/2000... Training loss: 0.5287\n",
      "Epoch: 949/2000... Training loss: 0.6492\n",
      "Epoch: 950/2000... Training loss: 0.4819\n",
      "Epoch: 950/2000... Training loss: 0.5732\n",
      "Epoch: 950/2000... Training loss: 0.4352\n",
      "Epoch: 950/2000... Training loss: 0.5488\n",
      "Epoch: 950/2000... Training loss: 0.4998\n",
      "Epoch: 950/2000... Training loss: 0.5034\n",
      "Epoch: 950/2000... Training loss: 0.5358\n",
      "Epoch: 950/2000... Training loss: 0.5042\n",
      "Epoch: 950/2000... Training loss: 0.6772\n",
      "Epoch: 950/2000... Training loss: 0.6127\n",
      "Epoch: 950/2000... Training loss: 0.4267\n",
      "Epoch: 950/2000... Training loss: 0.4766\n",
      "Epoch: 950/2000... Training loss: 0.5048\n",
      "Epoch: 950/2000... Training loss: 0.5251\n",
      "Epoch: 950/2000... Training loss: 0.4780\n",
      "Epoch: 950/2000... Training loss: 0.4545\n",
      "Epoch: 950/2000... Training loss: 0.5770\n",
      "Epoch: 950/2000... Training loss: 0.4551\n",
      "Epoch: 950/2000... Training loss: 0.5671\n",
      "Epoch: 950/2000... Training loss: 0.4894\n",
      "Epoch: 950/2000... Training loss: 0.5200\n",
      "Epoch: 950/2000... Training loss: 0.6210\n",
      "Epoch: 950/2000... Training loss: 0.7599\n",
      "Epoch: 950/2000... Training loss: 0.6695\n",
      "Epoch: 950/2000... Training loss: 0.5117\n",
      "Epoch: 950/2000... Training loss: 0.5465\n",
      "Epoch: 950/2000... Training loss: 0.6745\n",
      "Epoch: 950/2000... Training loss: 0.4707\n",
      "Epoch: 950/2000... Training loss: 0.6804\n",
      "Epoch: 950/2000... Training loss: 0.4645\n",
      "Epoch: 950/2000... Training loss: 0.5044\n",
      "Epoch: 951/2000... Training loss: 0.5555\n",
      "Epoch: 951/2000... Training loss: 0.5781\n",
      "Epoch: 951/2000... Training loss: 0.5610\n",
      "Epoch: 951/2000... Training loss: 0.5300\n",
      "Epoch: 951/2000... Training loss: 0.5039\n",
      "Epoch: 951/2000... Training loss: 0.6180\n",
      "Epoch: 951/2000... Training loss: 0.5226\n",
      "Epoch: 951/2000... Training loss: 0.4152\n",
      "Epoch: 951/2000... Training loss: 0.2507\n",
      "Epoch: 951/2000... Training loss: 0.6436\n",
      "Epoch: 951/2000... Training loss: 0.4534\n",
      "Epoch: 951/2000... Training loss: 0.5347\n",
      "Epoch: 951/2000... Training loss: 0.6575\n",
      "Epoch: 951/2000... Training loss: 0.4263\n",
      "Epoch: 951/2000... Training loss: 0.3973\n",
      "Epoch: 951/2000... Training loss: 0.4243\n",
      "Epoch: 951/2000... Training loss: 0.5670\n",
      "Epoch: 951/2000... Training loss: 0.6025\n",
      "Epoch: 951/2000... Training loss: 0.4518\n",
      "Epoch: 951/2000... Training loss: 0.4174\n",
      "Epoch: 951/2000... Training loss: 0.4434\n",
      "Epoch: 951/2000... Training loss: 0.4649\n",
      "Epoch: 951/2000... Training loss: 0.6074\n",
      "Epoch: 951/2000... Training loss: 0.5495\n",
      "Epoch: 951/2000... Training loss: 0.3700\n",
      "Epoch: 951/2000... Training loss: 0.5502\n",
      "Epoch: 951/2000... Training loss: 0.6509\n",
      "Epoch: 951/2000... Training loss: 0.4149\n",
      "Epoch: 951/2000... Training loss: 0.4326\n",
      "Epoch: 951/2000... Training loss: 0.4532\n",
      "Epoch: 951/2000... Training loss: 0.3720\n",
      "Epoch: 952/2000... Training loss: 0.4154\n",
      "Epoch: 952/2000... Training loss: 0.4518\n",
      "Epoch: 952/2000... Training loss: 0.4959\n",
      "Epoch: 952/2000... Training loss: 0.5785\n",
      "Epoch: 952/2000... Training loss: 0.5514\n",
      "Epoch: 952/2000... Training loss: 0.3979\n",
      "Epoch: 952/2000... Training loss: 0.6467\n",
      "Epoch: 952/2000... Training loss: 0.4940\n",
      "Epoch: 952/2000... Training loss: 0.5717\n",
      "Epoch: 952/2000... Training loss: 0.4600\n",
      "Epoch: 952/2000... Training loss: 0.5143\n",
      "Epoch: 952/2000... Training loss: 0.6282\n",
      "Epoch: 952/2000... Training loss: 0.5499\n",
      "Epoch: 952/2000... Training loss: 0.6426\n",
      "Epoch: 952/2000... Training loss: 0.5706\n",
      "Epoch: 952/2000... Training loss: 0.5146\n",
      "Epoch: 952/2000... Training loss: 0.5287\n",
      "Epoch: 952/2000... Training loss: 0.4700\n",
      "Epoch: 952/2000... Training loss: 0.4956\n",
      "Epoch: 952/2000... Training loss: 0.6551\n",
      "Epoch: 952/2000... Training loss: 0.6192\n",
      "Epoch: 952/2000... Training loss: 0.5413\n",
      "Epoch: 952/2000... Training loss: 0.4319\n",
      "Epoch: 952/2000... Training loss: 0.5080\n",
      "Epoch: 952/2000... Training loss: 0.5834\n",
      "Epoch: 952/2000... Training loss: 0.7188\n",
      "Epoch: 952/2000... Training loss: 0.5868\n",
      "Epoch: 952/2000... Training loss: 0.4299\n",
      "Epoch: 952/2000... Training loss: 0.4638\n",
      "Epoch: 952/2000... Training loss: 0.4787\n",
      "Epoch: 952/2000... Training loss: 0.5307\n",
      "Epoch: 953/2000... Training loss: 0.5318\n",
      "Epoch: 953/2000... Training loss: 0.5175\n",
      "Epoch: 953/2000... Training loss: 0.6407\n",
      "Epoch: 953/2000... Training loss: 0.6372\n",
      "Epoch: 953/2000... Training loss: 0.4620\n",
      "Epoch: 953/2000... Training loss: 0.4127\n",
      "Epoch: 953/2000... Training loss: 0.5038\n",
      "Epoch: 953/2000... Training loss: 0.4485\n",
      "Epoch: 953/2000... Training loss: 0.6246\n",
      "Epoch: 953/2000... Training loss: 0.6030\n",
      "Epoch: 953/2000... Training loss: 0.4300\n",
      "Epoch: 953/2000... Training loss: 0.5087\n",
      "Epoch: 953/2000... Training loss: 0.9215\n",
      "Epoch: 953/2000... Training loss: 0.6289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 953/2000... Training loss: 0.6266\n",
      "Epoch: 953/2000... Training loss: 0.3623\n",
      "Epoch: 953/2000... Training loss: 0.6298\n",
      "Epoch: 953/2000... Training loss: 0.5385\n",
      "Epoch: 953/2000... Training loss: 0.4152\n",
      "Epoch: 953/2000... Training loss: 0.5972\n",
      "Epoch: 953/2000... Training loss: 0.5343\n",
      "Epoch: 953/2000... Training loss: 0.3314\n",
      "Epoch: 953/2000... Training loss: 0.5985\n",
      "Epoch: 953/2000... Training loss: 0.4863\n",
      "Epoch: 953/2000... Training loss: 0.5013\n",
      "Epoch: 953/2000... Training loss: 0.4659\n",
      "Epoch: 953/2000... Training loss: 0.6158\n",
      "Epoch: 953/2000... Training loss: 0.6531\n",
      "Epoch: 953/2000... Training loss: 0.3829\n",
      "Epoch: 953/2000... Training loss: 0.5737\n",
      "Epoch: 953/2000... Training loss: 0.6168\n",
      "Epoch: 954/2000... Training loss: 0.5345\n",
      "Epoch: 954/2000... Training loss: 0.6390\n",
      "Epoch: 954/2000... Training loss: 0.4800\n",
      "Epoch: 954/2000... Training loss: 0.5989\n",
      "Epoch: 954/2000... Training loss: 0.5640\n",
      "Epoch: 954/2000... Training loss: 0.3574\n",
      "Epoch: 954/2000... Training loss: 0.4837\n",
      "Epoch: 954/2000... Training loss: 0.4486\n",
      "Epoch: 954/2000... Training loss: 0.4282\n",
      "Epoch: 954/2000... Training loss: 0.4063\n",
      "Epoch: 954/2000... Training loss: 0.4271\n",
      "Epoch: 954/2000... Training loss: 0.5024\n",
      "Epoch: 954/2000... Training loss: 0.4258\n",
      "Epoch: 954/2000... Training loss: 0.3433\n",
      "Epoch: 954/2000... Training loss: 0.4564\n",
      "Epoch: 954/2000... Training loss: 0.4604\n",
      "Epoch: 954/2000... Training loss: 0.6119\n",
      "Epoch: 954/2000... Training loss: 0.5252\n",
      "Epoch: 954/2000... Training loss: 0.7052\n",
      "Epoch: 954/2000... Training loss: 0.8472\n",
      "Epoch: 954/2000... Training loss: 0.5337\n",
      "Epoch: 954/2000... Training loss: 0.4337\n",
      "Epoch: 954/2000... Training loss: 0.5324\n",
      "Epoch: 954/2000... Training loss: 0.4093\n",
      "Epoch: 954/2000... Training loss: 0.4366\n",
      "Epoch: 954/2000... Training loss: 0.5765\n",
      "Epoch: 954/2000... Training loss: 0.3713\n",
      "Epoch: 954/2000... Training loss: 0.5748\n",
      "Epoch: 954/2000... Training loss: 0.5587\n",
      "Epoch: 954/2000... Training loss: 0.5289\n",
      "Epoch: 954/2000... Training loss: 0.5061\n",
      "Epoch: 955/2000... Training loss: 0.4770\n",
      "Epoch: 955/2000... Training loss: 0.4696\n",
      "Epoch: 955/2000... Training loss: 0.6155\n",
      "Epoch: 955/2000... Training loss: 0.4911\n",
      "Epoch: 955/2000... Training loss: 0.6060\n",
      "Epoch: 955/2000... Training loss: 0.5842\n",
      "Epoch: 955/2000... Training loss: 0.3757\n",
      "Epoch: 955/2000... Training loss: 0.7248\n",
      "Epoch: 955/2000... Training loss: 0.5047\n",
      "Epoch: 955/2000... Training loss: 0.5241\n",
      "Epoch: 955/2000... Training loss: 0.5418\n",
      "Epoch: 955/2000... Training loss: 0.4549\n",
      "Epoch: 955/2000... Training loss: 0.4046\n",
      "Epoch: 955/2000... Training loss: 0.5352\n",
      "Epoch: 955/2000... Training loss: 0.4932\n",
      "Epoch: 955/2000... Training loss: 0.5044\n",
      "Epoch: 955/2000... Training loss: 0.6472\n",
      "Epoch: 955/2000... Training loss: 0.7899\n",
      "Epoch: 955/2000... Training loss: 0.4257\n",
      "Epoch: 955/2000... Training loss: 0.4202\n",
      "Epoch: 955/2000... Training loss: 0.3966\n",
      "Epoch: 955/2000... Training loss: 0.7086\n",
      "Epoch: 955/2000... Training loss: 0.5690\n",
      "Epoch: 955/2000... Training loss: 0.5055\n",
      "Epoch: 955/2000... Training loss: 0.6074\n",
      "Epoch: 955/2000... Training loss: 0.4673\n",
      "Epoch: 955/2000... Training loss: 0.5946\n",
      "Epoch: 955/2000... Training loss: 0.6386\n",
      "Epoch: 955/2000... Training loss: 0.8288\n",
      "Epoch: 955/2000... Training loss: 0.6735\n",
      "Epoch: 955/2000... Training loss: 0.6113\n",
      "Epoch: 956/2000... Training loss: 0.5053\n",
      "Epoch: 956/2000... Training loss: 0.4644\n",
      "Epoch: 956/2000... Training loss: 0.5377\n",
      "Epoch: 956/2000... Training loss: 0.4269\n",
      "Epoch: 956/2000... Training loss: 0.6344\n",
      "Epoch: 956/2000... Training loss: 0.4502\n",
      "Epoch: 956/2000... Training loss: 0.6471\n",
      "Epoch: 956/2000... Training loss: 0.6741\n",
      "Epoch: 956/2000... Training loss: 0.5424\n",
      "Epoch: 956/2000... Training loss: 0.5360\n",
      "Epoch: 956/2000... Training loss: 0.5156\n",
      "Epoch: 956/2000... Training loss: 0.7074\n",
      "Epoch: 956/2000... Training loss: 0.5638\n",
      "Epoch: 956/2000... Training loss: 0.8559\n",
      "Epoch: 956/2000... Training loss: 0.5641\n",
      "Epoch: 956/2000... Training loss: 0.6851\n",
      "Epoch: 956/2000... Training loss: 0.3787\n",
      "Epoch: 956/2000... Training loss: 0.4404\n",
      "Epoch: 956/2000... Training loss: 0.5328\n",
      "Epoch: 956/2000... Training loss: 0.5155\n",
      "Epoch: 956/2000... Training loss: 0.5449\n",
      "Epoch: 956/2000... Training loss: 0.7153\n",
      "Epoch: 956/2000... Training loss: 0.9320\n",
      "Epoch: 956/2000... Training loss: 0.5786\n",
      "Epoch: 956/2000... Training loss: 0.5014\n",
      "Epoch: 956/2000... Training loss: 0.6636\n",
      "Epoch: 956/2000... Training loss: 0.3471\n",
      "Epoch: 956/2000... Training loss: 0.5398\n",
      "Epoch: 956/2000... Training loss: 0.7347\n",
      "Epoch: 956/2000... Training loss: 0.4383\n",
      "Epoch: 956/2000... Training loss: 0.5021\n",
      "Epoch: 957/2000... Training loss: 0.6408\n",
      "Epoch: 957/2000... Training loss: 0.5082\n",
      "Epoch: 957/2000... Training loss: 0.7122\n",
      "Epoch: 957/2000... Training loss: 0.5025\n",
      "Epoch: 957/2000... Training loss: 0.4644\n",
      "Epoch: 957/2000... Training loss: 0.4830\n",
      "Epoch: 957/2000... Training loss: 0.6335\n",
      "Epoch: 957/2000... Training loss: 0.5534\n",
      "Epoch: 957/2000... Training loss: 0.5259\n",
      "Epoch: 957/2000... Training loss: 0.4767\n",
      "Epoch: 957/2000... Training loss: 0.5655\n",
      "Epoch: 957/2000... Training loss: 0.4270\n",
      "Epoch: 957/2000... Training loss: 0.4781\n",
      "Epoch: 957/2000... Training loss: 0.5178\n",
      "Epoch: 957/2000... Training loss: 0.5543\n",
      "Epoch: 957/2000... Training loss: 0.6749\n",
      "Epoch: 957/2000... Training loss: 0.5592\n",
      "Epoch: 957/2000... Training loss: 0.6797\n",
      "Epoch: 957/2000... Training loss: 0.3681\n",
      "Epoch: 957/2000... Training loss: 0.3838\n",
      "Epoch: 957/2000... Training loss: 0.6826\n",
      "Epoch: 957/2000... Training loss: 0.5616\n",
      "Epoch: 957/2000... Training loss: 0.4703\n",
      "Epoch: 957/2000... Training loss: 0.4756\n",
      "Epoch: 957/2000... Training loss: 0.5890\n",
      "Epoch: 957/2000... Training loss: 0.5021\n",
      "Epoch: 957/2000... Training loss: 0.5884\n",
      "Epoch: 957/2000... Training loss: 0.6507\n",
      "Epoch: 957/2000... Training loss: 0.5335\n",
      "Epoch: 957/2000... Training loss: 0.6503\n",
      "Epoch: 957/2000... Training loss: 0.4473\n",
      "Epoch: 958/2000... Training loss: 0.8482\n",
      "Epoch: 958/2000... Training loss: 0.4358\n",
      "Epoch: 958/2000... Training loss: 0.5869\n",
      "Epoch: 958/2000... Training loss: 0.6204\n",
      "Epoch: 958/2000... Training loss: 0.7252\n",
      "Epoch: 958/2000... Training loss: 0.5009\n",
      "Epoch: 958/2000... Training loss: 0.5876\n",
      "Epoch: 958/2000... Training loss: 0.4830\n",
      "Epoch: 958/2000... Training loss: 0.5793\n",
      "Epoch: 958/2000... Training loss: 0.3855\n",
      "Epoch: 958/2000... Training loss: 0.3827\n",
      "Epoch: 958/2000... Training loss: 0.6446\n",
      "Epoch: 958/2000... Training loss: 0.5058\n",
      "Epoch: 958/2000... Training loss: 0.4414\n",
      "Epoch: 958/2000... Training loss: 0.6166\n",
      "Epoch: 958/2000... Training loss: 0.5827\n",
      "Epoch: 958/2000... Training loss: 0.4143\n",
      "Epoch: 958/2000... Training loss: 0.4209\n",
      "Epoch: 958/2000... Training loss: 0.5946\n",
      "Epoch: 958/2000... Training loss: 0.5452\n",
      "Epoch: 958/2000... Training loss: 0.3938\n",
      "Epoch: 958/2000... Training loss: 0.4334\n",
      "Epoch: 958/2000... Training loss: 0.5407\n",
      "Epoch: 958/2000... Training loss: 0.6449\n",
      "Epoch: 958/2000... Training loss: 0.4967\n",
      "Epoch: 958/2000... Training loss: 0.5319\n",
      "Epoch: 958/2000... Training loss: 0.4544\n",
      "Epoch: 958/2000... Training loss: 0.6607\n",
      "Epoch: 958/2000... Training loss: 0.3392\n",
      "Epoch: 958/2000... Training loss: 0.6656\n",
      "Epoch: 958/2000... Training loss: 0.5006\n",
      "Epoch: 959/2000... Training loss: 0.5382\n",
      "Epoch: 959/2000... Training loss: 0.5233\n",
      "Epoch: 959/2000... Training loss: 0.4980\n",
      "Epoch: 959/2000... Training loss: 0.4479\n",
      "Epoch: 959/2000... Training loss: 0.8646\n",
      "Epoch: 959/2000... Training loss: 0.5897\n",
      "Epoch: 959/2000... Training loss: 0.7635\n",
      "Epoch: 959/2000... Training loss: 0.5675\n",
      "Epoch: 959/2000... Training loss: 0.6210\n",
      "Epoch: 959/2000... Training loss: 0.5710\n",
      "Epoch: 959/2000... Training loss: 0.5184\n",
      "Epoch: 959/2000... Training loss: 0.5695\n",
      "Epoch: 959/2000... Training loss: 0.4531\n",
      "Epoch: 959/2000... Training loss: 0.4661\n",
      "Epoch: 959/2000... Training loss: 0.4804\n",
      "Epoch: 959/2000... Training loss: 0.4776\n",
      "Epoch: 959/2000... Training loss: 0.6565\n",
      "Epoch: 959/2000... Training loss: 0.4032\n",
      "Epoch: 959/2000... Training loss: 0.5315\n",
      "Epoch: 959/2000... Training loss: 0.6718\n",
      "Epoch: 959/2000... Training loss: 0.5514\n",
      "Epoch: 959/2000... Training loss: 0.4825\n",
      "Epoch: 959/2000... Training loss: 0.4644\n",
      "Epoch: 959/2000... Training loss: 0.5329\n",
      "Epoch: 959/2000... Training loss: 0.4044\n",
      "Epoch: 959/2000... Training loss: 0.7414\n",
      "Epoch: 959/2000... Training loss: 0.5428\n",
      "Epoch: 959/2000... Training loss: 0.5116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 959/2000... Training loss: 0.6092\n",
      "Epoch: 959/2000... Training loss: 0.5241\n",
      "Epoch: 959/2000... Training loss: 0.5926\n",
      "Epoch: 960/2000... Training loss: 0.8593\n",
      "Epoch: 960/2000... Training loss: 0.3804\n",
      "Epoch: 960/2000... Training loss: 0.4177\n",
      "Epoch: 960/2000... Training loss: 0.6346\n",
      "Epoch: 960/2000... Training loss: 0.4316\n",
      "Epoch: 960/2000... Training loss: 0.5493\n",
      "Epoch: 960/2000... Training loss: 0.5612\n",
      "Epoch: 960/2000... Training loss: 0.4911\n",
      "Epoch: 960/2000... Training loss: 0.7145\n",
      "Epoch: 960/2000... Training loss: 0.4740\n",
      "Epoch: 960/2000... Training loss: 0.5857\n",
      "Epoch: 960/2000... Training loss: 0.7442\n",
      "Epoch: 960/2000... Training loss: 0.4848\n",
      "Epoch: 960/2000... Training loss: 0.6993\n",
      "Epoch: 960/2000... Training loss: 0.6360\n",
      "Epoch: 960/2000... Training loss: 0.8047\n",
      "Epoch: 960/2000... Training loss: 0.7094\n",
      "Epoch: 960/2000... Training loss: 0.7808\n",
      "Epoch: 960/2000... Training loss: 0.6025\n",
      "Epoch: 960/2000... Training loss: 0.5590\n",
      "Epoch: 960/2000... Training loss: 0.3623\n",
      "Epoch: 960/2000... Training loss: 0.4103\n",
      "Epoch: 960/2000... Training loss: 0.6680\n",
      "Epoch: 960/2000... Training loss: 0.3899\n",
      "Epoch: 960/2000... Training loss: 0.5964\n",
      "Epoch: 960/2000... Training loss: 0.4688\n",
      "Epoch: 960/2000... Training loss: 0.5154\n",
      "Epoch: 960/2000... Training loss: 0.4597\n",
      "Epoch: 960/2000... Training loss: 0.4666\n",
      "Epoch: 960/2000... Training loss: 0.6017\n",
      "Epoch: 960/2000... Training loss: 0.5538\n",
      "Epoch: 961/2000... Training loss: 0.6349\n",
      "Epoch: 961/2000... Training loss: 0.6720\n",
      "Epoch: 961/2000... Training loss: 0.5241\n",
      "Epoch: 961/2000... Training loss: 0.5528\n",
      "Epoch: 961/2000... Training loss: 0.3476\n",
      "Epoch: 961/2000... Training loss: 0.3731\n",
      "Epoch: 961/2000... Training loss: 0.7696\n",
      "Epoch: 961/2000... Training loss: 0.5027\n",
      "Epoch: 961/2000... Training loss: 0.7085\n",
      "Epoch: 961/2000... Training loss: 0.5316\n",
      "Epoch: 961/2000... Training loss: 0.6537\n",
      "Epoch: 961/2000... Training loss: 0.5955\n",
      "Epoch: 961/2000... Training loss: 0.4841\n",
      "Epoch: 961/2000... Training loss: 0.5246\n",
      "Epoch: 961/2000... Training loss: 0.6529\n",
      "Epoch: 961/2000... Training loss: 0.4617\n",
      "Epoch: 961/2000... Training loss: 0.5398\n",
      "Epoch: 961/2000... Training loss: 0.5010\n",
      "Epoch: 961/2000... Training loss: 0.4154\n",
      "Epoch: 961/2000... Training loss: 0.5083\n",
      "Epoch: 961/2000... Training loss: 0.3549\n",
      "Epoch: 961/2000... Training loss: 0.4262\n",
      "Epoch: 961/2000... Training loss: 0.4978\n",
      "Epoch: 961/2000... Training loss: 0.6033\n",
      "Epoch: 961/2000... Training loss: 0.5520\n",
      "Epoch: 961/2000... Training loss: 0.4989\n",
      "Epoch: 961/2000... Training loss: 0.3528\n",
      "Epoch: 961/2000... Training loss: 0.5995\n",
      "Epoch: 961/2000... Training loss: 0.3425\n",
      "Epoch: 961/2000... Training loss: 0.5307\n",
      "Epoch: 961/2000... Training loss: 0.4757\n",
      "Epoch: 962/2000... Training loss: 0.3594\n",
      "Epoch: 962/2000... Training loss: 0.4560\n",
      "Epoch: 962/2000... Training loss: 0.6809\n",
      "Epoch: 962/2000... Training loss: 0.6895\n",
      "Epoch: 962/2000... Training loss: 0.6622\n",
      "Epoch: 962/2000... Training loss: 0.5591\n",
      "Epoch: 962/2000... Training loss: 0.5891\n",
      "Epoch: 962/2000... Training loss: 0.4212\n",
      "Epoch: 962/2000... Training loss: 0.5103\n",
      "Epoch: 962/2000... Training loss: 0.5471\n",
      "Epoch: 962/2000... Training loss: 0.5440\n",
      "Epoch: 962/2000... Training loss: 0.5914\n",
      "Epoch: 962/2000... Training loss: 0.5104\n",
      "Epoch: 962/2000... Training loss: 0.5164\n",
      "Epoch: 962/2000... Training loss: 0.6325\n",
      "Epoch: 962/2000... Training loss: 0.4479\n",
      "Epoch: 962/2000... Training loss: 0.3702\n",
      "Epoch: 962/2000... Training loss: 0.4434\n",
      "Epoch: 962/2000... Training loss: 0.7317\n",
      "Epoch: 962/2000... Training loss: 0.6442\n",
      "Epoch: 962/2000... Training loss: 0.5259\n",
      "Epoch: 962/2000... Training loss: 0.4880\n",
      "Epoch: 962/2000... Training loss: 0.5752\n",
      "Epoch: 962/2000... Training loss: 0.3854\n",
      "Epoch: 962/2000... Training loss: 0.5021\n",
      "Epoch: 962/2000... Training loss: 0.4666\n",
      "Epoch: 962/2000... Training loss: 0.5150\n",
      "Epoch: 962/2000... Training loss: 0.6924\n",
      "Epoch: 962/2000... Training loss: 0.7017\n",
      "Epoch: 962/2000... Training loss: 0.5174\n",
      "Epoch: 962/2000... Training loss: 0.5498\n",
      "Epoch: 963/2000... Training loss: 0.5286\n",
      "Epoch: 963/2000... Training loss: 0.4885\n",
      "Epoch: 963/2000... Training loss: 0.4963\n",
      "Epoch: 963/2000... Training loss: 0.6585\n",
      "Epoch: 963/2000... Training loss: 0.3909\n",
      "Epoch: 963/2000... Training loss: 0.6506\n",
      "Epoch: 963/2000... Training loss: 0.5159\n",
      "Epoch: 963/2000... Training loss: 0.5306\n",
      "Epoch: 963/2000... Training loss: 0.5802\n",
      "Epoch: 963/2000... Training loss: 0.4624\n",
      "Epoch: 963/2000... Training loss: 0.5338\n",
      "Epoch: 963/2000... Training loss: 0.4955\n",
      "Epoch: 963/2000... Training loss: 0.5913\n",
      "Epoch: 963/2000... Training loss: 0.4716\n",
      "Epoch: 963/2000... Training loss: 0.4809\n",
      "Epoch: 963/2000... Training loss: 0.4848\n",
      "Epoch: 963/2000... Training loss: 0.4278\n",
      "Epoch: 963/2000... Training loss: 0.4154\n",
      "Epoch: 963/2000... Training loss: 0.5253\n",
      "Epoch: 963/2000... Training loss: 0.5146\n",
      "Epoch: 963/2000... Training loss: 0.6549\n",
      "Epoch: 963/2000... Training loss: 0.7493\n",
      "Epoch: 963/2000... Training loss: 0.5535\n",
      "Epoch: 963/2000... Training loss: 0.4093\n",
      "Epoch: 963/2000... Training loss: 0.3153\n",
      "Epoch: 963/2000... Training loss: 0.6364\n",
      "Epoch: 963/2000... Training loss: 0.5309\n",
      "Epoch: 963/2000... Training loss: 0.5491\n",
      "Epoch: 963/2000... Training loss: 0.4934\n",
      "Epoch: 963/2000... Training loss: 0.4004\n",
      "Epoch: 963/2000... Training loss: 0.6207\n",
      "Epoch: 964/2000... Training loss: 0.5037\n",
      "Epoch: 964/2000... Training loss: 0.7434\n",
      "Epoch: 964/2000... Training loss: 0.4956\n",
      "Epoch: 964/2000... Training loss: 0.5173\n",
      "Epoch: 964/2000... Training loss: 0.5475\n",
      "Epoch: 964/2000... Training loss: 0.4948\n",
      "Epoch: 964/2000... Training loss: 0.6342\n",
      "Epoch: 964/2000... Training loss: 0.6881\n",
      "Epoch: 964/2000... Training loss: 0.6953\n",
      "Epoch: 964/2000... Training loss: 0.5927\n",
      "Epoch: 964/2000... Training loss: 0.5226\n",
      "Epoch: 964/2000... Training loss: 0.6532\n",
      "Epoch: 964/2000... Training loss: 0.4386\n",
      "Epoch: 964/2000... Training loss: 0.6089\n",
      "Epoch: 964/2000... Training loss: 0.5135\n",
      "Epoch: 964/2000... Training loss: 0.3579\n",
      "Epoch: 964/2000... Training loss: 0.6854\n",
      "Epoch: 964/2000... Training loss: 0.4358\n",
      "Epoch: 964/2000... Training loss: 0.5200\n",
      "Epoch: 964/2000... Training loss: 0.7993\n",
      "Epoch: 964/2000... Training loss: 0.4436\n",
      "Epoch: 964/2000... Training loss: 0.3754\n",
      "Epoch: 964/2000... Training loss: 0.5676\n",
      "Epoch: 964/2000... Training loss: 0.6460\n",
      "Epoch: 964/2000... Training loss: 0.5014\n",
      "Epoch: 964/2000... Training loss: 0.4371\n",
      "Epoch: 964/2000... Training loss: 0.5671\n",
      "Epoch: 964/2000... Training loss: 0.5033\n",
      "Epoch: 964/2000... Training loss: 0.5244\n",
      "Epoch: 964/2000... Training loss: 0.4995\n",
      "Epoch: 964/2000... Training loss: 0.4891\n",
      "Epoch: 965/2000... Training loss: 0.5662\n",
      "Epoch: 965/2000... Training loss: 0.5088\n",
      "Epoch: 965/2000... Training loss: 0.5964\n",
      "Epoch: 965/2000... Training loss: 0.5827\n",
      "Epoch: 965/2000... Training loss: 0.6326\n",
      "Epoch: 965/2000... Training loss: 0.4020\n",
      "Epoch: 965/2000... Training loss: 0.4719\n",
      "Epoch: 965/2000... Training loss: 0.3767\n",
      "Epoch: 965/2000... Training loss: 0.7987\n",
      "Epoch: 965/2000... Training loss: 0.6200\n",
      "Epoch: 965/2000... Training loss: 0.5154\n",
      "Epoch: 965/2000... Training loss: 0.5836\n",
      "Epoch: 965/2000... Training loss: 0.4836\n",
      "Epoch: 965/2000... Training loss: 0.5279\n",
      "Epoch: 965/2000... Training loss: 0.3898\n",
      "Epoch: 965/2000... Training loss: 0.4869\n",
      "Epoch: 965/2000... Training loss: 0.4523\n",
      "Epoch: 965/2000... Training loss: 0.6632\n",
      "Epoch: 965/2000... Training loss: 0.5221\n",
      "Epoch: 965/2000... Training loss: 0.6945\n",
      "Epoch: 965/2000... Training loss: 0.7044\n",
      "Epoch: 965/2000... Training loss: 0.4109\n",
      "Epoch: 965/2000... Training loss: 0.4825\n",
      "Epoch: 965/2000... Training loss: 0.5006\n",
      "Epoch: 965/2000... Training loss: 0.5328\n",
      "Epoch: 965/2000... Training loss: 0.6960\n",
      "Epoch: 965/2000... Training loss: 0.3772\n",
      "Epoch: 965/2000... Training loss: 0.6514\n",
      "Epoch: 965/2000... Training loss: 0.5894\n",
      "Epoch: 965/2000... Training loss: 0.3851\n",
      "Epoch: 965/2000... Training loss: 0.5141\n",
      "Epoch: 966/2000... Training loss: 0.5405\n",
      "Epoch: 966/2000... Training loss: 0.6829\n",
      "Epoch: 966/2000... Training loss: 0.4680\n",
      "Epoch: 966/2000... Training loss: 0.6534\n",
      "Epoch: 966/2000... Training loss: 0.2985\n",
      "Epoch: 966/2000... Training loss: 0.4180\n",
      "Epoch: 966/2000... Training loss: 0.4034\n",
      "Epoch: 966/2000... Training loss: 0.6021\n",
      "Epoch: 966/2000... Training loss: 0.4476\n",
      "Epoch: 966/2000... Training loss: 0.5353\n",
      "Epoch: 966/2000... Training loss: 0.6312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 966/2000... Training loss: 0.4548\n",
      "Epoch: 966/2000... Training loss: 0.3897\n",
      "Epoch: 966/2000... Training loss: 0.6035\n",
      "Epoch: 966/2000... Training loss: 0.6119\n",
      "Epoch: 966/2000... Training loss: 0.4825\n",
      "Epoch: 966/2000... Training loss: 0.4834\n",
      "Epoch: 966/2000... Training loss: 0.5455\n",
      "Epoch: 966/2000... Training loss: 0.5008\n",
      "Epoch: 966/2000... Training loss: 0.4806\n",
      "Epoch: 966/2000... Training loss: 0.4197\n",
      "Epoch: 966/2000... Training loss: 0.5632\n",
      "Epoch: 966/2000... Training loss: 0.8325\n",
      "Epoch: 966/2000... Training loss: 0.5711\n",
      "Epoch: 966/2000... Training loss: 0.5555\n",
      "Epoch: 966/2000... Training loss: 0.4812\n",
      "Epoch: 966/2000... Training loss: 0.3913\n",
      "Epoch: 966/2000... Training loss: 0.6687\n",
      "Epoch: 966/2000... Training loss: 0.6216\n",
      "Epoch: 966/2000... Training loss: 0.4937\n",
      "Epoch: 966/2000... Training loss: 0.4544\n",
      "Epoch: 967/2000... Training loss: 0.5235\n",
      "Epoch: 967/2000... Training loss: 0.5126\n",
      "Epoch: 967/2000... Training loss: 0.6137\n",
      "Epoch: 967/2000... Training loss: 0.4864\n",
      "Epoch: 967/2000... Training loss: 0.3888\n",
      "Epoch: 967/2000... Training loss: 0.7699\n",
      "Epoch: 967/2000... Training loss: 0.5475\n",
      "Epoch: 967/2000... Training loss: 0.3277\n",
      "Epoch: 967/2000... Training loss: 0.5712\n",
      "Epoch: 967/2000... Training loss: 0.4989\n",
      "Epoch: 967/2000... Training loss: 0.5583\n",
      "Epoch: 967/2000... Training loss: 0.6233\n",
      "Epoch: 967/2000... Training loss: 0.5588\n",
      "Epoch: 967/2000... Training loss: 0.6480\n",
      "Epoch: 967/2000... Training loss: 0.5014\n",
      "Epoch: 967/2000... Training loss: 0.4439\n",
      "Epoch: 967/2000... Training loss: 0.7827\n",
      "Epoch: 967/2000... Training loss: 0.7656\n",
      "Epoch: 967/2000... Training loss: 0.6950\n",
      "Epoch: 967/2000... Training loss: 0.4255\n",
      "Epoch: 967/2000... Training loss: 0.5910\n",
      "Epoch: 967/2000... Training loss: 0.6147\n",
      "Epoch: 967/2000... Training loss: 0.3346\n",
      "Epoch: 967/2000... Training loss: 0.6227\n",
      "Epoch: 967/2000... Training loss: 0.5740\n",
      "Epoch: 967/2000... Training loss: 0.4479\n",
      "Epoch: 967/2000... Training loss: 0.4466\n",
      "Epoch: 967/2000... Training loss: 0.3958\n",
      "Epoch: 967/2000... Training loss: 0.6105\n",
      "Epoch: 967/2000... Training loss: 0.6166\n",
      "Epoch: 967/2000... Training loss: 0.8159\n",
      "Epoch: 968/2000... Training loss: 0.5744\n",
      "Epoch: 968/2000... Training loss: 0.4859\n",
      "Epoch: 968/2000... Training loss: 0.6327\n",
      "Epoch: 968/2000... Training loss: 0.4753\n",
      "Epoch: 968/2000... Training loss: 0.5278\n",
      "Epoch: 968/2000... Training loss: 0.4045\n",
      "Epoch: 968/2000... Training loss: 0.6040\n",
      "Epoch: 968/2000... Training loss: 0.3836\n",
      "Epoch: 968/2000... Training loss: 0.6653\n",
      "Epoch: 968/2000... Training loss: 0.5801\n",
      "Epoch: 968/2000... Training loss: 0.5358\n",
      "Epoch: 968/2000... Training loss: 0.4529\n",
      "Epoch: 968/2000... Training loss: 0.5293\n",
      "Epoch: 968/2000... Training loss: 0.7389\n",
      "Epoch: 968/2000... Training loss: 0.4779\n",
      "Epoch: 968/2000... Training loss: 0.6246\n",
      "Epoch: 968/2000... Training loss: 0.4866\n",
      "Epoch: 968/2000... Training loss: 0.6512\n",
      "Epoch: 968/2000... Training loss: 0.7880\n",
      "Epoch: 968/2000... Training loss: 0.6200\n",
      "Epoch: 968/2000... Training loss: 0.5438\n",
      "Epoch: 968/2000... Training loss: 0.4904\n",
      "Epoch: 968/2000... Training loss: 0.5683\n",
      "Epoch: 968/2000... Training loss: 0.5312\n",
      "Epoch: 968/2000... Training loss: 0.5318\n",
      "Epoch: 968/2000... Training loss: 0.4682\n",
      "Epoch: 968/2000... Training loss: 0.6586\n",
      "Epoch: 968/2000... Training loss: 0.5209\n",
      "Epoch: 968/2000... Training loss: 0.5270\n",
      "Epoch: 968/2000... Training loss: 0.5767\n",
      "Epoch: 968/2000... Training loss: 0.4713\n",
      "Epoch: 969/2000... Training loss: 0.7954\n",
      "Epoch: 969/2000... Training loss: 0.6224\n",
      "Epoch: 969/2000... Training loss: 0.6174\n",
      "Epoch: 969/2000... Training loss: 0.3499\n",
      "Epoch: 969/2000... Training loss: 0.5615\n",
      "Epoch: 969/2000... Training loss: 0.4895\n",
      "Epoch: 969/2000... Training loss: 0.5085\n",
      "Epoch: 969/2000... Training loss: 0.3742\n",
      "Epoch: 969/2000... Training loss: 0.5003\n",
      "Epoch: 969/2000... Training loss: 0.5862\n",
      "Epoch: 969/2000... Training loss: 0.4407\n",
      "Epoch: 969/2000... Training loss: 0.5655\n",
      "Epoch: 969/2000... Training loss: 0.7159\n",
      "Epoch: 969/2000... Training loss: 0.6892\n",
      "Epoch: 969/2000... Training loss: 0.3990\n",
      "Epoch: 969/2000... Training loss: 0.4678\n",
      "Epoch: 969/2000... Training loss: 0.5784\n",
      "Epoch: 969/2000... Training loss: 0.3244\n",
      "Epoch: 969/2000... Training loss: 0.5584\n",
      "Epoch: 969/2000... Training loss: 0.5618\n",
      "Epoch: 969/2000... Training loss: 0.3654\n",
      "Epoch: 969/2000... Training loss: 0.4069\n",
      "Epoch: 969/2000... Training loss: 0.4621\n",
      "Epoch: 969/2000... Training loss: 0.4535\n",
      "Epoch: 969/2000... Training loss: 0.5121\n",
      "Epoch: 969/2000... Training loss: 0.4864\n",
      "Epoch: 969/2000... Training loss: 0.4887\n",
      "Epoch: 969/2000... Training loss: 0.5878\n",
      "Epoch: 969/2000... Training loss: 0.6762\n",
      "Epoch: 969/2000... Training loss: 0.5525\n",
      "Epoch: 969/2000... Training loss: 0.4532\n",
      "Epoch: 970/2000... Training loss: 0.4427\n",
      "Epoch: 970/2000... Training loss: 0.3725\n",
      "Epoch: 970/2000... Training loss: 0.4660\n",
      "Epoch: 970/2000... Training loss: 0.5240\n",
      "Epoch: 970/2000... Training loss: 0.5085\n",
      "Epoch: 970/2000... Training loss: 0.5698\n",
      "Epoch: 970/2000... Training loss: 0.4224\n",
      "Epoch: 970/2000... Training loss: 0.4799\n",
      "Epoch: 970/2000... Training loss: 0.7118\n",
      "Epoch: 970/2000... Training loss: 0.6158\n",
      "Epoch: 970/2000... Training loss: 0.6238\n",
      "Epoch: 970/2000... Training loss: 0.6446\n",
      "Epoch: 970/2000... Training loss: 0.3509\n",
      "Epoch: 970/2000... Training loss: 0.5520\n",
      "Epoch: 970/2000... Training loss: 0.5089\n",
      "Epoch: 970/2000... Training loss: 0.4315\n",
      "Epoch: 970/2000... Training loss: 0.6980\n",
      "Epoch: 970/2000... Training loss: 0.5890\n",
      "Epoch: 970/2000... Training loss: 0.5145\n",
      "Epoch: 970/2000... Training loss: 0.7482\n",
      "Epoch: 970/2000... Training loss: 0.4424\n",
      "Epoch: 970/2000... Training loss: 0.4810\n",
      "Epoch: 970/2000... Training loss: 0.4710\n",
      "Epoch: 970/2000... Training loss: 0.3955\n",
      "Epoch: 970/2000... Training loss: 0.5094\n",
      "Epoch: 970/2000... Training loss: 0.5585\n",
      "Epoch: 970/2000... Training loss: 0.3875\n",
      "Epoch: 970/2000... Training loss: 0.4232\n",
      "Epoch: 970/2000... Training loss: 0.4801\n",
      "Epoch: 970/2000... Training loss: 0.4942\n",
      "Epoch: 970/2000... Training loss: 0.4238\n",
      "Epoch: 971/2000... Training loss: 0.5135\n",
      "Epoch: 971/2000... Training loss: 0.6358\n",
      "Epoch: 971/2000... Training loss: 0.6553\n",
      "Epoch: 971/2000... Training loss: 0.5826\n",
      "Epoch: 971/2000... Training loss: 0.5028\n",
      "Epoch: 971/2000... Training loss: 0.5597\n",
      "Epoch: 971/2000... Training loss: 0.7465\n",
      "Epoch: 971/2000... Training loss: 0.3549\n",
      "Epoch: 971/2000... Training loss: 0.5751\n",
      "Epoch: 971/2000... Training loss: 0.6939\n",
      "Epoch: 971/2000... Training loss: 0.7199\n",
      "Epoch: 971/2000... Training loss: 0.4569\n",
      "Epoch: 971/2000... Training loss: 0.4421\n",
      "Epoch: 971/2000... Training loss: 0.6901\n",
      "Epoch: 971/2000... Training loss: 0.5183\n",
      "Epoch: 971/2000... Training loss: 0.4788\n",
      "Epoch: 971/2000... Training loss: 0.4894\n",
      "Epoch: 971/2000... Training loss: 0.6290\n",
      "Epoch: 971/2000... Training loss: 0.4724\n",
      "Epoch: 971/2000... Training loss: 0.4759\n",
      "Epoch: 971/2000... Training loss: 0.6541\n",
      "Epoch: 971/2000... Training loss: 0.4708\n",
      "Epoch: 971/2000... Training loss: 0.4670\n",
      "Epoch: 971/2000... Training loss: 0.4498\n",
      "Epoch: 971/2000... Training loss: 0.5095\n",
      "Epoch: 971/2000... Training loss: 0.5982\n",
      "Epoch: 971/2000... Training loss: 0.3761\n",
      "Epoch: 971/2000... Training loss: 0.5747\n",
      "Epoch: 971/2000... Training loss: 0.4050\n",
      "Epoch: 971/2000... Training loss: 0.3691\n",
      "Epoch: 971/2000... Training loss: 0.4833\n",
      "Epoch: 972/2000... Training loss: 0.4545\n",
      "Epoch: 972/2000... Training loss: 0.5094\n",
      "Epoch: 972/2000... Training loss: 0.5487\n",
      "Epoch: 972/2000... Training loss: 0.5642\n",
      "Epoch: 972/2000... Training loss: 0.7254\n",
      "Epoch: 972/2000... Training loss: 0.4575\n",
      "Epoch: 972/2000... Training loss: 0.4979\n",
      "Epoch: 972/2000... Training loss: 0.5636\n",
      "Epoch: 972/2000... Training loss: 0.3295\n",
      "Epoch: 972/2000... Training loss: 0.4605\n",
      "Epoch: 972/2000... Training loss: 0.4924\n",
      "Epoch: 972/2000... Training loss: 0.5942\n",
      "Epoch: 972/2000... Training loss: 0.5504\n",
      "Epoch: 972/2000... Training loss: 0.5658\n",
      "Epoch: 972/2000... Training loss: 0.4506\n",
      "Epoch: 972/2000... Training loss: 0.6238\n",
      "Epoch: 972/2000... Training loss: 0.3343\n",
      "Epoch: 972/2000... Training loss: 0.4060\n",
      "Epoch: 972/2000... Training loss: 0.4382\n",
      "Epoch: 972/2000... Training loss: 0.5578\n",
      "Epoch: 972/2000... Training loss: 0.4311\n",
      "Epoch: 972/2000... Training loss: 0.4783\n",
      "Epoch: 972/2000... Training loss: 0.4725\n",
      "Epoch: 972/2000... Training loss: 0.5291\n",
      "Epoch: 972/2000... Training loss: 0.5150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 972/2000... Training loss: 0.6150\n",
      "Epoch: 972/2000... Training loss: 0.5452\n",
      "Epoch: 972/2000... Training loss: 0.4834\n",
      "Epoch: 972/2000... Training loss: 0.7343\n",
      "Epoch: 972/2000... Training loss: 0.4302\n",
      "Epoch: 972/2000... Training loss: 0.4319\n",
      "Epoch: 973/2000... Training loss: 0.4171\n",
      "Epoch: 973/2000... Training loss: 0.4333\n",
      "Epoch: 973/2000... Training loss: 0.5431\n",
      "Epoch: 973/2000... Training loss: 0.4412\n",
      "Epoch: 973/2000... Training loss: 0.5305\n",
      "Epoch: 973/2000... Training loss: 0.6510\n",
      "Epoch: 973/2000... Training loss: 0.5727\n",
      "Epoch: 973/2000... Training loss: 0.6631\n",
      "Epoch: 973/2000... Training loss: 0.6507\n",
      "Epoch: 973/2000... Training loss: 0.5510\n",
      "Epoch: 973/2000... Training loss: 0.7031\n",
      "Epoch: 973/2000... Training loss: 0.4443\n",
      "Epoch: 973/2000... Training loss: 0.5177\n",
      "Epoch: 973/2000... Training loss: 0.5071\n",
      "Epoch: 973/2000... Training loss: 0.4281\n",
      "Epoch: 973/2000... Training loss: 0.5917\n",
      "Epoch: 973/2000... Training loss: 0.3802\n",
      "Epoch: 973/2000... Training loss: 0.3774\n",
      "Epoch: 973/2000... Training loss: 0.6008\n",
      "Epoch: 973/2000... Training loss: 0.6796\n",
      "Epoch: 973/2000... Training loss: 0.5706\n",
      "Epoch: 973/2000... Training loss: 0.5555\n",
      "Epoch: 973/2000... Training loss: 0.5080\n",
      "Epoch: 973/2000... Training loss: 0.5096\n",
      "Epoch: 973/2000... Training loss: 0.3886\n",
      "Epoch: 973/2000... Training loss: 0.5225\n",
      "Epoch: 973/2000... Training loss: 0.5043\n",
      "Epoch: 973/2000... Training loss: 0.5433\n",
      "Epoch: 973/2000... Training loss: 0.6990\n",
      "Epoch: 973/2000... Training loss: 0.5409\n",
      "Epoch: 973/2000... Training loss: 0.4131\n",
      "Epoch: 974/2000... Training loss: 0.5325\n",
      "Epoch: 974/2000... Training loss: 0.4369\n",
      "Epoch: 974/2000... Training loss: 0.3921\n",
      "Epoch: 974/2000... Training loss: 0.6980\n",
      "Epoch: 974/2000... Training loss: 0.6147\n",
      "Epoch: 974/2000... Training loss: 0.4487\n",
      "Epoch: 974/2000... Training loss: 0.6802\n",
      "Epoch: 974/2000... Training loss: 0.6329\n",
      "Epoch: 974/2000... Training loss: 0.5736\n",
      "Epoch: 974/2000... Training loss: 0.6537\n",
      "Epoch: 974/2000... Training loss: 0.6054\n",
      "Epoch: 974/2000... Training loss: 0.5884\n",
      "Epoch: 974/2000... Training loss: 0.5290\n",
      "Epoch: 974/2000... Training loss: 0.4783\n",
      "Epoch: 974/2000... Training loss: 0.7035\n",
      "Epoch: 974/2000... Training loss: 0.5289\n",
      "Epoch: 974/2000... Training loss: 0.4281\n",
      "Epoch: 974/2000... Training loss: 0.6750\n",
      "Epoch: 974/2000... Training loss: 0.4385\n",
      "Epoch: 974/2000... Training loss: 0.5420\n",
      "Epoch: 974/2000... Training loss: 0.5517\n",
      "Epoch: 974/2000... Training loss: 0.5294\n",
      "Epoch: 974/2000... Training loss: 0.5227\n",
      "Epoch: 974/2000... Training loss: 0.5934\n",
      "Epoch: 974/2000... Training loss: 0.4342\n",
      "Epoch: 974/2000... Training loss: 0.6233\n",
      "Epoch: 974/2000... Training loss: 0.4307\n",
      "Epoch: 974/2000... Training loss: 0.5222\n",
      "Epoch: 974/2000... Training loss: 0.5703\n",
      "Epoch: 974/2000... Training loss: 0.5042\n",
      "Epoch: 974/2000... Training loss: 0.6790\n",
      "Epoch: 975/2000... Training loss: 0.5361\n",
      "Epoch: 975/2000... Training loss: 0.6854\n",
      "Epoch: 975/2000... Training loss: 0.5820\n",
      "Epoch: 975/2000... Training loss: 0.4039\n",
      "Epoch: 975/2000... Training loss: 0.8932\n",
      "Epoch: 975/2000... Training loss: 0.3646\n",
      "Epoch: 975/2000... Training loss: 0.4957\n",
      "Epoch: 975/2000... Training loss: 0.5250\n",
      "Epoch: 975/2000... Training loss: 0.4297\n",
      "Epoch: 975/2000... Training loss: 0.3556\n",
      "Epoch: 975/2000... Training loss: 0.5076\n",
      "Epoch: 975/2000... Training loss: 0.4319\n",
      "Epoch: 975/2000... Training loss: 0.5417\n",
      "Epoch: 975/2000... Training loss: 0.5372\n",
      "Epoch: 975/2000... Training loss: 0.4815\n",
      "Epoch: 975/2000... Training loss: 0.6007\n",
      "Epoch: 975/2000... Training loss: 0.5527\n",
      "Epoch: 975/2000... Training loss: 0.6424\n",
      "Epoch: 975/2000... Training loss: 0.7051\n",
      "Epoch: 975/2000... Training loss: 0.5678\n",
      "Epoch: 975/2000... Training loss: 0.5308\n",
      "Epoch: 975/2000... Training loss: 0.5610\n",
      "Epoch: 975/2000... Training loss: 0.5520\n",
      "Epoch: 975/2000... Training loss: 0.4428\n",
      "Epoch: 975/2000... Training loss: 0.4725\n",
      "Epoch: 975/2000... Training loss: 0.3416\n",
      "Epoch: 975/2000... Training loss: 0.6438\n",
      "Epoch: 975/2000... Training loss: 0.6088\n",
      "Epoch: 975/2000... Training loss: 0.4972\n",
      "Epoch: 975/2000... Training loss: 0.6328\n",
      "Epoch: 975/2000... Training loss: 0.3284\n",
      "Epoch: 976/2000... Training loss: 0.4840\n",
      "Epoch: 976/2000... Training loss: 0.5118\n",
      "Epoch: 976/2000... Training loss: 0.4858\n",
      "Epoch: 976/2000... Training loss: 0.3999\n",
      "Epoch: 976/2000... Training loss: 0.6915\n",
      "Epoch: 976/2000... Training loss: 0.4789\n",
      "Epoch: 976/2000... Training loss: 0.4359\n",
      "Epoch: 976/2000... Training loss: 0.4741\n",
      "Epoch: 976/2000... Training loss: 0.3975\n",
      "Epoch: 976/2000... Training loss: 0.5516\n",
      "Epoch: 976/2000... Training loss: 0.5149\n",
      "Epoch: 976/2000... Training loss: 0.4862\n",
      "Epoch: 976/2000... Training loss: 0.4702\n",
      "Epoch: 976/2000... Training loss: 0.6858\n",
      "Epoch: 976/2000... Training loss: 0.4126\n",
      "Epoch: 976/2000... Training loss: 0.3728\n",
      "Epoch: 976/2000... Training loss: 0.4040\n",
      "Epoch: 976/2000... Training loss: 0.2756\n",
      "Epoch: 976/2000... Training loss: 0.4678\n",
      "Epoch: 976/2000... Training loss: 0.4816\n",
      "Epoch: 976/2000... Training loss: 0.6150\n",
      "Epoch: 976/2000... Training loss: 0.5729\n",
      "Epoch: 976/2000... Training loss: 0.4857\n",
      "Epoch: 976/2000... Training loss: 0.7261\n",
      "Epoch: 976/2000... Training loss: 0.4166\n",
      "Epoch: 976/2000... Training loss: 0.5842\n",
      "Epoch: 976/2000... Training loss: 0.5957\n",
      "Epoch: 976/2000... Training loss: 0.6379\n",
      "Epoch: 976/2000... Training loss: 0.5037\n",
      "Epoch: 976/2000... Training loss: 0.3554\n",
      "Epoch: 976/2000... Training loss: 0.4984\n",
      "Epoch: 977/2000... Training loss: 0.5259\n",
      "Epoch: 977/2000... Training loss: 0.4766\n",
      "Epoch: 977/2000... Training loss: 0.6131\n",
      "Epoch: 977/2000... Training loss: 0.3409\n",
      "Epoch: 977/2000... Training loss: 0.4989\n",
      "Epoch: 977/2000... Training loss: 0.5591\n",
      "Epoch: 977/2000... Training loss: 0.4991\n",
      "Epoch: 977/2000... Training loss: 0.3679\n",
      "Epoch: 977/2000... Training loss: 0.5872\n",
      "Epoch: 977/2000... Training loss: 0.5840\n",
      "Epoch: 977/2000... Training loss: 0.4734\n",
      "Epoch: 977/2000... Training loss: 0.5986\n",
      "Epoch: 977/2000... Training loss: 0.4848\n",
      "Epoch: 977/2000... Training loss: 0.3721\n",
      "Epoch: 977/2000... Training loss: 0.5611\n",
      "Epoch: 977/2000... Training loss: 0.3490\n",
      "Epoch: 977/2000... Training loss: 0.6506\n",
      "Epoch: 977/2000... Training loss: 0.5764\n",
      "Epoch: 977/2000... Training loss: 0.3074\n",
      "Epoch: 977/2000... Training loss: 0.5746\n",
      "Epoch: 977/2000... Training loss: 0.6802\n",
      "Epoch: 977/2000... Training loss: 0.6702\n",
      "Epoch: 977/2000... Training loss: 0.7039\n",
      "Epoch: 977/2000... Training loss: 0.5627\n",
      "Epoch: 977/2000... Training loss: 0.5707\n",
      "Epoch: 977/2000... Training loss: 0.4679\n",
      "Epoch: 977/2000... Training loss: 0.6406\n",
      "Epoch: 977/2000... Training loss: 0.5422\n",
      "Epoch: 977/2000... Training loss: 0.5170\n",
      "Epoch: 977/2000... Training loss: 0.4975\n",
      "Epoch: 977/2000... Training loss: 0.5610\n",
      "Epoch: 978/2000... Training loss: 0.6127\n",
      "Epoch: 978/2000... Training loss: 0.5872\n",
      "Epoch: 978/2000... Training loss: 0.4933\n",
      "Epoch: 978/2000... Training loss: 0.5563\n",
      "Epoch: 978/2000... Training loss: 0.4736\n",
      "Epoch: 978/2000... Training loss: 0.5894\n",
      "Epoch: 978/2000... Training loss: 0.5037\n",
      "Epoch: 978/2000... Training loss: 0.6530\n",
      "Epoch: 978/2000... Training loss: 0.4986\n",
      "Epoch: 978/2000... Training loss: 0.5403\n",
      "Epoch: 978/2000... Training loss: 0.3424\n",
      "Epoch: 978/2000... Training loss: 0.5270\n",
      "Epoch: 978/2000... Training loss: 0.5471\n",
      "Epoch: 978/2000... Training loss: 0.5828\n",
      "Epoch: 978/2000... Training loss: 0.6239\n",
      "Epoch: 978/2000... Training loss: 0.5116\n",
      "Epoch: 978/2000... Training loss: 0.4630\n",
      "Epoch: 978/2000... Training loss: 0.4826\n",
      "Epoch: 978/2000... Training loss: 0.6818\n",
      "Epoch: 978/2000... Training loss: 0.5562\n",
      "Epoch: 978/2000... Training loss: 0.6167\n",
      "Epoch: 978/2000... Training loss: 0.7040\n",
      "Epoch: 978/2000... Training loss: 0.6913\n",
      "Epoch: 978/2000... Training loss: 0.6846\n",
      "Epoch: 978/2000... Training loss: 0.7957\n",
      "Epoch: 978/2000... Training loss: 0.6828\n",
      "Epoch: 978/2000... Training loss: 0.3845\n",
      "Epoch: 978/2000... Training loss: 0.5928\n",
      "Epoch: 978/2000... Training loss: 0.5895\n",
      "Epoch: 978/2000... Training loss: 0.6235\n",
      "Epoch: 978/2000... Training loss: 0.4282\n",
      "Epoch: 979/2000... Training loss: 0.6625\n",
      "Epoch: 979/2000... Training loss: 0.5347\n",
      "Epoch: 979/2000... Training loss: 0.5066\n",
      "Epoch: 979/2000... Training loss: 0.5019\n",
      "Epoch: 979/2000... Training loss: 0.4557\n",
      "Epoch: 979/2000... Training loss: 0.5114\n",
      "Epoch: 979/2000... Training loss: 0.3221\n",
      "Epoch: 979/2000... Training loss: 0.6497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 979/2000... Training loss: 0.4266\n",
      "Epoch: 979/2000... Training loss: 0.5786\n",
      "Epoch: 979/2000... Training loss: 0.5894\n",
      "Epoch: 979/2000... Training loss: 0.4589\n",
      "Epoch: 979/2000... Training loss: 0.5280\n",
      "Epoch: 979/2000... Training loss: 0.5671\n",
      "Epoch: 979/2000... Training loss: 0.4494\n",
      "Epoch: 979/2000... Training loss: 0.5347\n",
      "Epoch: 979/2000... Training loss: 0.4772\n",
      "Epoch: 979/2000... Training loss: 0.4770\n",
      "Epoch: 979/2000... Training loss: 0.6987\n",
      "Epoch: 979/2000... Training loss: 0.5053\n",
      "Epoch: 979/2000... Training loss: 0.5792\n",
      "Epoch: 979/2000... Training loss: 0.5340\n",
      "Epoch: 979/2000... Training loss: 0.6099\n",
      "Epoch: 979/2000... Training loss: 0.5517\n",
      "Epoch: 979/2000... Training loss: 0.6167\n",
      "Epoch: 979/2000... Training loss: 0.4536\n",
      "Epoch: 979/2000... Training loss: 0.4814\n",
      "Epoch: 979/2000... Training loss: 0.5731\n",
      "Epoch: 979/2000... Training loss: 0.5600\n",
      "Epoch: 979/2000... Training loss: 0.3716\n",
      "Epoch: 979/2000... Training loss: 0.5587\n",
      "Epoch: 980/2000... Training loss: 0.6056\n",
      "Epoch: 980/2000... Training loss: 0.4678\n",
      "Epoch: 980/2000... Training loss: 0.5160\n",
      "Epoch: 980/2000... Training loss: 0.4782\n",
      "Epoch: 980/2000... Training loss: 0.4472\n",
      "Epoch: 980/2000... Training loss: 0.5194\n",
      "Epoch: 980/2000... Training loss: 0.6507\n",
      "Epoch: 980/2000... Training loss: 0.4844\n",
      "Epoch: 980/2000... Training loss: 0.3535\n",
      "Epoch: 980/2000... Training loss: 0.6022\n",
      "Epoch: 980/2000... Training loss: 0.6277\n",
      "Epoch: 980/2000... Training loss: 0.5093\n",
      "Epoch: 980/2000... Training loss: 0.5298\n",
      "Epoch: 980/2000... Training loss: 0.5765\n",
      "Epoch: 980/2000... Training loss: 0.5020\n",
      "Epoch: 980/2000... Training loss: 0.4259\n",
      "Epoch: 980/2000... Training loss: 0.4023\n",
      "Epoch: 980/2000... Training loss: 0.5600\n",
      "Epoch: 980/2000... Training loss: 0.6839\n",
      "Epoch: 980/2000... Training loss: 0.5138\n",
      "Epoch: 980/2000... Training loss: 0.7359\n",
      "Epoch: 980/2000... Training loss: 0.4992\n",
      "Epoch: 980/2000... Training loss: 0.4903\n",
      "Epoch: 980/2000... Training loss: 0.7545\n",
      "Epoch: 980/2000... Training loss: 0.5917\n",
      "Epoch: 980/2000... Training loss: 0.7077\n",
      "Epoch: 980/2000... Training loss: 0.3485\n",
      "Epoch: 980/2000... Training loss: 0.6204\n",
      "Epoch: 980/2000... Training loss: 0.4907\n",
      "Epoch: 980/2000... Training loss: 0.7094\n",
      "Epoch: 980/2000... Training loss: 0.7809\n",
      "Epoch: 981/2000... Training loss: 0.4249\n",
      "Epoch: 981/2000... Training loss: 0.5571\n",
      "Epoch: 981/2000... Training loss: 0.6009\n",
      "Epoch: 981/2000... Training loss: 0.4409\n",
      "Epoch: 981/2000... Training loss: 0.5764\n",
      "Epoch: 981/2000... Training loss: 0.6058\n",
      "Epoch: 981/2000... Training loss: 0.5152\n",
      "Epoch: 981/2000... Training loss: 0.4951\n",
      "Epoch: 981/2000... Training loss: 0.6991\n",
      "Epoch: 981/2000... Training loss: 0.6556\n",
      "Epoch: 981/2000... Training loss: 0.6410\n",
      "Epoch: 981/2000... Training loss: 0.7715\n",
      "Epoch: 981/2000... Training loss: 0.4846\n",
      "Epoch: 981/2000... Training loss: 0.5186\n",
      "Epoch: 981/2000... Training loss: 0.5513\n",
      "Epoch: 981/2000... Training loss: 0.5435\n",
      "Epoch: 981/2000... Training loss: 0.5223\n",
      "Epoch: 981/2000... Training loss: 0.4452\n",
      "Epoch: 981/2000... Training loss: 0.4407\n",
      "Epoch: 981/2000... Training loss: 0.5598\n",
      "Epoch: 981/2000... Training loss: 0.4457\n",
      "Epoch: 981/2000... Training loss: 0.5137\n",
      "Epoch: 981/2000... Training loss: 0.4975\n",
      "Epoch: 981/2000... Training loss: 0.5576\n",
      "Epoch: 981/2000... Training loss: 0.7314\n",
      "Epoch: 981/2000... Training loss: 0.5744\n",
      "Epoch: 981/2000... Training loss: 0.6231\n",
      "Epoch: 981/2000... Training loss: 0.3806\n",
      "Epoch: 981/2000... Training loss: 0.7746\n",
      "Epoch: 981/2000... Training loss: 0.4725\n",
      "Epoch: 981/2000... Training loss: 0.4739\n",
      "Epoch: 982/2000... Training loss: 0.4566\n",
      "Epoch: 982/2000... Training loss: 0.6427\n",
      "Epoch: 982/2000... Training loss: 0.4901\n",
      "Epoch: 982/2000... Training loss: 0.5910\n",
      "Epoch: 982/2000... Training loss: 0.5256\n",
      "Epoch: 982/2000... Training loss: 0.4960\n",
      "Epoch: 982/2000... Training loss: 0.5251\n",
      "Epoch: 982/2000... Training loss: 0.7044\n",
      "Epoch: 982/2000... Training loss: 0.5719\n",
      "Epoch: 982/2000... Training loss: 0.5068\n",
      "Epoch: 982/2000... Training loss: 0.4957\n",
      "Epoch: 982/2000... Training loss: 0.4060\n",
      "Epoch: 982/2000... Training loss: 0.5778\n",
      "Epoch: 982/2000... Training loss: 0.3639\n",
      "Epoch: 982/2000... Training loss: 0.4075\n",
      "Epoch: 982/2000... Training loss: 0.5055\n",
      "Epoch: 982/2000... Training loss: 0.7082\n",
      "Epoch: 982/2000... Training loss: 0.6204\n",
      "Epoch: 982/2000... Training loss: 0.4617\n",
      "Epoch: 982/2000... Training loss: 0.5729\n",
      "Epoch: 982/2000... Training loss: 0.4633\n",
      "Epoch: 982/2000... Training loss: 0.5141\n",
      "Epoch: 982/2000... Training loss: 0.7508\n",
      "Epoch: 982/2000... Training loss: 0.4559\n",
      "Epoch: 982/2000... Training loss: 0.5633\n",
      "Epoch: 982/2000... Training loss: 0.5330\n",
      "Epoch: 982/2000... Training loss: 0.5876\n",
      "Epoch: 982/2000... Training loss: 0.4706\n",
      "Epoch: 982/2000... Training loss: 0.6198\n",
      "Epoch: 982/2000... Training loss: 0.6107\n",
      "Epoch: 982/2000... Training loss: 0.7419\n",
      "Epoch: 983/2000... Training loss: 0.8271\n",
      "Epoch: 983/2000... Training loss: 0.5663\n",
      "Epoch: 983/2000... Training loss: 0.5518\n",
      "Epoch: 983/2000... Training loss: 0.4757\n",
      "Epoch: 983/2000... Training loss: 0.4714\n",
      "Epoch: 983/2000... Training loss: 0.4059\n",
      "Epoch: 983/2000... Training loss: 0.7020\n",
      "Epoch: 983/2000... Training loss: 0.4045\n",
      "Epoch: 983/2000... Training loss: 0.3346\n",
      "Epoch: 983/2000... Training loss: 0.3973\n",
      "Epoch: 983/2000... Training loss: 0.6413\n",
      "Epoch: 983/2000... Training loss: 0.4437\n",
      "Epoch: 983/2000... Training loss: 0.6365\n",
      "Epoch: 983/2000... Training loss: 0.5578\n",
      "Epoch: 983/2000... Training loss: 0.4212\n",
      "Epoch: 983/2000... Training loss: 0.5880\n",
      "Epoch: 983/2000... Training loss: 0.5673\n",
      "Epoch: 983/2000... Training loss: 0.6852\n",
      "Epoch: 983/2000... Training loss: 0.5458\n",
      "Epoch: 983/2000... Training loss: 0.6579\n",
      "Epoch: 983/2000... Training loss: 0.4910\n",
      "Epoch: 983/2000... Training loss: 0.4689\n",
      "Epoch: 983/2000... Training loss: 0.4744\n",
      "Epoch: 983/2000... Training loss: 0.6440\n",
      "Epoch: 983/2000... Training loss: 0.4634\n",
      "Epoch: 983/2000... Training loss: 0.4095\n",
      "Epoch: 983/2000... Training loss: 0.6564\n",
      "Epoch: 983/2000... Training loss: 0.5235\n",
      "Epoch: 983/2000... Training loss: 0.6963\n",
      "Epoch: 983/2000... Training loss: 0.4185\n",
      "Epoch: 983/2000... Training loss: 0.7132\n",
      "Epoch: 984/2000... Training loss: 0.7044\n",
      "Epoch: 984/2000... Training loss: 0.6177\n",
      "Epoch: 984/2000... Training loss: 0.4678\n",
      "Epoch: 984/2000... Training loss: 0.6172\n",
      "Epoch: 984/2000... Training loss: 0.3889\n",
      "Epoch: 984/2000... Training loss: 0.4348\n",
      "Epoch: 984/2000... Training loss: 0.4995\n",
      "Epoch: 984/2000... Training loss: 0.5490\n",
      "Epoch: 984/2000... Training loss: 0.4734\n",
      "Epoch: 984/2000... Training loss: 0.6257\n",
      "Epoch: 984/2000... Training loss: 0.4895\n",
      "Epoch: 984/2000... Training loss: 0.5759\n",
      "Epoch: 984/2000... Training loss: 0.3975\n",
      "Epoch: 984/2000... Training loss: 0.5668\n",
      "Epoch: 984/2000... Training loss: 0.4289\n",
      "Epoch: 984/2000... Training loss: 0.6289\n",
      "Epoch: 984/2000... Training loss: 0.4686\n",
      "Epoch: 984/2000... Training loss: 0.3215\n",
      "Epoch: 984/2000... Training loss: 0.4686\n",
      "Epoch: 984/2000... Training loss: 0.5676\n",
      "Epoch: 984/2000... Training loss: 0.5906\n",
      "Epoch: 984/2000... Training loss: 0.6879\n",
      "Epoch: 984/2000... Training loss: 0.3955\n",
      "Epoch: 984/2000... Training loss: 0.6558\n",
      "Epoch: 984/2000... Training loss: 0.4327\n",
      "Epoch: 984/2000... Training loss: 0.5010\n",
      "Epoch: 984/2000... Training loss: 0.8204\n",
      "Epoch: 984/2000... Training loss: 0.7630\n",
      "Epoch: 984/2000... Training loss: 0.4678\n",
      "Epoch: 984/2000... Training loss: 0.5427\n",
      "Epoch: 984/2000... Training loss: 0.4802\n",
      "Epoch: 985/2000... Training loss: 0.5773\n",
      "Epoch: 985/2000... Training loss: 0.4309\n",
      "Epoch: 985/2000... Training loss: 0.6178\n",
      "Epoch: 985/2000... Training loss: 0.5628\n",
      "Epoch: 985/2000... Training loss: 0.4374\n",
      "Epoch: 985/2000... Training loss: 0.6683\n",
      "Epoch: 985/2000... Training loss: 0.5908\n",
      "Epoch: 985/2000... Training loss: 0.5326\n",
      "Epoch: 985/2000... Training loss: 0.6039\n",
      "Epoch: 985/2000... Training loss: 0.5508\n",
      "Epoch: 985/2000... Training loss: 0.5100\n",
      "Epoch: 985/2000... Training loss: 0.5815\n",
      "Epoch: 985/2000... Training loss: 0.4441\n",
      "Epoch: 985/2000... Training loss: 0.4588\n",
      "Epoch: 985/2000... Training loss: 0.5362\n",
      "Epoch: 985/2000... Training loss: 0.5667\n",
      "Epoch: 985/2000... Training loss: 0.5656\n",
      "Epoch: 985/2000... Training loss: 0.6452\n",
      "Epoch: 985/2000... Training loss: 0.5968\n",
      "Epoch: 985/2000... Training loss: 0.5851\n",
      "Epoch: 985/2000... Training loss: 0.3155\n",
      "Epoch: 985/2000... Training loss: 0.5270\n",
      "Epoch: 985/2000... Training loss: 0.4929\n",
      "Epoch: 985/2000... Training loss: 0.5757\n",
      "Epoch: 985/2000... Training loss: 0.6931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 985/2000... Training loss: 0.5092\n",
      "Epoch: 985/2000... Training loss: 0.4907\n",
      "Epoch: 985/2000... Training loss: 0.5095\n",
      "Epoch: 985/2000... Training loss: 0.5066\n",
      "Epoch: 985/2000... Training loss: 0.2574\n",
      "Epoch: 985/2000... Training loss: 0.5348\n",
      "Epoch: 986/2000... Training loss: 0.4516\n",
      "Epoch: 986/2000... Training loss: 0.4789\n",
      "Epoch: 986/2000... Training loss: 0.4603\n",
      "Epoch: 986/2000... Training loss: 0.6725\n",
      "Epoch: 986/2000... Training loss: 0.4500\n",
      "Epoch: 986/2000... Training loss: 0.4380\n",
      "Epoch: 986/2000... Training loss: 0.5682\n",
      "Epoch: 986/2000... Training loss: 0.5247\n",
      "Epoch: 986/2000... Training loss: 0.5515\n",
      "Epoch: 986/2000... Training loss: 0.3978\n",
      "Epoch: 986/2000... Training loss: 0.4213\n",
      "Epoch: 986/2000... Training loss: 0.4936\n",
      "Epoch: 986/2000... Training loss: 0.5748\n",
      "Epoch: 986/2000... Training loss: 0.4775\n",
      "Epoch: 986/2000... Training loss: 0.4210\n",
      "Epoch: 986/2000... Training loss: 0.5450\n",
      "Epoch: 986/2000... Training loss: 0.6105\n",
      "Epoch: 986/2000... Training loss: 0.5015\n",
      "Epoch: 986/2000... Training loss: 0.6511\n",
      "Epoch: 986/2000... Training loss: 0.6722\n",
      "Epoch: 986/2000... Training loss: 0.5294\n",
      "Epoch: 986/2000... Training loss: 0.6103\n",
      "Epoch: 986/2000... Training loss: 0.3785\n",
      "Epoch: 986/2000... Training loss: 0.4462\n",
      "Epoch: 986/2000... Training loss: 0.4623\n",
      "Epoch: 986/2000... Training loss: 0.5436\n",
      "Epoch: 986/2000... Training loss: 0.5262\n",
      "Epoch: 986/2000... Training loss: 0.3900\n",
      "Epoch: 986/2000... Training loss: 0.4183\n",
      "Epoch: 986/2000... Training loss: 0.5764\n",
      "Epoch: 986/2000... Training loss: 0.5295\n",
      "Epoch: 987/2000... Training loss: 0.5444\n",
      "Epoch: 987/2000... Training loss: 0.6286\n",
      "Epoch: 987/2000... Training loss: 0.5643\n",
      "Epoch: 987/2000... Training loss: 0.4394\n",
      "Epoch: 987/2000... Training loss: 0.4423\n",
      "Epoch: 987/2000... Training loss: 0.6822\n",
      "Epoch: 987/2000... Training loss: 0.4689\n",
      "Epoch: 987/2000... Training loss: 0.5103\n",
      "Epoch: 987/2000... Training loss: 0.4917\n",
      "Epoch: 987/2000... Training loss: 0.4560\n",
      "Epoch: 987/2000... Training loss: 0.5571\n",
      "Epoch: 987/2000... Training loss: 0.6908\n",
      "Epoch: 987/2000... Training loss: 0.4059\n",
      "Epoch: 987/2000... Training loss: 0.7046\n",
      "Epoch: 987/2000... Training loss: 0.6374\n",
      "Epoch: 987/2000... Training loss: 0.5047\n",
      "Epoch: 987/2000... Training loss: 0.5627\n",
      "Epoch: 987/2000... Training loss: 0.4757\n",
      "Epoch: 987/2000... Training loss: 0.4058\n",
      "Epoch: 987/2000... Training loss: 0.5915\n",
      "Epoch: 987/2000... Training loss: 0.5103\n",
      "Epoch: 987/2000... Training loss: 0.6120\n",
      "Epoch: 987/2000... Training loss: 0.5257\n",
      "Epoch: 987/2000... Training loss: 0.4776\n",
      "Epoch: 987/2000... Training loss: 0.6077\n",
      "Epoch: 987/2000... Training loss: 0.6522\n",
      "Epoch: 987/2000... Training loss: 0.4670\n",
      "Epoch: 987/2000... Training loss: 0.5641\n",
      "Epoch: 987/2000... Training loss: 0.6868\n",
      "Epoch: 987/2000... Training loss: 0.7939\n",
      "Epoch: 987/2000... Training loss: 0.4874\n",
      "Epoch: 988/2000... Training loss: 0.3882\n",
      "Epoch: 988/2000... Training loss: 0.5571\n",
      "Epoch: 988/2000... Training loss: 0.5745\n",
      "Epoch: 988/2000... Training loss: 0.4214\n",
      "Epoch: 988/2000... Training loss: 0.6176\n",
      "Epoch: 988/2000... Training loss: 0.7318\n",
      "Epoch: 988/2000... Training loss: 0.3719\n",
      "Epoch: 988/2000... Training loss: 0.4911\n",
      "Epoch: 988/2000... Training loss: 0.5362\n",
      "Epoch: 988/2000... Training loss: 0.4459\n",
      "Epoch: 988/2000... Training loss: 0.4617\n",
      "Epoch: 988/2000... Training loss: 0.5319\n",
      "Epoch: 988/2000... Training loss: 0.6369\n",
      "Epoch: 988/2000... Training loss: 0.4824\n",
      "Epoch: 988/2000... Training loss: 0.3816\n",
      "Epoch: 988/2000... Training loss: 0.3356\n",
      "Epoch: 988/2000... Training loss: 0.4590\n",
      "Epoch: 988/2000... Training loss: 0.5479\n",
      "Epoch: 988/2000... Training loss: 0.6378\n",
      "Epoch: 988/2000... Training loss: 0.5259\n",
      "Epoch: 988/2000... Training loss: 0.2917\n",
      "Epoch: 988/2000... Training loss: 0.6333\n",
      "Epoch: 988/2000... Training loss: 0.6201\n",
      "Epoch: 988/2000... Training loss: 0.4093\n",
      "Epoch: 988/2000... Training loss: 0.5485\n",
      "Epoch: 988/2000... Training loss: 0.6635\n",
      "Epoch: 988/2000... Training loss: 0.5386\n",
      "Epoch: 988/2000... Training loss: 0.6030\n",
      "Epoch: 988/2000... Training loss: 0.4131\n",
      "Epoch: 988/2000... Training loss: 0.5221\n",
      "Epoch: 988/2000... Training loss: 0.5698\n",
      "Epoch: 989/2000... Training loss: 0.3566\n",
      "Epoch: 989/2000... Training loss: 0.2919\n",
      "Epoch: 989/2000... Training loss: 0.5886\n",
      "Epoch: 989/2000... Training loss: 0.6626\n",
      "Epoch: 989/2000... Training loss: 0.5032\n",
      "Epoch: 989/2000... Training loss: 0.6843\n",
      "Epoch: 989/2000... Training loss: 0.5815\n",
      "Epoch: 989/2000... Training loss: 0.3968\n",
      "Epoch: 989/2000... Training loss: 0.4751\n",
      "Epoch: 989/2000... Training loss: 0.5312\n",
      "Epoch: 989/2000... Training loss: 0.5432\n",
      "Epoch: 989/2000... Training loss: 0.6204\n",
      "Epoch: 989/2000... Training loss: 0.4144\n",
      "Epoch: 989/2000... Training loss: 0.6026\n",
      "Epoch: 989/2000... Training loss: 0.4932\n",
      "Epoch: 989/2000... Training loss: 0.4677\n",
      "Epoch: 989/2000... Training loss: 0.4385\n",
      "Epoch: 989/2000... Training loss: 0.4955\n",
      "Epoch: 989/2000... Training loss: 0.3734\n",
      "Epoch: 989/2000... Training loss: 0.7311\n",
      "Epoch: 989/2000... Training loss: 0.5061\n",
      "Epoch: 989/2000... Training loss: 0.6262\n",
      "Epoch: 989/2000... Training loss: 0.5312\n",
      "Epoch: 989/2000... Training loss: 0.4581\n",
      "Epoch: 989/2000... Training loss: 0.4130\n",
      "Epoch: 989/2000... Training loss: 0.4499\n",
      "Epoch: 989/2000... Training loss: 0.5411\n",
      "Epoch: 989/2000... Training loss: 0.5768\n",
      "Epoch: 989/2000... Training loss: 0.5288\n",
      "Epoch: 989/2000... Training loss: 0.4961\n",
      "Epoch: 989/2000... Training loss: 0.4765\n",
      "Epoch: 990/2000... Training loss: 0.6944\n",
      "Epoch: 990/2000... Training loss: 0.4041\n",
      "Epoch: 990/2000... Training loss: 0.6124\n",
      "Epoch: 990/2000... Training loss: 0.5467\n",
      "Epoch: 990/2000... Training loss: 0.5310\n",
      "Epoch: 990/2000... Training loss: 0.4642\n",
      "Epoch: 990/2000... Training loss: 0.4599\n",
      "Epoch: 990/2000... Training loss: 0.4551\n",
      "Epoch: 990/2000... Training loss: 0.5583\n",
      "Epoch: 990/2000... Training loss: 0.4911\n",
      "Epoch: 990/2000... Training loss: 0.3826\n",
      "Epoch: 990/2000... Training loss: 0.6347\n",
      "Epoch: 990/2000... Training loss: 0.5792\n",
      "Epoch: 990/2000... Training loss: 0.6137\n",
      "Epoch: 990/2000... Training loss: 0.5486\n",
      "Epoch: 990/2000... Training loss: 0.5594\n",
      "Epoch: 990/2000... Training loss: 0.4923\n",
      "Epoch: 990/2000... Training loss: 0.4011\n",
      "Epoch: 990/2000... Training loss: 0.5675\n",
      "Epoch: 990/2000... Training loss: 0.5317\n",
      "Epoch: 990/2000... Training loss: 0.5539\n",
      "Epoch: 990/2000... Training loss: 0.5706\n",
      "Epoch: 990/2000... Training loss: 0.4022\n",
      "Epoch: 990/2000... Training loss: 0.6971\n",
      "Epoch: 990/2000... Training loss: 0.6008\n",
      "Epoch: 990/2000... Training loss: 0.4707\n",
      "Epoch: 990/2000... Training loss: 0.3325\n",
      "Epoch: 990/2000... Training loss: 0.5950\n",
      "Epoch: 990/2000... Training loss: 0.3318\n",
      "Epoch: 990/2000... Training loss: 0.6380\n",
      "Epoch: 990/2000... Training loss: 0.7217\n",
      "Epoch: 991/2000... Training loss: 0.4255\n",
      "Epoch: 991/2000... Training loss: 0.5004\n",
      "Epoch: 991/2000... Training loss: 0.4118\n",
      "Epoch: 991/2000... Training loss: 0.5220\n",
      "Epoch: 991/2000... Training loss: 0.6395\n",
      "Epoch: 991/2000... Training loss: 0.5827\n",
      "Epoch: 991/2000... Training loss: 0.6672\n",
      "Epoch: 991/2000... Training loss: 0.3928\n",
      "Epoch: 991/2000... Training loss: 0.3284\n",
      "Epoch: 991/2000... Training loss: 0.5533\n",
      "Epoch: 991/2000... Training loss: 0.5970\n",
      "Epoch: 991/2000... Training loss: 0.4459\n",
      "Epoch: 991/2000... Training loss: 0.5400\n",
      "Epoch: 991/2000... Training loss: 0.6277\n",
      "Epoch: 991/2000... Training loss: 0.4763\n",
      "Epoch: 991/2000... Training loss: 0.4410\n",
      "Epoch: 991/2000... Training loss: 0.4238\n",
      "Epoch: 991/2000... Training loss: 0.4420\n",
      "Epoch: 991/2000... Training loss: 0.4022\n",
      "Epoch: 991/2000... Training loss: 0.6100\n",
      "Epoch: 991/2000... Training loss: 0.4084\n",
      "Epoch: 991/2000... Training loss: 0.5254\n",
      "Epoch: 991/2000... Training loss: 0.5517\n",
      "Epoch: 991/2000... Training loss: 0.3844\n",
      "Epoch: 991/2000... Training loss: 0.4923\n",
      "Epoch: 991/2000... Training loss: 0.5172\n",
      "Epoch: 991/2000... Training loss: 0.5998\n",
      "Epoch: 991/2000... Training loss: 0.6114\n",
      "Epoch: 991/2000... Training loss: 0.5486\n",
      "Epoch: 991/2000... Training loss: 0.6489\n",
      "Epoch: 991/2000... Training loss: 0.5336\n",
      "Epoch: 992/2000... Training loss: 0.5889\n",
      "Epoch: 992/2000... Training loss: 0.4916\n",
      "Epoch: 992/2000... Training loss: 0.4634\n",
      "Epoch: 992/2000... Training loss: 0.4103\n",
      "Epoch: 992/2000... Training loss: 0.4741\n",
      "Epoch: 992/2000... Training loss: 0.4273\n",
      "Epoch: 992/2000... Training loss: 0.5640\n",
      "Epoch: 992/2000... Training loss: 0.6153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 992/2000... Training loss: 0.5448\n",
      "Epoch: 992/2000... Training loss: 0.5116\n",
      "Epoch: 992/2000... Training loss: 0.4750\n",
      "Epoch: 992/2000... Training loss: 0.5328\n",
      "Epoch: 992/2000... Training loss: 0.6545\n",
      "Epoch: 992/2000... Training loss: 0.4788\n",
      "Epoch: 992/2000... Training loss: 0.6062\n",
      "Epoch: 992/2000... Training loss: 0.6120\n",
      "Epoch: 992/2000... Training loss: 0.6387\n",
      "Epoch: 992/2000... Training loss: 0.3867\n",
      "Epoch: 992/2000... Training loss: 0.5230\n",
      "Epoch: 992/2000... Training loss: 0.5490\n",
      "Epoch: 992/2000... Training loss: 0.5761\n",
      "Epoch: 992/2000... Training loss: 0.3755\n",
      "Epoch: 992/2000... Training loss: 0.3439\n",
      "Epoch: 992/2000... Training loss: 0.4555\n",
      "Epoch: 992/2000... Training loss: 0.4360\n",
      "Epoch: 992/2000... Training loss: 0.5602\n",
      "Epoch: 992/2000... Training loss: 0.4692\n",
      "Epoch: 992/2000... Training loss: 0.5422\n",
      "Epoch: 992/2000... Training loss: 0.4692\n",
      "Epoch: 992/2000... Training loss: 0.5116\n",
      "Epoch: 992/2000... Training loss: 0.3935\n",
      "Epoch: 993/2000... Training loss: 0.4139\n",
      "Epoch: 993/2000... Training loss: 0.6301\n",
      "Epoch: 993/2000... Training loss: 0.4966\n",
      "Epoch: 993/2000... Training loss: 0.5043\n",
      "Epoch: 993/2000... Training loss: 0.4118\n",
      "Epoch: 993/2000... Training loss: 0.4398\n",
      "Epoch: 993/2000... Training loss: 0.5061\n",
      "Epoch: 993/2000... Training loss: 0.6366\n",
      "Epoch: 993/2000... Training loss: 0.4775\n",
      "Epoch: 993/2000... Training loss: 0.4618\n",
      "Epoch: 993/2000... Training loss: 0.6514\n",
      "Epoch: 993/2000... Training loss: 0.6118\n",
      "Epoch: 993/2000... Training loss: 0.5120\n",
      "Epoch: 993/2000... Training loss: 0.6920\n",
      "Epoch: 993/2000... Training loss: 0.5834\n",
      "Epoch: 993/2000... Training loss: 0.4138\n",
      "Epoch: 993/2000... Training loss: 0.4371\n",
      "Epoch: 993/2000... Training loss: 0.5285\n",
      "Epoch: 993/2000... Training loss: 0.6133\n",
      "Epoch: 993/2000... Training loss: 0.4975\n",
      "Epoch: 993/2000... Training loss: 0.6881\n",
      "Epoch: 993/2000... Training loss: 0.5020\n",
      "Epoch: 993/2000... Training loss: 0.3711\n",
      "Epoch: 993/2000... Training loss: 0.5727\n",
      "Epoch: 993/2000... Training loss: 0.5819\n",
      "Epoch: 993/2000... Training loss: 0.4820\n",
      "Epoch: 993/2000... Training loss: 0.5632\n",
      "Epoch: 993/2000... Training loss: 0.6284\n",
      "Epoch: 993/2000... Training loss: 0.6189\n",
      "Epoch: 993/2000... Training loss: 0.5249\n",
      "Epoch: 993/2000... Training loss: 0.4848\n",
      "Epoch: 994/2000... Training loss: 0.4818\n",
      "Epoch: 994/2000... Training loss: 0.3676\n",
      "Epoch: 994/2000... Training loss: 0.5595\n",
      "Epoch: 994/2000... Training loss: 0.4569\n",
      "Epoch: 994/2000... Training loss: 0.4121\n",
      "Epoch: 994/2000... Training loss: 0.6403\n",
      "Epoch: 994/2000... Training loss: 0.6619\n",
      "Epoch: 994/2000... Training loss: 0.4422\n",
      "Epoch: 994/2000... Training loss: 0.4480\n",
      "Epoch: 994/2000... Training loss: 0.4801\n",
      "Epoch: 994/2000... Training loss: 0.8283\n",
      "Epoch: 994/2000... Training loss: 0.5320\n",
      "Epoch: 994/2000... Training loss: 0.4888\n",
      "Epoch: 994/2000... Training loss: 0.6160\n",
      "Epoch: 994/2000... Training loss: 0.5265\n",
      "Epoch: 994/2000... Training loss: 0.4448\n",
      "Epoch: 994/2000... Training loss: 0.4823\n",
      "Epoch: 994/2000... Training loss: 0.4938\n",
      "Epoch: 994/2000... Training loss: 0.5029\n",
      "Epoch: 994/2000... Training loss: 0.4451\n",
      "Epoch: 994/2000... Training loss: 0.4196\n",
      "Epoch: 994/2000... Training loss: 0.5140\n",
      "Epoch: 994/2000... Training loss: 0.6081\n",
      "Epoch: 994/2000... Training loss: 0.5077\n",
      "Epoch: 994/2000... Training loss: 0.4518\n",
      "Epoch: 994/2000... Training loss: 0.4301\n",
      "Epoch: 994/2000... Training loss: 0.6433\n",
      "Epoch: 994/2000... Training loss: 0.5476\n",
      "Epoch: 994/2000... Training loss: 0.3982\n",
      "Epoch: 994/2000... Training loss: 0.3380\n",
      "Epoch: 994/2000... Training loss: 0.6110\n",
      "Epoch: 995/2000... Training loss: 0.5549\n",
      "Epoch: 995/2000... Training loss: 0.3013\n",
      "Epoch: 995/2000... Training loss: 0.3967\n",
      "Epoch: 995/2000... Training loss: 0.6254\n",
      "Epoch: 995/2000... Training loss: 0.7564\n",
      "Epoch: 995/2000... Training loss: 0.4692\n",
      "Epoch: 995/2000... Training loss: 0.3703\n",
      "Epoch: 995/2000... Training loss: 0.5233\n",
      "Epoch: 995/2000... Training loss: 0.7645\n",
      "Epoch: 995/2000... Training loss: 0.5483\n",
      "Epoch: 995/2000... Training loss: 0.7550\n",
      "Epoch: 995/2000... Training loss: 0.5578\n",
      "Epoch: 995/2000... Training loss: 0.4558\n",
      "Epoch: 995/2000... Training loss: 0.5692\n",
      "Epoch: 995/2000... Training loss: 0.4159\n",
      "Epoch: 995/2000... Training loss: 0.3956\n",
      "Epoch: 995/2000... Training loss: 0.4172\n",
      "Epoch: 995/2000... Training loss: 0.5669\n",
      "Epoch: 995/2000... Training loss: 0.7144\n",
      "Epoch: 995/2000... Training loss: 0.6421\n",
      "Epoch: 995/2000... Training loss: 0.3946\n",
      "Epoch: 995/2000... Training loss: 0.4252\n",
      "Epoch: 995/2000... Training loss: 0.4795\n",
      "Epoch: 995/2000... Training loss: 0.4695\n",
      "Epoch: 995/2000... Training loss: 0.7675\n",
      "Epoch: 995/2000... Training loss: 0.4985\n",
      "Epoch: 995/2000... Training loss: 0.5722\n",
      "Epoch: 995/2000... Training loss: 0.5046\n",
      "Epoch: 995/2000... Training loss: 0.6388\n",
      "Epoch: 995/2000... Training loss: 0.5691\n",
      "Epoch: 995/2000... Training loss: 0.8835\n",
      "Epoch: 996/2000... Training loss: 0.2971\n",
      "Epoch: 996/2000... Training loss: 0.6260\n",
      "Epoch: 996/2000... Training loss: 0.3757\n",
      "Epoch: 996/2000... Training loss: 0.7078\n",
      "Epoch: 996/2000... Training loss: 0.4162\n",
      "Epoch: 996/2000... Training loss: 0.4669\n",
      "Epoch: 996/2000... Training loss: 0.6206\n",
      "Epoch: 996/2000... Training loss: 0.5137\n",
      "Epoch: 996/2000... Training loss: 0.7424\n",
      "Epoch: 996/2000... Training loss: 0.7006\n",
      "Epoch: 996/2000... Training loss: 0.3846\n",
      "Epoch: 996/2000... Training loss: 0.5243\n",
      "Epoch: 996/2000... Training loss: 0.3630\n",
      "Epoch: 996/2000... Training loss: 0.5481\n",
      "Epoch: 996/2000... Training loss: 0.4753\n",
      "Epoch: 996/2000... Training loss: 0.2935\n",
      "Epoch: 996/2000... Training loss: 0.4789\n",
      "Epoch: 996/2000... Training loss: 0.7104\n",
      "Epoch: 996/2000... Training loss: 0.3661\n",
      "Epoch: 996/2000... Training loss: 0.9020\n",
      "Epoch: 996/2000... Training loss: 0.6822\n",
      "Epoch: 996/2000... Training loss: 0.4176\n",
      "Epoch: 996/2000... Training loss: 0.5408\n",
      "Epoch: 996/2000... Training loss: 0.3819\n",
      "Epoch: 996/2000... Training loss: 0.6087\n",
      "Epoch: 996/2000... Training loss: 0.3804\n",
      "Epoch: 996/2000... Training loss: 0.5210\n",
      "Epoch: 996/2000... Training loss: 0.4935\n",
      "Epoch: 996/2000... Training loss: 0.6740\n",
      "Epoch: 996/2000... Training loss: 0.5441\n",
      "Epoch: 996/2000... Training loss: 0.5969\n",
      "Epoch: 997/2000... Training loss: 0.6779\n",
      "Epoch: 997/2000... Training loss: 0.3235\n",
      "Epoch: 997/2000... Training loss: 0.5843\n",
      "Epoch: 997/2000... Training loss: 0.5407\n",
      "Epoch: 997/2000... Training loss: 0.6265\n",
      "Epoch: 997/2000... Training loss: 0.5863\n",
      "Epoch: 997/2000... Training loss: 0.6497\n",
      "Epoch: 997/2000... Training loss: 0.3772\n",
      "Epoch: 997/2000... Training loss: 0.5422\n",
      "Epoch: 997/2000... Training loss: 0.3701\n",
      "Epoch: 997/2000... Training loss: 0.5654\n",
      "Epoch: 997/2000... Training loss: 0.6932\n",
      "Epoch: 997/2000... Training loss: 0.6178\n",
      "Epoch: 997/2000... Training loss: 0.5462\n",
      "Epoch: 997/2000... Training loss: 0.5177\n",
      "Epoch: 997/2000... Training loss: 0.4777\n",
      "Epoch: 997/2000... Training loss: 0.3697\n",
      "Epoch: 997/2000... Training loss: 0.5262\n",
      "Epoch: 997/2000... Training loss: 0.4838\n",
      "Epoch: 997/2000... Training loss: 0.4335\n",
      "Epoch: 997/2000... Training loss: 0.6580\n",
      "Epoch: 997/2000... Training loss: 0.4355\n",
      "Epoch: 997/2000... Training loss: 0.4505\n",
      "Epoch: 997/2000... Training loss: 0.7434\n",
      "Epoch: 997/2000... Training loss: 0.6035\n",
      "Epoch: 997/2000... Training loss: 0.3687\n",
      "Epoch: 997/2000... Training loss: 0.5350\n",
      "Epoch: 997/2000... Training loss: 0.6521\n",
      "Epoch: 997/2000... Training loss: 0.3754\n",
      "Epoch: 997/2000... Training loss: 0.3938\n",
      "Epoch: 997/2000... Training loss: 0.5843\n",
      "Epoch: 998/2000... Training loss: 0.4283\n",
      "Epoch: 998/2000... Training loss: 0.4811\n",
      "Epoch: 998/2000... Training loss: 0.6089\n",
      "Epoch: 998/2000... Training loss: 0.5722\n",
      "Epoch: 998/2000... Training loss: 0.5337\n",
      "Epoch: 998/2000... Training loss: 0.3406\n",
      "Epoch: 998/2000... Training loss: 0.5341\n",
      "Epoch: 998/2000... Training loss: 0.4713\n",
      "Epoch: 998/2000... Training loss: 0.5702\n",
      "Epoch: 998/2000... Training loss: 0.5717\n",
      "Epoch: 998/2000... Training loss: 0.6103\n",
      "Epoch: 998/2000... Training loss: 0.6991\n",
      "Epoch: 998/2000... Training loss: 0.6446\n",
      "Epoch: 998/2000... Training loss: 0.4868\n",
      "Epoch: 998/2000... Training loss: 0.5650\n",
      "Epoch: 998/2000... Training loss: 0.4647\n",
      "Epoch: 998/2000... Training loss: 0.4858\n",
      "Epoch: 998/2000... Training loss: 0.6942\n",
      "Epoch: 998/2000... Training loss: 0.6062\n",
      "Epoch: 998/2000... Training loss: 0.6646\n",
      "Epoch: 998/2000... Training loss: 0.4498\n",
      "Epoch: 998/2000... Training loss: 0.5496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 998/2000... Training loss: 0.5345\n",
      "Epoch: 998/2000... Training loss: 0.3221\n",
      "Epoch: 998/2000... Training loss: 0.6279\n",
      "Epoch: 998/2000... Training loss: 0.5029\n",
      "Epoch: 998/2000... Training loss: 0.4793\n",
      "Epoch: 998/2000... Training loss: 0.4666\n",
      "Epoch: 998/2000... Training loss: 0.5766\n",
      "Epoch: 998/2000... Training loss: 0.5293\n",
      "Epoch: 998/2000... Training loss: 0.4419\n",
      "Epoch: 999/2000... Training loss: 0.4865\n",
      "Epoch: 999/2000... Training loss: 0.5777\n",
      "Epoch: 999/2000... Training loss: 0.4709\n",
      "Epoch: 999/2000... Training loss: 0.5351\n",
      "Epoch: 999/2000... Training loss: 0.3771\n",
      "Epoch: 999/2000... Training loss: 0.5155\n",
      "Epoch: 999/2000... Training loss: 0.5416\n",
      "Epoch: 999/2000... Training loss: 0.5486\n",
      "Epoch: 999/2000... Training loss: 0.5460\n",
      "Epoch: 999/2000... Training loss: 0.5661\n",
      "Epoch: 999/2000... Training loss: 0.6207\n",
      "Epoch: 999/2000... Training loss: 0.6311\n",
      "Epoch: 999/2000... Training loss: 0.5083\n",
      "Epoch: 999/2000... Training loss: 0.5686\n",
      "Epoch: 999/2000... Training loss: 0.7059\n",
      "Epoch: 999/2000... Training loss: 0.5494\n",
      "Epoch: 999/2000... Training loss: 0.5352\n",
      "Epoch: 999/2000... Training loss: 0.4155\n",
      "Epoch: 999/2000... Training loss: 0.3354\n",
      "Epoch: 999/2000... Training loss: 0.5992\n",
      "Epoch: 999/2000... Training loss: 0.3906\n",
      "Epoch: 999/2000... Training loss: 0.6276\n",
      "Epoch: 999/2000... Training loss: 0.5629\n",
      "Epoch: 999/2000... Training loss: 0.5239\n",
      "Epoch: 999/2000... Training loss: 0.4313\n",
      "Epoch: 999/2000... Training loss: 0.5217\n",
      "Epoch: 999/2000... Training loss: 0.4999\n",
      "Epoch: 999/2000... Training loss: 0.5026\n",
      "Epoch: 999/2000... Training loss: 0.3866\n",
      "Epoch: 999/2000... Training loss: 0.5296\n",
      "Epoch: 999/2000... Training loss: 0.6950\n",
      "Epoch: 1000/2000... Training loss: 0.5505\n",
      "Epoch: 1000/2000... Training loss: 0.4223\n",
      "Epoch: 1000/2000... Training loss: 0.3331\n",
      "Epoch: 1000/2000... Training loss: 0.5675\n",
      "Epoch: 1000/2000... Training loss: 0.4269\n",
      "Epoch: 1000/2000... Training loss: 0.5377\n",
      "Epoch: 1000/2000... Training loss: 0.5743\n",
      "Epoch: 1000/2000... Training loss: 0.4636\n",
      "Epoch: 1000/2000... Training loss: 0.6365\n",
      "Epoch: 1000/2000... Training loss: 0.5785\n",
      "Epoch: 1000/2000... Training loss: 0.6859\n",
      "Epoch: 1000/2000... Training loss: 0.4265\n",
      "Epoch: 1000/2000... Training loss: 0.5543\n",
      "Epoch: 1000/2000... Training loss: 0.3692\n",
      "Epoch: 1000/2000... Training loss: 0.4103\n",
      "Epoch: 1000/2000... Training loss: 0.4443\n",
      "Epoch: 1000/2000... Training loss: 0.5374\n",
      "Epoch: 1000/2000... Training loss: 0.6236\n",
      "Epoch: 1000/2000... Training loss: 0.4635\n",
      "Epoch: 1000/2000... Training loss: 0.6884\n",
      "Epoch: 1000/2000... Training loss: 0.5439\n",
      "Epoch: 1000/2000... Training loss: 0.4926\n",
      "Epoch: 1000/2000... Training loss: 0.6281\n",
      "Epoch: 1000/2000... Training loss: 0.4327\n",
      "Epoch: 1000/2000... Training loss: 0.4889\n",
      "Epoch: 1000/2000... Training loss: 0.4984\n",
      "Epoch: 1000/2000... Training loss: 0.4567\n",
      "Epoch: 1000/2000... Training loss: 0.5984\n",
      "Epoch: 1000/2000... Training loss: 0.5546\n",
      "Epoch: 1000/2000... Training loss: 0.6195\n",
      "Epoch: 1000/2000... Training loss: 0.4776\n",
      "Epoch: 1001/2000... Training loss: 0.5839\n",
      "Epoch: 1001/2000... Training loss: 0.5342\n",
      "Epoch: 1001/2000... Training loss: 0.5128\n",
      "Epoch: 1001/2000... Training loss: 0.4497\n",
      "Epoch: 1001/2000... Training loss: 0.6158\n",
      "Epoch: 1001/2000... Training loss: 0.5365\n",
      "Epoch: 1001/2000... Training loss: 0.4462\n",
      "Epoch: 1001/2000... Training loss: 0.4179\n",
      "Epoch: 1001/2000... Training loss: 0.6592\n",
      "Epoch: 1001/2000... Training loss: 0.6690\n",
      "Epoch: 1001/2000... Training loss: 0.7363\n",
      "Epoch: 1001/2000... Training loss: 0.6061\n",
      "Epoch: 1001/2000... Training loss: 0.5221\n",
      "Epoch: 1001/2000... Training loss: 0.6224\n",
      "Epoch: 1001/2000... Training loss: 0.5331\n",
      "Epoch: 1001/2000... Training loss: 0.3843\n",
      "Epoch: 1001/2000... Training loss: 0.5776\n",
      "Epoch: 1001/2000... Training loss: 0.5916\n",
      "Epoch: 1001/2000... Training loss: 0.6205\n",
      "Epoch: 1001/2000... Training loss: 0.5495\n",
      "Epoch: 1001/2000... Training loss: 0.4952\n",
      "Epoch: 1001/2000... Training loss: 0.3974\n",
      "Epoch: 1001/2000... Training loss: 0.4674\n",
      "Epoch: 1001/2000... Training loss: 0.4920\n",
      "Epoch: 1001/2000... Training loss: 0.6593\n",
      "Epoch: 1001/2000... Training loss: 0.5403\n",
      "Epoch: 1001/2000... Training loss: 0.3927\n",
      "Epoch: 1001/2000... Training loss: 0.4433\n",
      "Epoch: 1001/2000... Training loss: 0.5194\n",
      "Epoch: 1001/2000... Training loss: 0.5997\n",
      "Epoch: 1001/2000... Training loss: 0.7578\n",
      "Epoch: 1002/2000... Training loss: 0.3832\n",
      "Epoch: 1002/2000... Training loss: 0.3334\n",
      "Epoch: 1002/2000... Training loss: 0.5084\n",
      "Epoch: 1002/2000... Training loss: 0.3461\n",
      "Epoch: 1002/2000... Training loss: 0.3919\n",
      "Epoch: 1002/2000... Training loss: 0.5571\n",
      "Epoch: 1002/2000... Training loss: 0.6153\n",
      "Epoch: 1002/2000... Training loss: 0.4304\n",
      "Epoch: 1002/2000... Training loss: 0.4392\n",
      "Epoch: 1002/2000... Training loss: 0.4432\n",
      "Epoch: 1002/2000... Training loss: 0.4562\n",
      "Epoch: 1002/2000... Training loss: 0.5394\n",
      "Epoch: 1002/2000... Training loss: 0.4336\n",
      "Epoch: 1002/2000... Training loss: 0.6085\n",
      "Epoch: 1002/2000... Training loss: 0.6466\n",
      "Epoch: 1002/2000... Training loss: 0.5282\n",
      "Epoch: 1002/2000... Training loss: 0.5492\n",
      "Epoch: 1002/2000... Training loss: 0.4481\n",
      "Epoch: 1002/2000... Training loss: 0.5703\n",
      "Epoch: 1002/2000... Training loss: 0.4159\n",
      "Epoch: 1002/2000... Training loss: 0.4883\n",
      "Epoch: 1002/2000... Training loss: 0.5009\n",
      "Epoch: 1002/2000... Training loss: 0.3308\n",
      "Epoch: 1002/2000... Training loss: 0.4983\n",
      "Epoch: 1002/2000... Training loss: 0.4636\n",
      "Epoch: 1002/2000... Training loss: 0.4141\n",
      "Epoch: 1002/2000... Training loss: 0.3747\n",
      "Epoch: 1002/2000... Training loss: 0.4035\n",
      "Epoch: 1002/2000... Training loss: 0.7217\n",
      "Epoch: 1002/2000... Training loss: 0.4497\n",
      "Epoch: 1002/2000... Training loss: 0.5377\n",
      "Epoch: 1003/2000... Training loss: 0.6779\n",
      "Epoch: 1003/2000... Training loss: 0.4912\n",
      "Epoch: 1003/2000... Training loss: 0.6013\n",
      "Epoch: 1003/2000... Training loss: 0.4918\n",
      "Epoch: 1003/2000... Training loss: 0.4471\n",
      "Epoch: 1003/2000... Training loss: 0.6885\n",
      "Epoch: 1003/2000... Training loss: 0.5542\n",
      "Epoch: 1003/2000... Training loss: 0.5021\n",
      "Epoch: 1003/2000... Training loss: 0.3368\n",
      "Epoch: 1003/2000... Training loss: 0.5349\n",
      "Epoch: 1003/2000... Training loss: 0.6744\n",
      "Epoch: 1003/2000... Training loss: 0.4849\n",
      "Epoch: 1003/2000... Training loss: 0.3602\n",
      "Epoch: 1003/2000... Training loss: 0.5590\n",
      "Epoch: 1003/2000... Training loss: 0.7151\n",
      "Epoch: 1003/2000... Training loss: 0.4489\n",
      "Epoch: 1003/2000... Training loss: 0.5545\n",
      "Epoch: 1003/2000... Training loss: 0.4709\n",
      "Epoch: 1003/2000... Training loss: 0.4926\n",
      "Epoch: 1003/2000... Training loss: 0.5869\n",
      "Epoch: 1003/2000... Training loss: 0.3715\n",
      "Epoch: 1003/2000... Training loss: 0.6832\n",
      "Epoch: 1003/2000... Training loss: 0.5006\n",
      "Epoch: 1003/2000... Training loss: 0.4713\n",
      "Epoch: 1003/2000... Training loss: 0.4287\n",
      "Epoch: 1003/2000... Training loss: 0.5893\n",
      "Epoch: 1003/2000... Training loss: 0.5717\n",
      "Epoch: 1003/2000... Training loss: 0.5524\n",
      "Epoch: 1003/2000... Training loss: 0.3866\n",
      "Epoch: 1003/2000... Training loss: 0.5102\n",
      "Epoch: 1003/2000... Training loss: 0.3873\n",
      "Epoch: 1004/2000... Training loss: 0.4526\n",
      "Epoch: 1004/2000... Training loss: 0.4167\n",
      "Epoch: 1004/2000... Training loss: 0.3898\n",
      "Epoch: 1004/2000... Training loss: 0.5992\n",
      "Epoch: 1004/2000... Training loss: 0.4925\n",
      "Epoch: 1004/2000... Training loss: 0.4819\n",
      "Epoch: 1004/2000... Training loss: 0.5584\n",
      "Epoch: 1004/2000... Training loss: 0.4880\n",
      "Epoch: 1004/2000... Training loss: 0.6250\n",
      "Epoch: 1004/2000... Training loss: 0.6411\n",
      "Epoch: 1004/2000... Training loss: 0.7573\n",
      "Epoch: 1004/2000... Training loss: 0.4353\n",
      "Epoch: 1004/2000... Training loss: 0.5289\n",
      "Epoch: 1004/2000... Training loss: 0.4737\n",
      "Epoch: 1004/2000... Training loss: 0.4583\n",
      "Epoch: 1004/2000... Training loss: 0.6821\n",
      "Epoch: 1004/2000... Training loss: 0.4619\n",
      "Epoch: 1004/2000... Training loss: 0.5185\n",
      "Epoch: 1004/2000... Training loss: 0.6100\n",
      "Epoch: 1004/2000... Training loss: 0.6047\n",
      "Epoch: 1004/2000... Training loss: 0.3933\n",
      "Epoch: 1004/2000... Training loss: 0.4325\n",
      "Epoch: 1004/2000... Training loss: 0.5688\n",
      "Epoch: 1004/2000... Training loss: 0.6499\n",
      "Epoch: 1004/2000... Training loss: 0.5516\n",
      "Epoch: 1004/2000... Training loss: 0.4930\n",
      "Epoch: 1004/2000... Training loss: 0.4653\n",
      "Epoch: 1004/2000... Training loss: 0.4434\n",
      "Epoch: 1004/2000... Training loss: 0.4881\n",
      "Epoch: 1004/2000... Training loss: 0.4291\n",
      "Epoch: 1004/2000... Training loss: 0.5719\n",
      "Epoch: 1005/2000... Training loss: 0.4000\n",
      "Epoch: 1005/2000... Training loss: 0.6300\n",
      "Epoch: 1005/2000... Training loss: 0.4315\n",
      "Epoch: 1005/2000... Training loss: 0.4037\n",
      "Epoch: 1005/2000... Training loss: 0.4575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1005/2000... Training loss: 0.4839\n",
      "Epoch: 1005/2000... Training loss: 0.6807\n",
      "Epoch: 1005/2000... Training loss: 0.6146\n",
      "Epoch: 1005/2000... Training loss: 0.5636\n",
      "Epoch: 1005/2000... Training loss: 0.3701\n",
      "Epoch: 1005/2000... Training loss: 0.4058\n",
      "Epoch: 1005/2000... Training loss: 0.5794\n",
      "Epoch: 1005/2000... Training loss: 0.5127\n",
      "Epoch: 1005/2000... Training loss: 0.4057\n",
      "Epoch: 1005/2000... Training loss: 0.6554\n",
      "Epoch: 1005/2000... Training loss: 0.6129\n",
      "Epoch: 1005/2000... Training loss: 0.4906\n",
      "Epoch: 1005/2000... Training loss: 0.4655\n",
      "Epoch: 1005/2000... Training loss: 0.4195\n",
      "Epoch: 1005/2000... Training loss: 0.4219\n",
      "Epoch: 1005/2000... Training loss: 0.3654\n",
      "Epoch: 1005/2000... Training loss: 0.5798\n",
      "Epoch: 1005/2000... Training loss: 0.3804\n",
      "Epoch: 1005/2000... Training loss: 0.4674\n",
      "Epoch: 1005/2000... Training loss: 0.6267\n",
      "Epoch: 1005/2000... Training loss: 0.6182\n",
      "Epoch: 1005/2000... Training loss: 0.4017\n",
      "Epoch: 1005/2000... Training loss: 0.4876\n",
      "Epoch: 1005/2000... Training loss: 0.5317\n",
      "Epoch: 1005/2000... Training loss: 0.3600\n",
      "Epoch: 1005/2000... Training loss: 0.5368\n",
      "Epoch: 1006/2000... Training loss: 0.6172\n",
      "Epoch: 1006/2000... Training loss: 0.3944\n",
      "Epoch: 1006/2000... Training loss: 0.6433\n",
      "Epoch: 1006/2000... Training loss: 0.7141\n",
      "Epoch: 1006/2000... Training loss: 0.6627\n",
      "Epoch: 1006/2000... Training loss: 0.5956\n",
      "Epoch: 1006/2000... Training loss: 0.4879\n",
      "Epoch: 1006/2000... Training loss: 0.3352\n",
      "Epoch: 1006/2000... Training loss: 0.5990\n",
      "Epoch: 1006/2000... Training loss: 0.5189\n",
      "Epoch: 1006/2000... Training loss: 0.3644\n",
      "Epoch: 1006/2000... Training loss: 0.5610\n",
      "Epoch: 1006/2000... Training loss: 0.4472\n",
      "Epoch: 1006/2000... Training loss: 0.4412\n",
      "Epoch: 1006/2000... Training loss: 0.3641\n",
      "Epoch: 1006/2000... Training loss: 0.4861\n",
      "Epoch: 1006/2000... Training loss: 0.4615\n",
      "Epoch: 1006/2000... Training loss: 0.4459\n",
      "Epoch: 1006/2000... Training loss: 0.6815\n",
      "Epoch: 1006/2000... Training loss: 0.4347\n",
      "Epoch: 1006/2000... Training loss: 0.4832\n",
      "Epoch: 1006/2000... Training loss: 0.3814\n",
      "Epoch: 1006/2000... Training loss: 0.5863\n",
      "Epoch: 1006/2000... Training loss: 0.5265\n",
      "Epoch: 1006/2000... Training loss: 0.4602\n",
      "Epoch: 1006/2000... Training loss: 0.4941\n",
      "Epoch: 1006/2000... Training loss: 0.4179\n",
      "Epoch: 1006/2000... Training loss: 0.4344\n",
      "Epoch: 1006/2000... Training loss: 0.6447\n",
      "Epoch: 1006/2000... Training loss: 0.6968\n",
      "Epoch: 1006/2000... Training loss: 0.3926\n",
      "Epoch: 1007/2000... Training loss: 0.5868\n",
      "Epoch: 1007/2000... Training loss: 0.5341\n",
      "Epoch: 1007/2000... Training loss: 0.5032\n",
      "Epoch: 1007/2000... Training loss: 0.6367\n",
      "Epoch: 1007/2000... Training loss: 0.4310\n",
      "Epoch: 1007/2000... Training loss: 0.6469\n",
      "Epoch: 1007/2000... Training loss: 0.5438\n",
      "Epoch: 1007/2000... Training loss: 0.6380\n",
      "Epoch: 1007/2000... Training loss: 0.5548\n",
      "Epoch: 1007/2000... Training loss: 0.6345\n",
      "Epoch: 1007/2000... Training loss: 0.5814\n",
      "Epoch: 1007/2000... Training loss: 0.5402\n",
      "Epoch: 1007/2000... Training loss: 0.4395\n",
      "Epoch: 1007/2000... Training loss: 0.7422\n",
      "Epoch: 1007/2000... Training loss: 0.5665\n",
      "Epoch: 1007/2000... Training loss: 0.5879\n",
      "Epoch: 1007/2000... Training loss: 0.2919\n",
      "Epoch: 1007/2000... Training loss: 0.5765\n",
      "Epoch: 1007/2000... Training loss: 0.4586\n",
      "Epoch: 1007/2000... Training loss: 0.7045\n",
      "Epoch: 1007/2000... Training loss: 0.3257\n",
      "Epoch: 1007/2000... Training loss: 0.3943\n",
      "Epoch: 1007/2000... Training loss: 0.5954\n",
      "Epoch: 1007/2000... Training loss: 0.2759\n",
      "Epoch: 1007/2000... Training loss: 0.6324\n",
      "Epoch: 1007/2000... Training loss: 0.4702\n",
      "Epoch: 1007/2000... Training loss: 0.5920\n",
      "Epoch: 1007/2000... Training loss: 0.5524\n",
      "Epoch: 1007/2000... Training loss: 0.6036\n",
      "Epoch: 1007/2000... Training loss: 0.4772\n",
      "Epoch: 1007/2000... Training loss: 0.4793\n",
      "Epoch: 1008/2000... Training loss: 0.6684\n",
      "Epoch: 1008/2000... Training loss: 0.4508\n",
      "Epoch: 1008/2000... Training loss: 0.4358\n",
      "Epoch: 1008/2000... Training loss: 0.4434\n",
      "Epoch: 1008/2000... Training loss: 0.3882\n",
      "Epoch: 1008/2000... Training loss: 0.5158\n",
      "Epoch: 1008/2000... Training loss: 0.3813\n",
      "Epoch: 1008/2000... Training loss: 0.6262\n",
      "Epoch: 1008/2000... Training loss: 0.4895\n",
      "Epoch: 1008/2000... Training loss: 0.3971\n",
      "Epoch: 1008/2000... Training loss: 0.4474\n",
      "Epoch: 1008/2000... Training loss: 0.6395\n",
      "Epoch: 1008/2000... Training loss: 0.4824\n",
      "Epoch: 1008/2000... Training loss: 0.4563\n",
      "Epoch: 1008/2000... Training loss: 0.5629\n",
      "Epoch: 1008/2000... Training loss: 0.2708\n",
      "Epoch: 1008/2000... Training loss: 0.4597\n",
      "Epoch: 1008/2000... Training loss: 0.3973\n",
      "Epoch: 1008/2000... Training loss: 0.4965\n",
      "Epoch: 1008/2000... Training loss: 0.6014\n",
      "Epoch: 1008/2000... Training loss: 0.5085\n",
      "Epoch: 1008/2000... Training loss: 0.5774\n",
      "Epoch: 1008/2000... Training loss: 0.4728\n",
      "Epoch: 1008/2000... Training loss: 0.4166\n",
      "Epoch: 1008/2000... Training loss: 0.5026\n",
      "Epoch: 1008/2000... Training loss: 0.4709\n",
      "Epoch: 1008/2000... Training loss: 0.5669\n",
      "Epoch: 1008/2000... Training loss: 0.6354\n",
      "Epoch: 1008/2000... Training loss: 0.5813\n",
      "Epoch: 1008/2000... Training loss: 0.6907\n",
      "Epoch: 1008/2000... Training loss: 0.4472\n",
      "Epoch: 1009/2000... Training loss: 0.7195\n",
      "Epoch: 1009/2000... Training loss: 0.5760\n",
      "Epoch: 1009/2000... Training loss: 0.4084\n",
      "Epoch: 1009/2000... Training loss: 0.5079\n",
      "Epoch: 1009/2000... Training loss: 0.6757\n",
      "Epoch: 1009/2000... Training loss: 0.7298\n",
      "Epoch: 1009/2000... Training loss: 0.3770\n",
      "Epoch: 1009/2000... Training loss: 0.4985\n",
      "Epoch: 1009/2000... Training loss: 0.2844\n",
      "Epoch: 1009/2000... Training loss: 0.3925\n",
      "Epoch: 1009/2000... Training loss: 0.5034\n",
      "Epoch: 1009/2000... Training loss: 0.5636\n",
      "Epoch: 1009/2000... Training loss: 0.3196\n",
      "Epoch: 1009/2000... Training loss: 0.5438\n",
      "Epoch: 1009/2000... Training loss: 0.6172\n",
      "Epoch: 1009/2000... Training loss: 0.4329\n",
      "Epoch: 1009/2000... Training loss: 0.4685\n",
      "Epoch: 1009/2000... Training loss: 0.5358\n",
      "Epoch: 1009/2000... Training loss: 0.4940\n",
      "Epoch: 1009/2000... Training loss: 0.5290\n",
      "Epoch: 1009/2000... Training loss: 0.7149\n",
      "Epoch: 1009/2000... Training loss: 0.5321\n",
      "Epoch: 1009/2000... Training loss: 0.4597\n",
      "Epoch: 1009/2000... Training loss: 0.6449\n",
      "Epoch: 1009/2000... Training loss: 0.6255\n",
      "Epoch: 1009/2000... Training loss: 0.5337\n",
      "Epoch: 1009/2000... Training loss: 0.4838\n",
      "Epoch: 1009/2000... Training loss: 0.4287\n",
      "Epoch: 1009/2000... Training loss: 0.4995\n",
      "Epoch: 1009/2000... Training loss: 0.5320\n",
      "Epoch: 1009/2000... Training loss: 0.4933\n",
      "Epoch: 1010/2000... Training loss: 0.6040\n",
      "Epoch: 1010/2000... Training loss: 0.5844\n",
      "Epoch: 1010/2000... Training loss: 0.4854\n",
      "Epoch: 1010/2000... Training loss: 0.5570\n",
      "Epoch: 1010/2000... Training loss: 0.3378\n",
      "Epoch: 1010/2000... Training loss: 0.3781\n",
      "Epoch: 1010/2000... Training loss: 0.3782\n",
      "Epoch: 1010/2000... Training loss: 0.3688\n",
      "Epoch: 1010/2000... Training loss: 0.6925\n",
      "Epoch: 1010/2000... Training loss: 0.6613\n",
      "Epoch: 1010/2000... Training loss: 0.5253\n",
      "Epoch: 1010/2000... Training loss: 0.4717\n",
      "Epoch: 1010/2000... Training loss: 0.5524\n",
      "Epoch: 1010/2000... Training loss: 0.4589\n",
      "Epoch: 1010/2000... Training loss: 0.5461\n",
      "Epoch: 1010/2000... Training loss: 0.5808\n",
      "Epoch: 1010/2000... Training loss: 0.4483\n",
      "Epoch: 1010/2000... Training loss: 0.6364\n",
      "Epoch: 1010/2000... Training loss: 0.4315\n",
      "Epoch: 1010/2000... Training loss: 0.4560\n",
      "Epoch: 1010/2000... Training loss: 0.5896\n",
      "Epoch: 1010/2000... Training loss: 0.5239\n",
      "Epoch: 1010/2000... Training loss: 0.5715\n",
      "Epoch: 1010/2000... Training loss: 0.4920\n",
      "Epoch: 1010/2000... Training loss: 0.7036\n",
      "Epoch: 1010/2000... Training loss: 0.6007\n",
      "Epoch: 1010/2000... Training loss: 0.4834\n",
      "Epoch: 1010/2000... Training loss: 0.6350\n",
      "Epoch: 1010/2000... Training loss: 0.4737\n",
      "Epoch: 1010/2000... Training loss: 0.5519\n",
      "Epoch: 1010/2000... Training loss: 0.4757\n",
      "Epoch: 1011/2000... Training loss: 0.3637\n",
      "Epoch: 1011/2000... Training loss: 0.8336\n",
      "Epoch: 1011/2000... Training loss: 0.5823\n",
      "Epoch: 1011/2000... Training loss: 0.4565\n",
      "Epoch: 1011/2000... Training loss: 0.5853\n",
      "Epoch: 1011/2000... Training loss: 0.6002\n",
      "Epoch: 1011/2000... Training loss: 0.5096\n",
      "Epoch: 1011/2000... Training loss: 0.7686\n",
      "Epoch: 1011/2000... Training loss: 0.4843\n",
      "Epoch: 1011/2000... Training loss: 0.6406\n",
      "Epoch: 1011/2000... Training loss: 0.3899\n",
      "Epoch: 1011/2000... Training loss: 0.3498\n",
      "Epoch: 1011/2000... Training loss: 0.3190\n",
      "Epoch: 1011/2000... Training loss: 0.6852\n",
      "Epoch: 1011/2000... Training loss: 0.5016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1011/2000... Training loss: 0.4241\n",
      "Epoch: 1011/2000... Training loss: 0.4287\n",
      "Epoch: 1011/2000... Training loss: 0.5515\n",
      "Epoch: 1011/2000... Training loss: 0.4800\n",
      "Epoch: 1011/2000... Training loss: 0.5446\n",
      "Epoch: 1011/2000... Training loss: 0.5617\n",
      "Epoch: 1011/2000... Training loss: 0.4293\n",
      "Epoch: 1011/2000... Training loss: 0.5471\n",
      "Epoch: 1011/2000... Training loss: 0.4667\n",
      "Epoch: 1011/2000... Training loss: 0.7185\n",
      "Epoch: 1011/2000... Training loss: 0.6312\n",
      "Epoch: 1011/2000... Training loss: 0.5217\n",
      "Epoch: 1011/2000... Training loss: 0.3852\n",
      "Epoch: 1011/2000... Training loss: 0.5337\n",
      "Epoch: 1011/2000... Training loss: 0.6387\n",
      "Epoch: 1011/2000... Training loss: 0.5652\n",
      "Epoch: 1012/2000... Training loss: 0.5329\n",
      "Epoch: 1012/2000... Training loss: 0.5276\n",
      "Epoch: 1012/2000... Training loss: 0.4265\n",
      "Epoch: 1012/2000... Training loss: 0.3696\n",
      "Epoch: 1012/2000... Training loss: 0.5853\n",
      "Epoch: 1012/2000... Training loss: 0.4065\n",
      "Epoch: 1012/2000... Training loss: 0.4709\n",
      "Epoch: 1012/2000... Training loss: 0.5402\n",
      "Epoch: 1012/2000... Training loss: 0.6523\n",
      "Epoch: 1012/2000... Training loss: 0.4331\n",
      "Epoch: 1012/2000... Training loss: 0.6459\n",
      "Epoch: 1012/2000... Training loss: 0.5896\n",
      "Epoch: 1012/2000... Training loss: 0.5963\n",
      "Epoch: 1012/2000... Training loss: 0.3930\n",
      "Epoch: 1012/2000... Training loss: 0.4722\n",
      "Epoch: 1012/2000... Training loss: 0.6104\n",
      "Epoch: 1012/2000... Training loss: 0.4301\n",
      "Epoch: 1012/2000... Training loss: 0.5337\n",
      "Epoch: 1012/2000... Training loss: 0.3215\n",
      "Epoch: 1012/2000... Training loss: 0.4431\n",
      "Epoch: 1012/2000... Training loss: 0.5004\n",
      "Epoch: 1012/2000... Training loss: 0.5620\n",
      "Epoch: 1012/2000... Training loss: 0.5711\n",
      "Epoch: 1012/2000... Training loss: 0.5238\n",
      "Epoch: 1012/2000... Training loss: 0.5287\n",
      "Epoch: 1012/2000... Training loss: 0.5027\n",
      "Epoch: 1012/2000... Training loss: 0.8185\n",
      "Epoch: 1012/2000... Training loss: 0.6773\n",
      "Epoch: 1012/2000... Training loss: 0.3739\n",
      "Epoch: 1012/2000... Training loss: 0.4030\n",
      "Epoch: 1012/2000... Training loss: 0.5480\n",
      "Epoch: 1013/2000... Training loss: 0.3690\n",
      "Epoch: 1013/2000... Training loss: 0.4182\n",
      "Epoch: 1013/2000... Training loss: 0.4240\n",
      "Epoch: 1013/2000... Training loss: 0.3346\n",
      "Epoch: 1013/2000... Training loss: 0.5924\n",
      "Epoch: 1013/2000... Training loss: 0.4891\n",
      "Epoch: 1013/2000... Training loss: 0.5187\n",
      "Epoch: 1013/2000... Training loss: 0.4409\n",
      "Epoch: 1013/2000... Training loss: 0.5021\n",
      "Epoch: 1013/2000... Training loss: 0.5765\n",
      "Epoch: 1013/2000... Training loss: 0.5937\n",
      "Epoch: 1013/2000... Training loss: 0.5769\n",
      "Epoch: 1013/2000... Training loss: 0.4163\n",
      "Epoch: 1013/2000... Training loss: 0.7236\n",
      "Epoch: 1013/2000... Training loss: 0.4280\n",
      "Epoch: 1013/2000... Training loss: 0.5473\n",
      "Epoch: 1013/2000... Training loss: 0.6895\n",
      "Epoch: 1013/2000... Training loss: 0.4623\n",
      "Epoch: 1013/2000... Training loss: 0.5359\n",
      "Epoch: 1013/2000... Training loss: 0.5451\n",
      "Epoch: 1013/2000... Training loss: 0.4200\n",
      "Epoch: 1013/2000... Training loss: 0.4449\n",
      "Epoch: 1013/2000... Training loss: 0.7460\n",
      "Epoch: 1013/2000... Training loss: 0.5417\n",
      "Epoch: 1013/2000... Training loss: 0.5297\n",
      "Epoch: 1013/2000... Training loss: 0.4576\n",
      "Epoch: 1013/2000... Training loss: 0.4128\n",
      "Epoch: 1013/2000... Training loss: 0.3908\n",
      "Epoch: 1013/2000... Training loss: 0.3631\n",
      "Epoch: 1013/2000... Training loss: 0.5229\n",
      "Epoch: 1013/2000... Training loss: 0.6343\n",
      "Epoch: 1014/2000... Training loss: 0.4691\n",
      "Epoch: 1014/2000... Training loss: 0.4503\n",
      "Epoch: 1014/2000... Training loss: 0.6733\n",
      "Epoch: 1014/2000... Training loss: 0.4048\n",
      "Epoch: 1014/2000... Training loss: 0.5333\n",
      "Epoch: 1014/2000... Training loss: 0.4819\n",
      "Epoch: 1014/2000... Training loss: 0.3811\n",
      "Epoch: 1014/2000... Training loss: 0.4869\n",
      "Epoch: 1014/2000... Training loss: 0.6053\n",
      "Epoch: 1014/2000... Training loss: 0.3892\n",
      "Epoch: 1014/2000... Training loss: 0.5025\n",
      "Epoch: 1014/2000... Training loss: 0.3948\n",
      "Epoch: 1014/2000... Training loss: 0.6223\n",
      "Epoch: 1014/2000... Training loss: 0.5020\n",
      "Epoch: 1014/2000... Training loss: 0.4204\n",
      "Epoch: 1014/2000... Training loss: 0.5295\n",
      "Epoch: 1014/2000... Training loss: 0.4361\n",
      "Epoch: 1014/2000... Training loss: 0.4524\n",
      "Epoch: 1014/2000... Training loss: 0.6749\n",
      "Epoch: 1014/2000... Training loss: 0.5774\n",
      "Epoch: 1014/2000... Training loss: 0.5589\n",
      "Epoch: 1014/2000... Training loss: 0.5904\n",
      "Epoch: 1014/2000... Training loss: 0.4569\n",
      "Epoch: 1014/2000... Training loss: 0.3943\n",
      "Epoch: 1014/2000... Training loss: 0.6703\n",
      "Epoch: 1014/2000... Training loss: 0.6042\n",
      "Epoch: 1014/2000... Training loss: 0.4787\n",
      "Epoch: 1014/2000... Training loss: 0.4329\n",
      "Epoch: 1014/2000... Training loss: 0.5167\n",
      "Epoch: 1014/2000... Training loss: 0.4132\n",
      "Epoch: 1014/2000... Training loss: 0.6716\n",
      "Epoch: 1015/2000... Training loss: 0.6036\n",
      "Epoch: 1015/2000... Training loss: 0.3840\n",
      "Epoch: 1015/2000... Training loss: 0.4325\n",
      "Epoch: 1015/2000... Training loss: 0.4142\n",
      "Epoch: 1015/2000... Training loss: 0.5260\n",
      "Epoch: 1015/2000... Training loss: 0.4362\n",
      "Epoch: 1015/2000... Training loss: 0.4931\n",
      "Epoch: 1015/2000... Training loss: 0.5345\n",
      "Epoch: 1015/2000... Training loss: 0.4182\n",
      "Epoch: 1015/2000... Training loss: 0.5423\n",
      "Epoch: 1015/2000... Training loss: 0.3754\n",
      "Epoch: 1015/2000... Training loss: 0.4716\n",
      "Epoch: 1015/2000... Training loss: 0.6352\n",
      "Epoch: 1015/2000... Training loss: 0.6223\n",
      "Epoch: 1015/2000... Training loss: 0.5282\n",
      "Epoch: 1015/2000... Training loss: 0.3612\n",
      "Epoch: 1015/2000... Training loss: 0.7192\n",
      "Epoch: 1015/2000... Training loss: 0.5249\n",
      "Epoch: 1015/2000... Training loss: 0.3946\n",
      "Epoch: 1015/2000... Training loss: 0.4827\n",
      "Epoch: 1015/2000... Training loss: 0.6471\n",
      "Epoch: 1015/2000... Training loss: 0.5302\n",
      "Epoch: 1015/2000... Training loss: 0.4857\n",
      "Epoch: 1015/2000... Training loss: 0.5828\n",
      "Epoch: 1015/2000... Training loss: 0.7825\n",
      "Epoch: 1015/2000... Training loss: 0.5953\n",
      "Epoch: 1015/2000... Training loss: 0.3996\n",
      "Epoch: 1015/2000... Training loss: 0.6750\n",
      "Epoch: 1015/2000... Training loss: 0.6108\n",
      "Epoch: 1015/2000... Training loss: 0.5265\n",
      "Epoch: 1015/2000... Training loss: 0.4321\n",
      "Epoch: 1016/2000... Training loss: 0.5223\n",
      "Epoch: 1016/2000... Training loss: 0.5760\n",
      "Epoch: 1016/2000... Training loss: 0.5956\n",
      "Epoch: 1016/2000... Training loss: 0.5124\n",
      "Epoch: 1016/2000... Training loss: 0.5864\n",
      "Epoch: 1016/2000... Training loss: 0.5564\n",
      "Epoch: 1016/2000... Training loss: 0.4485\n",
      "Epoch: 1016/2000... Training loss: 0.4830\n",
      "Epoch: 1016/2000... Training loss: 0.6307\n",
      "Epoch: 1016/2000... Training loss: 0.5591\n",
      "Epoch: 1016/2000... Training loss: 0.4336\n",
      "Epoch: 1016/2000... Training loss: 0.4599\n",
      "Epoch: 1016/2000... Training loss: 0.4567\n",
      "Epoch: 1016/2000... Training loss: 0.6340\n",
      "Epoch: 1016/2000... Training loss: 0.4799\n",
      "Epoch: 1016/2000... Training loss: 0.5391\n",
      "Epoch: 1016/2000... Training loss: 0.5739\n",
      "Epoch: 1016/2000... Training loss: 0.3716\n",
      "Epoch: 1016/2000... Training loss: 0.4852\n",
      "Epoch: 1016/2000... Training loss: 0.6315\n",
      "Epoch: 1016/2000... Training loss: 0.5672\n",
      "Epoch: 1016/2000... Training loss: 0.5413\n",
      "Epoch: 1016/2000... Training loss: 0.5401\n",
      "Epoch: 1016/2000... Training loss: 0.5244\n",
      "Epoch: 1016/2000... Training loss: 0.4740\n",
      "Epoch: 1016/2000... Training loss: 0.7718\n",
      "Epoch: 1016/2000... Training loss: 0.6139\n",
      "Epoch: 1016/2000... Training loss: 0.4492\n",
      "Epoch: 1016/2000... Training loss: 0.3881\n",
      "Epoch: 1016/2000... Training loss: 0.6105\n",
      "Epoch: 1016/2000... Training loss: 0.8586\n",
      "Epoch: 1017/2000... Training loss: 0.5802\n",
      "Epoch: 1017/2000... Training loss: 0.5628\n",
      "Epoch: 1017/2000... Training loss: 0.6670\n",
      "Epoch: 1017/2000... Training loss: 0.7201\n",
      "Epoch: 1017/2000... Training loss: 0.6378\n",
      "Epoch: 1017/2000... Training loss: 0.5945\n",
      "Epoch: 1017/2000... Training loss: 0.5790\n",
      "Epoch: 1017/2000... Training loss: 0.3860\n",
      "Epoch: 1017/2000... Training loss: 0.5390\n",
      "Epoch: 1017/2000... Training loss: 0.5175\n",
      "Epoch: 1017/2000... Training loss: 0.4984\n",
      "Epoch: 1017/2000... Training loss: 0.3174\n",
      "Epoch: 1017/2000... Training loss: 0.5855\n",
      "Epoch: 1017/2000... Training loss: 0.4466\n",
      "Epoch: 1017/2000... Training loss: 0.4714\n",
      "Epoch: 1017/2000... Training loss: 0.5807\n",
      "Epoch: 1017/2000... Training loss: 0.4658\n",
      "Epoch: 1017/2000... Training loss: 0.4160\n",
      "Epoch: 1017/2000... Training loss: 0.5550\n",
      "Epoch: 1017/2000... Training loss: 0.4205\n",
      "Epoch: 1017/2000... Training loss: 0.5372\n",
      "Epoch: 1017/2000... Training loss: 0.5019\n",
      "Epoch: 1017/2000... Training loss: 0.5075\n",
      "Epoch: 1017/2000... Training loss: 0.5992\n",
      "Epoch: 1017/2000... Training loss: 0.5316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1017/2000... Training loss: 0.4745\n",
      "Epoch: 1017/2000... Training loss: 0.6028\n",
      "Epoch: 1017/2000... Training loss: 0.4491\n",
      "Epoch: 1017/2000... Training loss: 0.6147\n",
      "Epoch: 1017/2000... Training loss: 0.8040\n",
      "Epoch: 1017/2000... Training loss: 0.5492\n",
      "Epoch: 1018/2000... Training loss: 0.6606\n",
      "Epoch: 1018/2000... Training loss: 0.6291\n",
      "Epoch: 1018/2000... Training loss: 0.6671\n",
      "Epoch: 1018/2000... Training loss: 0.5177\n",
      "Epoch: 1018/2000... Training loss: 0.5234\n",
      "Epoch: 1018/2000... Training loss: 0.3187\n",
      "Epoch: 1018/2000... Training loss: 0.5530\n",
      "Epoch: 1018/2000... Training loss: 0.5572\n",
      "Epoch: 1018/2000... Training loss: 0.4313\n",
      "Epoch: 1018/2000... Training loss: 0.5663\n",
      "Epoch: 1018/2000... Training loss: 0.4307\n",
      "Epoch: 1018/2000... Training loss: 0.5593\n",
      "Epoch: 1018/2000... Training loss: 0.5359\n",
      "Epoch: 1018/2000... Training loss: 0.5041\n",
      "Epoch: 1018/2000... Training loss: 0.6342\n",
      "Epoch: 1018/2000... Training loss: 0.3217\n",
      "Epoch: 1018/2000... Training loss: 0.6438\n",
      "Epoch: 1018/2000... Training loss: 0.4222\n",
      "Epoch: 1018/2000... Training loss: 0.4696\n",
      "Epoch: 1018/2000... Training loss: 0.6325\n",
      "Epoch: 1018/2000... Training loss: 0.4502\n",
      "Epoch: 1018/2000... Training loss: 0.4650\n",
      "Epoch: 1018/2000... Training loss: 0.4245\n",
      "Epoch: 1018/2000... Training loss: 0.4156\n",
      "Epoch: 1018/2000... Training loss: 0.5123\n",
      "Epoch: 1018/2000... Training loss: 0.7206\n",
      "Epoch: 1018/2000... Training loss: 0.4759\n",
      "Epoch: 1018/2000... Training loss: 0.5428\n",
      "Epoch: 1018/2000... Training loss: 0.4569\n",
      "Epoch: 1018/2000... Training loss: 0.8811\n",
      "Epoch: 1018/2000... Training loss: 0.6097\n",
      "Epoch: 1019/2000... Training loss: 0.5359\n",
      "Epoch: 1019/2000... Training loss: 0.6368\n",
      "Epoch: 1019/2000... Training loss: 0.5720\n",
      "Epoch: 1019/2000... Training loss: 0.5476\n",
      "Epoch: 1019/2000... Training loss: 0.5156\n",
      "Epoch: 1019/2000... Training loss: 0.6699\n",
      "Epoch: 1019/2000... Training loss: 0.5204\n",
      "Epoch: 1019/2000... Training loss: 0.4606\n",
      "Epoch: 1019/2000... Training loss: 0.6352\n",
      "Epoch: 1019/2000... Training loss: 0.5412\n",
      "Epoch: 1019/2000... Training loss: 0.4100\n",
      "Epoch: 1019/2000... Training loss: 0.4925\n",
      "Epoch: 1019/2000... Training loss: 0.6475\n",
      "Epoch: 1019/2000... Training loss: 0.6137\n",
      "Epoch: 1019/2000... Training loss: 0.4533\n",
      "Epoch: 1019/2000... Training loss: 0.5784\n",
      "Epoch: 1019/2000... Training loss: 0.4314\n",
      "Epoch: 1019/2000... Training loss: 0.4773\n",
      "Epoch: 1019/2000... Training loss: 0.5987\n",
      "Epoch: 1019/2000... Training loss: 0.5053\n",
      "Epoch: 1019/2000... Training loss: 0.5391\n",
      "Epoch: 1019/2000... Training loss: 0.4256\n",
      "Epoch: 1019/2000... Training loss: 0.6919\n",
      "Epoch: 1019/2000... Training loss: 0.5629\n",
      "Epoch: 1019/2000... Training loss: 0.5645\n",
      "Epoch: 1019/2000... Training loss: 0.3760\n",
      "Epoch: 1019/2000... Training loss: 0.6237\n",
      "Epoch: 1019/2000... Training loss: 0.6174\n",
      "Epoch: 1019/2000... Training loss: 0.5504\n",
      "Epoch: 1019/2000... Training loss: 0.3377\n",
      "Epoch: 1019/2000... Training loss: 0.5835\n",
      "Epoch: 1020/2000... Training loss: 0.6581\n",
      "Epoch: 1020/2000... Training loss: 0.5100\n",
      "Epoch: 1020/2000... Training loss: 0.6408\n",
      "Epoch: 1020/2000... Training loss: 0.3634\n",
      "Epoch: 1020/2000... Training loss: 0.5532\n",
      "Epoch: 1020/2000... Training loss: 0.5088\n",
      "Epoch: 1020/2000... Training loss: 0.5111\n",
      "Epoch: 1020/2000... Training loss: 0.3721\n",
      "Epoch: 1020/2000... Training loss: 0.6068\n",
      "Epoch: 1020/2000... Training loss: 0.5764\n",
      "Epoch: 1020/2000... Training loss: 0.4085\n",
      "Epoch: 1020/2000... Training loss: 0.6642\n",
      "Epoch: 1020/2000... Training loss: 0.5695\n",
      "Epoch: 1020/2000... Training loss: 0.6849\n",
      "Epoch: 1020/2000... Training loss: 0.4684\n",
      "Epoch: 1020/2000... Training loss: 0.6114\n",
      "Epoch: 1020/2000... Training loss: 0.4587\n",
      "Epoch: 1020/2000... Training loss: 0.3110\n",
      "Epoch: 1020/2000... Training loss: 0.5868\n",
      "Epoch: 1020/2000... Training loss: 0.6899\n",
      "Epoch: 1020/2000... Training loss: 0.5417\n",
      "Epoch: 1020/2000... Training loss: 0.5670\n",
      "Epoch: 1020/2000... Training loss: 0.5166\n",
      "Epoch: 1020/2000... Training loss: 0.4568\n",
      "Epoch: 1020/2000... Training loss: 0.6266\n",
      "Epoch: 1020/2000... Training loss: 0.5002\n",
      "Epoch: 1020/2000... Training loss: 0.4221\n",
      "Epoch: 1020/2000... Training loss: 0.4096\n",
      "Epoch: 1020/2000... Training loss: 0.5447\n",
      "Epoch: 1020/2000... Training loss: 0.5302\n",
      "Epoch: 1020/2000... Training loss: 0.5261\n",
      "Epoch: 1021/2000... Training loss: 0.5467\n",
      "Epoch: 1021/2000... Training loss: 0.5575\n",
      "Epoch: 1021/2000... Training loss: 0.5768\n",
      "Epoch: 1021/2000... Training loss: 0.7067\n",
      "Epoch: 1021/2000... Training loss: 0.4304\n",
      "Epoch: 1021/2000... Training loss: 0.4184\n",
      "Epoch: 1021/2000... Training loss: 0.4419\n",
      "Epoch: 1021/2000... Training loss: 0.5102\n",
      "Epoch: 1021/2000... Training loss: 0.4230\n",
      "Epoch: 1021/2000... Training loss: 0.6689\n",
      "Epoch: 1021/2000... Training loss: 0.4666\n",
      "Epoch: 1021/2000... Training loss: 0.6571\n",
      "Epoch: 1021/2000... Training loss: 0.4932\n",
      "Epoch: 1021/2000... Training loss: 0.5372\n",
      "Epoch: 1021/2000... Training loss: 0.4383\n",
      "Epoch: 1021/2000... Training loss: 0.5539\n",
      "Epoch: 1021/2000... Training loss: 0.3575\n",
      "Epoch: 1021/2000... Training loss: 0.4731\n",
      "Epoch: 1021/2000... Training loss: 0.5131\n",
      "Epoch: 1021/2000... Training loss: 0.5555\n",
      "Epoch: 1021/2000... Training loss: 0.5753\n",
      "Epoch: 1021/2000... Training loss: 0.5024\n",
      "Epoch: 1021/2000... Training loss: 0.6078\n",
      "Epoch: 1021/2000... Training loss: 0.5154\n",
      "Epoch: 1021/2000... Training loss: 0.6354\n",
      "Epoch: 1021/2000... Training loss: 0.4901\n",
      "Epoch: 1021/2000... Training loss: 0.4909\n",
      "Epoch: 1021/2000... Training loss: 0.6346\n",
      "Epoch: 1021/2000... Training loss: 0.3115\n",
      "Epoch: 1021/2000... Training loss: 0.5836\n",
      "Epoch: 1021/2000... Training loss: 0.5113\n",
      "Epoch: 1022/2000... Training loss: 0.4412\n",
      "Epoch: 1022/2000... Training loss: 0.4461\n",
      "Epoch: 1022/2000... Training loss: 0.4724\n",
      "Epoch: 1022/2000... Training loss: 0.4438\n",
      "Epoch: 1022/2000... Training loss: 0.4255\n",
      "Epoch: 1022/2000... Training loss: 0.3347\n",
      "Epoch: 1022/2000... Training loss: 0.3175\n",
      "Epoch: 1022/2000... Training loss: 0.4420\n",
      "Epoch: 1022/2000... Training loss: 0.9605\n",
      "Epoch: 1022/2000... Training loss: 0.4569\n",
      "Epoch: 1022/2000... Training loss: 0.5529\n",
      "Epoch: 1022/2000... Training loss: 0.4594\n",
      "Epoch: 1022/2000... Training loss: 0.5183\n",
      "Epoch: 1022/2000... Training loss: 0.4657\n",
      "Epoch: 1022/2000... Training loss: 0.5212\n",
      "Epoch: 1022/2000... Training loss: 0.3749\n",
      "Epoch: 1022/2000... Training loss: 0.6183\n",
      "Epoch: 1022/2000... Training loss: 0.4697\n",
      "Epoch: 1022/2000... Training loss: 0.6683\n",
      "Epoch: 1022/2000... Training loss: 0.7293\n",
      "Epoch: 1022/2000... Training loss: 0.5256\n",
      "Epoch: 1022/2000... Training loss: 0.5895\n",
      "Epoch: 1022/2000... Training loss: 0.4412\n",
      "Epoch: 1022/2000... Training loss: 0.4082\n",
      "Epoch: 1022/2000... Training loss: 0.4645\n",
      "Epoch: 1022/2000... Training loss: 0.4689\n",
      "Epoch: 1022/2000... Training loss: 0.4341\n",
      "Epoch: 1022/2000... Training loss: 0.3641\n",
      "Epoch: 1022/2000... Training loss: 0.6870\n",
      "Epoch: 1022/2000... Training loss: 0.5936\n",
      "Epoch: 1022/2000... Training loss: 0.6411\n",
      "Epoch: 1023/2000... Training loss: 0.6057\n",
      "Epoch: 1023/2000... Training loss: 0.5388\n",
      "Epoch: 1023/2000... Training loss: 0.4007\n",
      "Epoch: 1023/2000... Training loss: 0.5843\n",
      "Epoch: 1023/2000... Training loss: 0.6204\n",
      "Epoch: 1023/2000... Training loss: 0.7372\n",
      "Epoch: 1023/2000... Training loss: 0.6251\n",
      "Epoch: 1023/2000... Training loss: 0.5426\n",
      "Epoch: 1023/2000... Training loss: 0.5563\n",
      "Epoch: 1023/2000... Training loss: 0.5869\n",
      "Epoch: 1023/2000... Training loss: 0.5182\n",
      "Epoch: 1023/2000... Training loss: 0.5078\n",
      "Epoch: 1023/2000... Training loss: 0.5484\n",
      "Epoch: 1023/2000... Training loss: 0.6171\n",
      "Epoch: 1023/2000... Training loss: 0.5399\n",
      "Epoch: 1023/2000... Training loss: 0.3995\n",
      "Epoch: 1023/2000... Training loss: 0.4450\n",
      "Epoch: 1023/2000... Training loss: 0.4976\n",
      "Epoch: 1023/2000... Training loss: 0.4716\n",
      "Epoch: 1023/2000... Training loss: 0.5643\n",
      "Epoch: 1023/2000... Training loss: 0.5132\n",
      "Epoch: 1023/2000... Training loss: 0.4367\n",
      "Epoch: 1023/2000... Training loss: 0.5219\n",
      "Epoch: 1023/2000... Training loss: 0.4823\n",
      "Epoch: 1023/2000... Training loss: 0.5765\n",
      "Epoch: 1023/2000... Training loss: 0.5925\n",
      "Epoch: 1023/2000... Training loss: 0.5210\n",
      "Epoch: 1023/2000... Training loss: 0.5186\n",
      "Epoch: 1023/2000... Training loss: 0.6163\n",
      "Epoch: 1023/2000... Training loss: 0.4904\n",
      "Epoch: 1023/2000... Training loss: 0.4028\n",
      "Epoch: 1024/2000... Training loss: 0.3682\n",
      "Epoch: 1024/2000... Training loss: 0.4931\n",
      "Epoch: 1024/2000... Training loss: 0.5359\n",
      "Epoch: 1024/2000... Training loss: 0.4380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1024/2000... Training loss: 0.3536\n",
      "Epoch: 1024/2000... Training loss: 0.6912\n",
      "Epoch: 1024/2000... Training loss: 0.4555\n",
      "Epoch: 1024/2000... Training loss: 0.4833\n",
      "Epoch: 1024/2000... Training loss: 0.5241\n",
      "Epoch: 1024/2000... Training loss: 0.4806\n",
      "Epoch: 1024/2000... Training loss: 0.4678\n",
      "Epoch: 1024/2000... Training loss: 0.6280\n",
      "Epoch: 1024/2000... Training loss: 0.4690\n",
      "Epoch: 1024/2000... Training loss: 0.6269\n",
      "Epoch: 1024/2000... Training loss: 0.4140\n",
      "Epoch: 1024/2000... Training loss: 0.4510\n",
      "Epoch: 1024/2000... Training loss: 0.4065\n",
      "Epoch: 1024/2000... Training loss: 0.6030\n",
      "Epoch: 1024/2000... Training loss: 0.4860\n",
      "Epoch: 1024/2000... Training loss: 0.5343\n",
      "Epoch: 1024/2000... Training loss: 0.6071\n",
      "Epoch: 1024/2000... Training loss: 0.4577\n",
      "Epoch: 1024/2000... Training loss: 0.6523\n",
      "Epoch: 1024/2000... Training loss: 0.4918\n",
      "Epoch: 1024/2000... Training loss: 0.5290\n",
      "Epoch: 1024/2000... Training loss: 0.5526\n",
      "Epoch: 1024/2000... Training loss: 0.4010\n",
      "Epoch: 1024/2000... Training loss: 0.5766\n",
      "Epoch: 1024/2000... Training loss: 0.3898\n",
      "Epoch: 1024/2000... Training loss: 0.6922\n",
      "Epoch: 1024/2000... Training loss: 0.6359\n",
      "Epoch: 1025/2000... Training loss: 0.6351\n",
      "Epoch: 1025/2000... Training loss: 0.6338\n",
      "Epoch: 1025/2000... Training loss: 0.6360\n",
      "Epoch: 1025/2000... Training loss: 0.6223\n",
      "Epoch: 1025/2000... Training loss: 0.4105\n",
      "Epoch: 1025/2000... Training loss: 0.4493\n",
      "Epoch: 1025/2000... Training loss: 0.3342\n",
      "Epoch: 1025/2000... Training loss: 0.4825\n",
      "Epoch: 1025/2000... Training loss: 0.4414\n",
      "Epoch: 1025/2000... Training loss: 0.6491\n",
      "Epoch: 1025/2000... Training loss: 0.5808\n",
      "Epoch: 1025/2000... Training loss: 0.5486\n",
      "Epoch: 1025/2000... Training loss: 0.4788\n",
      "Epoch: 1025/2000... Training loss: 0.5844\n",
      "Epoch: 1025/2000... Training loss: 0.5073\n",
      "Epoch: 1025/2000... Training loss: 0.5816\n",
      "Epoch: 1025/2000... Training loss: 0.5559\n",
      "Epoch: 1025/2000... Training loss: 0.4118\n",
      "Epoch: 1025/2000... Training loss: 0.5570\n",
      "Epoch: 1025/2000... Training loss: 0.6528\n",
      "Epoch: 1025/2000... Training loss: 0.5584\n",
      "Epoch: 1025/2000... Training loss: 0.5463\n",
      "Epoch: 1025/2000... Training loss: 0.3854\n",
      "Epoch: 1025/2000... Training loss: 0.5021\n",
      "Epoch: 1025/2000... Training loss: 0.5195\n",
      "Epoch: 1025/2000... Training loss: 0.3762\n",
      "Epoch: 1025/2000... Training loss: 0.6004\n",
      "Epoch: 1025/2000... Training loss: 0.6348\n",
      "Epoch: 1025/2000... Training loss: 0.6158\n",
      "Epoch: 1025/2000... Training loss: 0.3754\n",
      "Epoch: 1025/2000... Training loss: 0.5379\n",
      "Epoch: 1026/2000... Training loss: 0.7316\n",
      "Epoch: 1026/2000... Training loss: 0.4521\n",
      "Epoch: 1026/2000... Training loss: 0.4933\n",
      "Epoch: 1026/2000... Training loss: 0.4625\n",
      "Epoch: 1026/2000... Training loss: 0.4477\n",
      "Epoch: 1026/2000... Training loss: 0.3134\n",
      "Epoch: 1026/2000... Training loss: 0.4215\n",
      "Epoch: 1026/2000... Training loss: 0.3036\n",
      "Epoch: 1026/2000... Training loss: 0.5995\n",
      "Epoch: 1026/2000... Training loss: 0.5961\n",
      "Epoch: 1026/2000... Training loss: 0.5482\n",
      "Epoch: 1026/2000... Training loss: 0.6081\n",
      "Epoch: 1026/2000... Training loss: 0.5193\n",
      "Epoch: 1026/2000... Training loss: 0.3957\n",
      "Epoch: 1026/2000... Training loss: 0.5128\n",
      "Epoch: 1026/2000... Training loss: 0.5625\n",
      "Epoch: 1026/2000... Training loss: 0.4421\n",
      "Epoch: 1026/2000... Training loss: 0.5131\n",
      "Epoch: 1026/2000... Training loss: 0.5108\n",
      "Epoch: 1026/2000... Training loss: 0.6155\n",
      "Epoch: 1026/2000... Training loss: 0.5132\n",
      "Epoch: 1026/2000... Training loss: 0.4919\n",
      "Epoch: 1026/2000... Training loss: 0.5167\n",
      "Epoch: 1026/2000... Training loss: 0.5003\n",
      "Epoch: 1026/2000... Training loss: 0.5280\n",
      "Epoch: 1026/2000... Training loss: 0.6307\n",
      "Epoch: 1026/2000... Training loss: 0.4672\n",
      "Epoch: 1026/2000... Training loss: 0.7258\n",
      "Epoch: 1026/2000... Training loss: 0.3891\n",
      "Epoch: 1026/2000... Training loss: 0.5146\n",
      "Epoch: 1026/2000... Training loss: 0.5548\n",
      "Epoch: 1027/2000... Training loss: 0.3325\n",
      "Epoch: 1027/2000... Training loss: 0.7097\n",
      "Epoch: 1027/2000... Training loss: 0.4926\n",
      "Epoch: 1027/2000... Training loss: 0.4895\n",
      "Epoch: 1027/2000... Training loss: 0.6855\n",
      "Epoch: 1027/2000... Training loss: 0.5575\n",
      "Epoch: 1027/2000... Training loss: 0.4215\n",
      "Epoch: 1027/2000... Training loss: 0.5720\n",
      "Epoch: 1027/2000... Training loss: 0.5449\n",
      "Epoch: 1027/2000... Training loss: 0.5273\n",
      "Epoch: 1027/2000... Training loss: 0.5117\n",
      "Epoch: 1027/2000... Training loss: 0.5063\n",
      "Epoch: 1027/2000... Training loss: 0.4554\n",
      "Epoch: 1027/2000... Training loss: 0.5143\n",
      "Epoch: 1027/2000... Training loss: 0.5142\n",
      "Epoch: 1027/2000... Training loss: 0.6533\n",
      "Epoch: 1027/2000... Training loss: 0.5149\n",
      "Epoch: 1027/2000... Training loss: 0.3947\n",
      "Epoch: 1027/2000... Training loss: 0.4489\n",
      "Epoch: 1027/2000... Training loss: 0.4818\n",
      "Epoch: 1027/2000... Training loss: 0.4820\n",
      "Epoch: 1027/2000... Training loss: 0.4501\n",
      "Epoch: 1027/2000... Training loss: 0.4584\n",
      "Epoch: 1027/2000... Training loss: 0.4625\n",
      "Epoch: 1027/2000... Training loss: 0.6153\n",
      "Epoch: 1027/2000... Training loss: 0.4234\n",
      "Epoch: 1027/2000... Training loss: 0.5300\n",
      "Epoch: 1027/2000... Training loss: 0.5324\n",
      "Epoch: 1027/2000... Training loss: 0.6526\n",
      "Epoch: 1027/2000... Training loss: 0.4471\n",
      "Epoch: 1027/2000... Training loss: 0.6068\n",
      "Epoch: 1028/2000... Training loss: 0.4513\n",
      "Epoch: 1028/2000... Training loss: 0.6188\n",
      "Epoch: 1028/2000... Training loss: 0.5401\n",
      "Epoch: 1028/2000... Training loss: 0.7071\n",
      "Epoch: 1028/2000... Training loss: 0.4253\n",
      "Epoch: 1028/2000... Training loss: 0.5276\n",
      "Epoch: 1028/2000... Training loss: 0.5453\n",
      "Epoch: 1028/2000... Training loss: 0.4378\n",
      "Epoch: 1028/2000... Training loss: 0.3932\n",
      "Epoch: 1028/2000... Training loss: 0.4592\n",
      "Epoch: 1028/2000... Training loss: 0.5848\n",
      "Epoch: 1028/2000... Training loss: 0.4797\n",
      "Epoch: 1028/2000... Training loss: 0.4875\n",
      "Epoch: 1028/2000... Training loss: 0.5642\n",
      "Epoch: 1028/2000... Training loss: 0.4952\n",
      "Epoch: 1028/2000... Training loss: 0.4888\n",
      "Epoch: 1028/2000... Training loss: 0.3581\n",
      "Epoch: 1028/2000... Training loss: 0.6307\n",
      "Epoch: 1028/2000... Training loss: 0.4734\n",
      "Epoch: 1028/2000... Training loss: 0.5920\n",
      "Epoch: 1028/2000... Training loss: 0.3512\n",
      "Epoch: 1028/2000... Training loss: 0.5010\n",
      "Epoch: 1028/2000... Training loss: 0.7551\n",
      "Epoch: 1028/2000... Training loss: 0.4734\n",
      "Epoch: 1028/2000... Training loss: 0.6255\n",
      "Epoch: 1028/2000... Training loss: 0.7371\n",
      "Epoch: 1028/2000... Training loss: 0.6124\n",
      "Epoch: 1028/2000... Training loss: 0.5897\n",
      "Epoch: 1028/2000... Training loss: 0.4382\n",
      "Epoch: 1028/2000... Training loss: 0.5546\n",
      "Epoch: 1028/2000... Training loss: 0.5481\n",
      "Epoch: 1029/2000... Training loss: 0.5494\n",
      "Epoch: 1029/2000... Training loss: 0.6250\n",
      "Epoch: 1029/2000... Training loss: 0.7680\n",
      "Epoch: 1029/2000... Training loss: 0.5037\n",
      "Epoch: 1029/2000... Training loss: 0.4775\n",
      "Epoch: 1029/2000... Training loss: 0.6762\n",
      "Epoch: 1029/2000... Training loss: 0.3671\n",
      "Epoch: 1029/2000... Training loss: 0.3403\n",
      "Epoch: 1029/2000... Training loss: 0.4694\n",
      "Epoch: 1029/2000... Training loss: 0.5055\n",
      "Epoch: 1029/2000... Training loss: 0.5622\n",
      "Epoch: 1029/2000... Training loss: 0.5119\n",
      "Epoch: 1029/2000... Training loss: 0.4648\n",
      "Epoch: 1029/2000... Training loss: 0.5303\n",
      "Epoch: 1029/2000... Training loss: 0.5164\n",
      "Epoch: 1029/2000... Training loss: 0.3060\n",
      "Epoch: 1029/2000... Training loss: 0.5031\n",
      "Epoch: 1029/2000... Training loss: 0.4243\n",
      "Epoch: 1029/2000... Training loss: 0.6105\n",
      "Epoch: 1029/2000... Training loss: 0.4820\n",
      "Epoch: 1029/2000... Training loss: 0.4621\n",
      "Epoch: 1029/2000... Training loss: 0.3675\n",
      "Epoch: 1029/2000... Training loss: 0.5435\n",
      "Epoch: 1029/2000... Training loss: 0.6631\n",
      "Epoch: 1029/2000... Training loss: 0.4236\n",
      "Epoch: 1029/2000... Training loss: 0.4549\n",
      "Epoch: 1029/2000... Training loss: 0.3989\n",
      "Epoch: 1029/2000... Training loss: 0.3898\n",
      "Epoch: 1029/2000... Training loss: 0.4265\n",
      "Epoch: 1029/2000... Training loss: 0.4058\n",
      "Epoch: 1029/2000... Training loss: 0.4292\n",
      "Epoch: 1030/2000... Training loss: 0.2957\n",
      "Epoch: 1030/2000... Training loss: 0.5899\n",
      "Epoch: 1030/2000... Training loss: 0.5351\n",
      "Epoch: 1030/2000... Training loss: 0.4369\n",
      "Epoch: 1030/2000... Training loss: 0.7219\n",
      "Epoch: 1030/2000... Training loss: 0.5204\n",
      "Epoch: 1030/2000... Training loss: 0.4267\n",
      "Epoch: 1030/2000... Training loss: 0.6188\n",
      "Epoch: 1030/2000... Training loss: 0.6544\n",
      "Epoch: 1030/2000... Training loss: 0.6543\n",
      "Epoch: 1030/2000... Training loss: 0.5682\n",
      "Epoch: 1030/2000... Training loss: 0.4432\n",
      "Epoch: 1030/2000... Training loss: 0.4480\n",
      "Epoch: 1030/2000... Training loss: 0.4299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1030/2000... Training loss: 0.6200\n",
      "Epoch: 1030/2000... Training loss: 0.5583\n",
      "Epoch: 1030/2000... Training loss: 0.5572\n",
      "Epoch: 1030/2000... Training loss: 0.4488\n",
      "Epoch: 1030/2000... Training loss: 0.5389\n",
      "Epoch: 1030/2000... Training loss: 0.5282\n",
      "Epoch: 1030/2000... Training loss: 0.4519\n",
      "Epoch: 1030/2000... Training loss: 0.5481\n",
      "Epoch: 1030/2000... Training loss: 0.4651\n",
      "Epoch: 1030/2000... Training loss: 0.3736\n",
      "Epoch: 1030/2000... Training loss: 0.3711\n",
      "Epoch: 1030/2000... Training loss: 0.5532\n",
      "Epoch: 1030/2000... Training loss: 0.7183\n",
      "Epoch: 1030/2000... Training loss: 0.6725\n",
      "Epoch: 1030/2000... Training loss: 0.5570\n",
      "Epoch: 1030/2000... Training loss: 0.4681\n",
      "Epoch: 1030/2000... Training loss: 0.3906\n",
      "Epoch: 1031/2000... Training loss: 0.4359\n",
      "Epoch: 1031/2000... Training loss: 0.4619\n",
      "Epoch: 1031/2000... Training loss: 0.4261\n",
      "Epoch: 1031/2000... Training loss: 0.4289\n",
      "Epoch: 1031/2000... Training loss: 0.4084\n",
      "Epoch: 1031/2000... Training loss: 0.3373\n",
      "Epoch: 1031/2000... Training loss: 0.4854\n",
      "Epoch: 1031/2000... Training loss: 0.4640\n",
      "Epoch: 1031/2000... Training loss: 0.5871\n",
      "Epoch: 1031/2000... Training loss: 0.6698\n",
      "Epoch: 1031/2000... Training loss: 0.4393\n",
      "Epoch: 1031/2000... Training loss: 0.3985\n",
      "Epoch: 1031/2000... Training loss: 0.3556\n",
      "Epoch: 1031/2000... Training loss: 0.5080\n",
      "Epoch: 1031/2000... Training loss: 0.4935\n",
      "Epoch: 1031/2000... Training loss: 0.5393\n",
      "Epoch: 1031/2000... Training loss: 0.5276\n",
      "Epoch: 1031/2000... Training loss: 0.5311\n",
      "Epoch: 1031/2000... Training loss: 0.6151\n",
      "Epoch: 1031/2000... Training loss: 0.5985\n",
      "Epoch: 1031/2000... Training loss: 0.5480\n",
      "Epoch: 1031/2000... Training loss: 0.4503\n",
      "Epoch: 1031/2000... Training loss: 0.3702\n",
      "Epoch: 1031/2000... Training loss: 0.4117\n",
      "Epoch: 1031/2000... Training loss: 0.5331\n",
      "Epoch: 1031/2000... Training loss: 0.6104\n",
      "Epoch: 1031/2000... Training loss: 0.3154\n",
      "Epoch: 1031/2000... Training loss: 0.5904\n",
      "Epoch: 1031/2000... Training loss: 0.4538\n",
      "Epoch: 1031/2000... Training loss: 0.4457\n",
      "Epoch: 1031/2000... Training loss: 0.4761\n",
      "Epoch: 1032/2000... Training loss: 0.5623\n",
      "Epoch: 1032/2000... Training loss: 0.6897\n",
      "Epoch: 1032/2000... Training loss: 0.5060\n",
      "Epoch: 1032/2000... Training loss: 0.6180\n",
      "Epoch: 1032/2000... Training loss: 0.5631\n",
      "Epoch: 1032/2000... Training loss: 0.4048\n",
      "Epoch: 1032/2000... Training loss: 0.7490\n",
      "Epoch: 1032/2000... Training loss: 0.4085\n",
      "Epoch: 1032/2000... Training loss: 0.5705\n",
      "Epoch: 1032/2000... Training loss: 0.4328\n",
      "Epoch: 1032/2000... Training loss: 0.7122\n",
      "Epoch: 1032/2000... Training loss: 0.6729\n",
      "Epoch: 1032/2000... Training loss: 0.5436\n",
      "Epoch: 1032/2000... Training loss: 0.4779\n",
      "Epoch: 1032/2000... Training loss: 0.4488\n",
      "Epoch: 1032/2000... Training loss: 0.5647\n",
      "Epoch: 1032/2000... Training loss: 0.5580\n",
      "Epoch: 1032/2000... Training loss: 0.3658\n",
      "Epoch: 1032/2000... Training loss: 0.5753\n",
      "Epoch: 1032/2000... Training loss: 0.5071\n",
      "Epoch: 1032/2000... Training loss: 0.5288\n",
      "Epoch: 1032/2000... Training loss: 0.4135\n",
      "Epoch: 1032/2000... Training loss: 0.5863\n",
      "Epoch: 1032/2000... Training loss: 0.6697\n",
      "Epoch: 1032/2000... Training loss: 0.3845\n",
      "Epoch: 1032/2000... Training loss: 0.5114\n",
      "Epoch: 1032/2000... Training loss: 0.6280\n",
      "Epoch: 1032/2000... Training loss: 0.3605\n",
      "Epoch: 1032/2000... Training loss: 0.5393\n",
      "Epoch: 1032/2000... Training loss: 0.3397\n",
      "Epoch: 1032/2000... Training loss: 0.6794\n",
      "Epoch: 1033/2000... Training loss: 0.6262\n",
      "Epoch: 1033/2000... Training loss: 0.3542\n",
      "Epoch: 1033/2000... Training loss: 0.4872\n",
      "Epoch: 1033/2000... Training loss: 0.5831\n",
      "Epoch: 1033/2000... Training loss: 0.4701\n",
      "Epoch: 1033/2000... Training loss: 0.5335\n",
      "Epoch: 1033/2000... Training loss: 0.5154\n",
      "Epoch: 1033/2000... Training loss: 0.5696\n",
      "Epoch: 1033/2000... Training loss: 0.4340\n",
      "Epoch: 1033/2000... Training loss: 0.5083\n",
      "Epoch: 1033/2000... Training loss: 0.4085\n",
      "Epoch: 1033/2000... Training loss: 0.6738\n",
      "Epoch: 1033/2000... Training loss: 0.5024\n",
      "Epoch: 1033/2000... Training loss: 0.5397\n",
      "Epoch: 1033/2000... Training loss: 0.6182\n",
      "Epoch: 1033/2000... Training loss: 0.4757\n",
      "Epoch: 1033/2000... Training loss: 0.4304\n",
      "Epoch: 1033/2000... Training loss: 0.5610\n",
      "Epoch: 1033/2000... Training loss: 0.4733\n",
      "Epoch: 1033/2000... Training loss: 0.5408\n",
      "Epoch: 1033/2000... Training loss: 0.4196\n",
      "Epoch: 1033/2000... Training loss: 0.6463\n",
      "Epoch: 1033/2000... Training loss: 0.6016\n",
      "Epoch: 1033/2000... Training loss: 0.3177\n",
      "Epoch: 1033/2000... Training loss: 0.5464\n",
      "Epoch: 1033/2000... Training loss: 0.5159\n",
      "Epoch: 1033/2000... Training loss: 0.6139\n",
      "Epoch: 1033/2000... Training loss: 0.5499\n",
      "Epoch: 1033/2000... Training loss: 0.4767\n",
      "Epoch: 1033/2000... Training loss: 0.5793\n",
      "Epoch: 1033/2000... Training loss: 0.3770\n",
      "Epoch: 1034/2000... Training loss: 0.4980\n",
      "Epoch: 1034/2000... Training loss: 0.5088\n",
      "Epoch: 1034/2000... Training loss: 0.4822\n",
      "Epoch: 1034/2000... Training loss: 0.5732\n",
      "Epoch: 1034/2000... Training loss: 0.5409\n",
      "Epoch: 1034/2000... Training loss: 0.6268\n",
      "Epoch: 1034/2000... Training loss: 0.5470\n",
      "Epoch: 1034/2000... Training loss: 0.4680\n",
      "Epoch: 1034/2000... Training loss: 0.7112\n",
      "Epoch: 1034/2000... Training loss: 0.6409\n",
      "Epoch: 1034/2000... Training loss: 0.5696\n",
      "Epoch: 1034/2000... Training loss: 0.3541\n",
      "Epoch: 1034/2000... Training loss: 0.7136\n",
      "Epoch: 1034/2000... Training loss: 0.5170\n",
      "Epoch: 1034/2000... Training loss: 0.6136\n",
      "Epoch: 1034/2000... Training loss: 0.6209\n",
      "Epoch: 1034/2000... Training loss: 0.3714\n",
      "Epoch: 1034/2000... Training loss: 0.6116\n",
      "Epoch: 1034/2000... Training loss: 0.5826\n",
      "Epoch: 1034/2000... Training loss: 0.3349\n",
      "Epoch: 1034/2000... Training loss: 0.4402\n",
      "Epoch: 1034/2000... Training loss: 0.6820\n",
      "Epoch: 1034/2000... Training loss: 0.4232\n",
      "Epoch: 1034/2000... Training loss: 0.5454\n",
      "Epoch: 1034/2000... Training loss: 0.6350\n",
      "Epoch: 1034/2000... Training loss: 0.5179\n",
      "Epoch: 1034/2000... Training loss: 0.5819\n",
      "Epoch: 1034/2000... Training loss: 0.5291\n",
      "Epoch: 1034/2000... Training loss: 0.4922\n",
      "Epoch: 1034/2000... Training loss: 0.5766\n",
      "Epoch: 1034/2000... Training loss: 0.5587\n",
      "Epoch: 1035/2000... Training loss: 0.4195\n",
      "Epoch: 1035/2000... Training loss: 0.4467\n",
      "Epoch: 1035/2000... Training loss: 0.4894\n",
      "Epoch: 1035/2000... Training loss: 0.4711\n",
      "Epoch: 1035/2000... Training loss: 0.4472\n",
      "Epoch: 1035/2000... Training loss: 0.4994\n",
      "Epoch: 1035/2000... Training loss: 0.3864\n",
      "Epoch: 1035/2000... Training loss: 0.5945\n",
      "Epoch: 1035/2000... Training loss: 0.4780\n",
      "Epoch: 1035/2000... Training loss: 0.4136\n",
      "Epoch: 1035/2000... Training loss: 0.5643\n",
      "Epoch: 1035/2000... Training loss: 0.4912\n",
      "Epoch: 1035/2000... Training loss: 0.3976\n",
      "Epoch: 1035/2000... Training loss: 0.4779\n",
      "Epoch: 1035/2000... Training loss: 0.5154\n",
      "Epoch: 1035/2000... Training loss: 0.5036\n",
      "Epoch: 1035/2000... Training loss: 0.5414\n",
      "Epoch: 1035/2000... Training loss: 0.4842\n",
      "Epoch: 1035/2000... Training loss: 0.6155\n",
      "Epoch: 1035/2000... Training loss: 0.5648\n",
      "Epoch: 1035/2000... Training loss: 0.4523\n",
      "Epoch: 1035/2000... Training loss: 0.5113\n",
      "Epoch: 1035/2000... Training loss: 0.5302\n",
      "Epoch: 1035/2000... Training loss: 0.5218\n",
      "Epoch: 1035/2000... Training loss: 0.4219\n",
      "Epoch: 1035/2000... Training loss: 0.6645\n",
      "Epoch: 1035/2000... Training loss: 0.4010\n",
      "Epoch: 1035/2000... Training loss: 0.4201\n",
      "Epoch: 1035/2000... Training loss: 0.5071\n",
      "Epoch: 1035/2000... Training loss: 0.5362\n",
      "Epoch: 1035/2000... Training loss: 0.4895\n",
      "Epoch: 1036/2000... Training loss: 0.7400\n",
      "Epoch: 1036/2000... Training loss: 0.5742\n",
      "Epoch: 1036/2000... Training loss: 0.4913\n",
      "Epoch: 1036/2000... Training loss: 0.5599\n",
      "Epoch: 1036/2000... Training loss: 0.5602\n",
      "Epoch: 1036/2000... Training loss: 0.6520\n",
      "Epoch: 1036/2000... Training loss: 0.5371\n",
      "Epoch: 1036/2000... Training loss: 0.5397\n",
      "Epoch: 1036/2000... Training loss: 0.2632\n",
      "Epoch: 1036/2000... Training loss: 0.3858\n",
      "Epoch: 1036/2000... Training loss: 0.3949\n",
      "Epoch: 1036/2000... Training loss: 0.4371\n",
      "Epoch: 1036/2000... Training loss: 0.6653\n",
      "Epoch: 1036/2000... Training loss: 0.4789\n",
      "Epoch: 1036/2000... Training loss: 0.3883\n",
      "Epoch: 1036/2000... Training loss: 0.5160\n",
      "Epoch: 1036/2000... Training loss: 0.5691\n",
      "Epoch: 1036/2000... Training loss: 0.3356\n",
      "Epoch: 1036/2000... Training loss: 0.5555\n",
      "Epoch: 1036/2000... Training loss: 0.5353\n",
      "Epoch: 1036/2000... Training loss: 0.4209\n",
      "Epoch: 1036/2000... Training loss: 0.4552\n",
      "Epoch: 1036/2000... Training loss: 0.4348\n",
      "Epoch: 1036/2000... Training loss: 0.4721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1036/2000... Training loss: 0.4435\n",
      "Epoch: 1036/2000... Training loss: 0.4219\n",
      "Epoch: 1036/2000... Training loss: 0.5075\n",
      "Epoch: 1036/2000... Training loss: 0.6329\n",
      "Epoch: 1036/2000... Training loss: 0.3866\n",
      "Epoch: 1036/2000... Training loss: 0.4434\n",
      "Epoch: 1036/2000... Training loss: 0.4855\n",
      "Epoch: 1037/2000... Training loss: 0.4421\n",
      "Epoch: 1037/2000... Training loss: 0.5498\n",
      "Epoch: 1037/2000... Training loss: 0.4935\n",
      "Epoch: 1037/2000... Training loss: 0.4020\n",
      "Epoch: 1037/2000... Training loss: 0.5045\n",
      "Epoch: 1037/2000... Training loss: 0.5399\n",
      "Epoch: 1037/2000... Training loss: 0.5925\n",
      "Epoch: 1037/2000... Training loss: 0.5458\n",
      "Epoch: 1037/2000... Training loss: 0.4622\n",
      "Epoch: 1037/2000... Training loss: 0.6312\n",
      "Epoch: 1037/2000... Training loss: 0.7789\n",
      "Epoch: 1037/2000... Training loss: 0.5122\n",
      "Epoch: 1037/2000... Training loss: 0.4783\n",
      "Epoch: 1037/2000... Training loss: 0.4198\n",
      "Epoch: 1037/2000... Training loss: 0.5012\n",
      "Epoch: 1037/2000... Training loss: 0.5422\n",
      "Epoch: 1037/2000... Training loss: 0.4251\n",
      "Epoch: 1037/2000... Training loss: 0.4888\n",
      "Epoch: 1037/2000... Training loss: 0.4718\n",
      "Epoch: 1037/2000... Training loss: 0.4138\n",
      "Epoch: 1037/2000... Training loss: 0.4075\n",
      "Epoch: 1037/2000... Training loss: 0.3217\n",
      "Epoch: 1037/2000... Training loss: 0.6867\n",
      "Epoch: 1037/2000... Training loss: 0.6499\n",
      "Epoch: 1037/2000... Training loss: 0.4214\n",
      "Epoch: 1037/2000... Training loss: 0.5727\n",
      "Epoch: 1037/2000... Training loss: 0.4788\n",
      "Epoch: 1037/2000... Training loss: 0.4744\n",
      "Epoch: 1037/2000... Training loss: 0.4593\n",
      "Epoch: 1037/2000... Training loss: 0.5410\n",
      "Epoch: 1037/2000... Training loss: 0.3548\n",
      "Epoch: 1038/2000... Training loss: 0.3712\n",
      "Epoch: 1038/2000... Training loss: 0.5666\n",
      "Epoch: 1038/2000... Training loss: 0.4377\n",
      "Epoch: 1038/2000... Training loss: 0.6236\n",
      "Epoch: 1038/2000... Training loss: 0.4604\n",
      "Epoch: 1038/2000... Training loss: 0.4761\n",
      "Epoch: 1038/2000... Training loss: 0.3992\n",
      "Epoch: 1038/2000... Training loss: 0.3920\n",
      "Epoch: 1038/2000... Training loss: 0.5449\n",
      "Epoch: 1038/2000... Training loss: 0.4231\n",
      "Epoch: 1038/2000... Training loss: 0.4577\n",
      "Epoch: 1038/2000... Training loss: 0.6053\n",
      "Epoch: 1038/2000... Training loss: 0.5298\n",
      "Epoch: 1038/2000... Training loss: 0.5218\n",
      "Epoch: 1038/2000... Training loss: 0.5484\n",
      "Epoch: 1038/2000... Training loss: 0.5928\n",
      "Epoch: 1038/2000... Training loss: 0.5054\n",
      "Epoch: 1038/2000... Training loss: 0.5138\n",
      "Epoch: 1038/2000... Training loss: 0.4437\n",
      "Epoch: 1038/2000... Training loss: 0.6808\n",
      "Epoch: 1038/2000... Training loss: 0.4802\n",
      "Epoch: 1038/2000... Training loss: 0.4771\n",
      "Epoch: 1038/2000... Training loss: 0.4058\n",
      "Epoch: 1038/2000... Training loss: 0.6010\n",
      "Epoch: 1038/2000... Training loss: 0.5957\n",
      "Epoch: 1038/2000... Training loss: 0.4702\n",
      "Epoch: 1038/2000... Training loss: 0.3869\n",
      "Epoch: 1038/2000... Training loss: 0.5379\n",
      "Epoch: 1038/2000... Training loss: 0.4905\n",
      "Epoch: 1038/2000... Training loss: 0.4062\n",
      "Epoch: 1038/2000... Training loss: 0.5161\n",
      "Epoch: 1039/2000... Training loss: 0.6491\n",
      "Epoch: 1039/2000... Training loss: 0.5219\n",
      "Epoch: 1039/2000... Training loss: 0.3701\n",
      "Epoch: 1039/2000... Training loss: 0.5835\n",
      "Epoch: 1039/2000... Training loss: 0.5427\n",
      "Epoch: 1039/2000... Training loss: 0.6170\n",
      "Epoch: 1039/2000... Training loss: 0.4656\n",
      "Epoch: 1039/2000... Training loss: 0.4611\n",
      "Epoch: 1039/2000... Training loss: 0.4783\n",
      "Epoch: 1039/2000... Training loss: 0.5373\n",
      "Epoch: 1039/2000... Training loss: 0.4042\n",
      "Epoch: 1039/2000... Training loss: 0.5331\n",
      "Epoch: 1039/2000... Training loss: 0.5049\n",
      "Epoch: 1039/2000... Training loss: 0.4847\n",
      "Epoch: 1039/2000... Training loss: 0.5344\n",
      "Epoch: 1039/2000... Training loss: 0.6128\n",
      "Epoch: 1039/2000... Training loss: 0.5546\n",
      "Epoch: 1039/2000... Training loss: 0.7546\n",
      "Epoch: 1039/2000... Training loss: 0.4312\n",
      "Epoch: 1039/2000... Training loss: 0.5531\n",
      "Epoch: 1039/2000... Training loss: 0.6546\n",
      "Epoch: 1039/2000... Training loss: 0.4345\n",
      "Epoch: 1039/2000... Training loss: 0.6088\n",
      "Epoch: 1039/2000... Training loss: 0.4757\n",
      "Epoch: 1039/2000... Training loss: 0.3937\n",
      "Epoch: 1039/2000... Training loss: 0.5248\n",
      "Epoch: 1039/2000... Training loss: 0.6333\n",
      "Epoch: 1039/2000... Training loss: 0.6422\n",
      "Epoch: 1039/2000... Training loss: 0.5243\n",
      "Epoch: 1039/2000... Training loss: 0.5727\n",
      "Epoch: 1039/2000... Training loss: 0.4786\n",
      "Epoch: 1040/2000... Training loss: 0.3892\n",
      "Epoch: 1040/2000... Training loss: 0.5668\n",
      "Epoch: 1040/2000... Training loss: 0.7243\n",
      "Epoch: 1040/2000... Training loss: 0.3345\n",
      "Epoch: 1040/2000... Training loss: 0.3533\n",
      "Epoch: 1040/2000... Training loss: 0.4223\n",
      "Epoch: 1040/2000... Training loss: 0.4594\n",
      "Epoch: 1040/2000... Training loss: 0.5426\n",
      "Epoch: 1040/2000... Training loss: 0.5402\n",
      "Epoch: 1040/2000... Training loss: 0.7525\n",
      "Epoch: 1040/2000... Training loss: 0.4610\n",
      "Epoch: 1040/2000... Training loss: 0.6435\n",
      "Epoch: 1040/2000... Training loss: 0.5383\n",
      "Epoch: 1040/2000... Training loss: 0.5548\n",
      "Epoch: 1040/2000... Training loss: 0.3915\n",
      "Epoch: 1040/2000... Training loss: 0.4415\n",
      "Epoch: 1040/2000... Training loss: 0.5041\n",
      "Epoch: 1040/2000... Training loss: 0.3824\n",
      "Epoch: 1040/2000... Training loss: 0.4085\n",
      "Epoch: 1040/2000... Training loss: 0.6088\n",
      "Epoch: 1040/2000... Training loss: 0.6080\n",
      "Epoch: 1040/2000... Training loss: 0.5004\n",
      "Epoch: 1040/2000... Training loss: 0.5586\n",
      "Epoch: 1040/2000... Training loss: 0.4890\n",
      "Epoch: 1040/2000... Training loss: 0.4731\n",
      "Epoch: 1040/2000... Training loss: 0.5040\n",
      "Epoch: 1040/2000... Training loss: 0.5248\n",
      "Epoch: 1040/2000... Training loss: 0.4221\n",
      "Epoch: 1040/2000... Training loss: 0.4719\n",
      "Epoch: 1040/2000... Training loss: 0.5959\n",
      "Epoch: 1040/2000... Training loss: 0.4183\n",
      "Epoch: 1041/2000... Training loss: 0.3173\n",
      "Epoch: 1041/2000... Training loss: 0.4222\n",
      "Epoch: 1041/2000... Training loss: 0.5198\n",
      "Epoch: 1041/2000... Training loss: 0.5362\n",
      "Epoch: 1041/2000... Training loss: 0.5702\n",
      "Epoch: 1041/2000... Training loss: 0.4690\n",
      "Epoch: 1041/2000... Training loss: 0.4848\n",
      "Epoch: 1041/2000... Training loss: 0.4023\n",
      "Epoch: 1041/2000... Training loss: 0.5243\n",
      "Epoch: 1041/2000... Training loss: 0.4429\n",
      "Epoch: 1041/2000... Training loss: 0.4849\n",
      "Epoch: 1041/2000... Training loss: 0.5428\n",
      "Epoch: 1041/2000... Training loss: 0.4329\n",
      "Epoch: 1041/2000... Training loss: 0.4974\n",
      "Epoch: 1041/2000... Training loss: 0.6408\n",
      "Epoch: 1041/2000... Training loss: 0.4838\n",
      "Epoch: 1041/2000... Training loss: 0.3958\n",
      "Epoch: 1041/2000... Training loss: 0.5836\n",
      "Epoch: 1041/2000... Training loss: 0.3972\n",
      "Epoch: 1041/2000... Training loss: 0.5335\n",
      "Epoch: 1041/2000... Training loss: 0.4642\n",
      "Epoch: 1041/2000... Training loss: 0.7296\n",
      "Epoch: 1041/2000... Training loss: 0.3961\n",
      "Epoch: 1041/2000... Training loss: 0.6160\n",
      "Epoch: 1041/2000... Training loss: 0.5386\n",
      "Epoch: 1041/2000... Training loss: 0.4400\n",
      "Epoch: 1041/2000... Training loss: 0.4584\n",
      "Epoch: 1041/2000... Training loss: 0.3809\n",
      "Epoch: 1041/2000... Training loss: 0.3673\n",
      "Epoch: 1041/2000... Training loss: 0.3163\n",
      "Epoch: 1041/2000... Training loss: 0.5454\n",
      "Epoch: 1042/2000... Training loss: 0.4717\n",
      "Epoch: 1042/2000... Training loss: 0.4112\n",
      "Epoch: 1042/2000... Training loss: 0.3647\n",
      "Epoch: 1042/2000... Training loss: 0.5623\n",
      "Epoch: 1042/2000... Training loss: 0.7300\n",
      "Epoch: 1042/2000... Training loss: 0.3273\n",
      "Epoch: 1042/2000... Training loss: 0.4309\n",
      "Epoch: 1042/2000... Training loss: 0.3871\n",
      "Epoch: 1042/2000... Training loss: 0.5057\n",
      "Epoch: 1042/2000... Training loss: 0.5628\n",
      "Epoch: 1042/2000... Training loss: 0.6107\n",
      "Epoch: 1042/2000... Training loss: 0.5953\n",
      "Epoch: 1042/2000... Training loss: 0.4219\n",
      "Epoch: 1042/2000... Training loss: 0.5238\n",
      "Epoch: 1042/2000... Training loss: 0.6362\n",
      "Epoch: 1042/2000... Training loss: 0.5327\n",
      "Epoch: 1042/2000... Training loss: 0.4013\n",
      "Epoch: 1042/2000... Training loss: 0.3612\n",
      "Epoch: 1042/2000... Training loss: 0.4082\n",
      "Epoch: 1042/2000... Training loss: 0.5182\n",
      "Epoch: 1042/2000... Training loss: 0.5995\n",
      "Epoch: 1042/2000... Training loss: 0.5585\n",
      "Epoch: 1042/2000... Training loss: 0.5010\n",
      "Epoch: 1042/2000... Training loss: 0.5762\n",
      "Epoch: 1042/2000... Training loss: 0.2843\n",
      "Epoch: 1042/2000... Training loss: 0.6263\n",
      "Epoch: 1042/2000... Training loss: 0.4215\n",
      "Epoch: 1042/2000... Training loss: 0.6171\n",
      "Epoch: 1042/2000... Training loss: 0.3432\n",
      "Epoch: 1042/2000... Training loss: 0.4992\n",
      "Epoch: 1042/2000... Training loss: 0.4872\n",
      "Epoch: 1043/2000... Training loss: 0.5473\n",
      "Epoch: 1043/2000... Training loss: 0.7006\n",
      "Epoch: 1043/2000... Training loss: 0.4167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1043/2000... Training loss: 0.5209\n",
      "Epoch: 1043/2000... Training loss: 0.3864\n",
      "Epoch: 1043/2000... Training loss: 0.4230\n",
      "Epoch: 1043/2000... Training loss: 0.4255\n",
      "Epoch: 1043/2000... Training loss: 0.6242\n",
      "Epoch: 1043/2000... Training loss: 0.5115\n",
      "Epoch: 1043/2000... Training loss: 0.4721\n",
      "Epoch: 1043/2000... Training loss: 0.5099\n",
      "Epoch: 1043/2000... Training loss: 0.3951\n",
      "Epoch: 1043/2000... Training loss: 0.5292\n",
      "Epoch: 1043/2000... Training loss: 0.7178\n",
      "Epoch: 1043/2000... Training loss: 0.6337\n",
      "Epoch: 1043/2000... Training loss: 0.6669\n",
      "Epoch: 1043/2000... Training loss: 0.4765\n",
      "Epoch: 1043/2000... Training loss: 0.4288\n",
      "Epoch: 1043/2000... Training loss: 0.7369\n",
      "Epoch: 1043/2000... Training loss: 0.4935\n",
      "Epoch: 1043/2000... Training loss: 0.4045\n",
      "Epoch: 1043/2000... Training loss: 0.4703\n",
      "Epoch: 1043/2000... Training loss: 0.5258\n",
      "Epoch: 1043/2000... Training loss: 0.4847\n",
      "Epoch: 1043/2000... Training loss: 0.4233\n",
      "Epoch: 1043/2000... Training loss: 0.5254\n",
      "Epoch: 1043/2000... Training loss: 0.4726\n",
      "Epoch: 1043/2000... Training loss: 0.5554\n",
      "Epoch: 1043/2000... Training loss: 0.5847\n",
      "Epoch: 1043/2000... Training loss: 0.3413\n",
      "Epoch: 1043/2000... Training loss: 0.6679\n",
      "Epoch: 1044/2000... Training loss: 0.5593\n",
      "Epoch: 1044/2000... Training loss: 0.4098\n",
      "Epoch: 1044/2000... Training loss: 0.4813\n",
      "Epoch: 1044/2000... Training loss: 0.3876\n",
      "Epoch: 1044/2000... Training loss: 0.4863\n",
      "Epoch: 1044/2000... Training loss: 0.6099\n",
      "Epoch: 1044/2000... Training loss: 0.5531\n",
      "Epoch: 1044/2000... Training loss: 0.6935\n",
      "Epoch: 1044/2000... Training loss: 0.5249\n",
      "Epoch: 1044/2000... Training loss: 0.4131\n",
      "Epoch: 1044/2000... Training loss: 0.5394\n",
      "Epoch: 1044/2000... Training loss: 0.5032\n",
      "Epoch: 1044/2000... Training loss: 0.4508\n",
      "Epoch: 1044/2000... Training loss: 0.4700\n",
      "Epoch: 1044/2000... Training loss: 0.5283\n",
      "Epoch: 1044/2000... Training loss: 0.6636\n",
      "Epoch: 1044/2000... Training loss: 0.5872\n",
      "Epoch: 1044/2000... Training loss: 0.5544\n",
      "Epoch: 1044/2000... Training loss: 0.5054\n",
      "Epoch: 1044/2000... Training loss: 0.7381\n",
      "Epoch: 1044/2000... Training loss: 0.6722\n",
      "Epoch: 1044/2000... Training loss: 0.6109\n",
      "Epoch: 1044/2000... Training loss: 0.5018\n",
      "Epoch: 1044/2000... Training loss: 0.6049\n",
      "Epoch: 1044/2000... Training loss: 0.4916\n",
      "Epoch: 1044/2000... Training loss: 0.4445\n",
      "Epoch: 1044/2000... Training loss: 0.5768\n",
      "Epoch: 1044/2000... Training loss: 0.6006\n",
      "Epoch: 1044/2000... Training loss: 0.7276\n",
      "Epoch: 1044/2000... Training loss: 0.4495\n",
      "Epoch: 1044/2000... Training loss: 0.4065\n",
      "Epoch: 1045/2000... Training loss: 0.4216\n",
      "Epoch: 1045/2000... Training loss: 0.4795\n",
      "Epoch: 1045/2000... Training loss: 0.3389\n",
      "Epoch: 1045/2000... Training loss: 0.3235\n",
      "Epoch: 1045/2000... Training loss: 0.4332\n",
      "Epoch: 1045/2000... Training loss: 0.4729\n",
      "Epoch: 1045/2000... Training loss: 0.5250\n",
      "Epoch: 1045/2000... Training loss: 0.4420\n",
      "Epoch: 1045/2000... Training loss: 0.5510\n",
      "Epoch: 1045/2000... Training loss: 0.4293\n",
      "Epoch: 1045/2000... Training loss: 0.6674\n",
      "Epoch: 1045/2000... Training loss: 0.4833\n",
      "Epoch: 1045/2000... Training loss: 0.4296\n",
      "Epoch: 1045/2000... Training loss: 0.6277\n",
      "Epoch: 1045/2000... Training loss: 0.4978\n",
      "Epoch: 1045/2000... Training loss: 0.4570\n",
      "Epoch: 1045/2000... Training loss: 0.4632\n",
      "Epoch: 1045/2000... Training loss: 0.5865\n",
      "Epoch: 1045/2000... Training loss: 0.6230\n",
      "Epoch: 1045/2000... Training loss: 0.4131\n",
      "Epoch: 1045/2000... Training loss: 0.3203\n",
      "Epoch: 1045/2000... Training loss: 0.4084\n",
      "Epoch: 1045/2000... Training loss: 0.4551\n",
      "Epoch: 1045/2000... Training loss: 0.5357\n",
      "Epoch: 1045/2000... Training loss: 0.5352\n",
      "Epoch: 1045/2000... Training loss: 0.4826\n",
      "Epoch: 1045/2000... Training loss: 0.4600\n",
      "Epoch: 1045/2000... Training loss: 0.5596\n",
      "Epoch: 1045/2000... Training loss: 0.2612\n",
      "Epoch: 1045/2000... Training loss: 0.6223\n",
      "Epoch: 1045/2000... Training loss: 0.4788\n",
      "Epoch: 1046/2000... Training loss: 0.5132\n",
      "Epoch: 1046/2000... Training loss: 0.5732\n",
      "Epoch: 1046/2000... Training loss: 0.7110\n",
      "Epoch: 1046/2000... Training loss: 0.3265\n",
      "Epoch: 1046/2000... Training loss: 0.4662\n",
      "Epoch: 1046/2000... Training loss: 0.4795\n",
      "Epoch: 1046/2000... Training loss: 0.4864\n",
      "Epoch: 1046/2000... Training loss: 0.4863\n",
      "Epoch: 1046/2000... Training loss: 0.5869\n",
      "Epoch: 1046/2000... Training loss: 0.4978\n",
      "Epoch: 1046/2000... Training loss: 0.3431\n",
      "Epoch: 1046/2000... Training loss: 0.4646\n",
      "Epoch: 1046/2000... Training loss: 0.4894\n",
      "Epoch: 1046/2000... Training loss: 0.5725\n",
      "Epoch: 1046/2000... Training loss: 0.3349\n",
      "Epoch: 1046/2000... Training loss: 0.4569\n",
      "Epoch: 1046/2000... Training loss: 0.5404\n",
      "Epoch: 1046/2000... Training loss: 0.4407\n",
      "Epoch: 1046/2000... Training loss: 0.6581\n",
      "Epoch: 1046/2000... Training loss: 0.3833\n",
      "Epoch: 1046/2000... Training loss: 0.6066\n",
      "Epoch: 1046/2000... Training loss: 0.3777\n",
      "Epoch: 1046/2000... Training loss: 0.4392\n",
      "Epoch: 1046/2000... Training loss: 0.5568\n",
      "Epoch: 1046/2000... Training loss: 0.4823\n",
      "Epoch: 1046/2000... Training loss: 0.5244\n",
      "Epoch: 1046/2000... Training loss: 0.5588\n",
      "Epoch: 1046/2000... Training loss: 0.4788\n",
      "Epoch: 1046/2000... Training loss: 0.6728\n",
      "Epoch: 1046/2000... Training loss: 0.4024\n",
      "Epoch: 1046/2000... Training loss: 0.3105\n",
      "Epoch: 1047/2000... Training loss: 0.4944\n",
      "Epoch: 1047/2000... Training loss: 0.6219\n",
      "Epoch: 1047/2000... Training loss: 0.4819\n",
      "Epoch: 1047/2000... Training loss: 0.2926\n",
      "Epoch: 1047/2000... Training loss: 0.6139\n",
      "Epoch: 1047/2000... Training loss: 0.2982\n",
      "Epoch: 1047/2000... Training loss: 0.6696\n",
      "Epoch: 1047/2000... Training loss: 0.3664\n",
      "Epoch: 1047/2000... Training loss: 0.3337\n",
      "Epoch: 1047/2000... Training loss: 0.5413\n",
      "Epoch: 1047/2000... Training loss: 0.4369\n",
      "Epoch: 1047/2000... Training loss: 0.4719\n",
      "Epoch: 1047/2000... Training loss: 0.3971\n",
      "Epoch: 1047/2000... Training loss: 0.3911\n",
      "Epoch: 1047/2000... Training loss: 0.4480\n",
      "Epoch: 1047/2000... Training loss: 0.3520\n",
      "Epoch: 1047/2000... Training loss: 0.4269\n",
      "Epoch: 1047/2000... Training loss: 0.6340\n",
      "Epoch: 1047/2000... Training loss: 0.4676\n",
      "Epoch: 1047/2000... Training loss: 0.4610\n",
      "Epoch: 1047/2000... Training loss: 0.3303\n",
      "Epoch: 1047/2000... Training loss: 0.5282\n",
      "Epoch: 1047/2000... Training loss: 0.5598\n",
      "Epoch: 1047/2000... Training loss: 0.4668\n",
      "Epoch: 1047/2000... Training loss: 0.4281\n",
      "Epoch: 1047/2000... Training loss: 0.5108\n",
      "Epoch: 1047/2000... Training loss: 0.5635\n",
      "Epoch: 1047/2000... Training loss: 0.4139\n",
      "Epoch: 1047/2000... Training loss: 0.7020\n",
      "Epoch: 1047/2000... Training loss: 0.3809\n",
      "Epoch: 1047/2000... Training loss: 0.2910\n",
      "Epoch: 1048/2000... Training loss: 0.5894\n",
      "Epoch: 1048/2000... Training loss: 0.4256\n",
      "Epoch: 1048/2000... Training loss: 0.4104\n",
      "Epoch: 1048/2000... Training loss: 0.4012\n",
      "Epoch: 1048/2000... Training loss: 0.5075\n",
      "Epoch: 1048/2000... Training loss: 0.4289\n",
      "Epoch: 1048/2000... Training loss: 0.6582\n",
      "Epoch: 1048/2000... Training loss: 0.6924\n",
      "Epoch: 1048/2000... Training loss: 0.4845\n",
      "Epoch: 1048/2000... Training loss: 0.6423\n",
      "Epoch: 1048/2000... Training loss: 0.4675\n",
      "Epoch: 1048/2000... Training loss: 0.4601\n",
      "Epoch: 1048/2000... Training loss: 0.5744\n",
      "Epoch: 1048/2000... Training loss: 0.3693\n",
      "Epoch: 1048/2000... Training loss: 0.3045\n",
      "Epoch: 1048/2000... Training loss: 0.5193\n",
      "Epoch: 1048/2000... Training loss: 0.5478\n",
      "Epoch: 1048/2000... Training loss: 0.3767\n",
      "Epoch: 1048/2000... Training loss: 0.3923\n",
      "Epoch: 1048/2000... Training loss: 0.4310\n",
      "Epoch: 1048/2000... Training loss: 0.3625\n",
      "Epoch: 1048/2000... Training loss: 0.3939\n",
      "Epoch: 1048/2000... Training loss: 0.3576\n",
      "Epoch: 1048/2000... Training loss: 0.5202\n",
      "Epoch: 1048/2000... Training loss: 0.5081\n",
      "Epoch: 1048/2000... Training loss: 0.3229\n",
      "Epoch: 1048/2000... Training loss: 0.6207\n",
      "Epoch: 1048/2000... Training loss: 0.5323\n",
      "Epoch: 1048/2000... Training loss: 0.3871\n",
      "Epoch: 1048/2000... Training loss: 0.4843\n",
      "Epoch: 1048/2000... Training loss: 0.5228\n",
      "Epoch: 1049/2000... Training loss: 0.5244\n",
      "Epoch: 1049/2000... Training loss: 0.5257\n",
      "Epoch: 1049/2000... Training loss: 0.5935\n",
      "Epoch: 1049/2000... Training loss: 0.4445\n",
      "Epoch: 1049/2000... Training loss: 0.5400\n",
      "Epoch: 1049/2000... Training loss: 0.4883\n",
      "Epoch: 1049/2000... Training loss: 0.4209\n",
      "Epoch: 1049/2000... Training loss: 0.4348\n",
      "Epoch: 1049/2000... Training loss: 0.5577\n",
      "Epoch: 1049/2000... Training loss: 0.4299\n",
      "Epoch: 1049/2000... Training loss: 0.5528\n",
      "Epoch: 1049/2000... Training loss: 0.4681\n",
      "Epoch: 1049/2000... Training loss: 0.5872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1049/2000... Training loss: 0.4937\n",
      "Epoch: 1049/2000... Training loss: 0.5946\n",
      "Epoch: 1049/2000... Training loss: 0.4421\n",
      "Epoch: 1049/2000... Training loss: 0.5448\n",
      "Epoch: 1049/2000... Training loss: 0.4204\n",
      "Epoch: 1049/2000... Training loss: 0.4213\n",
      "Epoch: 1049/2000... Training loss: 0.4538\n",
      "Epoch: 1049/2000... Training loss: 0.4384\n",
      "Epoch: 1049/2000... Training loss: 0.3278\n",
      "Epoch: 1049/2000... Training loss: 0.4893\n",
      "Epoch: 1049/2000... Training loss: 0.4001\n",
      "Epoch: 1049/2000... Training loss: 0.5175\n",
      "Epoch: 1049/2000... Training loss: 0.5167\n",
      "Epoch: 1049/2000... Training loss: 0.5132\n",
      "Epoch: 1049/2000... Training loss: 0.5597\n",
      "Epoch: 1049/2000... Training loss: 0.6405\n",
      "Epoch: 1049/2000... Training loss: 0.4636\n",
      "Epoch: 1049/2000... Training loss: 0.6556\n",
      "Epoch: 1050/2000... Training loss: 0.8268\n",
      "Epoch: 1050/2000... Training loss: 0.4094\n",
      "Epoch: 1050/2000... Training loss: 0.6118\n",
      "Epoch: 1050/2000... Training loss: 0.5319\n",
      "Epoch: 1050/2000... Training loss: 0.4987\n",
      "Epoch: 1050/2000... Training loss: 0.4631\n",
      "Epoch: 1050/2000... Training loss: 0.5069\n",
      "Epoch: 1050/2000... Training loss: 0.5824\n",
      "Epoch: 1050/2000... Training loss: 0.4743\n",
      "Epoch: 1050/2000... Training loss: 0.5301\n",
      "Epoch: 1050/2000... Training loss: 0.5946\n",
      "Epoch: 1050/2000... Training loss: 0.4356\n",
      "Epoch: 1050/2000... Training loss: 0.6865\n",
      "Epoch: 1050/2000... Training loss: 0.5572\n",
      "Epoch: 1050/2000... Training loss: 0.5043\n",
      "Epoch: 1050/2000... Training loss: 0.4263\n",
      "Epoch: 1050/2000... Training loss: 0.5070\n",
      "Epoch: 1050/2000... Training loss: 0.4910\n",
      "Epoch: 1050/2000... Training loss: 0.6459\n",
      "Epoch: 1050/2000... Training loss: 0.5856\n",
      "Epoch: 1050/2000... Training loss: 0.6144\n",
      "Epoch: 1050/2000... Training loss: 0.5091\n",
      "Epoch: 1050/2000... Training loss: 0.3802\n",
      "Epoch: 1050/2000... Training loss: 0.3835\n",
      "Epoch: 1050/2000... Training loss: 0.4378\n",
      "Epoch: 1050/2000... Training loss: 0.5149\n",
      "Epoch: 1050/2000... Training loss: 0.4371\n",
      "Epoch: 1050/2000... Training loss: 0.5386\n",
      "Epoch: 1050/2000... Training loss: 0.5258\n",
      "Epoch: 1050/2000... Training loss: 0.5031\n",
      "Epoch: 1050/2000... Training loss: 0.5384\n",
      "Epoch: 1051/2000... Training loss: 0.6289\n",
      "Epoch: 1051/2000... Training loss: 0.5421\n",
      "Epoch: 1051/2000... Training loss: 0.2470\n",
      "Epoch: 1051/2000... Training loss: 0.3340\n",
      "Epoch: 1051/2000... Training loss: 0.6086\n",
      "Epoch: 1051/2000... Training loss: 0.3679\n",
      "Epoch: 1051/2000... Training loss: 0.3766\n",
      "Epoch: 1051/2000... Training loss: 0.5627\n",
      "Epoch: 1051/2000... Training loss: 0.7836\n",
      "Epoch: 1051/2000... Training loss: 0.5423\n",
      "Epoch: 1051/2000... Training loss: 0.4954\n",
      "Epoch: 1051/2000... Training loss: 0.4718\n",
      "Epoch: 1051/2000... Training loss: 0.6693\n",
      "Epoch: 1051/2000... Training loss: 0.6041\n",
      "Epoch: 1051/2000... Training loss: 0.4947\n",
      "Epoch: 1051/2000... Training loss: 0.4447\n",
      "Epoch: 1051/2000... Training loss: 0.6024\n",
      "Epoch: 1051/2000... Training loss: 0.6017\n",
      "Epoch: 1051/2000... Training loss: 0.5870\n",
      "Epoch: 1051/2000... Training loss: 0.4594\n",
      "Epoch: 1051/2000... Training loss: 0.4737\n",
      "Epoch: 1051/2000... Training loss: 0.4947\n",
      "Epoch: 1051/2000... Training loss: 0.4835\n",
      "Epoch: 1051/2000... Training loss: 0.4837\n",
      "Epoch: 1051/2000... Training loss: 0.4937\n",
      "Epoch: 1051/2000... Training loss: 0.5448\n",
      "Epoch: 1051/2000... Training loss: 0.5302\n",
      "Epoch: 1051/2000... Training loss: 0.5845\n",
      "Epoch: 1051/2000... Training loss: 0.5211\n",
      "Epoch: 1051/2000... Training loss: 0.4948\n",
      "Epoch: 1051/2000... Training loss: 0.3171\n",
      "Epoch: 1052/2000... Training loss: 0.3970\n",
      "Epoch: 1052/2000... Training loss: 0.5045\n",
      "Epoch: 1052/2000... Training loss: 0.5534\n",
      "Epoch: 1052/2000... Training loss: 0.6035\n",
      "Epoch: 1052/2000... Training loss: 0.4756\n",
      "Epoch: 1052/2000... Training loss: 0.5241\n",
      "Epoch: 1052/2000... Training loss: 0.3688\n",
      "Epoch: 1052/2000... Training loss: 0.4197\n",
      "Epoch: 1052/2000... Training loss: 0.5057\n",
      "Epoch: 1052/2000... Training loss: 0.5527\n",
      "Epoch: 1052/2000... Training loss: 0.4997\n",
      "Epoch: 1052/2000... Training loss: 0.4299\n",
      "Epoch: 1052/2000... Training loss: 0.3284\n",
      "Epoch: 1052/2000... Training loss: 0.4704\n",
      "Epoch: 1052/2000... Training loss: 0.5066\n",
      "Epoch: 1052/2000... Training loss: 0.4254\n",
      "Epoch: 1052/2000... Training loss: 0.4639\n",
      "Epoch: 1052/2000... Training loss: 0.7853\n",
      "Epoch: 1052/2000... Training loss: 0.5472\n",
      "Epoch: 1052/2000... Training loss: 0.5194\n",
      "Epoch: 1052/2000... Training loss: 0.4565\n",
      "Epoch: 1052/2000... Training loss: 0.4680\n",
      "Epoch: 1052/2000... Training loss: 0.4870\n",
      "Epoch: 1052/2000... Training loss: 0.4240\n",
      "Epoch: 1052/2000... Training loss: 0.6346\n",
      "Epoch: 1052/2000... Training loss: 0.4153\n",
      "Epoch: 1052/2000... Training loss: 0.5911\n",
      "Epoch: 1052/2000... Training loss: 0.5827\n",
      "Epoch: 1052/2000... Training loss: 0.4553\n",
      "Epoch: 1052/2000... Training loss: 0.4322\n",
      "Epoch: 1052/2000... Training loss: 0.4613\n",
      "Epoch: 1053/2000... Training loss: 0.3638\n",
      "Epoch: 1053/2000... Training loss: 0.4548\n",
      "Epoch: 1053/2000... Training loss: 0.6549\n",
      "Epoch: 1053/2000... Training loss: 0.5264\n",
      "Epoch: 1053/2000... Training loss: 0.6028\n",
      "Epoch: 1053/2000... Training loss: 0.3452\n",
      "Epoch: 1053/2000... Training loss: 0.5085\n",
      "Epoch: 1053/2000... Training loss: 0.4573\n",
      "Epoch: 1053/2000... Training loss: 0.4558\n",
      "Epoch: 1053/2000... Training loss: 0.5942\n",
      "Epoch: 1053/2000... Training loss: 0.7198\n",
      "Epoch: 1053/2000... Training loss: 0.5730\n",
      "Epoch: 1053/2000... Training loss: 0.6005\n",
      "Epoch: 1053/2000... Training loss: 0.4054\n",
      "Epoch: 1053/2000... Training loss: 0.3501\n",
      "Epoch: 1053/2000... Training loss: 0.6018\n",
      "Epoch: 1053/2000... Training loss: 0.3493\n",
      "Epoch: 1053/2000... Training loss: 0.4144\n",
      "Epoch: 1053/2000... Training loss: 0.5142\n",
      "Epoch: 1053/2000... Training loss: 0.4853\n",
      "Epoch: 1053/2000... Training loss: 0.7129\n",
      "Epoch: 1053/2000... Training loss: 0.4240\n",
      "Epoch: 1053/2000... Training loss: 0.5361\n",
      "Epoch: 1053/2000... Training loss: 0.5291\n",
      "Epoch: 1053/2000... Training loss: 0.2993\n",
      "Epoch: 1053/2000... Training loss: 0.5754\n",
      "Epoch: 1053/2000... Training loss: 0.5908\n",
      "Epoch: 1053/2000... Training loss: 0.4677\n",
      "Epoch: 1053/2000... Training loss: 0.4221\n",
      "Epoch: 1053/2000... Training loss: 0.4176\n",
      "Epoch: 1053/2000... Training loss: 0.5142\n",
      "Epoch: 1054/2000... Training loss: 0.4219\n",
      "Epoch: 1054/2000... Training loss: 0.5582\n",
      "Epoch: 1054/2000... Training loss: 0.3941\n",
      "Epoch: 1054/2000... Training loss: 0.4577\n",
      "Epoch: 1054/2000... Training loss: 0.3995\n",
      "Epoch: 1054/2000... Training loss: 0.7096\n",
      "Epoch: 1054/2000... Training loss: 0.3550\n",
      "Epoch: 1054/2000... Training loss: 0.3932\n",
      "Epoch: 1054/2000... Training loss: 0.4782\n",
      "Epoch: 1054/2000... Training loss: 0.5788\n",
      "Epoch: 1054/2000... Training loss: 0.6456\n",
      "Epoch: 1054/2000... Training loss: 0.4107\n",
      "Epoch: 1054/2000... Training loss: 0.4509\n",
      "Epoch: 1054/2000... Training loss: 0.4093\n",
      "Epoch: 1054/2000... Training loss: 0.5320\n",
      "Epoch: 1054/2000... Training loss: 0.3587\n",
      "Epoch: 1054/2000... Training loss: 0.5617\n",
      "Epoch: 1054/2000... Training loss: 0.4537\n",
      "Epoch: 1054/2000... Training loss: 0.5555\n",
      "Epoch: 1054/2000... Training loss: 0.4586\n",
      "Epoch: 1054/2000... Training loss: 0.4692\n",
      "Epoch: 1054/2000... Training loss: 0.6677\n",
      "Epoch: 1054/2000... Training loss: 0.4994\n",
      "Epoch: 1054/2000... Training loss: 0.5276\n",
      "Epoch: 1054/2000... Training loss: 0.4705\n",
      "Epoch: 1054/2000... Training loss: 0.6688\n",
      "Epoch: 1054/2000... Training loss: 0.4419\n",
      "Epoch: 1054/2000... Training loss: 0.5870\n",
      "Epoch: 1054/2000... Training loss: 0.6886\n",
      "Epoch: 1054/2000... Training loss: 0.6593\n",
      "Epoch: 1054/2000... Training loss: 0.3651\n",
      "Epoch: 1055/2000... Training loss: 0.5280\n",
      "Epoch: 1055/2000... Training loss: 0.5199\n",
      "Epoch: 1055/2000... Training loss: 0.4428\n",
      "Epoch: 1055/2000... Training loss: 0.5068\n",
      "Epoch: 1055/2000... Training loss: 0.6885\n",
      "Epoch: 1055/2000... Training loss: 0.4427\n",
      "Epoch: 1055/2000... Training loss: 0.6435\n",
      "Epoch: 1055/2000... Training loss: 0.4730\n",
      "Epoch: 1055/2000... Training loss: 0.5971\n",
      "Epoch: 1055/2000... Training loss: 0.3966\n",
      "Epoch: 1055/2000... Training loss: 0.4355\n",
      "Epoch: 1055/2000... Training loss: 0.5690\n",
      "Epoch: 1055/2000... Training loss: 0.4601\n",
      "Epoch: 1055/2000... Training loss: 0.5018\n",
      "Epoch: 1055/2000... Training loss: 0.5883\n",
      "Epoch: 1055/2000... Training loss: 0.3469\n",
      "Epoch: 1055/2000... Training loss: 0.4791\n",
      "Epoch: 1055/2000... Training loss: 0.4744\n",
      "Epoch: 1055/2000... Training loss: 0.4752\n",
      "Epoch: 1055/2000... Training loss: 0.6831\n",
      "Epoch: 1055/2000... Training loss: 0.5944\n",
      "Epoch: 1055/2000... Training loss: 0.4921\n",
      "Epoch: 1055/2000... Training loss: 0.6661\n",
      "Epoch: 1055/2000... Training loss: 0.6191\n",
      "Epoch: 1055/2000... Training loss: 0.5273\n",
      "Epoch: 1055/2000... Training loss: 0.3704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1055/2000... Training loss: 0.4740\n",
      "Epoch: 1055/2000... Training loss: 0.4661\n",
      "Epoch: 1055/2000... Training loss: 0.3661\n",
      "Epoch: 1055/2000... Training loss: 0.2357\n",
      "Epoch: 1055/2000... Training loss: 0.5875\n",
      "Epoch: 1056/2000... Training loss: 0.4810\n",
      "Epoch: 1056/2000... Training loss: 0.3420\n",
      "Epoch: 1056/2000... Training loss: 0.5031\n",
      "Epoch: 1056/2000... Training loss: 0.4799\n",
      "Epoch: 1056/2000... Training loss: 0.6081\n",
      "Epoch: 1056/2000... Training loss: 0.4622\n",
      "Epoch: 1056/2000... Training loss: 0.3549\n",
      "Epoch: 1056/2000... Training loss: 0.4246\n",
      "Epoch: 1056/2000... Training loss: 0.4370\n",
      "Epoch: 1056/2000... Training loss: 0.4357\n",
      "Epoch: 1056/2000... Training loss: 0.4011\n",
      "Epoch: 1056/2000... Training loss: 0.3265\n",
      "Epoch: 1056/2000... Training loss: 0.4010\n",
      "Epoch: 1056/2000... Training loss: 0.4581\n",
      "Epoch: 1056/2000... Training loss: 0.5773\n",
      "Epoch: 1056/2000... Training loss: 0.4241\n",
      "Epoch: 1056/2000... Training loss: 0.5139\n",
      "Epoch: 1056/2000... Training loss: 0.4176\n",
      "Epoch: 1056/2000... Training loss: 0.3485\n",
      "Epoch: 1056/2000... Training loss: 0.3807\n",
      "Epoch: 1056/2000... Training loss: 0.4495\n",
      "Epoch: 1056/2000... Training loss: 0.5218\n",
      "Epoch: 1056/2000... Training loss: 0.6252\n",
      "Epoch: 1056/2000... Training loss: 0.4777\n",
      "Epoch: 1056/2000... Training loss: 0.5182\n",
      "Epoch: 1056/2000... Training loss: 0.5803\n",
      "Epoch: 1056/2000... Training loss: 0.4560\n",
      "Epoch: 1056/2000... Training loss: 0.5892\n",
      "Epoch: 1056/2000... Training loss: 0.5894\n",
      "Epoch: 1056/2000... Training loss: 0.5376\n",
      "Epoch: 1056/2000... Training loss: 0.4621\n",
      "Epoch: 1057/2000... Training loss: 0.5148\n",
      "Epoch: 1057/2000... Training loss: 0.4605\n",
      "Epoch: 1057/2000... Training loss: 0.4415\n",
      "Epoch: 1057/2000... Training loss: 0.3632\n",
      "Epoch: 1057/2000... Training loss: 0.4203\n",
      "Epoch: 1057/2000... Training loss: 0.4823\n",
      "Epoch: 1057/2000... Training loss: 0.5670\n",
      "Epoch: 1057/2000... Training loss: 0.7751\n",
      "Epoch: 1057/2000... Training loss: 0.4609\n",
      "Epoch: 1057/2000... Training loss: 0.4196\n",
      "Epoch: 1057/2000... Training loss: 0.6070\n",
      "Epoch: 1057/2000... Training loss: 0.4037\n",
      "Epoch: 1057/2000... Training loss: 0.5336\n",
      "Epoch: 1057/2000... Training loss: 0.4906\n",
      "Epoch: 1057/2000... Training loss: 0.3674\n",
      "Epoch: 1057/2000... Training loss: 0.4552\n",
      "Epoch: 1057/2000... Training loss: 0.4626\n",
      "Epoch: 1057/2000... Training loss: 0.3850\n",
      "Epoch: 1057/2000... Training loss: 0.3983\n",
      "Epoch: 1057/2000... Training loss: 0.5571\n",
      "Epoch: 1057/2000... Training loss: 0.6853\n",
      "Epoch: 1057/2000... Training loss: 0.4683\n",
      "Epoch: 1057/2000... Training loss: 0.5896\n",
      "Epoch: 1057/2000... Training loss: 0.5096\n",
      "Epoch: 1057/2000... Training loss: 0.6280\n",
      "Epoch: 1057/2000... Training loss: 0.5781\n",
      "Epoch: 1057/2000... Training loss: 0.5669\n",
      "Epoch: 1057/2000... Training loss: 0.4513\n",
      "Epoch: 1057/2000... Training loss: 0.4091\n",
      "Epoch: 1057/2000... Training loss: 0.3872\n",
      "Epoch: 1057/2000... Training loss: 0.4845\n",
      "Epoch: 1058/2000... Training loss: 0.6904\n",
      "Epoch: 1058/2000... Training loss: 0.4318\n",
      "Epoch: 1058/2000... Training loss: 0.4651\n",
      "Epoch: 1058/2000... Training loss: 0.4438\n",
      "Epoch: 1058/2000... Training loss: 0.5378\n",
      "Epoch: 1058/2000... Training loss: 0.2857\n",
      "Epoch: 1058/2000... Training loss: 0.5837\n",
      "Epoch: 1058/2000... Training loss: 0.4775\n",
      "Epoch: 1058/2000... Training loss: 0.3685\n",
      "Epoch: 1058/2000... Training loss: 0.5714\n",
      "Epoch: 1058/2000... Training loss: 0.5432\n",
      "Epoch: 1058/2000... Training loss: 0.6678\n",
      "Epoch: 1058/2000... Training loss: 0.3748\n",
      "Epoch: 1058/2000... Training loss: 0.4469\n",
      "Epoch: 1058/2000... Training loss: 0.6336\n",
      "Epoch: 1058/2000... Training loss: 0.3808\n",
      "Epoch: 1058/2000... Training loss: 0.4640\n",
      "Epoch: 1058/2000... Training loss: 0.4895\n",
      "Epoch: 1058/2000... Training loss: 0.4790\n",
      "Epoch: 1058/2000... Training loss: 0.5062\n",
      "Epoch: 1058/2000... Training loss: 0.5141\n",
      "Epoch: 1058/2000... Training loss: 0.5464\n",
      "Epoch: 1058/2000... Training loss: 0.6081\n",
      "Epoch: 1058/2000... Training loss: 0.4271\n",
      "Epoch: 1058/2000... Training loss: 0.5692\n",
      "Epoch: 1058/2000... Training loss: 0.5137\n",
      "Epoch: 1058/2000... Training loss: 0.4701\n",
      "Epoch: 1058/2000... Training loss: 0.4732\n",
      "Epoch: 1058/2000... Training loss: 0.6137\n",
      "Epoch: 1058/2000... Training loss: 0.4607\n",
      "Epoch: 1058/2000... Training loss: 0.4020\n",
      "Epoch: 1059/2000... Training loss: 0.5015\n",
      "Epoch: 1059/2000... Training loss: 0.3762\n",
      "Epoch: 1059/2000... Training loss: 0.3683\n",
      "Epoch: 1059/2000... Training loss: 0.5599\n",
      "Epoch: 1059/2000... Training loss: 0.3891\n",
      "Epoch: 1059/2000... Training loss: 0.4402\n",
      "Epoch: 1059/2000... Training loss: 0.4147\n",
      "Epoch: 1059/2000... Training loss: 0.4441\n",
      "Epoch: 1059/2000... Training loss: 0.6884\n",
      "Epoch: 1059/2000... Training loss: 0.5446\n",
      "Epoch: 1059/2000... Training loss: 0.4146\n",
      "Epoch: 1059/2000... Training loss: 0.6258\n",
      "Epoch: 1059/2000... Training loss: 0.5752\n",
      "Epoch: 1059/2000... Training loss: 0.3148\n",
      "Epoch: 1059/2000... Training loss: 0.3755\n",
      "Epoch: 1059/2000... Training loss: 0.6875\n",
      "Epoch: 1059/2000... Training loss: 0.5084\n",
      "Epoch: 1059/2000... Training loss: 0.5148\n",
      "Epoch: 1059/2000... Training loss: 0.5301\n",
      "Epoch: 1059/2000... Training loss: 0.4137\n",
      "Epoch: 1059/2000... Training loss: 0.3389\n",
      "Epoch: 1059/2000... Training loss: 0.3816\n",
      "Epoch: 1059/2000... Training loss: 0.4040\n",
      "Epoch: 1059/2000... Training loss: 0.4308\n",
      "Epoch: 1059/2000... Training loss: 0.5215\n",
      "Epoch: 1059/2000... Training loss: 0.6531\n",
      "Epoch: 1059/2000... Training loss: 0.5487\n",
      "Epoch: 1059/2000... Training loss: 0.3995\n",
      "Epoch: 1059/2000... Training loss: 0.4957\n",
      "Epoch: 1059/2000... Training loss: 0.4810\n",
      "Epoch: 1059/2000... Training loss: 0.5643\n",
      "Epoch: 1060/2000... Training loss: 0.5406\n",
      "Epoch: 1060/2000... Training loss: 0.6480\n",
      "Epoch: 1060/2000... Training loss: 0.5410\n",
      "Epoch: 1060/2000... Training loss: 0.5397\n",
      "Epoch: 1060/2000... Training loss: 0.3354\n",
      "Epoch: 1060/2000... Training loss: 0.5809\n",
      "Epoch: 1060/2000... Training loss: 0.4743\n",
      "Epoch: 1060/2000... Training loss: 0.2627\n",
      "Epoch: 1060/2000... Training loss: 0.3349\n",
      "Epoch: 1060/2000... Training loss: 0.5513\n",
      "Epoch: 1060/2000... Training loss: 0.5105\n",
      "Epoch: 1060/2000... Training loss: 0.4896\n",
      "Epoch: 1060/2000... Training loss: 0.6318\n",
      "Epoch: 1060/2000... Training loss: 0.7624\n",
      "Epoch: 1060/2000... Training loss: 0.5575\n",
      "Epoch: 1060/2000... Training loss: 0.5553\n",
      "Epoch: 1060/2000... Training loss: 0.5740\n",
      "Epoch: 1060/2000... Training loss: 0.4654\n",
      "Epoch: 1060/2000... Training loss: 0.5901\n",
      "Epoch: 1060/2000... Training loss: 0.4480\n",
      "Epoch: 1060/2000... Training loss: 0.5056\n",
      "Epoch: 1060/2000... Training loss: 0.3876\n",
      "Epoch: 1060/2000... Training loss: 0.4290\n",
      "Epoch: 1060/2000... Training loss: 0.6572\n",
      "Epoch: 1060/2000... Training loss: 0.5839\n",
      "Epoch: 1060/2000... Training loss: 0.6453\n",
      "Epoch: 1060/2000... Training loss: 0.4720\n",
      "Epoch: 1060/2000... Training loss: 0.4491\n",
      "Epoch: 1060/2000... Training loss: 0.5193\n",
      "Epoch: 1060/2000... Training loss: 0.4347\n",
      "Epoch: 1060/2000... Training loss: 0.5258\n",
      "Epoch: 1061/2000... Training loss: 0.4853\n",
      "Epoch: 1061/2000... Training loss: 0.6176\n",
      "Epoch: 1061/2000... Training loss: 0.5051\n",
      "Epoch: 1061/2000... Training loss: 0.4937\n",
      "Epoch: 1061/2000... Training loss: 0.5123\n",
      "Epoch: 1061/2000... Training loss: 0.6825\n",
      "Epoch: 1061/2000... Training loss: 0.4672\n",
      "Epoch: 1061/2000... Training loss: 0.5823\n",
      "Epoch: 1061/2000... Training loss: 0.5199\n",
      "Epoch: 1061/2000... Training loss: 0.5791\n",
      "Epoch: 1061/2000... Training loss: 0.4672\n",
      "Epoch: 1061/2000... Training loss: 0.6011\n",
      "Epoch: 1061/2000... Training loss: 0.5172\n",
      "Epoch: 1061/2000... Training loss: 0.6707\n",
      "Epoch: 1061/2000... Training loss: 0.4857\n",
      "Epoch: 1061/2000... Training loss: 0.5046\n",
      "Epoch: 1061/2000... Training loss: 0.3303\n",
      "Epoch: 1061/2000... Training loss: 0.4671\n",
      "Epoch: 1061/2000... Training loss: 0.5488\n",
      "Epoch: 1061/2000... Training loss: 0.4041\n",
      "Epoch: 1061/2000... Training loss: 0.6986\n",
      "Epoch: 1061/2000... Training loss: 0.4463\n",
      "Epoch: 1061/2000... Training loss: 0.6091\n",
      "Epoch: 1061/2000... Training loss: 0.4583\n",
      "Epoch: 1061/2000... Training loss: 0.5143\n",
      "Epoch: 1061/2000... Training loss: 0.4381\n",
      "Epoch: 1061/2000... Training loss: 0.5795\n",
      "Epoch: 1061/2000... Training loss: 0.4695\n",
      "Epoch: 1061/2000... Training loss: 0.6187\n",
      "Epoch: 1061/2000... Training loss: 0.4896\n",
      "Epoch: 1061/2000... Training loss: 0.2966\n",
      "Epoch: 1062/2000... Training loss: 0.4209\n",
      "Epoch: 1062/2000... Training loss: 0.4359\n",
      "Epoch: 1062/2000... Training loss: 0.3791\n",
      "Epoch: 1062/2000... Training loss: 0.5326\n",
      "Epoch: 1062/2000... Training loss: 0.3703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1062/2000... Training loss: 0.4674\n",
      "Epoch: 1062/2000... Training loss: 0.5661\n",
      "Epoch: 1062/2000... Training loss: 0.7287\n",
      "Epoch: 1062/2000... Training loss: 0.6562\n",
      "Epoch: 1062/2000... Training loss: 0.6503\n",
      "Epoch: 1062/2000... Training loss: 0.6093\n",
      "Epoch: 1062/2000... Training loss: 0.4970\n",
      "Epoch: 1062/2000... Training loss: 0.6740\n",
      "Epoch: 1062/2000... Training loss: 0.4546\n",
      "Epoch: 1062/2000... Training loss: 0.5360\n",
      "Epoch: 1062/2000... Training loss: 0.3897\n",
      "Epoch: 1062/2000... Training loss: 0.6650\n",
      "Epoch: 1062/2000... Training loss: 0.6645\n",
      "Epoch: 1062/2000... Training loss: 0.6312\n",
      "Epoch: 1062/2000... Training loss: 0.4321\n",
      "Epoch: 1062/2000... Training loss: 0.4852\n",
      "Epoch: 1062/2000... Training loss: 0.4605\n",
      "Epoch: 1062/2000... Training loss: 0.4944\n",
      "Epoch: 1062/2000... Training loss: 0.6415\n",
      "Epoch: 1062/2000... Training loss: 0.6629\n",
      "Epoch: 1062/2000... Training loss: 0.6085\n",
      "Epoch: 1062/2000... Training loss: 0.4717\n",
      "Epoch: 1062/2000... Training loss: 0.4945\n",
      "Epoch: 1062/2000... Training loss: 0.3976\n",
      "Epoch: 1062/2000... Training loss: 0.5459\n",
      "Epoch: 1062/2000... Training loss: 0.5299\n",
      "Epoch: 1063/2000... Training loss: 0.5265\n",
      "Epoch: 1063/2000... Training loss: 0.6207\n",
      "Epoch: 1063/2000... Training loss: 0.6183\n",
      "Epoch: 1063/2000... Training loss: 0.7204\n",
      "Epoch: 1063/2000... Training loss: 0.5223\n",
      "Epoch: 1063/2000... Training loss: 0.7395\n",
      "Epoch: 1063/2000... Training loss: 0.4995\n",
      "Epoch: 1063/2000... Training loss: 0.6085\n",
      "Epoch: 1063/2000... Training loss: 0.5136\n",
      "Epoch: 1063/2000... Training loss: 0.4386\n",
      "Epoch: 1063/2000... Training loss: 0.5423\n",
      "Epoch: 1063/2000... Training loss: 0.4936\n",
      "Epoch: 1063/2000... Training loss: 0.4393\n",
      "Epoch: 1063/2000... Training loss: 0.4145\n",
      "Epoch: 1063/2000... Training loss: 0.3933\n",
      "Epoch: 1063/2000... Training loss: 0.6672\n",
      "Epoch: 1063/2000... Training loss: 0.3626\n",
      "Epoch: 1063/2000... Training loss: 0.3739\n",
      "Epoch: 1063/2000... Training loss: 0.5963\n",
      "Epoch: 1063/2000... Training loss: 0.5877\n",
      "Epoch: 1063/2000... Training loss: 0.5147\n",
      "Epoch: 1063/2000... Training loss: 0.5697\n",
      "Epoch: 1063/2000... Training loss: 0.5217\n",
      "Epoch: 1063/2000... Training loss: 0.6088\n",
      "Epoch: 1063/2000... Training loss: 0.4880\n",
      "Epoch: 1063/2000... Training loss: 0.2680\n",
      "Epoch: 1063/2000... Training loss: 0.5778\n",
      "Epoch: 1063/2000... Training loss: 0.6142\n",
      "Epoch: 1063/2000... Training loss: 0.4590\n",
      "Epoch: 1063/2000... Training loss: 0.4082\n",
      "Epoch: 1063/2000... Training loss: 0.5199\n",
      "Epoch: 1064/2000... Training loss: 0.4953\n",
      "Epoch: 1064/2000... Training loss: 0.4703\n",
      "Epoch: 1064/2000... Training loss: 0.4965\n",
      "Epoch: 1064/2000... Training loss: 0.4982\n",
      "Epoch: 1064/2000... Training loss: 0.4232\n",
      "Epoch: 1064/2000... Training loss: 0.5706\n",
      "Epoch: 1064/2000... Training loss: 0.2656\n",
      "Epoch: 1064/2000... Training loss: 0.5040\n",
      "Epoch: 1064/2000... Training loss: 0.4710\n",
      "Epoch: 1064/2000... Training loss: 0.5766\n",
      "Epoch: 1064/2000... Training loss: 0.3898\n",
      "Epoch: 1064/2000... Training loss: 0.4225\n",
      "Epoch: 1064/2000... Training loss: 0.6076\n",
      "Epoch: 1064/2000... Training loss: 0.4942\n",
      "Epoch: 1064/2000... Training loss: 0.4744\n",
      "Epoch: 1064/2000... Training loss: 0.4910\n",
      "Epoch: 1064/2000... Training loss: 0.6041\n",
      "Epoch: 1064/2000... Training loss: 0.4313\n",
      "Epoch: 1064/2000... Training loss: 0.3133\n",
      "Epoch: 1064/2000... Training loss: 0.6300\n",
      "Epoch: 1064/2000... Training loss: 0.4751\n",
      "Epoch: 1064/2000... Training loss: 0.5855\n",
      "Epoch: 1064/2000... Training loss: 0.5362\n",
      "Epoch: 1064/2000... Training loss: 0.5688\n",
      "Epoch: 1064/2000... Training loss: 0.5766\n",
      "Epoch: 1064/2000... Training loss: 0.6082\n",
      "Epoch: 1064/2000... Training loss: 0.4435\n",
      "Epoch: 1064/2000... Training loss: 0.5816\n",
      "Epoch: 1064/2000... Training loss: 0.4280\n",
      "Epoch: 1064/2000... Training loss: 0.3578\n",
      "Epoch: 1064/2000... Training loss: 0.6042\n",
      "Epoch: 1065/2000... Training loss: 0.4870\n",
      "Epoch: 1065/2000... Training loss: 0.4681\n",
      "Epoch: 1065/2000... Training loss: 0.3797\n",
      "Epoch: 1065/2000... Training loss: 0.4346\n",
      "Epoch: 1065/2000... Training loss: 0.4635\n",
      "Epoch: 1065/2000... Training loss: 0.4760\n",
      "Epoch: 1065/2000... Training loss: 0.4819\n",
      "Epoch: 1065/2000... Training loss: 0.4308\n",
      "Epoch: 1065/2000... Training loss: 0.5516\n",
      "Epoch: 1065/2000... Training loss: 0.6388\n",
      "Epoch: 1065/2000... Training loss: 0.4569\n",
      "Epoch: 1065/2000... Training loss: 0.4573\n",
      "Epoch: 1065/2000... Training loss: 0.4124\n",
      "Epoch: 1065/2000... Training loss: 0.6135\n",
      "Epoch: 1065/2000... Training loss: 0.6763\n",
      "Epoch: 1065/2000... Training loss: 0.4550\n",
      "Epoch: 1065/2000... Training loss: 0.5381\n",
      "Epoch: 1065/2000... Training loss: 0.5675\n",
      "Epoch: 1065/2000... Training loss: 0.3398\n",
      "Epoch: 1065/2000... Training loss: 0.6795\n",
      "Epoch: 1065/2000... Training loss: 0.3930\n",
      "Epoch: 1065/2000... Training loss: 0.4391\n",
      "Epoch: 1065/2000... Training loss: 0.4679\n",
      "Epoch: 1065/2000... Training loss: 0.3326\n",
      "Epoch: 1065/2000... Training loss: 0.5077\n",
      "Epoch: 1065/2000... Training loss: 0.5022\n",
      "Epoch: 1065/2000... Training loss: 0.5587\n",
      "Epoch: 1065/2000... Training loss: 0.3729\n",
      "Epoch: 1065/2000... Training loss: 0.6076\n",
      "Epoch: 1065/2000... Training loss: 0.5954\n",
      "Epoch: 1065/2000... Training loss: 0.4044\n",
      "Epoch: 1066/2000... Training loss: 0.6755\n",
      "Epoch: 1066/2000... Training loss: 0.6737\n",
      "Epoch: 1066/2000... Training loss: 0.3386\n",
      "Epoch: 1066/2000... Training loss: 0.5407\n",
      "Epoch: 1066/2000... Training loss: 0.6014\n",
      "Epoch: 1066/2000... Training loss: 0.4146\n",
      "Epoch: 1066/2000... Training loss: 0.4850\n",
      "Epoch: 1066/2000... Training loss: 0.6204\n",
      "Epoch: 1066/2000... Training loss: 0.5760\n",
      "Epoch: 1066/2000... Training loss: 0.3544\n",
      "Epoch: 1066/2000... Training loss: 0.5831\n",
      "Epoch: 1066/2000... Training loss: 0.4134\n",
      "Epoch: 1066/2000... Training loss: 0.4369\n",
      "Epoch: 1066/2000... Training loss: 0.4252\n",
      "Epoch: 1066/2000... Training loss: 0.5743\n",
      "Epoch: 1066/2000... Training loss: 0.3483\n",
      "Epoch: 1066/2000... Training loss: 0.6466\n",
      "Epoch: 1066/2000... Training loss: 0.4646\n",
      "Epoch: 1066/2000... Training loss: 0.5439\n",
      "Epoch: 1066/2000... Training loss: 0.4182\n",
      "Epoch: 1066/2000... Training loss: 0.5586\n",
      "Epoch: 1066/2000... Training loss: 0.3263\n",
      "Epoch: 1066/2000... Training loss: 0.3550\n",
      "Epoch: 1066/2000... Training loss: 0.5570\n",
      "Epoch: 1066/2000... Training loss: 0.5606\n",
      "Epoch: 1066/2000... Training loss: 0.4212\n",
      "Epoch: 1066/2000... Training loss: 0.6442\n",
      "Epoch: 1066/2000... Training loss: 0.5857\n",
      "Epoch: 1066/2000... Training loss: 0.4202\n",
      "Epoch: 1066/2000... Training loss: 0.5568\n",
      "Epoch: 1066/2000... Training loss: 0.5718\n",
      "Epoch: 1067/2000... Training loss: 0.4591\n",
      "Epoch: 1067/2000... Training loss: 0.4781\n",
      "Epoch: 1067/2000... Training loss: 0.3478\n",
      "Epoch: 1067/2000... Training loss: 0.5633\n",
      "Epoch: 1067/2000... Training loss: 0.5179\n",
      "Epoch: 1067/2000... Training loss: 0.6604\n",
      "Epoch: 1067/2000... Training loss: 0.4680\n",
      "Epoch: 1067/2000... Training loss: 0.4319\n",
      "Epoch: 1067/2000... Training loss: 0.5728\n",
      "Epoch: 1067/2000... Training loss: 0.4958\n",
      "Epoch: 1067/2000... Training loss: 0.4958\n",
      "Epoch: 1067/2000... Training loss: 0.4885\n",
      "Epoch: 1067/2000... Training loss: 0.3803\n",
      "Epoch: 1067/2000... Training loss: 0.5553\n",
      "Epoch: 1067/2000... Training loss: 0.3337\n",
      "Epoch: 1067/2000... Training loss: 0.4360\n",
      "Epoch: 1067/2000... Training loss: 0.6661\n",
      "Epoch: 1067/2000... Training loss: 0.4214\n",
      "Epoch: 1067/2000... Training loss: 0.4961\n",
      "Epoch: 1067/2000... Training loss: 0.5484\n",
      "Epoch: 1067/2000... Training loss: 0.5094\n",
      "Epoch: 1067/2000... Training loss: 0.5201\n",
      "Epoch: 1067/2000... Training loss: 0.7477\n",
      "Epoch: 1067/2000... Training loss: 0.5275\n",
      "Epoch: 1067/2000... Training loss: 0.4879\n",
      "Epoch: 1067/2000... Training loss: 0.5445\n",
      "Epoch: 1067/2000... Training loss: 0.4483\n",
      "Epoch: 1067/2000... Training loss: 0.4140\n",
      "Epoch: 1067/2000... Training loss: 0.3771\n",
      "Epoch: 1067/2000... Training loss: 0.5290\n",
      "Epoch: 1067/2000... Training loss: 0.3901\n",
      "Epoch: 1068/2000... Training loss: 0.4308\n",
      "Epoch: 1068/2000... Training loss: 0.3268\n",
      "Epoch: 1068/2000... Training loss: 0.4948\n",
      "Epoch: 1068/2000... Training loss: 0.4335\n",
      "Epoch: 1068/2000... Training loss: 0.5025\n",
      "Epoch: 1068/2000... Training loss: 0.5144\n",
      "Epoch: 1068/2000... Training loss: 0.5447\n",
      "Epoch: 1068/2000... Training loss: 0.4855\n",
      "Epoch: 1068/2000... Training loss: 0.6268\n",
      "Epoch: 1068/2000... Training loss: 0.5296\n",
      "Epoch: 1068/2000... Training loss: 0.5805\n",
      "Epoch: 1068/2000... Training loss: 0.4172\n",
      "Epoch: 1068/2000... Training loss: 0.4913\n",
      "Epoch: 1068/2000... Training loss: 0.4266\n",
      "Epoch: 1068/2000... Training loss: 0.3958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1068/2000... Training loss: 0.5565\n",
      "Epoch: 1068/2000... Training loss: 0.3659\n",
      "Epoch: 1068/2000... Training loss: 0.5342\n",
      "Epoch: 1068/2000... Training loss: 0.4005\n",
      "Epoch: 1068/2000... Training loss: 0.5334\n",
      "Epoch: 1068/2000... Training loss: 0.3589\n",
      "Epoch: 1068/2000... Training loss: 0.6384\n",
      "Epoch: 1068/2000... Training loss: 0.4754\n",
      "Epoch: 1068/2000... Training loss: 0.6816\n",
      "Epoch: 1068/2000... Training loss: 0.5549\n",
      "Epoch: 1068/2000... Training loss: 0.5405\n",
      "Epoch: 1068/2000... Training loss: 0.3489\n",
      "Epoch: 1068/2000... Training loss: 0.6926\n",
      "Epoch: 1068/2000... Training loss: 0.4222\n",
      "Epoch: 1068/2000... Training loss: 0.4858\n",
      "Epoch: 1068/2000... Training loss: 0.4983\n",
      "Epoch: 1069/2000... Training loss: 0.5889\n",
      "Epoch: 1069/2000... Training loss: 0.4150\n",
      "Epoch: 1069/2000... Training loss: 0.4855\n",
      "Epoch: 1069/2000... Training loss: 0.5268\n",
      "Epoch: 1069/2000... Training loss: 0.3256\n",
      "Epoch: 1069/2000... Training loss: 0.3696\n",
      "Epoch: 1069/2000... Training loss: 0.4829\n",
      "Epoch: 1069/2000... Training loss: 0.3987\n",
      "Epoch: 1069/2000... Training loss: 0.4795\n",
      "Epoch: 1069/2000... Training loss: 0.3704\n",
      "Epoch: 1069/2000... Training loss: 0.4732\n",
      "Epoch: 1069/2000... Training loss: 0.7377\n",
      "Epoch: 1069/2000... Training loss: 0.5888\n",
      "Epoch: 1069/2000... Training loss: 0.6359\n",
      "Epoch: 1069/2000... Training loss: 0.5902\n",
      "Epoch: 1069/2000... Training loss: 0.5724\n",
      "Epoch: 1069/2000... Training loss: 0.4561\n",
      "Epoch: 1069/2000... Training loss: 0.3998\n",
      "Epoch: 1069/2000... Training loss: 0.6434\n",
      "Epoch: 1069/2000... Training loss: 0.4860\n",
      "Epoch: 1069/2000... Training loss: 0.4662\n",
      "Epoch: 1069/2000... Training loss: 0.6594\n",
      "Epoch: 1069/2000... Training loss: 0.4535\n",
      "Epoch: 1069/2000... Training loss: 0.4103\n",
      "Epoch: 1069/2000... Training loss: 0.4512\n",
      "Epoch: 1069/2000... Training loss: 0.5292\n",
      "Epoch: 1069/2000... Training loss: 0.4501\n",
      "Epoch: 1069/2000... Training loss: 0.3864\n",
      "Epoch: 1069/2000... Training loss: 0.3729\n",
      "Epoch: 1069/2000... Training loss: 0.4478\n",
      "Epoch: 1069/2000... Training loss: 0.6115\n",
      "Epoch: 1070/2000... Training loss: 0.5913\n",
      "Epoch: 1070/2000... Training loss: 0.5634\n",
      "Epoch: 1070/2000... Training loss: 0.3677\n",
      "Epoch: 1070/2000... Training loss: 0.5235\n",
      "Epoch: 1070/2000... Training loss: 0.5627\n",
      "Epoch: 1070/2000... Training loss: 0.4987\n",
      "Epoch: 1070/2000... Training loss: 0.2513\n",
      "Epoch: 1070/2000... Training loss: 0.5072\n",
      "Epoch: 1070/2000... Training loss: 0.4102\n",
      "Epoch: 1070/2000... Training loss: 0.4829\n",
      "Epoch: 1070/2000... Training loss: 0.4107\n",
      "Epoch: 1070/2000... Training loss: 0.2758\n",
      "Epoch: 1070/2000... Training loss: 0.5536\n",
      "Epoch: 1070/2000... Training loss: 0.3908\n",
      "Epoch: 1070/2000... Training loss: 0.2901\n",
      "Epoch: 1070/2000... Training loss: 0.3973\n",
      "Epoch: 1070/2000... Training loss: 0.4119\n",
      "Epoch: 1070/2000... Training loss: 0.3740\n",
      "Epoch: 1070/2000... Training loss: 0.8083\n",
      "Epoch: 1070/2000... Training loss: 0.2974\n",
      "Epoch: 1070/2000... Training loss: 0.4881\n",
      "Epoch: 1070/2000... Training loss: 0.3935\n",
      "Epoch: 1070/2000... Training loss: 0.4579\n",
      "Epoch: 1070/2000... Training loss: 0.5043\n",
      "Epoch: 1070/2000... Training loss: 0.4368\n",
      "Epoch: 1070/2000... Training loss: 0.5057\n",
      "Epoch: 1070/2000... Training loss: 0.6480\n",
      "Epoch: 1070/2000... Training loss: 0.4486\n",
      "Epoch: 1070/2000... Training loss: 0.4421\n",
      "Epoch: 1070/2000... Training loss: 0.4094\n",
      "Epoch: 1070/2000... Training loss: 0.5015\n",
      "Epoch: 1071/2000... Training loss: 0.5583\n",
      "Epoch: 1071/2000... Training loss: 0.4874\n",
      "Epoch: 1071/2000... Training loss: 0.5508\n",
      "Epoch: 1071/2000... Training loss: 0.4498\n",
      "Epoch: 1071/2000... Training loss: 0.6326\n",
      "Epoch: 1071/2000... Training loss: 0.4498\n",
      "Epoch: 1071/2000... Training loss: 0.5820\n",
      "Epoch: 1071/2000... Training loss: 0.3881\n",
      "Epoch: 1071/2000... Training loss: 0.5040\n",
      "Epoch: 1071/2000... Training loss: 0.5762\n",
      "Epoch: 1071/2000... Training loss: 0.3641\n",
      "Epoch: 1071/2000... Training loss: 0.5506\n",
      "Epoch: 1071/2000... Training loss: 0.5336\n",
      "Epoch: 1071/2000... Training loss: 0.6212\n",
      "Epoch: 1071/2000... Training loss: 0.5889\n",
      "Epoch: 1071/2000... Training loss: 0.3421\n",
      "Epoch: 1071/2000... Training loss: 0.6859\n",
      "Epoch: 1071/2000... Training loss: 0.4123\n",
      "Epoch: 1071/2000... Training loss: 0.5157\n",
      "Epoch: 1071/2000... Training loss: 0.6145\n",
      "Epoch: 1071/2000... Training loss: 0.5135\n",
      "Epoch: 1071/2000... Training loss: 0.4463\n",
      "Epoch: 1071/2000... Training loss: 0.3744\n",
      "Epoch: 1071/2000... Training loss: 0.6005\n",
      "Epoch: 1071/2000... Training loss: 0.4619\n",
      "Epoch: 1071/2000... Training loss: 0.5533\n",
      "Epoch: 1071/2000... Training loss: 0.6564\n",
      "Epoch: 1071/2000... Training loss: 0.6433\n",
      "Epoch: 1071/2000... Training loss: 0.5815\n",
      "Epoch: 1071/2000... Training loss: 0.3811\n",
      "Epoch: 1071/2000... Training loss: 0.4585\n",
      "Epoch: 1072/2000... Training loss: 0.5350\n",
      "Epoch: 1072/2000... Training loss: 0.4142\n",
      "Epoch: 1072/2000... Training loss: 0.6667\n",
      "Epoch: 1072/2000... Training loss: 0.5035\n",
      "Epoch: 1072/2000... Training loss: 0.5299\n",
      "Epoch: 1072/2000... Training loss: 0.5847\n",
      "Epoch: 1072/2000... Training loss: 0.3141\n",
      "Epoch: 1072/2000... Training loss: 0.4810\n",
      "Epoch: 1072/2000... Training loss: 0.4938\n",
      "Epoch: 1072/2000... Training loss: 0.6114\n",
      "Epoch: 1072/2000... Training loss: 0.3806\n",
      "Epoch: 1072/2000... Training loss: 0.7644\n",
      "Epoch: 1072/2000... Training loss: 0.4180\n",
      "Epoch: 1072/2000... Training loss: 0.4069\n",
      "Epoch: 1072/2000... Training loss: 0.5580\n",
      "Epoch: 1072/2000... Training loss: 0.4636\n",
      "Epoch: 1072/2000... Training loss: 0.3045\n",
      "Epoch: 1072/2000... Training loss: 0.4818\n",
      "Epoch: 1072/2000... Training loss: 0.5036\n",
      "Epoch: 1072/2000... Training loss: 0.7450\n",
      "Epoch: 1072/2000... Training loss: 0.3374\n",
      "Epoch: 1072/2000... Training loss: 0.6359\n",
      "Epoch: 1072/2000... Training loss: 0.3971\n",
      "Epoch: 1072/2000... Training loss: 0.4348\n",
      "Epoch: 1072/2000... Training loss: 0.6391\n",
      "Epoch: 1072/2000... Training loss: 0.5314\n",
      "Epoch: 1072/2000... Training loss: 0.5291\n",
      "Epoch: 1072/2000... Training loss: 0.4909\n",
      "Epoch: 1072/2000... Training loss: 0.7624\n",
      "Epoch: 1072/2000... Training loss: 0.3983\n",
      "Epoch: 1072/2000... Training loss: 0.5087\n",
      "Epoch: 1073/2000... Training loss: 0.4356\n",
      "Epoch: 1073/2000... Training loss: 0.4635\n",
      "Epoch: 1073/2000... Training loss: 0.4412\n",
      "Epoch: 1073/2000... Training loss: 0.5880\n",
      "Epoch: 1073/2000... Training loss: 0.4841\n",
      "Epoch: 1073/2000... Training loss: 0.4988\n",
      "Epoch: 1073/2000... Training loss: 0.5372\n",
      "Epoch: 1073/2000... Training loss: 0.5580\n",
      "Epoch: 1073/2000... Training loss: 0.4331\n",
      "Epoch: 1073/2000... Training loss: 0.5195\n",
      "Epoch: 1073/2000... Training loss: 0.5619\n",
      "Epoch: 1073/2000... Training loss: 0.3981\n",
      "Epoch: 1073/2000... Training loss: 0.3891\n",
      "Epoch: 1073/2000... Training loss: 0.5150\n",
      "Epoch: 1073/2000... Training loss: 0.5076\n",
      "Epoch: 1073/2000... Training loss: 0.5305\n",
      "Epoch: 1073/2000... Training loss: 0.4373\n",
      "Epoch: 1073/2000... Training loss: 0.5773\n",
      "Epoch: 1073/2000... Training loss: 0.3870\n",
      "Epoch: 1073/2000... Training loss: 0.5860\n",
      "Epoch: 1073/2000... Training loss: 0.7627\n",
      "Epoch: 1073/2000... Training loss: 0.6323\n",
      "Epoch: 1073/2000... Training loss: 0.5947\n",
      "Epoch: 1073/2000... Training loss: 0.4769\n",
      "Epoch: 1073/2000... Training loss: 0.5348\n",
      "Epoch: 1073/2000... Training loss: 0.2971\n",
      "Epoch: 1073/2000... Training loss: 0.4347\n",
      "Epoch: 1073/2000... Training loss: 0.5176\n",
      "Epoch: 1073/2000... Training loss: 0.5563\n",
      "Epoch: 1073/2000... Training loss: 0.3720\n",
      "Epoch: 1073/2000... Training loss: 0.3920\n",
      "Epoch: 1074/2000... Training loss: 0.5808\n",
      "Epoch: 1074/2000... Training loss: 0.5200\n",
      "Epoch: 1074/2000... Training loss: 0.3876\n",
      "Epoch: 1074/2000... Training loss: 0.5167\n",
      "Epoch: 1074/2000... Training loss: 0.3698\n",
      "Epoch: 1074/2000... Training loss: 0.5124\n",
      "Epoch: 1074/2000... Training loss: 0.4911\n",
      "Epoch: 1074/2000... Training loss: 0.5756\n",
      "Epoch: 1074/2000... Training loss: 0.8558\n",
      "Epoch: 1074/2000... Training loss: 0.5559\n",
      "Epoch: 1074/2000... Training loss: 0.4810\n",
      "Epoch: 1074/2000... Training loss: 0.3526\n",
      "Epoch: 1074/2000... Training loss: 0.4313\n",
      "Epoch: 1074/2000... Training loss: 0.4360\n",
      "Epoch: 1074/2000... Training loss: 0.3998\n",
      "Epoch: 1074/2000... Training loss: 0.3627\n",
      "Epoch: 1074/2000... Training loss: 0.4779\n",
      "Epoch: 1074/2000... Training loss: 0.4578\n",
      "Epoch: 1074/2000... Training loss: 0.5730\n",
      "Epoch: 1074/2000... Training loss: 0.5883\n",
      "Epoch: 1074/2000... Training loss: 0.4295\n",
      "Epoch: 1074/2000... Training loss: 0.4075\n",
      "Epoch: 1074/2000... Training loss: 0.4624\n",
      "Epoch: 1074/2000... Training loss: 0.4926\n",
      "Epoch: 1074/2000... Training loss: 0.7055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1074/2000... Training loss: 0.8159\n",
      "Epoch: 1074/2000... Training loss: 0.3394\n",
      "Epoch: 1074/2000... Training loss: 0.3935\n",
      "Epoch: 1074/2000... Training loss: 0.5420\n",
      "Epoch: 1074/2000... Training loss: 0.4463\n",
      "Epoch: 1074/2000... Training loss: 0.5373\n",
      "Epoch: 1075/2000... Training loss: 0.5446\n",
      "Epoch: 1075/2000... Training loss: 0.5544\n",
      "Epoch: 1075/2000... Training loss: 0.3516\n",
      "Epoch: 1075/2000... Training loss: 0.4964\n",
      "Epoch: 1075/2000... Training loss: 0.4563\n",
      "Epoch: 1075/2000... Training loss: 0.5031\n",
      "Epoch: 1075/2000... Training loss: 0.4499\n",
      "Epoch: 1075/2000... Training loss: 0.4079\n",
      "Epoch: 1075/2000... Training loss: 0.5594\n",
      "Epoch: 1075/2000... Training loss: 0.4750\n",
      "Epoch: 1075/2000... Training loss: 0.5592\n",
      "Epoch: 1075/2000... Training loss: 0.6306\n",
      "Epoch: 1075/2000... Training loss: 0.5806\n",
      "Epoch: 1075/2000... Training loss: 0.5879\n",
      "Epoch: 1075/2000... Training loss: 0.6027\n",
      "Epoch: 1075/2000... Training loss: 0.5580\n",
      "Epoch: 1075/2000... Training loss: 0.5628\n",
      "Epoch: 1075/2000... Training loss: 0.3493\n",
      "Epoch: 1075/2000... Training loss: 0.4269\n",
      "Epoch: 1075/2000... Training loss: 0.5488\n",
      "Epoch: 1075/2000... Training loss: 0.4856\n",
      "Epoch: 1075/2000... Training loss: 0.5743\n",
      "Epoch: 1075/2000... Training loss: 0.4791\n",
      "Epoch: 1075/2000... Training loss: 0.5096\n",
      "Epoch: 1075/2000... Training loss: 0.3304\n",
      "Epoch: 1075/2000... Training loss: 0.5725\n",
      "Epoch: 1075/2000... Training loss: 0.4521\n",
      "Epoch: 1075/2000... Training loss: 0.3916\n",
      "Epoch: 1075/2000... Training loss: 0.4562\n",
      "Epoch: 1075/2000... Training loss: 0.6640\n",
      "Epoch: 1075/2000... Training loss: 0.5634\n",
      "Epoch: 1076/2000... Training loss: 0.5110\n",
      "Epoch: 1076/2000... Training loss: 0.6014\n",
      "Epoch: 1076/2000... Training loss: 0.5346\n",
      "Epoch: 1076/2000... Training loss: 0.5881\n",
      "Epoch: 1076/2000... Training loss: 0.4224\n",
      "Epoch: 1076/2000... Training loss: 0.5409\n",
      "Epoch: 1076/2000... Training loss: 0.6355\n",
      "Epoch: 1076/2000... Training loss: 0.6307\n",
      "Epoch: 1076/2000... Training loss: 0.5818\n",
      "Epoch: 1076/2000... Training loss: 0.3873\n",
      "Epoch: 1076/2000... Training loss: 0.4102\n",
      "Epoch: 1076/2000... Training loss: 0.5066\n",
      "Epoch: 1076/2000... Training loss: 0.5635\n",
      "Epoch: 1076/2000... Training loss: 0.5641\n",
      "Epoch: 1076/2000... Training loss: 0.5213\n",
      "Epoch: 1076/2000... Training loss: 0.3852\n",
      "Epoch: 1076/2000... Training loss: 0.4538\n",
      "Epoch: 1076/2000... Training loss: 0.4207\n",
      "Epoch: 1076/2000... Training loss: 0.4581\n",
      "Epoch: 1076/2000... Training loss: 0.6939\n",
      "Epoch: 1076/2000... Training loss: 0.3745\n",
      "Epoch: 1076/2000... Training loss: 0.5182\n",
      "Epoch: 1076/2000... Training loss: 0.4639\n",
      "Epoch: 1076/2000... Training loss: 0.5253\n",
      "Epoch: 1076/2000... Training loss: 0.4805\n",
      "Epoch: 1076/2000... Training loss: 0.4501\n",
      "Epoch: 1076/2000... Training loss: 0.4811\n",
      "Epoch: 1076/2000... Training loss: 0.4535\n",
      "Epoch: 1076/2000... Training loss: 0.4091\n",
      "Epoch: 1076/2000... Training loss: 0.8441\n",
      "Epoch: 1076/2000... Training loss: 0.4947\n",
      "Epoch: 1077/2000... Training loss: 0.4744\n",
      "Epoch: 1077/2000... Training loss: 0.4022\n",
      "Epoch: 1077/2000... Training loss: 0.3245\n",
      "Epoch: 1077/2000... Training loss: 0.6041\n",
      "Epoch: 1077/2000... Training loss: 0.5908\n",
      "Epoch: 1077/2000... Training loss: 0.5207\n",
      "Epoch: 1077/2000... Training loss: 0.4723\n",
      "Epoch: 1077/2000... Training loss: 0.4166\n",
      "Epoch: 1077/2000... Training loss: 0.5968\n",
      "Epoch: 1077/2000... Training loss: 0.4416\n",
      "Epoch: 1077/2000... Training loss: 0.6635\n",
      "Epoch: 1077/2000... Training loss: 0.4930\n",
      "Epoch: 1077/2000... Training loss: 0.5809\n",
      "Epoch: 1077/2000... Training loss: 0.4769\n",
      "Epoch: 1077/2000... Training loss: 0.4594\n",
      "Epoch: 1077/2000... Training loss: 0.4716\n",
      "Epoch: 1077/2000... Training loss: 0.3951\n",
      "Epoch: 1077/2000... Training loss: 0.7058\n",
      "Epoch: 1077/2000... Training loss: 0.4328\n",
      "Epoch: 1077/2000... Training loss: 0.3911\n",
      "Epoch: 1077/2000... Training loss: 0.4759\n",
      "Epoch: 1077/2000... Training loss: 0.4040\n",
      "Epoch: 1077/2000... Training loss: 0.7536\n",
      "Epoch: 1077/2000... Training loss: 0.5290\n",
      "Epoch: 1077/2000... Training loss: 0.5669\n",
      "Epoch: 1077/2000... Training loss: 0.5338\n",
      "Epoch: 1077/2000... Training loss: 0.5453\n",
      "Epoch: 1077/2000... Training loss: 0.5060\n",
      "Epoch: 1077/2000... Training loss: 0.4689\n",
      "Epoch: 1077/2000... Training loss: 0.3810\n",
      "Epoch: 1077/2000... Training loss: 0.5144\n",
      "Epoch: 1078/2000... Training loss: 0.5646\n",
      "Epoch: 1078/2000... Training loss: 0.5358\n",
      "Epoch: 1078/2000... Training loss: 0.3178\n",
      "Epoch: 1078/2000... Training loss: 0.2427\n",
      "Epoch: 1078/2000... Training loss: 0.4509\n",
      "Epoch: 1078/2000... Training loss: 0.6601\n",
      "Epoch: 1078/2000... Training loss: 0.4754\n",
      "Epoch: 1078/2000... Training loss: 0.5552\n",
      "Epoch: 1078/2000... Training loss: 0.4327\n",
      "Epoch: 1078/2000... Training loss: 0.4131\n",
      "Epoch: 1078/2000... Training loss: 0.5415\n",
      "Epoch: 1078/2000... Training loss: 0.2961\n",
      "Epoch: 1078/2000... Training loss: 0.6017\n",
      "Epoch: 1078/2000... Training loss: 0.5904\n",
      "Epoch: 1078/2000... Training loss: 0.5069\n",
      "Epoch: 1078/2000... Training loss: 0.3987\n",
      "Epoch: 1078/2000... Training loss: 0.5349\n",
      "Epoch: 1078/2000... Training loss: 0.6586\n",
      "Epoch: 1078/2000... Training loss: 0.5109\n",
      "Epoch: 1078/2000... Training loss: 0.6488\n",
      "Epoch: 1078/2000... Training loss: 0.3419\n",
      "Epoch: 1078/2000... Training loss: 0.5991\n",
      "Epoch: 1078/2000... Training loss: 0.5624\n",
      "Epoch: 1078/2000... Training loss: 0.5108\n",
      "Epoch: 1078/2000... Training loss: 0.4501\n",
      "Epoch: 1078/2000... Training loss: 0.6022\n",
      "Epoch: 1078/2000... Training loss: 0.4483\n",
      "Epoch: 1078/2000... Training loss: 0.5680\n",
      "Epoch: 1078/2000... Training loss: 0.4762\n",
      "Epoch: 1078/2000... Training loss: 0.4887\n",
      "Epoch: 1078/2000... Training loss: 0.3834\n",
      "Epoch: 1079/2000... Training loss: 0.8486\n",
      "Epoch: 1079/2000... Training loss: 0.5252\n",
      "Epoch: 1079/2000... Training loss: 0.7043\n",
      "Epoch: 1079/2000... Training loss: 0.5458\n",
      "Epoch: 1079/2000... Training loss: 0.3879\n",
      "Epoch: 1079/2000... Training loss: 0.4191\n",
      "Epoch: 1079/2000... Training loss: 0.3449\n",
      "Epoch: 1079/2000... Training loss: 0.4766\n",
      "Epoch: 1079/2000... Training loss: 0.6464\n",
      "Epoch: 1079/2000... Training loss: 0.4775\n",
      "Epoch: 1079/2000... Training loss: 0.6770\n",
      "Epoch: 1079/2000... Training loss: 0.6375\n",
      "Epoch: 1079/2000... Training loss: 0.3409\n",
      "Epoch: 1079/2000... Training loss: 0.4268\n",
      "Epoch: 1079/2000... Training loss: 0.6358\n",
      "Epoch: 1079/2000... Training loss: 0.3455\n",
      "Epoch: 1079/2000... Training loss: 0.4828\n",
      "Epoch: 1079/2000... Training loss: 0.5185\n",
      "Epoch: 1079/2000... Training loss: 0.6823\n",
      "Epoch: 1079/2000... Training loss: 0.4819\n",
      "Epoch: 1079/2000... Training loss: 0.4175\n",
      "Epoch: 1079/2000... Training loss: 0.5951\n",
      "Epoch: 1079/2000... Training loss: 0.4910\n",
      "Epoch: 1079/2000... Training loss: 0.4697\n",
      "Epoch: 1079/2000... Training loss: 0.5260\n",
      "Epoch: 1079/2000... Training loss: 0.6010\n",
      "Epoch: 1079/2000... Training loss: 0.5313\n",
      "Epoch: 1079/2000... Training loss: 0.6504\n",
      "Epoch: 1079/2000... Training loss: 0.3964\n",
      "Epoch: 1079/2000... Training loss: 0.4331\n",
      "Epoch: 1079/2000... Training loss: 0.4997\n",
      "Epoch: 1080/2000... Training loss: 0.4675\n",
      "Epoch: 1080/2000... Training loss: 0.5428\n",
      "Epoch: 1080/2000... Training loss: 0.4928\n",
      "Epoch: 1080/2000... Training loss: 0.7436\n",
      "Epoch: 1080/2000... Training loss: 0.4091\n",
      "Epoch: 1080/2000... Training loss: 0.5740\n",
      "Epoch: 1080/2000... Training loss: 0.3852\n",
      "Epoch: 1080/2000... Training loss: 0.3470\n",
      "Epoch: 1080/2000... Training loss: 0.5233\n",
      "Epoch: 1080/2000... Training loss: 0.4670\n",
      "Epoch: 1080/2000... Training loss: 0.5317\n",
      "Epoch: 1080/2000... Training loss: 0.4909\n",
      "Epoch: 1080/2000... Training loss: 0.3964\n",
      "Epoch: 1080/2000... Training loss: 0.6133\n",
      "Epoch: 1080/2000... Training loss: 0.5435\n",
      "Epoch: 1080/2000... Training loss: 0.3900\n",
      "Epoch: 1080/2000... Training loss: 0.5395\n",
      "Epoch: 1080/2000... Training loss: 0.4347\n",
      "Epoch: 1080/2000... Training loss: 0.4496\n",
      "Epoch: 1080/2000... Training loss: 0.3523\n",
      "Epoch: 1080/2000... Training loss: 0.4481\n",
      "Epoch: 1080/2000... Training loss: 0.5273\n",
      "Epoch: 1080/2000... Training loss: 0.3395\n",
      "Epoch: 1080/2000... Training loss: 0.6790\n",
      "Epoch: 1080/2000... Training loss: 0.5377\n",
      "Epoch: 1080/2000... Training loss: 0.5927\n",
      "Epoch: 1080/2000... Training loss: 0.4145\n",
      "Epoch: 1080/2000... Training loss: 0.5476\n",
      "Epoch: 1080/2000... Training loss: 0.5326\n",
      "Epoch: 1080/2000... Training loss: 0.3428\n",
      "Epoch: 1080/2000... Training loss: 0.4926\n",
      "Epoch: 1081/2000... Training loss: 0.4400\n",
      "Epoch: 1081/2000... Training loss: 0.4157\n",
      "Epoch: 1081/2000... Training loss: 0.4384\n",
      "Epoch: 1081/2000... Training loss: 0.5428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1081/2000... Training loss: 0.4216\n",
      "Epoch: 1081/2000... Training loss: 0.6399\n",
      "Epoch: 1081/2000... Training loss: 0.5288\n",
      "Epoch: 1081/2000... Training loss: 0.4181\n",
      "Epoch: 1081/2000... Training loss: 0.6585\n",
      "Epoch: 1081/2000... Training loss: 0.5622\n",
      "Epoch: 1081/2000... Training loss: 0.4728\n",
      "Epoch: 1081/2000... Training loss: 0.4057\n",
      "Epoch: 1081/2000... Training loss: 0.6447\n",
      "Epoch: 1081/2000... Training loss: 0.3379\n",
      "Epoch: 1081/2000... Training loss: 0.5570\n",
      "Epoch: 1081/2000... Training loss: 0.7665\n",
      "Epoch: 1081/2000... Training loss: 0.3899\n",
      "Epoch: 1081/2000... Training loss: 0.3763\n",
      "Epoch: 1081/2000... Training loss: 0.6077\n",
      "Epoch: 1081/2000... Training loss: 0.4799\n",
      "Epoch: 1081/2000... Training loss: 0.5272\n",
      "Epoch: 1081/2000... Training loss: 0.4122\n",
      "Epoch: 1081/2000... Training loss: 0.5049\n",
      "Epoch: 1081/2000... Training loss: 0.5578\n",
      "Epoch: 1081/2000... Training loss: 0.5643\n",
      "Epoch: 1081/2000... Training loss: 0.5594\n",
      "Epoch: 1081/2000... Training loss: 0.4763\n",
      "Epoch: 1081/2000... Training loss: 0.6709\n",
      "Epoch: 1081/2000... Training loss: 0.5047\n",
      "Epoch: 1081/2000... Training loss: 0.6242\n",
      "Epoch: 1081/2000... Training loss: 0.4384\n",
      "Epoch: 1082/2000... Training loss: 0.6380\n",
      "Epoch: 1082/2000... Training loss: 0.4906\n",
      "Epoch: 1082/2000... Training loss: 0.4636\n",
      "Epoch: 1082/2000... Training loss: 0.3315\n",
      "Epoch: 1082/2000... Training loss: 0.7057\n",
      "Epoch: 1082/2000... Training loss: 0.5675\n",
      "Epoch: 1082/2000... Training loss: 0.4599\n",
      "Epoch: 1082/2000... Training loss: 0.4576\n",
      "Epoch: 1082/2000... Training loss: 0.5198\n",
      "Epoch: 1082/2000... Training loss: 0.4243\n",
      "Epoch: 1082/2000... Training loss: 0.4908\n",
      "Epoch: 1082/2000... Training loss: 0.3136\n",
      "Epoch: 1082/2000... Training loss: 0.6282\n",
      "Epoch: 1082/2000... Training loss: 0.4965\n",
      "Epoch: 1082/2000... Training loss: 0.4304\n",
      "Epoch: 1082/2000... Training loss: 0.5023\n",
      "Epoch: 1082/2000... Training loss: 0.3807\n",
      "Epoch: 1082/2000... Training loss: 0.6173\n",
      "Epoch: 1082/2000... Training loss: 0.3863\n",
      "Epoch: 1082/2000... Training loss: 0.3710\n",
      "Epoch: 1082/2000... Training loss: 0.6641\n",
      "Epoch: 1082/2000... Training loss: 0.4348\n",
      "Epoch: 1082/2000... Training loss: 0.4953\n",
      "Epoch: 1082/2000... Training loss: 0.5114\n",
      "Epoch: 1082/2000... Training loss: 0.5537\n",
      "Epoch: 1082/2000... Training loss: 0.6119\n",
      "Epoch: 1082/2000... Training loss: 0.4870\n",
      "Epoch: 1082/2000... Training loss: 0.5452\n",
      "Epoch: 1082/2000... Training loss: 0.3752\n",
      "Epoch: 1082/2000... Training loss: 0.5202\n",
      "Epoch: 1082/2000... Training loss: 0.6276\n",
      "Epoch: 1083/2000... Training loss: 0.3886\n",
      "Epoch: 1083/2000... Training loss: 0.5747\n",
      "Epoch: 1083/2000... Training loss: 0.3041\n",
      "Epoch: 1083/2000... Training loss: 0.6344\n",
      "Epoch: 1083/2000... Training loss: 0.4750\n",
      "Epoch: 1083/2000... Training loss: 0.5149\n",
      "Epoch: 1083/2000... Training loss: 0.5576\n",
      "Epoch: 1083/2000... Training loss: 0.4801\n",
      "Epoch: 1083/2000... Training loss: 0.3291\n",
      "Epoch: 1083/2000... Training loss: 0.6038\n",
      "Epoch: 1083/2000... Training loss: 0.5852\n",
      "Epoch: 1083/2000... Training loss: 0.5142\n",
      "Epoch: 1083/2000... Training loss: 0.4893\n",
      "Epoch: 1083/2000... Training loss: 0.4250\n",
      "Epoch: 1083/2000... Training loss: 0.4271\n",
      "Epoch: 1083/2000... Training loss: 0.3972\n",
      "Epoch: 1083/2000... Training loss: 0.4828\n",
      "Epoch: 1083/2000... Training loss: 0.3764\n",
      "Epoch: 1083/2000... Training loss: 0.4079\n",
      "Epoch: 1083/2000... Training loss: 0.6121\n",
      "Epoch: 1083/2000... Training loss: 0.3940\n",
      "Epoch: 1083/2000... Training loss: 0.4834\n",
      "Epoch: 1083/2000... Training loss: 0.5145\n",
      "Epoch: 1083/2000... Training loss: 0.4305\n",
      "Epoch: 1083/2000... Training loss: 0.3857\n",
      "Epoch: 1083/2000... Training loss: 0.5826\n",
      "Epoch: 1083/2000... Training loss: 0.3304\n",
      "Epoch: 1083/2000... Training loss: 0.4167\n",
      "Epoch: 1083/2000... Training loss: 0.4112\n",
      "Epoch: 1083/2000... Training loss: 0.4811\n",
      "Epoch: 1083/2000... Training loss: 0.4045\n",
      "Epoch: 1084/2000... Training loss: 0.4523\n",
      "Epoch: 1084/2000... Training loss: 0.5126\n",
      "Epoch: 1084/2000... Training loss: 0.6778\n",
      "Epoch: 1084/2000... Training loss: 0.5536\n",
      "Epoch: 1084/2000... Training loss: 0.3251\n",
      "Epoch: 1084/2000... Training loss: 0.4201\n",
      "Epoch: 1084/2000... Training loss: 0.3978\n",
      "Epoch: 1084/2000... Training loss: 0.4113\n",
      "Epoch: 1084/2000... Training loss: 0.6670\n",
      "Epoch: 1084/2000... Training loss: 0.6027\n",
      "Epoch: 1084/2000... Training loss: 0.4451\n",
      "Epoch: 1084/2000... Training loss: 0.4845\n",
      "Epoch: 1084/2000... Training loss: 0.6422\n",
      "Epoch: 1084/2000... Training loss: 0.4661\n",
      "Epoch: 1084/2000... Training loss: 0.4470\n",
      "Epoch: 1084/2000... Training loss: 0.3760\n",
      "Epoch: 1084/2000... Training loss: 0.3183\n",
      "Epoch: 1084/2000... Training loss: 0.4017\n",
      "Epoch: 1084/2000... Training loss: 0.5603\n",
      "Epoch: 1084/2000... Training loss: 0.5122\n",
      "Epoch: 1084/2000... Training loss: 0.2918\n",
      "Epoch: 1084/2000... Training loss: 0.6925\n",
      "Epoch: 1084/2000... Training loss: 0.4901\n",
      "Epoch: 1084/2000... Training loss: 0.6750\n",
      "Epoch: 1084/2000... Training loss: 0.5226\n",
      "Epoch: 1084/2000... Training loss: 0.4695\n",
      "Epoch: 1084/2000... Training loss: 0.5333\n",
      "Epoch: 1084/2000... Training loss: 0.5737\n",
      "Epoch: 1084/2000... Training loss: 0.4389\n",
      "Epoch: 1084/2000... Training loss: 0.5177\n",
      "Epoch: 1084/2000... Training loss: 0.6529\n",
      "Epoch: 1085/2000... Training loss: 0.5617\n",
      "Epoch: 1085/2000... Training loss: 0.3789\n",
      "Epoch: 1085/2000... Training loss: 0.4146\n",
      "Epoch: 1085/2000... Training loss: 0.5802\n",
      "Epoch: 1085/2000... Training loss: 0.4373\n",
      "Epoch: 1085/2000... Training loss: 0.5275\n",
      "Epoch: 1085/2000... Training loss: 0.3790\n",
      "Epoch: 1085/2000... Training loss: 0.4387\n",
      "Epoch: 1085/2000... Training loss: 0.4165\n",
      "Epoch: 1085/2000... Training loss: 0.3997\n",
      "Epoch: 1085/2000... Training loss: 0.3979\n",
      "Epoch: 1085/2000... Training loss: 0.5382\n",
      "Epoch: 1085/2000... Training loss: 0.3686\n",
      "Epoch: 1085/2000... Training loss: 0.4906\n",
      "Epoch: 1085/2000... Training loss: 0.4677\n",
      "Epoch: 1085/2000... Training loss: 0.4925\n",
      "Epoch: 1085/2000... Training loss: 0.4392\n",
      "Epoch: 1085/2000... Training loss: 0.3965\n",
      "Epoch: 1085/2000... Training loss: 0.4060\n",
      "Epoch: 1085/2000... Training loss: 0.5940\n",
      "Epoch: 1085/2000... Training loss: 0.6028\n",
      "Epoch: 1085/2000... Training loss: 0.4077\n",
      "Epoch: 1085/2000... Training loss: 0.5561\n",
      "Epoch: 1085/2000... Training loss: 0.5127\n",
      "Epoch: 1085/2000... Training loss: 0.4259\n",
      "Epoch: 1085/2000... Training loss: 0.5675\n",
      "Epoch: 1085/2000... Training loss: 0.5554\n",
      "Epoch: 1085/2000... Training loss: 0.3752\n",
      "Epoch: 1085/2000... Training loss: 0.3952\n",
      "Epoch: 1085/2000... Training loss: 0.3308\n",
      "Epoch: 1085/2000... Training loss: 0.3509\n",
      "Epoch: 1086/2000... Training loss: 0.3968\n",
      "Epoch: 1086/2000... Training loss: 0.5488\n",
      "Epoch: 1086/2000... Training loss: 0.5089\n",
      "Epoch: 1086/2000... Training loss: 0.6371\n",
      "Epoch: 1086/2000... Training loss: 0.5899\n",
      "Epoch: 1086/2000... Training loss: 0.4654\n",
      "Epoch: 1086/2000... Training loss: 0.5694\n",
      "Epoch: 1086/2000... Training loss: 0.5713\n",
      "Epoch: 1086/2000... Training loss: 0.3691\n",
      "Epoch: 1086/2000... Training loss: 0.4353\n",
      "Epoch: 1086/2000... Training loss: 0.4898\n",
      "Epoch: 1086/2000... Training loss: 0.5098\n",
      "Epoch: 1086/2000... Training loss: 0.4645\n",
      "Epoch: 1086/2000... Training loss: 0.4138\n",
      "Epoch: 1086/2000... Training loss: 0.4163\n",
      "Epoch: 1086/2000... Training loss: 0.4166\n",
      "Epoch: 1086/2000... Training loss: 0.3740\n",
      "Epoch: 1086/2000... Training loss: 0.4871\n",
      "Epoch: 1086/2000... Training loss: 0.3485\n",
      "Epoch: 1086/2000... Training loss: 0.3428\n",
      "Epoch: 1086/2000... Training loss: 0.5552\n",
      "Epoch: 1086/2000... Training loss: 0.4714\n",
      "Epoch: 1086/2000... Training loss: 0.5077\n",
      "Epoch: 1086/2000... Training loss: 0.3092\n",
      "Epoch: 1086/2000... Training loss: 0.5789\n",
      "Epoch: 1086/2000... Training loss: 0.3087\n",
      "Epoch: 1086/2000... Training loss: 0.5149\n",
      "Epoch: 1086/2000... Training loss: 0.4221\n",
      "Epoch: 1086/2000... Training loss: 0.7184\n",
      "Epoch: 1086/2000... Training loss: 0.6380\n",
      "Epoch: 1086/2000... Training loss: 0.5133\n",
      "Epoch: 1087/2000... Training loss: 0.3949\n",
      "Epoch: 1087/2000... Training loss: 0.5355\n",
      "Epoch: 1087/2000... Training loss: 0.4353\n",
      "Epoch: 1087/2000... Training loss: 0.4064\n",
      "Epoch: 1087/2000... Training loss: 0.4602\n",
      "Epoch: 1087/2000... Training loss: 0.4916\n",
      "Epoch: 1087/2000... Training loss: 0.4849\n",
      "Epoch: 1087/2000... Training loss: 0.3123\n",
      "Epoch: 1087/2000... Training loss: 0.5213\n",
      "Epoch: 1087/2000... Training loss: 0.6113\n",
      "Epoch: 1087/2000... Training loss: 0.4985\n",
      "Epoch: 1087/2000... Training loss: 0.4483\n",
      "Epoch: 1087/2000... Training loss: 0.5084\n",
      "Epoch: 1087/2000... Training loss: 0.5135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1087/2000... Training loss: 0.3781\n",
      "Epoch: 1087/2000... Training loss: 0.5844\n",
      "Epoch: 1087/2000... Training loss: 0.5758\n",
      "Epoch: 1087/2000... Training loss: 0.4222\n",
      "Epoch: 1087/2000... Training loss: 0.4271\n",
      "Epoch: 1087/2000... Training loss: 0.5304\n",
      "Epoch: 1087/2000... Training loss: 0.4876\n",
      "Epoch: 1087/2000... Training loss: 0.5616\n",
      "Epoch: 1087/2000... Training loss: 0.4442\n",
      "Epoch: 1087/2000... Training loss: 0.5300\n",
      "Epoch: 1087/2000... Training loss: 0.5143\n",
      "Epoch: 1087/2000... Training loss: 0.5138\n",
      "Epoch: 1087/2000... Training loss: 0.5352\n",
      "Epoch: 1087/2000... Training loss: 0.6256\n",
      "Epoch: 1087/2000... Training loss: 0.4839\n",
      "Epoch: 1087/2000... Training loss: 0.5236\n",
      "Epoch: 1087/2000... Training loss: 0.5117\n",
      "Epoch: 1088/2000... Training loss: 0.3834\n",
      "Epoch: 1088/2000... Training loss: 0.5979\n",
      "Epoch: 1088/2000... Training loss: 0.3617\n",
      "Epoch: 1088/2000... Training loss: 0.4685\n",
      "Epoch: 1088/2000... Training loss: 0.5257\n",
      "Epoch: 1088/2000... Training loss: 0.4473\n",
      "Epoch: 1088/2000... Training loss: 0.6864\n",
      "Epoch: 1088/2000... Training loss: 0.4832\n",
      "Epoch: 1088/2000... Training loss: 0.3996\n",
      "Epoch: 1088/2000... Training loss: 0.5307\n",
      "Epoch: 1088/2000... Training loss: 0.5436\n",
      "Epoch: 1088/2000... Training loss: 0.4224\n",
      "Epoch: 1088/2000... Training loss: 0.7021\n",
      "Epoch: 1088/2000... Training loss: 0.4388\n",
      "Epoch: 1088/2000... Training loss: 0.5410\n",
      "Epoch: 1088/2000... Training loss: 0.5303\n",
      "Epoch: 1088/2000... Training loss: 0.5647\n",
      "Epoch: 1088/2000... Training loss: 0.4955\n",
      "Epoch: 1088/2000... Training loss: 0.5470\n",
      "Epoch: 1088/2000... Training loss: 0.5180\n",
      "Epoch: 1088/2000... Training loss: 0.4454\n",
      "Epoch: 1088/2000... Training loss: 0.3603\n",
      "Epoch: 1088/2000... Training loss: 0.4313\n",
      "Epoch: 1088/2000... Training loss: 0.5515\n",
      "Epoch: 1088/2000... Training loss: 0.6059\n",
      "Epoch: 1088/2000... Training loss: 0.3500\n",
      "Epoch: 1088/2000... Training loss: 0.4646\n",
      "Epoch: 1088/2000... Training loss: 0.4434\n",
      "Epoch: 1088/2000... Training loss: 0.6377\n",
      "Epoch: 1088/2000... Training loss: 0.4043\n",
      "Epoch: 1088/2000... Training loss: 0.5586\n",
      "Epoch: 1089/2000... Training loss: 0.7400\n",
      "Epoch: 1089/2000... Training loss: 0.5371\n",
      "Epoch: 1089/2000... Training loss: 0.3773\n",
      "Epoch: 1089/2000... Training loss: 0.6272\n",
      "Epoch: 1089/2000... Training loss: 0.5905\n",
      "Epoch: 1089/2000... Training loss: 0.3608\n",
      "Epoch: 1089/2000... Training loss: 0.4209\n",
      "Epoch: 1089/2000... Training loss: 0.5094\n",
      "Epoch: 1089/2000... Training loss: 0.4833\n",
      "Epoch: 1089/2000... Training loss: 0.3407\n",
      "Epoch: 1089/2000... Training loss: 0.4876\n",
      "Epoch: 1089/2000... Training loss: 0.5656\n",
      "Epoch: 1089/2000... Training loss: 0.5893\n",
      "Epoch: 1089/2000... Training loss: 0.7245\n",
      "Epoch: 1089/2000... Training loss: 0.3718\n",
      "Epoch: 1089/2000... Training loss: 0.5359\n",
      "Epoch: 1089/2000... Training loss: 0.5124\n",
      "Epoch: 1089/2000... Training loss: 0.4161\n",
      "Epoch: 1089/2000... Training loss: 0.3975\n",
      "Epoch: 1089/2000... Training loss: 0.4252\n",
      "Epoch: 1089/2000... Training loss: 0.4354\n",
      "Epoch: 1089/2000... Training loss: 0.6097\n",
      "Epoch: 1089/2000... Training loss: 0.5031\n",
      "Epoch: 1089/2000... Training loss: 0.3671\n",
      "Epoch: 1089/2000... Training loss: 0.4417\n",
      "Epoch: 1089/2000... Training loss: 0.5841\n",
      "Epoch: 1089/2000... Training loss: 0.4467\n",
      "Epoch: 1089/2000... Training loss: 0.6013\n",
      "Epoch: 1089/2000... Training loss: 0.4200\n",
      "Epoch: 1089/2000... Training loss: 0.6690\n",
      "Epoch: 1089/2000... Training loss: 0.2903\n",
      "Epoch: 1090/2000... Training loss: 0.5481\n",
      "Epoch: 1090/2000... Training loss: 0.3515\n",
      "Epoch: 1090/2000... Training loss: 0.6391\n",
      "Epoch: 1090/2000... Training loss: 0.6091\n",
      "Epoch: 1090/2000... Training loss: 0.5280\n",
      "Epoch: 1090/2000... Training loss: 0.5598\n",
      "Epoch: 1090/2000... Training loss: 0.5128\n",
      "Epoch: 1090/2000... Training loss: 0.5676\n",
      "Epoch: 1090/2000... Training loss: 0.4860\n",
      "Epoch: 1090/2000... Training loss: 0.5828\n",
      "Epoch: 1090/2000... Training loss: 0.5349\n",
      "Epoch: 1090/2000... Training loss: 0.5536\n",
      "Epoch: 1090/2000... Training loss: 0.5989\n",
      "Epoch: 1090/2000... Training loss: 0.3820\n",
      "Epoch: 1090/2000... Training loss: 0.6353\n",
      "Epoch: 1090/2000... Training loss: 0.3585\n",
      "Epoch: 1090/2000... Training loss: 0.5945\n",
      "Epoch: 1090/2000... Training loss: 0.5867\n",
      "Epoch: 1090/2000... Training loss: 0.4629\n",
      "Epoch: 1090/2000... Training loss: 0.3047\n",
      "Epoch: 1090/2000... Training loss: 0.4761\n",
      "Epoch: 1090/2000... Training loss: 0.5532\n",
      "Epoch: 1090/2000... Training loss: 0.5260\n",
      "Epoch: 1090/2000... Training loss: 0.4816\n",
      "Epoch: 1090/2000... Training loss: 0.5147\n",
      "Epoch: 1090/2000... Training loss: 0.6152\n",
      "Epoch: 1090/2000... Training loss: 0.4431\n",
      "Epoch: 1090/2000... Training loss: 0.6023\n",
      "Epoch: 1090/2000... Training loss: 0.5565\n",
      "Epoch: 1090/2000... Training loss: 0.4276\n",
      "Epoch: 1090/2000... Training loss: 0.4202\n",
      "Epoch: 1091/2000... Training loss: 0.5244\n",
      "Epoch: 1091/2000... Training loss: 0.3071\n",
      "Epoch: 1091/2000... Training loss: 0.5335\n",
      "Epoch: 1091/2000... Training loss: 0.4088\n",
      "Epoch: 1091/2000... Training loss: 0.5879\n",
      "Epoch: 1091/2000... Training loss: 0.5293\n",
      "Epoch: 1091/2000... Training loss: 0.4948\n",
      "Epoch: 1091/2000... Training loss: 0.5661\n",
      "Epoch: 1091/2000... Training loss: 0.4072\n",
      "Epoch: 1091/2000... Training loss: 0.3179\n",
      "Epoch: 1091/2000... Training loss: 0.5466\n",
      "Epoch: 1091/2000... Training loss: 0.6272\n",
      "Epoch: 1091/2000... Training loss: 0.5514\n",
      "Epoch: 1091/2000... Training loss: 0.6088\n",
      "Epoch: 1091/2000... Training loss: 0.4784\n",
      "Epoch: 1091/2000... Training loss: 0.4674\n",
      "Epoch: 1091/2000... Training loss: 0.5835\n",
      "Epoch: 1091/2000... Training loss: 0.5691\n",
      "Epoch: 1091/2000... Training loss: 0.4920\n",
      "Epoch: 1091/2000... Training loss: 0.4508\n",
      "Epoch: 1091/2000... Training loss: 0.3947\n",
      "Epoch: 1091/2000... Training loss: 0.4599\n",
      "Epoch: 1091/2000... Training loss: 0.3827\n",
      "Epoch: 1091/2000... Training loss: 0.6349\n",
      "Epoch: 1091/2000... Training loss: 0.5844\n",
      "Epoch: 1091/2000... Training loss: 0.3839\n",
      "Epoch: 1091/2000... Training loss: 0.3372\n",
      "Epoch: 1091/2000... Training loss: 0.4195\n",
      "Epoch: 1091/2000... Training loss: 0.4216\n",
      "Epoch: 1091/2000... Training loss: 0.5314\n",
      "Epoch: 1091/2000... Training loss: 0.4291\n",
      "Epoch: 1092/2000... Training loss: 0.5972\n",
      "Epoch: 1092/2000... Training loss: 0.6929\n",
      "Epoch: 1092/2000... Training loss: 0.3670\n",
      "Epoch: 1092/2000... Training loss: 0.5198\n",
      "Epoch: 1092/2000... Training loss: 0.6111\n",
      "Epoch: 1092/2000... Training loss: 0.6052\n",
      "Epoch: 1092/2000... Training loss: 0.4996\n",
      "Epoch: 1092/2000... Training loss: 0.4014\n",
      "Epoch: 1092/2000... Training loss: 0.4590\n",
      "Epoch: 1092/2000... Training loss: 0.5112\n",
      "Epoch: 1092/2000... Training loss: 0.4359\n",
      "Epoch: 1092/2000... Training loss: 0.5308\n",
      "Epoch: 1092/2000... Training loss: 0.4513\n",
      "Epoch: 1092/2000... Training loss: 0.5643\n",
      "Epoch: 1092/2000... Training loss: 0.5724\n",
      "Epoch: 1092/2000... Training loss: 0.4152\n",
      "Epoch: 1092/2000... Training loss: 0.6160\n",
      "Epoch: 1092/2000... Training loss: 0.3421\n",
      "Epoch: 1092/2000... Training loss: 0.4603\n",
      "Epoch: 1092/2000... Training loss: 0.5286\n",
      "Epoch: 1092/2000... Training loss: 0.3372\n",
      "Epoch: 1092/2000... Training loss: 0.4408\n",
      "Epoch: 1092/2000... Training loss: 0.3510\n",
      "Epoch: 1092/2000... Training loss: 0.4236\n",
      "Epoch: 1092/2000... Training loss: 0.6068\n",
      "Epoch: 1092/2000... Training loss: 0.3816\n",
      "Epoch: 1092/2000... Training loss: 0.4493\n",
      "Epoch: 1092/2000... Training loss: 0.5891\n",
      "Epoch: 1092/2000... Training loss: 0.6353\n",
      "Epoch: 1092/2000... Training loss: 0.4307\n",
      "Epoch: 1092/2000... Training loss: 0.4886\n",
      "Epoch: 1093/2000... Training loss: 0.7129\n",
      "Epoch: 1093/2000... Training loss: 0.4663\n",
      "Epoch: 1093/2000... Training loss: 0.5637\n",
      "Epoch: 1093/2000... Training loss: 0.5866\n",
      "Epoch: 1093/2000... Training loss: 0.4418\n",
      "Epoch: 1093/2000... Training loss: 0.3631\n",
      "Epoch: 1093/2000... Training loss: 0.4688\n",
      "Epoch: 1093/2000... Training loss: 0.5215\n",
      "Epoch: 1093/2000... Training loss: 0.4664\n",
      "Epoch: 1093/2000... Training loss: 0.3127\n",
      "Epoch: 1093/2000... Training loss: 0.6401\n",
      "Epoch: 1093/2000... Training loss: 0.4673\n",
      "Epoch: 1093/2000... Training loss: 0.6107\n",
      "Epoch: 1093/2000... Training loss: 0.6087\n",
      "Epoch: 1093/2000... Training loss: 0.4731\n",
      "Epoch: 1093/2000... Training loss: 0.5317\n",
      "Epoch: 1093/2000... Training loss: 0.5355\n",
      "Epoch: 1093/2000... Training loss: 0.5297\n",
      "Epoch: 1093/2000... Training loss: 0.3694\n",
      "Epoch: 1093/2000... Training loss: 0.5045\n",
      "Epoch: 1093/2000... Training loss: 0.5056\n",
      "Epoch: 1093/2000... Training loss: 0.5746\n",
      "Epoch: 1093/2000... Training loss: 0.4524\n",
      "Epoch: 1093/2000... Training loss: 0.3703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1093/2000... Training loss: 0.3405\n",
      "Epoch: 1093/2000... Training loss: 0.5482\n",
      "Epoch: 1093/2000... Training loss: 0.4947\n",
      "Epoch: 1093/2000... Training loss: 0.5393\n",
      "Epoch: 1093/2000... Training loss: 0.4918\n",
      "Epoch: 1093/2000... Training loss: 0.5256\n",
      "Epoch: 1093/2000... Training loss: 0.5045\n",
      "Epoch: 1094/2000... Training loss: 0.6576\n",
      "Epoch: 1094/2000... Training loss: 0.5045\n",
      "Epoch: 1094/2000... Training loss: 0.4900\n",
      "Epoch: 1094/2000... Training loss: 0.4877\n",
      "Epoch: 1094/2000... Training loss: 0.4098\n",
      "Epoch: 1094/2000... Training loss: 0.4879\n",
      "Epoch: 1094/2000... Training loss: 0.4462\n",
      "Epoch: 1094/2000... Training loss: 0.4683\n",
      "Epoch: 1094/2000... Training loss: 0.6805\n",
      "Epoch: 1094/2000... Training loss: 0.5834\n",
      "Epoch: 1094/2000... Training loss: 0.3518\n",
      "Epoch: 1094/2000... Training loss: 0.6264\n",
      "Epoch: 1094/2000... Training loss: 0.5471\n",
      "Epoch: 1094/2000... Training loss: 0.4794\n",
      "Epoch: 1094/2000... Training loss: 0.5688\n",
      "Epoch: 1094/2000... Training loss: 0.4153\n",
      "Epoch: 1094/2000... Training loss: 0.5288\n",
      "Epoch: 1094/2000... Training loss: 0.3265\n",
      "Epoch: 1094/2000... Training loss: 0.4767\n",
      "Epoch: 1094/2000... Training loss: 0.5037\n",
      "Epoch: 1094/2000... Training loss: 0.4563\n",
      "Epoch: 1094/2000... Training loss: 0.4675\n",
      "Epoch: 1094/2000... Training loss: 0.6363\n",
      "Epoch: 1094/2000... Training loss: 0.4874\n",
      "Epoch: 1094/2000... Training loss: 0.6401\n",
      "Epoch: 1094/2000... Training loss: 0.4208\n",
      "Epoch: 1094/2000... Training loss: 0.6236\n",
      "Epoch: 1094/2000... Training loss: 0.4705\n",
      "Epoch: 1094/2000... Training loss: 0.6134\n",
      "Epoch: 1094/2000... Training loss: 0.3427\n",
      "Epoch: 1094/2000... Training loss: 0.4007\n",
      "Epoch: 1095/2000... Training loss: 0.5452\n",
      "Epoch: 1095/2000... Training loss: 0.5613\n",
      "Epoch: 1095/2000... Training loss: 0.4320\n",
      "Epoch: 1095/2000... Training loss: 0.5483\n",
      "Epoch: 1095/2000... Training loss: 0.5281\n",
      "Epoch: 1095/2000... Training loss: 0.5545\n",
      "Epoch: 1095/2000... Training loss: 0.5117\n",
      "Epoch: 1095/2000... Training loss: 0.4289\n",
      "Epoch: 1095/2000... Training loss: 0.3883\n",
      "Epoch: 1095/2000... Training loss: 0.5496\n",
      "Epoch: 1095/2000... Training loss: 0.3803\n",
      "Epoch: 1095/2000... Training loss: 0.5316\n",
      "Epoch: 1095/2000... Training loss: 0.6160\n",
      "Epoch: 1095/2000... Training loss: 0.4580\n",
      "Epoch: 1095/2000... Training loss: 0.3552\n",
      "Epoch: 1095/2000... Training loss: 0.4465\n",
      "Epoch: 1095/2000... Training loss: 0.5475\n",
      "Epoch: 1095/2000... Training loss: 0.5398\n",
      "Epoch: 1095/2000... Training loss: 0.3755\n",
      "Epoch: 1095/2000... Training loss: 0.5189\n",
      "Epoch: 1095/2000... Training loss: 0.5189\n",
      "Epoch: 1095/2000... Training loss: 0.6940\n",
      "Epoch: 1095/2000... Training loss: 0.4214\n",
      "Epoch: 1095/2000... Training loss: 0.5748\n",
      "Epoch: 1095/2000... Training loss: 0.5897\n",
      "Epoch: 1095/2000... Training loss: 0.5078\n",
      "Epoch: 1095/2000... Training loss: 0.5538\n",
      "Epoch: 1095/2000... Training loss: 0.4295\n",
      "Epoch: 1095/2000... Training loss: 0.3464\n",
      "Epoch: 1095/2000... Training loss: 0.3751\n",
      "Epoch: 1095/2000... Training loss: 0.4233\n",
      "Epoch: 1096/2000... Training loss: 0.5524\n",
      "Epoch: 1096/2000... Training loss: 0.7593\n",
      "Epoch: 1096/2000... Training loss: 0.5599\n",
      "Epoch: 1096/2000... Training loss: 0.4292\n",
      "Epoch: 1096/2000... Training loss: 0.3365\n",
      "Epoch: 1096/2000... Training loss: 0.4245\n",
      "Epoch: 1096/2000... Training loss: 0.4532\n",
      "Epoch: 1096/2000... Training loss: 0.4771\n",
      "Epoch: 1096/2000... Training loss: 0.6136\n",
      "Epoch: 1096/2000... Training loss: 0.5207\n",
      "Epoch: 1096/2000... Training loss: 0.4033\n",
      "Epoch: 1096/2000... Training loss: 0.6821\n",
      "Epoch: 1096/2000... Training loss: 0.4222\n",
      "Epoch: 1096/2000... Training loss: 0.3421\n",
      "Epoch: 1096/2000... Training loss: 0.4883\n",
      "Epoch: 1096/2000... Training loss: 0.5135\n",
      "Epoch: 1096/2000... Training loss: 0.5611\n",
      "Epoch: 1096/2000... Training loss: 0.4727\n",
      "Epoch: 1096/2000... Training loss: 0.4153\n",
      "Epoch: 1096/2000... Training loss: 0.3553\n",
      "Epoch: 1096/2000... Training loss: 0.5976\n",
      "Epoch: 1096/2000... Training loss: 0.4319\n",
      "Epoch: 1096/2000... Training loss: 0.4670\n",
      "Epoch: 1096/2000... Training loss: 0.4713\n",
      "Epoch: 1096/2000... Training loss: 0.4500\n",
      "Epoch: 1096/2000... Training loss: 0.5067\n",
      "Epoch: 1096/2000... Training loss: 0.5033\n",
      "Epoch: 1096/2000... Training loss: 0.5589\n",
      "Epoch: 1096/2000... Training loss: 0.3904\n",
      "Epoch: 1096/2000... Training loss: 0.3476\n",
      "Epoch: 1096/2000... Training loss: 0.4679\n",
      "Epoch: 1097/2000... Training loss: 0.3986\n",
      "Epoch: 1097/2000... Training loss: 0.6330\n",
      "Epoch: 1097/2000... Training loss: 0.4583\n",
      "Epoch: 1097/2000... Training loss: 0.3403\n",
      "Epoch: 1097/2000... Training loss: 0.3291\n",
      "Epoch: 1097/2000... Training loss: 0.4093\n",
      "Epoch: 1097/2000... Training loss: 0.6238\n",
      "Epoch: 1097/2000... Training loss: 0.4206\n",
      "Epoch: 1097/2000... Training loss: 0.4647\n",
      "Epoch: 1097/2000... Training loss: 0.4181\n",
      "Epoch: 1097/2000... Training loss: 0.5825\n",
      "Epoch: 1097/2000... Training loss: 0.3465\n",
      "Epoch: 1097/2000... Training loss: 0.5196\n",
      "Epoch: 1097/2000... Training loss: 0.4850\n",
      "Epoch: 1097/2000... Training loss: 0.6053\n",
      "Epoch: 1097/2000... Training loss: 0.3761\n",
      "Epoch: 1097/2000... Training loss: 0.3962\n",
      "Epoch: 1097/2000... Training loss: 0.3357\n",
      "Epoch: 1097/2000... Training loss: 0.3793\n",
      "Epoch: 1097/2000... Training loss: 0.4959\n",
      "Epoch: 1097/2000... Training loss: 0.4838\n",
      "Epoch: 1097/2000... Training loss: 0.4739\n",
      "Epoch: 1097/2000... Training loss: 0.5064\n",
      "Epoch: 1097/2000... Training loss: 0.6618\n",
      "Epoch: 1097/2000... Training loss: 0.4934\n",
      "Epoch: 1097/2000... Training loss: 0.6241\n",
      "Epoch: 1097/2000... Training loss: 0.4066\n",
      "Epoch: 1097/2000... Training loss: 0.4811\n",
      "Epoch: 1097/2000... Training loss: 0.4521\n",
      "Epoch: 1097/2000... Training loss: 0.3793\n",
      "Epoch: 1097/2000... Training loss: 0.6643\n",
      "Epoch: 1098/2000... Training loss: 0.5002\n",
      "Epoch: 1098/2000... Training loss: 0.4991\n",
      "Epoch: 1098/2000... Training loss: 0.3677\n",
      "Epoch: 1098/2000... Training loss: 0.3645\n",
      "Epoch: 1098/2000... Training loss: 0.4734\n",
      "Epoch: 1098/2000... Training loss: 0.5066\n",
      "Epoch: 1098/2000... Training loss: 0.3524\n",
      "Epoch: 1098/2000... Training loss: 0.4908\n",
      "Epoch: 1098/2000... Training loss: 0.3499\n",
      "Epoch: 1098/2000... Training loss: 0.4809\n",
      "Epoch: 1098/2000... Training loss: 0.4369\n",
      "Epoch: 1098/2000... Training loss: 0.4791\n",
      "Epoch: 1098/2000... Training loss: 0.4284\n",
      "Epoch: 1098/2000... Training loss: 0.5234\n",
      "Epoch: 1098/2000... Training loss: 0.6337\n",
      "Epoch: 1098/2000... Training loss: 0.7899\n",
      "Epoch: 1098/2000... Training loss: 0.5789\n",
      "Epoch: 1098/2000... Training loss: 0.6147\n",
      "Epoch: 1098/2000... Training loss: 0.6396\n",
      "Epoch: 1098/2000... Training loss: 0.3922\n",
      "Epoch: 1098/2000... Training loss: 0.3643\n",
      "Epoch: 1098/2000... Training loss: 0.6642\n",
      "Epoch: 1098/2000... Training loss: 0.3873\n",
      "Epoch: 1098/2000... Training loss: 0.4968\n",
      "Epoch: 1098/2000... Training loss: 0.4537\n",
      "Epoch: 1098/2000... Training loss: 0.6254\n",
      "Epoch: 1098/2000... Training loss: 0.3687\n",
      "Epoch: 1098/2000... Training loss: 0.4231\n",
      "Epoch: 1098/2000... Training loss: 0.5157\n",
      "Epoch: 1098/2000... Training loss: 0.4879\n",
      "Epoch: 1098/2000... Training loss: 0.5092\n",
      "Epoch: 1099/2000... Training loss: 0.4770\n",
      "Epoch: 1099/2000... Training loss: 0.5648\n",
      "Epoch: 1099/2000... Training loss: 0.4111\n",
      "Epoch: 1099/2000... Training loss: 0.3907\n",
      "Epoch: 1099/2000... Training loss: 0.4613\n",
      "Epoch: 1099/2000... Training loss: 0.5386\n",
      "Epoch: 1099/2000... Training loss: 0.6864\n",
      "Epoch: 1099/2000... Training loss: 0.4776\n",
      "Epoch: 1099/2000... Training loss: 0.5897\n",
      "Epoch: 1099/2000... Training loss: 0.3725\n",
      "Epoch: 1099/2000... Training loss: 0.6110\n",
      "Epoch: 1099/2000... Training loss: 0.4300\n",
      "Epoch: 1099/2000... Training loss: 0.2589\n",
      "Epoch: 1099/2000... Training loss: 0.4155\n",
      "Epoch: 1099/2000... Training loss: 0.3012\n",
      "Epoch: 1099/2000... Training loss: 0.4609\n",
      "Epoch: 1099/2000... Training loss: 0.5177\n",
      "Epoch: 1099/2000... Training loss: 0.3883\n",
      "Epoch: 1099/2000... Training loss: 0.5118\n",
      "Epoch: 1099/2000... Training loss: 0.4014\n",
      "Epoch: 1099/2000... Training loss: 0.4329\n",
      "Epoch: 1099/2000... Training loss: 0.4971\n",
      "Epoch: 1099/2000... Training loss: 0.5091\n",
      "Epoch: 1099/2000... Training loss: 0.5688\n",
      "Epoch: 1099/2000... Training loss: 0.4131\n",
      "Epoch: 1099/2000... Training loss: 0.4508\n",
      "Epoch: 1099/2000... Training loss: 0.4545\n",
      "Epoch: 1099/2000... Training loss: 0.5924\n",
      "Epoch: 1099/2000... Training loss: 0.5345\n",
      "Epoch: 1099/2000... Training loss: 0.3970\n",
      "Epoch: 1099/2000... Training loss: 0.5407\n",
      "Epoch: 1100/2000... Training loss: 0.5801\n",
      "Epoch: 1100/2000... Training loss: 0.4850\n",
      "Epoch: 1100/2000... Training loss: 0.6395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1100/2000... Training loss: 0.4902\n",
      "Epoch: 1100/2000... Training loss: 0.6276\n",
      "Epoch: 1100/2000... Training loss: 0.5581\n",
      "Epoch: 1100/2000... Training loss: 0.3857\n",
      "Epoch: 1100/2000... Training loss: 0.6660\n",
      "Epoch: 1100/2000... Training loss: 0.5142\n",
      "Epoch: 1100/2000... Training loss: 0.4566\n",
      "Epoch: 1100/2000... Training loss: 0.6507\n",
      "Epoch: 1100/2000... Training loss: 0.6469\n",
      "Epoch: 1100/2000... Training loss: 0.5726\n",
      "Epoch: 1100/2000... Training loss: 0.5765\n",
      "Epoch: 1100/2000... Training loss: 0.6360\n",
      "Epoch: 1100/2000... Training loss: 0.3619\n",
      "Epoch: 1100/2000... Training loss: 0.4852\n",
      "Epoch: 1100/2000... Training loss: 0.4889\n",
      "Epoch: 1100/2000... Training loss: 0.6456\n",
      "Epoch: 1100/2000... Training loss: 0.4535\n",
      "Epoch: 1100/2000... Training loss: 0.6011\n",
      "Epoch: 1100/2000... Training loss: 0.4314\n",
      "Epoch: 1100/2000... Training loss: 0.5317\n",
      "Epoch: 1100/2000... Training loss: 0.3923\n",
      "Epoch: 1100/2000... Training loss: 0.6739\n",
      "Epoch: 1100/2000... Training loss: 0.4022\n",
      "Epoch: 1100/2000... Training loss: 0.5414\n",
      "Epoch: 1100/2000... Training loss: 0.3987\n",
      "Epoch: 1100/2000... Training loss: 0.5277\n",
      "Epoch: 1100/2000... Training loss: 0.4451\n",
      "Epoch: 1100/2000... Training loss: 0.5233\n",
      "Epoch: 1101/2000... Training loss: 0.4188\n",
      "Epoch: 1101/2000... Training loss: 0.3608\n",
      "Epoch: 1101/2000... Training loss: 0.4211\n",
      "Epoch: 1101/2000... Training loss: 0.5266\n",
      "Epoch: 1101/2000... Training loss: 0.5696\n",
      "Epoch: 1101/2000... Training loss: 0.4504\n",
      "Epoch: 1101/2000... Training loss: 0.3296\n",
      "Epoch: 1101/2000... Training loss: 0.4670\n",
      "Epoch: 1101/2000... Training loss: 0.4158\n",
      "Epoch: 1101/2000... Training loss: 0.4678\n",
      "Epoch: 1101/2000... Training loss: 0.4127\n",
      "Epoch: 1101/2000... Training loss: 0.5112\n",
      "Epoch: 1101/2000... Training loss: 0.6005\n",
      "Epoch: 1101/2000... Training loss: 0.4606\n",
      "Epoch: 1101/2000... Training loss: 0.4179\n",
      "Epoch: 1101/2000... Training loss: 0.3759\n",
      "Epoch: 1101/2000... Training loss: 0.3410\n",
      "Epoch: 1101/2000... Training loss: 0.6133\n",
      "Epoch: 1101/2000... Training loss: 0.2455\n",
      "Epoch: 1101/2000... Training loss: 0.5628\n",
      "Epoch: 1101/2000... Training loss: 0.5611\n",
      "Epoch: 1101/2000... Training loss: 0.5894\n",
      "Epoch: 1101/2000... Training loss: 0.3703\n",
      "Epoch: 1101/2000... Training loss: 0.6622\n",
      "Epoch: 1101/2000... Training loss: 0.6403\n",
      "Epoch: 1101/2000... Training loss: 0.5522\n",
      "Epoch: 1101/2000... Training loss: 0.4164\n",
      "Epoch: 1101/2000... Training loss: 0.4281\n",
      "Epoch: 1101/2000... Training loss: 0.5244\n",
      "Epoch: 1101/2000... Training loss: 0.4598\n",
      "Epoch: 1101/2000... Training loss: 0.5438\n",
      "Epoch: 1102/2000... Training loss: 0.4791\n",
      "Epoch: 1102/2000... Training loss: 0.4050\n",
      "Epoch: 1102/2000... Training loss: 0.3379\n",
      "Epoch: 1102/2000... Training loss: 0.4046\n",
      "Epoch: 1102/2000... Training loss: 0.4426\n",
      "Epoch: 1102/2000... Training loss: 0.4412\n",
      "Epoch: 1102/2000... Training loss: 0.5171\n",
      "Epoch: 1102/2000... Training loss: 0.5092\n",
      "Epoch: 1102/2000... Training loss: 0.3993\n",
      "Epoch: 1102/2000... Training loss: 0.3954\n",
      "Epoch: 1102/2000... Training loss: 0.3661\n",
      "Epoch: 1102/2000... Training loss: 0.4674\n",
      "Epoch: 1102/2000... Training loss: 0.4540\n",
      "Epoch: 1102/2000... Training loss: 0.4162\n",
      "Epoch: 1102/2000... Training loss: 0.3067\n",
      "Epoch: 1102/2000... Training loss: 0.4694\n",
      "Epoch: 1102/2000... Training loss: 0.4641\n",
      "Epoch: 1102/2000... Training loss: 0.4734\n",
      "Epoch: 1102/2000... Training loss: 0.5864\n",
      "Epoch: 1102/2000... Training loss: 0.6128\n",
      "Epoch: 1102/2000... Training loss: 0.4761\n",
      "Epoch: 1102/2000... Training loss: 0.5289\n",
      "Epoch: 1102/2000... Training loss: 0.3877\n",
      "Epoch: 1102/2000... Training loss: 0.5806\n",
      "Epoch: 1102/2000... Training loss: 0.4679\n",
      "Epoch: 1102/2000... Training loss: 0.4322\n",
      "Epoch: 1102/2000... Training loss: 0.4490\n",
      "Epoch: 1102/2000... Training loss: 0.5245\n",
      "Epoch: 1102/2000... Training loss: 0.3523\n",
      "Epoch: 1102/2000... Training loss: 0.6204\n",
      "Epoch: 1102/2000... Training loss: 0.5241\n",
      "Epoch: 1103/2000... Training loss: 0.5980\n",
      "Epoch: 1103/2000... Training loss: 0.6249\n",
      "Epoch: 1103/2000... Training loss: 0.3685\n",
      "Epoch: 1103/2000... Training loss: 0.6552\n",
      "Epoch: 1103/2000... Training loss: 0.5033\n",
      "Epoch: 1103/2000... Training loss: 0.5265\n",
      "Epoch: 1103/2000... Training loss: 0.6811\n",
      "Epoch: 1103/2000... Training loss: 0.5785\n",
      "Epoch: 1103/2000... Training loss: 0.4693\n",
      "Epoch: 1103/2000... Training loss: 0.5469\n",
      "Epoch: 1103/2000... Training loss: 0.4196\n",
      "Epoch: 1103/2000... Training loss: 0.4945\n",
      "Epoch: 1103/2000... Training loss: 0.6188\n",
      "Epoch: 1103/2000... Training loss: 0.3486\n",
      "Epoch: 1103/2000... Training loss: 0.6337\n",
      "Epoch: 1103/2000... Training loss: 0.5445\n",
      "Epoch: 1103/2000... Training loss: 0.3847\n",
      "Epoch: 1103/2000... Training loss: 0.5029\n",
      "Epoch: 1103/2000... Training loss: 0.4878\n",
      "Epoch: 1103/2000... Training loss: 0.3993\n",
      "Epoch: 1103/2000... Training loss: 0.5845\n",
      "Epoch: 1103/2000... Training loss: 0.5240\n",
      "Epoch: 1103/2000... Training loss: 0.5088\n",
      "Epoch: 1103/2000... Training loss: 0.5803\n",
      "Epoch: 1103/2000... Training loss: 0.5370\n",
      "Epoch: 1103/2000... Training loss: 0.6276\n",
      "Epoch: 1103/2000... Training loss: 0.5261\n",
      "Epoch: 1103/2000... Training loss: 0.4375\n",
      "Epoch: 1103/2000... Training loss: 0.5340\n",
      "Epoch: 1103/2000... Training loss: 0.4527\n",
      "Epoch: 1103/2000... Training loss: 0.4852\n",
      "Epoch: 1104/2000... Training loss: 0.5270\n",
      "Epoch: 1104/2000... Training loss: 0.5084\n",
      "Epoch: 1104/2000... Training loss: 0.4324\n",
      "Epoch: 1104/2000... Training loss: 0.4984\n",
      "Epoch: 1104/2000... Training loss: 0.6038\n",
      "Epoch: 1104/2000... Training loss: 0.4862\n",
      "Epoch: 1104/2000... Training loss: 0.5667\n",
      "Epoch: 1104/2000... Training loss: 0.4805\n",
      "Epoch: 1104/2000... Training loss: 0.5529\n",
      "Epoch: 1104/2000... Training loss: 0.6059\n",
      "Epoch: 1104/2000... Training loss: 0.5477\n",
      "Epoch: 1104/2000... Training loss: 0.4435\n",
      "Epoch: 1104/2000... Training loss: 0.4472\n",
      "Epoch: 1104/2000... Training loss: 0.6303\n",
      "Epoch: 1104/2000... Training loss: 0.4207\n",
      "Epoch: 1104/2000... Training loss: 0.3219\n",
      "Epoch: 1104/2000... Training loss: 0.4989\n",
      "Epoch: 1104/2000... Training loss: 0.4548\n",
      "Epoch: 1104/2000... Training loss: 0.4941\n",
      "Epoch: 1104/2000... Training loss: 0.5391\n",
      "Epoch: 1104/2000... Training loss: 0.4743\n",
      "Epoch: 1104/2000... Training loss: 0.5047\n",
      "Epoch: 1104/2000... Training loss: 0.2561\n",
      "Epoch: 1104/2000... Training loss: 0.5598\n",
      "Epoch: 1104/2000... Training loss: 0.4944\n",
      "Epoch: 1104/2000... Training loss: 0.6548\n",
      "Epoch: 1104/2000... Training loss: 0.3021\n",
      "Epoch: 1104/2000... Training loss: 0.5727\n",
      "Epoch: 1104/2000... Training loss: 0.4704\n",
      "Epoch: 1104/2000... Training loss: 0.4894\n",
      "Epoch: 1104/2000... Training loss: 0.5630\n",
      "Epoch: 1105/2000... Training loss: 0.6772\n",
      "Epoch: 1105/2000... Training loss: 0.3271\n",
      "Epoch: 1105/2000... Training loss: 0.3927\n",
      "Epoch: 1105/2000... Training loss: 0.5478\n",
      "Epoch: 1105/2000... Training loss: 0.3512\n",
      "Epoch: 1105/2000... Training loss: 0.4947\n",
      "Epoch: 1105/2000... Training loss: 0.4674\n",
      "Epoch: 1105/2000... Training loss: 0.5011\n",
      "Epoch: 1105/2000... Training loss: 0.5262\n",
      "Epoch: 1105/2000... Training loss: 0.5381\n",
      "Epoch: 1105/2000... Training loss: 0.4535\n",
      "Epoch: 1105/2000... Training loss: 0.4814\n",
      "Epoch: 1105/2000... Training loss: 0.5020\n",
      "Epoch: 1105/2000... Training loss: 0.3418\n",
      "Epoch: 1105/2000... Training loss: 0.4909\n",
      "Epoch: 1105/2000... Training loss: 0.4143\n",
      "Epoch: 1105/2000... Training loss: 0.4534\n",
      "Epoch: 1105/2000... Training loss: 0.5828\n",
      "Epoch: 1105/2000... Training loss: 0.4908\n",
      "Epoch: 1105/2000... Training loss: 0.5580\n",
      "Epoch: 1105/2000... Training loss: 0.6413\n",
      "Epoch: 1105/2000... Training loss: 0.4767\n",
      "Epoch: 1105/2000... Training loss: 0.4281\n",
      "Epoch: 1105/2000... Training loss: 0.4770\n",
      "Epoch: 1105/2000... Training loss: 0.4625\n",
      "Epoch: 1105/2000... Training loss: 0.3699\n",
      "Epoch: 1105/2000... Training loss: 0.5454\n",
      "Epoch: 1105/2000... Training loss: 0.4888\n",
      "Epoch: 1105/2000... Training loss: 0.8041\n",
      "Epoch: 1105/2000... Training loss: 0.4868\n",
      "Epoch: 1105/2000... Training loss: 0.4547\n",
      "Epoch: 1106/2000... Training loss: 0.4938\n",
      "Epoch: 1106/2000... Training loss: 0.6667\n",
      "Epoch: 1106/2000... Training loss: 0.2849\n",
      "Epoch: 1106/2000... Training loss: 0.5429\n",
      "Epoch: 1106/2000... Training loss: 0.4234\n",
      "Epoch: 1106/2000... Training loss: 0.4919\n",
      "Epoch: 1106/2000... Training loss: 0.5132\n",
      "Epoch: 1106/2000... Training loss: 0.6060\n",
      "Epoch: 1106/2000... Training loss: 0.4742\n",
      "Epoch: 1106/2000... Training loss: 0.4110\n",
      "Epoch: 1106/2000... Training loss: 0.3751\n",
      "Epoch: 1106/2000... Training loss: 0.3822\n",
      "Epoch: 1106/2000... Training loss: 0.4174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1106/2000... Training loss: 0.4542\n",
      "Epoch: 1106/2000... Training loss: 0.5590\n",
      "Epoch: 1106/2000... Training loss: 0.4063\n",
      "Epoch: 1106/2000... Training loss: 0.6784\n",
      "Epoch: 1106/2000... Training loss: 0.4812\n",
      "Epoch: 1106/2000... Training loss: 0.5142\n",
      "Epoch: 1106/2000... Training loss: 0.3501\n",
      "Epoch: 1106/2000... Training loss: 0.5616\n",
      "Epoch: 1106/2000... Training loss: 0.4462\n",
      "Epoch: 1106/2000... Training loss: 0.6000\n",
      "Epoch: 1106/2000... Training loss: 0.5056\n",
      "Epoch: 1106/2000... Training loss: 0.6189\n",
      "Epoch: 1106/2000... Training loss: 0.4106\n",
      "Epoch: 1106/2000... Training loss: 0.5287\n",
      "Epoch: 1106/2000... Training loss: 0.6773\n",
      "Epoch: 1106/2000... Training loss: 0.6066\n",
      "Epoch: 1106/2000... Training loss: 0.4004\n",
      "Epoch: 1106/2000... Training loss: 0.5808\n",
      "Epoch: 1107/2000... Training loss: 0.4411\n",
      "Epoch: 1107/2000... Training loss: 0.3721\n",
      "Epoch: 1107/2000... Training loss: 0.6388\n",
      "Epoch: 1107/2000... Training loss: 0.5020\n",
      "Epoch: 1107/2000... Training loss: 0.4281\n",
      "Epoch: 1107/2000... Training loss: 0.3994\n",
      "Epoch: 1107/2000... Training loss: 0.4715\n",
      "Epoch: 1107/2000... Training loss: 0.4680\n",
      "Epoch: 1107/2000... Training loss: 0.4451\n",
      "Epoch: 1107/2000... Training loss: 0.5691\n",
      "Epoch: 1107/2000... Training loss: 0.5884\n",
      "Epoch: 1107/2000... Training loss: 0.5704\n",
      "Epoch: 1107/2000... Training loss: 0.3796\n",
      "Epoch: 1107/2000... Training loss: 0.4358\n",
      "Epoch: 1107/2000... Training loss: 0.5303\n",
      "Epoch: 1107/2000... Training loss: 0.6621\n",
      "Epoch: 1107/2000... Training loss: 0.5094\n",
      "Epoch: 1107/2000... Training loss: 0.5087\n",
      "Epoch: 1107/2000... Training loss: 0.3704\n",
      "Epoch: 1107/2000... Training loss: 0.4562\n",
      "Epoch: 1107/2000... Training loss: 0.4564\n",
      "Epoch: 1107/2000... Training loss: 0.7438\n",
      "Epoch: 1107/2000... Training loss: 0.5224\n",
      "Epoch: 1107/2000... Training loss: 0.6748\n",
      "Epoch: 1107/2000... Training loss: 0.4155\n",
      "Epoch: 1107/2000... Training loss: 0.5613\n",
      "Epoch: 1107/2000... Training loss: 0.3406\n",
      "Epoch: 1107/2000... Training loss: 0.5767\n",
      "Epoch: 1107/2000... Training loss: 0.3050\n",
      "Epoch: 1107/2000... Training loss: 0.4081\n",
      "Epoch: 1107/2000... Training loss: 0.5980\n",
      "Epoch: 1108/2000... Training loss: 0.3907\n",
      "Epoch: 1108/2000... Training loss: 0.4035\n",
      "Epoch: 1108/2000... Training loss: 0.4441\n",
      "Epoch: 1108/2000... Training loss: 0.4412\n",
      "Epoch: 1108/2000... Training loss: 0.2719\n",
      "Epoch: 1108/2000... Training loss: 0.5621\n",
      "Epoch: 1108/2000... Training loss: 0.4038\n",
      "Epoch: 1108/2000... Training loss: 0.5416\n",
      "Epoch: 1108/2000... Training loss: 0.3896\n",
      "Epoch: 1108/2000... Training loss: 0.4747\n",
      "Epoch: 1108/2000... Training loss: 0.3601\n",
      "Epoch: 1108/2000... Training loss: 0.5464\n",
      "Epoch: 1108/2000... Training loss: 0.3799\n",
      "Epoch: 1108/2000... Training loss: 0.5293\n",
      "Epoch: 1108/2000... Training loss: 0.5077\n",
      "Epoch: 1108/2000... Training loss: 0.4812\n",
      "Epoch: 1108/2000... Training loss: 0.4139\n",
      "Epoch: 1108/2000... Training loss: 0.4425\n",
      "Epoch: 1108/2000... Training loss: 0.4239\n",
      "Epoch: 1108/2000... Training loss: 0.4274\n",
      "Epoch: 1108/2000... Training loss: 0.5018\n",
      "Epoch: 1108/2000... Training loss: 0.4401\n",
      "Epoch: 1108/2000... Training loss: 0.5282\n",
      "Epoch: 1108/2000... Training loss: 0.3450\n",
      "Epoch: 1108/2000... Training loss: 0.5470\n",
      "Epoch: 1108/2000... Training loss: 0.4768\n",
      "Epoch: 1108/2000... Training loss: 0.4483\n",
      "Epoch: 1108/2000... Training loss: 0.4238\n",
      "Epoch: 1108/2000... Training loss: 0.3899\n",
      "Epoch: 1108/2000... Training loss: 0.4755\n",
      "Epoch: 1108/2000... Training loss: 0.5063\n",
      "Epoch: 1109/2000... Training loss: 0.6323\n",
      "Epoch: 1109/2000... Training loss: 0.5852\n",
      "Epoch: 1109/2000... Training loss: 0.4621\n",
      "Epoch: 1109/2000... Training loss: 0.4954\n",
      "Epoch: 1109/2000... Training loss: 0.6438\n",
      "Epoch: 1109/2000... Training loss: 0.5708\n",
      "Epoch: 1109/2000... Training loss: 0.4690\n",
      "Epoch: 1109/2000... Training loss: 0.6077\n",
      "Epoch: 1109/2000... Training loss: 0.5684\n",
      "Epoch: 1109/2000... Training loss: 0.4162\n",
      "Epoch: 1109/2000... Training loss: 0.6379\n",
      "Epoch: 1109/2000... Training loss: 0.6866\n",
      "Epoch: 1109/2000... Training loss: 0.4904\n",
      "Epoch: 1109/2000... Training loss: 0.5690\n",
      "Epoch: 1109/2000... Training loss: 0.3468\n",
      "Epoch: 1109/2000... Training loss: 0.6230\n",
      "Epoch: 1109/2000... Training loss: 0.4164\n",
      "Epoch: 1109/2000... Training loss: 0.4770\n",
      "Epoch: 1109/2000... Training loss: 0.4087\n",
      "Epoch: 1109/2000... Training loss: 0.4998\n",
      "Epoch: 1109/2000... Training loss: 0.4687\n",
      "Epoch: 1109/2000... Training loss: 0.4378\n",
      "Epoch: 1109/2000... Training loss: 0.4242\n",
      "Epoch: 1109/2000... Training loss: 0.6018\n",
      "Epoch: 1109/2000... Training loss: 0.5086\n",
      "Epoch: 1109/2000... Training loss: 0.2932\n",
      "Epoch: 1109/2000... Training loss: 0.4807\n",
      "Epoch: 1109/2000... Training loss: 0.4523\n",
      "Epoch: 1109/2000... Training loss: 0.5741\n",
      "Epoch: 1109/2000... Training loss: 0.3585\n",
      "Epoch: 1109/2000... Training loss: 0.4967\n",
      "Epoch: 1110/2000... Training loss: 0.4991\n",
      "Epoch: 1110/2000... Training loss: 0.4922\n",
      "Epoch: 1110/2000... Training loss: 0.6084\n",
      "Epoch: 1110/2000... Training loss: 0.4898\n",
      "Epoch: 1110/2000... Training loss: 0.4370\n",
      "Epoch: 1110/2000... Training loss: 0.5372\n",
      "Epoch: 1110/2000... Training loss: 0.6022\n",
      "Epoch: 1110/2000... Training loss: 0.4802\n",
      "Epoch: 1110/2000... Training loss: 0.4530\n",
      "Epoch: 1110/2000... Training loss: 0.6120\n",
      "Epoch: 1110/2000... Training loss: 0.4633\n",
      "Epoch: 1110/2000... Training loss: 0.4680\n",
      "Epoch: 1110/2000... Training loss: 0.4653\n",
      "Epoch: 1110/2000... Training loss: 0.4456\n",
      "Epoch: 1110/2000... Training loss: 0.6788\n",
      "Epoch: 1110/2000... Training loss: 0.4311\n",
      "Epoch: 1110/2000... Training loss: 0.5550\n",
      "Epoch: 1110/2000... Training loss: 0.4866\n",
      "Epoch: 1110/2000... Training loss: 0.4085\n",
      "Epoch: 1110/2000... Training loss: 0.2317\n",
      "Epoch: 1110/2000... Training loss: 0.4893\n",
      "Epoch: 1110/2000... Training loss: 0.7194\n",
      "Epoch: 1110/2000... Training loss: 0.6752\n",
      "Epoch: 1110/2000... Training loss: 0.4005\n",
      "Epoch: 1110/2000... Training loss: 0.3421\n",
      "Epoch: 1110/2000... Training loss: 0.5621\n",
      "Epoch: 1110/2000... Training loss: 0.4558\n",
      "Epoch: 1110/2000... Training loss: 0.4650\n",
      "Epoch: 1110/2000... Training loss: 0.2727\n",
      "Epoch: 1110/2000... Training loss: 0.5879\n",
      "Epoch: 1110/2000... Training loss: 0.5516\n",
      "Epoch: 1111/2000... Training loss: 0.5506\n",
      "Epoch: 1111/2000... Training loss: 0.5500\n",
      "Epoch: 1111/2000... Training loss: 0.3903\n",
      "Epoch: 1111/2000... Training loss: 0.4710\n",
      "Epoch: 1111/2000... Training loss: 0.5234\n",
      "Epoch: 1111/2000... Training loss: 0.4010\n",
      "Epoch: 1111/2000... Training loss: 0.4232\n",
      "Epoch: 1111/2000... Training loss: 0.4726\n",
      "Epoch: 1111/2000... Training loss: 0.5137\n",
      "Epoch: 1111/2000... Training loss: 0.4814\n",
      "Epoch: 1111/2000... Training loss: 0.3666\n",
      "Epoch: 1111/2000... Training loss: 0.5475\n",
      "Epoch: 1111/2000... Training loss: 0.5404\n",
      "Epoch: 1111/2000... Training loss: 0.3918\n",
      "Epoch: 1111/2000... Training loss: 0.3964\n",
      "Epoch: 1111/2000... Training loss: 0.4622\n",
      "Epoch: 1111/2000... Training loss: 0.3822\n",
      "Epoch: 1111/2000... Training loss: 0.4632\n",
      "Epoch: 1111/2000... Training loss: 0.5874\n",
      "Epoch: 1111/2000... Training loss: 0.5212\n",
      "Epoch: 1111/2000... Training loss: 0.4717\n",
      "Epoch: 1111/2000... Training loss: 0.6874\n",
      "Epoch: 1111/2000... Training loss: 0.4930\n",
      "Epoch: 1111/2000... Training loss: 0.4149\n",
      "Epoch: 1111/2000... Training loss: 0.4244\n",
      "Epoch: 1111/2000... Training loss: 0.4635\n",
      "Epoch: 1111/2000... Training loss: 0.4698\n",
      "Epoch: 1111/2000... Training loss: 0.5593\n",
      "Epoch: 1111/2000... Training loss: 0.4091\n",
      "Epoch: 1111/2000... Training loss: 0.5008\n",
      "Epoch: 1111/2000... Training loss: 0.5893\n",
      "Epoch: 1112/2000... Training loss: 0.3563\n",
      "Epoch: 1112/2000... Training loss: 0.4128\n",
      "Epoch: 1112/2000... Training loss: 0.3993\n",
      "Epoch: 1112/2000... Training loss: 0.4592\n",
      "Epoch: 1112/2000... Training loss: 0.5271\n",
      "Epoch: 1112/2000... Training loss: 0.4858\n",
      "Epoch: 1112/2000... Training loss: 0.5828\n",
      "Epoch: 1112/2000... Training loss: 0.5851\n",
      "Epoch: 1112/2000... Training loss: 0.4775\n",
      "Epoch: 1112/2000... Training loss: 0.5762\n",
      "Epoch: 1112/2000... Training loss: 0.4463\n",
      "Epoch: 1112/2000... Training loss: 0.5188\n",
      "Epoch: 1112/2000... Training loss: 0.5239\n",
      "Epoch: 1112/2000... Training loss: 0.4746\n",
      "Epoch: 1112/2000... Training loss: 0.5022\n",
      "Epoch: 1112/2000... Training loss: 0.4068\n",
      "Epoch: 1112/2000... Training loss: 0.3979\n",
      "Epoch: 1112/2000... Training loss: 0.4312\n",
      "Epoch: 1112/2000... Training loss: 0.6047\n",
      "Epoch: 1112/2000... Training loss: 0.4027\n",
      "Epoch: 1112/2000... Training loss: 0.5885\n",
      "Epoch: 1112/2000... Training loss: 0.6330\n",
      "Epoch: 1112/2000... Training loss: 0.5965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1112/2000... Training loss: 0.5154\n",
      "Epoch: 1112/2000... Training loss: 0.4565\n",
      "Epoch: 1112/2000... Training loss: 0.5455\n",
      "Epoch: 1112/2000... Training loss: 0.6266\n",
      "Epoch: 1112/2000... Training loss: 0.5922\n",
      "Epoch: 1112/2000... Training loss: 0.6502\n",
      "Epoch: 1112/2000... Training loss: 0.4745\n",
      "Epoch: 1112/2000... Training loss: 0.4609\n",
      "Epoch: 1113/2000... Training loss: 0.4939\n",
      "Epoch: 1113/2000... Training loss: 0.4721\n",
      "Epoch: 1113/2000... Training loss: 0.2755\n",
      "Epoch: 1113/2000... Training loss: 0.4438\n",
      "Epoch: 1113/2000... Training loss: 0.3694\n",
      "Epoch: 1113/2000... Training loss: 0.3606\n",
      "Epoch: 1113/2000... Training loss: 0.3052\n",
      "Epoch: 1113/2000... Training loss: 0.3547\n",
      "Epoch: 1113/2000... Training loss: 0.6295\n",
      "Epoch: 1113/2000... Training loss: 0.3701\n",
      "Epoch: 1113/2000... Training loss: 0.4050\n",
      "Epoch: 1113/2000... Training loss: 0.3791\n",
      "Epoch: 1113/2000... Training loss: 0.3312\n",
      "Epoch: 1113/2000... Training loss: 0.3822\n",
      "Epoch: 1113/2000... Training loss: 0.3549\n",
      "Epoch: 1113/2000... Training loss: 0.6215\n",
      "Epoch: 1113/2000... Training loss: 0.2810\n",
      "Epoch: 1113/2000... Training loss: 0.4023\n",
      "Epoch: 1113/2000... Training loss: 0.5354\n",
      "Epoch: 1113/2000... Training loss: 0.3083\n",
      "Epoch: 1113/2000... Training loss: 0.4532\n",
      "Epoch: 1113/2000... Training loss: 0.4392\n",
      "Epoch: 1113/2000... Training loss: 0.4703\n",
      "Epoch: 1113/2000... Training loss: 0.4978\n",
      "Epoch: 1113/2000... Training loss: 0.3874\n",
      "Epoch: 1113/2000... Training loss: 0.4523\n",
      "Epoch: 1113/2000... Training loss: 0.5385\n",
      "Epoch: 1113/2000... Training loss: 0.3270\n",
      "Epoch: 1113/2000... Training loss: 0.5608\n",
      "Epoch: 1113/2000... Training loss: 0.2389\n",
      "Epoch: 1113/2000... Training loss: 0.3680\n",
      "Epoch: 1114/2000... Training loss: 0.5622\n",
      "Epoch: 1114/2000... Training loss: 0.4864\n",
      "Epoch: 1114/2000... Training loss: 0.3996\n",
      "Epoch: 1114/2000... Training loss: 0.3256\n",
      "Epoch: 1114/2000... Training loss: 0.3661\n",
      "Epoch: 1114/2000... Training loss: 0.3991\n",
      "Epoch: 1114/2000... Training loss: 0.8439\n",
      "Epoch: 1114/2000... Training loss: 0.4282\n",
      "Epoch: 1114/2000... Training loss: 0.6256\n",
      "Epoch: 1114/2000... Training loss: 0.4255\n",
      "Epoch: 1114/2000... Training loss: 0.6927\n",
      "Epoch: 1114/2000... Training loss: 0.4686\n",
      "Epoch: 1114/2000... Training loss: 0.3993\n",
      "Epoch: 1114/2000... Training loss: 0.6070\n",
      "Epoch: 1114/2000... Training loss: 0.6144\n",
      "Epoch: 1114/2000... Training loss: 0.4962\n",
      "Epoch: 1114/2000... Training loss: 0.4331\n",
      "Epoch: 1114/2000... Training loss: 0.4308\n",
      "Epoch: 1114/2000... Training loss: 0.3394\n",
      "Epoch: 1114/2000... Training loss: 0.7076\n",
      "Epoch: 1114/2000... Training loss: 0.4428\n",
      "Epoch: 1114/2000... Training loss: 0.5129\n",
      "Epoch: 1114/2000... Training loss: 0.3167\n",
      "Epoch: 1114/2000... Training loss: 0.6233\n",
      "Epoch: 1114/2000... Training loss: 0.3926\n",
      "Epoch: 1114/2000... Training loss: 0.4454\n",
      "Epoch: 1114/2000... Training loss: 0.5625\n",
      "Epoch: 1114/2000... Training loss: 0.4907\n",
      "Epoch: 1114/2000... Training loss: 0.3566\n",
      "Epoch: 1114/2000... Training loss: 0.3703\n",
      "Epoch: 1114/2000... Training loss: 0.6605\n",
      "Epoch: 1115/2000... Training loss: 0.4072\n",
      "Epoch: 1115/2000... Training loss: 0.5588\n",
      "Epoch: 1115/2000... Training loss: 0.4530\n",
      "Epoch: 1115/2000... Training loss: 0.7775\n",
      "Epoch: 1115/2000... Training loss: 0.5907\n",
      "Epoch: 1115/2000... Training loss: 0.4326\n",
      "Epoch: 1115/2000... Training loss: 0.3853\n",
      "Epoch: 1115/2000... Training loss: 0.4651\n",
      "Epoch: 1115/2000... Training loss: 0.6129\n",
      "Epoch: 1115/2000... Training loss: 0.4508\n",
      "Epoch: 1115/2000... Training loss: 0.6828\n",
      "Epoch: 1115/2000... Training loss: 0.3291\n",
      "Epoch: 1115/2000... Training loss: 0.4534\n",
      "Epoch: 1115/2000... Training loss: 0.5998\n",
      "Epoch: 1115/2000... Training loss: 0.4719\n",
      "Epoch: 1115/2000... Training loss: 0.3435\n",
      "Epoch: 1115/2000... Training loss: 0.5867\n",
      "Epoch: 1115/2000... Training loss: 0.5141\n",
      "Epoch: 1115/2000... Training loss: 0.4589\n",
      "Epoch: 1115/2000... Training loss: 0.5382\n",
      "Epoch: 1115/2000... Training loss: 0.3338\n",
      "Epoch: 1115/2000... Training loss: 0.4290\n",
      "Epoch: 1115/2000... Training loss: 0.5735\n",
      "Epoch: 1115/2000... Training loss: 0.5750\n",
      "Epoch: 1115/2000... Training loss: 0.4089\n",
      "Epoch: 1115/2000... Training loss: 0.3501\n",
      "Epoch: 1115/2000... Training loss: 0.4823\n",
      "Epoch: 1115/2000... Training loss: 0.4532\n",
      "Epoch: 1115/2000... Training loss: 0.4300\n",
      "Epoch: 1115/2000... Training loss: 0.5876\n",
      "Epoch: 1115/2000... Training loss: 0.4266\n",
      "Epoch: 1116/2000... Training loss: 0.3749\n",
      "Epoch: 1116/2000... Training loss: 0.6720\n",
      "Epoch: 1116/2000... Training loss: 0.5404\n",
      "Epoch: 1116/2000... Training loss: 0.6002\n",
      "Epoch: 1116/2000... Training loss: 0.4792\n",
      "Epoch: 1116/2000... Training loss: 0.4780\n",
      "Epoch: 1116/2000... Training loss: 0.5807\n",
      "Epoch: 1116/2000... Training loss: 0.5278\n",
      "Epoch: 1116/2000... Training loss: 0.6646\n",
      "Epoch: 1116/2000... Training loss: 0.4426\n",
      "Epoch: 1116/2000... Training loss: 0.2826\n",
      "Epoch: 1116/2000... Training loss: 0.6177\n",
      "Epoch: 1116/2000... Training loss: 0.5102\n",
      "Epoch: 1116/2000... Training loss: 0.5397\n",
      "Epoch: 1116/2000... Training loss: 0.6036\n",
      "Epoch: 1116/2000... Training loss: 0.3900\n",
      "Epoch: 1116/2000... Training loss: 0.4398\n",
      "Epoch: 1116/2000... Training loss: 0.3981\n",
      "Epoch: 1116/2000... Training loss: 0.4816\n",
      "Epoch: 1116/2000... Training loss: 0.5317\n",
      "Epoch: 1116/2000... Training loss: 0.3270\n",
      "Epoch: 1116/2000... Training loss: 0.4749\n",
      "Epoch: 1116/2000... Training loss: 0.6790\n",
      "Epoch: 1116/2000... Training loss: 0.6904\n",
      "Epoch: 1116/2000... Training loss: 0.5825\n",
      "Epoch: 1116/2000... Training loss: 0.3800\n",
      "Epoch: 1116/2000... Training loss: 0.5473\n",
      "Epoch: 1116/2000... Training loss: 0.4427\n",
      "Epoch: 1116/2000... Training loss: 0.5619\n",
      "Epoch: 1116/2000... Training loss: 0.7450\n",
      "Epoch: 1116/2000... Training loss: 0.3686\n",
      "Epoch: 1117/2000... Training loss: 0.4892\n",
      "Epoch: 1117/2000... Training loss: 0.5878\n",
      "Epoch: 1117/2000... Training loss: 0.5448\n",
      "Epoch: 1117/2000... Training loss: 0.4842\n",
      "Epoch: 1117/2000... Training loss: 0.4239\n",
      "Epoch: 1117/2000... Training loss: 0.5347\n",
      "Epoch: 1117/2000... Training loss: 0.5953\n",
      "Epoch: 1117/2000... Training loss: 0.5064\n",
      "Epoch: 1117/2000... Training loss: 0.4902\n",
      "Epoch: 1117/2000... Training loss: 0.5744\n",
      "Epoch: 1117/2000... Training loss: 0.4299\n",
      "Epoch: 1117/2000... Training loss: 0.6190\n",
      "Epoch: 1117/2000... Training loss: 0.4924\n",
      "Epoch: 1117/2000... Training loss: 0.3836\n",
      "Epoch: 1117/2000... Training loss: 0.4016\n",
      "Epoch: 1117/2000... Training loss: 0.3904\n",
      "Epoch: 1117/2000... Training loss: 0.4461\n",
      "Epoch: 1117/2000... Training loss: 0.4633\n",
      "Epoch: 1117/2000... Training loss: 0.5647\n",
      "Epoch: 1117/2000... Training loss: 0.5636\n",
      "Epoch: 1117/2000... Training loss: 0.6797\n",
      "Epoch: 1117/2000... Training loss: 0.5094\n",
      "Epoch: 1117/2000... Training loss: 0.3943\n",
      "Epoch: 1117/2000... Training loss: 0.3563\n",
      "Epoch: 1117/2000... Training loss: 0.4923\n",
      "Epoch: 1117/2000... Training loss: 0.5987\n",
      "Epoch: 1117/2000... Training loss: 0.3906\n",
      "Epoch: 1117/2000... Training loss: 0.4873\n",
      "Epoch: 1117/2000... Training loss: 0.3808\n",
      "Epoch: 1117/2000... Training loss: 0.4662\n",
      "Epoch: 1117/2000... Training loss: 0.6726\n",
      "Epoch: 1118/2000... Training loss: 0.4786\n",
      "Epoch: 1118/2000... Training loss: 0.2940\n",
      "Epoch: 1118/2000... Training loss: 0.3457\n",
      "Epoch: 1118/2000... Training loss: 0.7342\n",
      "Epoch: 1118/2000... Training loss: 0.4409\n",
      "Epoch: 1118/2000... Training loss: 0.5713\n",
      "Epoch: 1118/2000... Training loss: 0.4507\n",
      "Epoch: 1118/2000... Training loss: 0.5354\n",
      "Epoch: 1118/2000... Training loss: 0.5256\n",
      "Epoch: 1118/2000... Training loss: 0.3472\n",
      "Epoch: 1118/2000... Training loss: 0.4484\n",
      "Epoch: 1118/2000... Training loss: 0.3786\n",
      "Epoch: 1118/2000... Training loss: 0.3675\n",
      "Epoch: 1118/2000... Training loss: 0.5100\n",
      "Epoch: 1118/2000... Training loss: 0.5211\n",
      "Epoch: 1118/2000... Training loss: 0.5616\n",
      "Epoch: 1118/2000... Training loss: 0.6401\n",
      "Epoch: 1118/2000... Training loss: 0.4844\n",
      "Epoch: 1118/2000... Training loss: 0.5495\n",
      "Epoch: 1118/2000... Training loss: 0.3998\n",
      "Epoch: 1118/2000... Training loss: 0.4387\n",
      "Epoch: 1118/2000... Training loss: 0.3930\n",
      "Epoch: 1118/2000... Training loss: 0.6687\n",
      "Epoch: 1118/2000... Training loss: 0.4757\n",
      "Epoch: 1118/2000... Training loss: 0.3372\n",
      "Epoch: 1118/2000... Training loss: 0.4576\n",
      "Epoch: 1118/2000... Training loss: 0.6021\n",
      "Epoch: 1118/2000... Training loss: 0.4918\n",
      "Epoch: 1118/2000... Training loss: 0.3503\n",
      "Epoch: 1118/2000... Training loss: 0.4452\n",
      "Epoch: 1118/2000... Training loss: 0.4946\n",
      "Epoch: 1119/2000... Training loss: 0.5561\n",
      "Epoch: 1119/2000... Training loss: 0.6121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1119/2000... Training loss: 0.5343\n",
      "Epoch: 1119/2000... Training loss: 0.3741\n",
      "Epoch: 1119/2000... Training loss: 0.5002\n",
      "Epoch: 1119/2000... Training loss: 0.6842\n",
      "Epoch: 1119/2000... Training loss: 0.5728\n",
      "Epoch: 1119/2000... Training loss: 0.4824\n",
      "Epoch: 1119/2000... Training loss: 0.4324\n",
      "Epoch: 1119/2000... Training loss: 0.5043\n",
      "Epoch: 1119/2000... Training loss: 0.3842\n",
      "Epoch: 1119/2000... Training loss: 0.6363\n",
      "Epoch: 1119/2000... Training loss: 0.6013\n",
      "Epoch: 1119/2000... Training loss: 0.5229\n",
      "Epoch: 1119/2000... Training loss: 0.4527\n",
      "Epoch: 1119/2000... Training loss: 0.2310\n",
      "Epoch: 1119/2000... Training loss: 0.4929\n",
      "Epoch: 1119/2000... Training loss: 0.4587\n",
      "Epoch: 1119/2000... Training loss: 0.6221\n",
      "Epoch: 1119/2000... Training loss: 0.5248\n",
      "Epoch: 1119/2000... Training loss: 0.5462\n",
      "Epoch: 1119/2000... Training loss: 0.6186\n",
      "Epoch: 1119/2000... Training loss: 0.3584\n",
      "Epoch: 1119/2000... Training loss: 0.6188\n",
      "Epoch: 1119/2000... Training loss: 0.4169\n",
      "Epoch: 1119/2000... Training loss: 0.4367\n",
      "Epoch: 1119/2000... Training loss: 0.2947\n",
      "Epoch: 1119/2000... Training loss: 0.4978\n",
      "Epoch: 1119/2000... Training loss: 0.3675\n",
      "Epoch: 1119/2000... Training loss: 0.4090\n",
      "Epoch: 1119/2000... Training loss: 0.4814\n",
      "Epoch: 1120/2000... Training loss: 0.5296\n",
      "Epoch: 1120/2000... Training loss: 0.3746\n",
      "Epoch: 1120/2000... Training loss: 0.4855\n",
      "Epoch: 1120/2000... Training loss: 0.4088\n",
      "Epoch: 1120/2000... Training loss: 0.4893\n",
      "Epoch: 1120/2000... Training loss: 0.6361\n",
      "Epoch: 1120/2000... Training loss: 0.4966\n",
      "Epoch: 1120/2000... Training loss: 0.4410\n",
      "Epoch: 1120/2000... Training loss: 0.5920\n",
      "Epoch: 1120/2000... Training loss: 0.5533\n",
      "Epoch: 1120/2000... Training loss: 0.3717\n",
      "Epoch: 1120/2000... Training loss: 0.5647\n",
      "Epoch: 1120/2000... Training loss: 0.4582\n",
      "Epoch: 1120/2000... Training loss: 0.5132\n",
      "Epoch: 1120/2000... Training loss: 0.4670\n",
      "Epoch: 1120/2000... Training loss: 0.4736\n",
      "Epoch: 1120/2000... Training loss: 0.4880\n",
      "Epoch: 1120/2000... Training loss: 0.2885\n",
      "Epoch: 1120/2000... Training loss: 0.3412\n",
      "Epoch: 1120/2000... Training loss: 0.4826\n",
      "Epoch: 1120/2000... Training loss: 0.3850\n",
      "Epoch: 1120/2000... Training loss: 0.4174\n",
      "Epoch: 1120/2000... Training loss: 0.6055\n",
      "Epoch: 1120/2000... Training loss: 0.6869\n",
      "Epoch: 1120/2000... Training loss: 0.3959\n",
      "Epoch: 1120/2000... Training loss: 0.4478\n",
      "Epoch: 1120/2000... Training loss: 0.4040\n",
      "Epoch: 1120/2000... Training loss: 0.4128\n",
      "Epoch: 1120/2000... Training loss: 0.3786\n",
      "Epoch: 1120/2000... Training loss: 0.3049\n",
      "Epoch: 1120/2000... Training loss: 0.4319\n",
      "Epoch: 1121/2000... Training loss: 0.4824\n",
      "Epoch: 1121/2000... Training loss: 0.5556\n",
      "Epoch: 1121/2000... Training loss: 0.3903\n",
      "Epoch: 1121/2000... Training loss: 0.5754\n",
      "Epoch: 1121/2000... Training loss: 0.3791\n",
      "Epoch: 1121/2000... Training loss: 0.5009\n",
      "Epoch: 1121/2000... Training loss: 0.4368\n",
      "Epoch: 1121/2000... Training loss: 0.4403\n",
      "Epoch: 1121/2000... Training loss: 0.5409\n",
      "Epoch: 1121/2000... Training loss: 0.5838\n",
      "Epoch: 1121/2000... Training loss: 0.4433\n",
      "Epoch: 1121/2000... Training loss: 0.3967\n",
      "Epoch: 1121/2000... Training loss: 0.2952\n",
      "Epoch: 1121/2000... Training loss: 0.6016\n",
      "Epoch: 1121/2000... Training loss: 0.3797\n",
      "Epoch: 1121/2000... Training loss: 0.3880\n",
      "Epoch: 1121/2000... Training loss: 0.5012\n",
      "Epoch: 1121/2000... Training loss: 0.6072\n",
      "Epoch: 1121/2000... Training loss: 0.5293\n",
      "Epoch: 1121/2000... Training loss: 0.2964\n",
      "Epoch: 1121/2000... Training loss: 0.3952\n",
      "Epoch: 1121/2000... Training loss: 0.4680\n",
      "Epoch: 1121/2000... Training loss: 0.5232\n",
      "Epoch: 1121/2000... Training loss: 0.4560\n",
      "Epoch: 1121/2000... Training loss: 0.4666\n",
      "Epoch: 1121/2000... Training loss: 0.2761\n",
      "Epoch: 1121/2000... Training loss: 0.4409\n",
      "Epoch: 1121/2000... Training loss: 0.3936\n",
      "Epoch: 1121/2000... Training loss: 0.5066\n",
      "Epoch: 1121/2000... Training loss: 0.6903\n",
      "Epoch: 1121/2000... Training loss: 0.4426\n",
      "Epoch: 1122/2000... Training loss: 0.3966\n",
      "Epoch: 1122/2000... Training loss: 0.4536\n",
      "Epoch: 1122/2000... Training loss: 0.3992\n",
      "Epoch: 1122/2000... Training loss: 0.5097\n",
      "Epoch: 1122/2000... Training loss: 0.6282\n",
      "Epoch: 1122/2000... Training loss: 0.4781\n",
      "Epoch: 1122/2000... Training loss: 0.4201\n",
      "Epoch: 1122/2000... Training loss: 0.5289\n",
      "Epoch: 1122/2000... Training loss: 0.4180\n",
      "Epoch: 1122/2000... Training loss: 0.4373\n",
      "Epoch: 1122/2000... Training loss: 0.4216\n",
      "Epoch: 1122/2000... Training loss: 0.5726\n",
      "Epoch: 1122/2000... Training loss: 0.5772\n",
      "Epoch: 1122/2000... Training loss: 0.3974\n",
      "Epoch: 1122/2000... Training loss: 0.3360\n",
      "Epoch: 1122/2000... Training loss: 0.3558\n",
      "Epoch: 1122/2000... Training loss: 0.6102\n",
      "Epoch: 1122/2000... Training loss: 0.5615\n",
      "Epoch: 1122/2000... Training loss: 0.4617\n",
      "Epoch: 1122/2000... Training loss: 0.5023\n",
      "Epoch: 1122/2000... Training loss: 0.5272\n",
      "Epoch: 1122/2000... Training loss: 0.4302\n",
      "Epoch: 1122/2000... Training loss: 0.5173\n",
      "Epoch: 1122/2000... Training loss: 0.5088\n",
      "Epoch: 1122/2000... Training loss: 0.6684\n",
      "Epoch: 1122/2000... Training loss: 0.5111\n",
      "Epoch: 1122/2000... Training loss: 0.6154\n",
      "Epoch: 1122/2000... Training loss: 0.7150\n",
      "Epoch: 1122/2000... Training loss: 0.5299\n",
      "Epoch: 1122/2000... Training loss: 0.4044\n",
      "Epoch: 1122/2000... Training loss: 0.5128\n",
      "Epoch: 1123/2000... Training loss: 0.5873\n",
      "Epoch: 1123/2000... Training loss: 0.3723\n",
      "Epoch: 1123/2000... Training loss: 0.3744\n",
      "Epoch: 1123/2000... Training loss: 0.4070\n",
      "Epoch: 1123/2000... Training loss: 0.4517\n",
      "Epoch: 1123/2000... Training loss: 0.4591\n",
      "Epoch: 1123/2000... Training loss: 0.3336\n",
      "Epoch: 1123/2000... Training loss: 0.4909\n",
      "Epoch: 1123/2000... Training loss: 0.4788\n",
      "Epoch: 1123/2000... Training loss: 0.4242\n",
      "Epoch: 1123/2000... Training loss: 0.3416\n",
      "Epoch: 1123/2000... Training loss: 0.5440\n",
      "Epoch: 1123/2000... Training loss: 0.5259\n",
      "Epoch: 1123/2000... Training loss: 0.5117\n",
      "Epoch: 1123/2000... Training loss: 0.5703\n",
      "Epoch: 1123/2000... Training loss: 0.3349\n",
      "Epoch: 1123/2000... Training loss: 0.5913\n",
      "Epoch: 1123/2000... Training loss: 0.4555\n",
      "Epoch: 1123/2000... Training loss: 0.5712\n",
      "Epoch: 1123/2000... Training loss: 0.4961\n",
      "Epoch: 1123/2000... Training loss: 0.4775\n",
      "Epoch: 1123/2000... Training loss: 0.4859\n",
      "Epoch: 1123/2000... Training loss: 0.4037\n",
      "Epoch: 1123/2000... Training loss: 0.4282\n",
      "Epoch: 1123/2000... Training loss: 0.3459\n",
      "Epoch: 1123/2000... Training loss: 0.5163\n",
      "Epoch: 1123/2000... Training loss: 0.3482\n",
      "Epoch: 1123/2000... Training loss: 0.5079\n",
      "Epoch: 1123/2000... Training loss: 0.3792\n",
      "Epoch: 1123/2000... Training loss: 0.4120\n",
      "Epoch: 1123/2000... Training loss: 0.5053\n",
      "Epoch: 1124/2000... Training loss: 0.5568\n",
      "Epoch: 1124/2000... Training loss: 0.6940\n",
      "Epoch: 1124/2000... Training loss: 0.5482\n",
      "Epoch: 1124/2000... Training loss: 0.4715\n",
      "Epoch: 1124/2000... Training loss: 0.5021\n",
      "Epoch: 1124/2000... Training loss: 0.6347\n",
      "Epoch: 1124/2000... Training loss: 0.6001\n",
      "Epoch: 1124/2000... Training loss: 0.3043\n",
      "Epoch: 1124/2000... Training loss: 0.4428\n",
      "Epoch: 1124/2000... Training loss: 0.5967\n",
      "Epoch: 1124/2000... Training loss: 0.4635\n",
      "Epoch: 1124/2000... Training loss: 0.5267\n",
      "Epoch: 1124/2000... Training loss: 0.5780\n",
      "Epoch: 1124/2000... Training loss: 0.6317\n",
      "Epoch: 1124/2000... Training loss: 0.4475\n",
      "Epoch: 1124/2000... Training loss: 0.4887\n",
      "Epoch: 1124/2000... Training loss: 0.4466\n",
      "Epoch: 1124/2000... Training loss: 0.4922\n",
      "Epoch: 1124/2000... Training loss: 0.5150\n",
      "Epoch: 1124/2000... Training loss: 0.5756\n",
      "Epoch: 1124/2000... Training loss: 0.4199\n",
      "Epoch: 1124/2000... Training loss: 0.5580\n",
      "Epoch: 1124/2000... Training loss: 0.4643\n",
      "Epoch: 1124/2000... Training loss: 0.6541\n",
      "Epoch: 1124/2000... Training loss: 0.4104\n",
      "Epoch: 1124/2000... Training loss: 0.5154\n",
      "Epoch: 1124/2000... Training loss: 0.5621\n",
      "Epoch: 1124/2000... Training loss: 0.4961\n",
      "Epoch: 1124/2000... Training loss: 0.5438\n",
      "Epoch: 1124/2000... Training loss: 0.5727\n",
      "Epoch: 1124/2000... Training loss: 0.5309\n",
      "Epoch: 1125/2000... Training loss: 0.3948\n",
      "Epoch: 1125/2000... Training loss: 0.5813\n",
      "Epoch: 1125/2000... Training loss: 0.3886\n",
      "Epoch: 1125/2000... Training loss: 0.5108\n",
      "Epoch: 1125/2000... Training loss: 0.5768\n",
      "Epoch: 1125/2000... Training loss: 0.6047\n",
      "Epoch: 1125/2000... Training loss: 0.3673\n",
      "Epoch: 1125/2000... Training loss: 0.4110\n",
      "Epoch: 1125/2000... Training loss: 0.6181\n",
      "Epoch: 1125/2000... Training loss: 0.5077\n",
      "Epoch: 1125/2000... Training loss: 0.5686\n",
      "Epoch: 1125/2000... Training loss: 0.4318\n",
      "Epoch: 1125/2000... Training loss: 0.5727\n",
      "Epoch: 1125/2000... Training loss: 0.4413\n",
      "Epoch: 1125/2000... Training loss: 0.4452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1125/2000... Training loss: 0.3315\n",
      "Epoch: 1125/2000... Training loss: 0.5550\n",
      "Epoch: 1125/2000... Training loss: 0.4137\n",
      "Epoch: 1125/2000... Training loss: 0.5252\n",
      "Epoch: 1125/2000... Training loss: 0.5311\n",
      "Epoch: 1125/2000... Training loss: 0.5056\n",
      "Epoch: 1125/2000... Training loss: 0.3964\n",
      "Epoch: 1125/2000... Training loss: 0.4154\n",
      "Epoch: 1125/2000... Training loss: 0.4115\n",
      "Epoch: 1125/2000... Training loss: 0.5592\n",
      "Epoch: 1125/2000... Training loss: 0.6010\n",
      "Epoch: 1125/2000... Training loss: 0.5074\n",
      "Epoch: 1125/2000... Training loss: 0.5877\n",
      "Epoch: 1125/2000... Training loss: 0.4178\n",
      "Epoch: 1125/2000... Training loss: 0.5380\n",
      "Epoch: 1125/2000... Training loss: 0.5191\n",
      "Epoch: 1126/2000... Training loss: 0.5437\n",
      "Epoch: 1126/2000... Training loss: 0.3969\n",
      "Epoch: 1126/2000... Training loss: 0.5661\n",
      "Epoch: 1126/2000... Training loss: 0.4742\n",
      "Epoch: 1126/2000... Training loss: 0.3536\n",
      "Epoch: 1126/2000... Training loss: 0.4233\n",
      "Epoch: 1126/2000... Training loss: 0.2903\n",
      "Epoch: 1126/2000... Training loss: 0.6337\n",
      "Epoch: 1126/2000... Training loss: 0.4172\n",
      "Epoch: 1126/2000... Training loss: 0.5066\n",
      "Epoch: 1126/2000... Training loss: 0.3618\n",
      "Epoch: 1126/2000... Training loss: 0.3578\n",
      "Epoch: 1126/2000... Training loss: 0.4687\n",
      "Epoch: 1126/2000... Training loss: 0.5719\n",
      "Epoch: 1126/2000... Training loss: 0.5393\n",
      "Epoch: 1126/2000... Training loss: 0.5143\n",
      "Epoch: 1126/2000... Training loss: 0.2960\n",
      "Epoch: 1126/2000... Training loss: 0.2727\n",
      "Epoch: 1126/2000... Training loss: 0.3953\n",
      "Epoch: 1126/2000... Training loss: 0.4361\n",
      "Epoch: 1126/2000... Training loss: 0.3773\n",
      "Epoch: 1126/2000... Training loss: 0.5143\n",
      "Epoch: 1126/2000... Training loss: 0.5687\n",
      "Epoch: 1126/2000... Training loss: 0.3697\n",
      "Epoch: 1126/2000... Training loss: 0.4894\n",
      "Epoch: 1126/2000... Training loss: 0.5207\n",
      "Epoch: 1126/2000... Training loss: 0.2549\n",
      "Epoch: 1126/2000... Training loss: 0.2489\n",
      "Epoch: 1126/2000... Training loss: 0.3423\n",
      "Epoch: 1126/2000... Training loss: 0.4372\n",
      "Epoch: 1126/2000... Training loss: 0.5315\n",
      "Epoch: 1127/2000... Training loss: 0.5501\n",
      "Epoch: 1127/2000... Training loss: 0.4638\n",
      "Epoch: 1127/2000... Training loss: 0.4611\n",
      "Epoch: 1127/2000... Training loss: 0.3872\n",
      "Epoch: 1127/2000... Training loss: 0.3365\n",
      "Epoch: 1127/2000... Training loss: 0.6325\n",
      "Epoch: 1127/2000... Training loss: 0.4019\n",
      "Epoch: 1127/2000... Training loss: 0.3933\n",
      "Epoch: 1127/2000... Training loss: 0.5751\n",
      "Epoch: 1127/2000... Training loss: 0.4325\n",
      "Epoch: 1127/2000... Training loss: 0.4701\n",
      "Epoch: 1127/2000... Training loss: 0.3448\n",
      "Epoch: 1127/2000... Training loss: 0.5520\n",
      "Epoch: 1127/2000... Training loss: 0.3368\n",
      "Epoch: 1127/2000... Training loss: 0.4414\n",
      "Epoch: 1127/2000... Training loss: 0.4526\n",
      "Epoch: 1127/2000... Training loss: 0.3769\n",
      "Epoch: 1127/2000... Training loss: 0.4728\n",
      "Epoch: 1127/2000... Training loss: 0.4637\n",
      "Epoch: 1127/2000... Training loss: 0.3532\n",
      "Epoch: 1127/2000... Training loss: 0.4284\n",
      "Epoch: 1127/2000... Training loss: 0.3632\n",
      "Epoch: 1127/2000... Training loss: 0.6778\n",
      "Epoch: 1127/2000... Training loss: 0.4756\n",
      "Epoch: 1127/2000... Training loss: 0.3895\n",
      "Epoch: 1127/2000... Training loss: 0.4094\n",
      "Epoch: 1127/2000... Training loss: 0.4620\n",
      "Epoch: 1127/2000... Training loss: 0.4000\n",
      "Epoch: 1127/2000... Training loss: 0.5605\n",
      "Epoch: 1127/2000... Training loss: 0.4186\n",
      "Epoch: 1127/2000... Training loss: 0.4944\n",
      "Epoch: 1128/2000... Training loss: 0.3523\n",
      "Epoch: 1128/2000... Training loss: 0.5086\n",
      "Epoch: 1128/2000... Training loss: 0.5670\n",
      "Epoch: 1128/2000... Training loss: 0.5283\n",
      "Epoch: 1128/2000... Training loss: 0.4682\n",
      "Epoch: 1128/2000... Training loss: 0.4157\n",
      "Epoch: 1128/2000... Training loss: 0.5050\n",
      "Epoch: 1128/2000... Training loss: 0.4869\n",
      "Epoch: 1128/2000... Training loss: 0.4366\n",
      "Epoch: 1128/2000... Training loss: 0.4883\n",
      "Epoch: 1128/2000... Training loss: 0.4388\n",
      "Epoch: 1128/2000... Training loss: 0.5663\n",
      "Epoch: 1128/2000... Training loss: 0.4034\n",
      "Epoch: 1128/2000... Training loss: 0.6404\n",
      "Epoch: 1128/2000... Training loss: 0.3196\n",
      "Epoch: 1128/2000... Training loss: 0.4306\n",
      "Epoch: 1128/2000... Training loss: 0.3965\n",
      "Epoch: 1128/2000... Training loss: 0.5000\n",
      "Epoch: 1128/2000... Training loss: 0.6223\n",
      "Epoch: 1128/2000... Training loss: 0.5434\n",
      "Epoch: 1128/2000... Training loss: 0.3816\n",
      "Epoch: 1128/2000... Training loss: 0.4840\n",
      "Epoch: 1128/2000... Training loss: 0.5342\n",
      "Epoch: 1128/2000... Training loss: 0.5472\n",
      "Epoch: 1128/2000... Training loss: 0.5061\n",
      "Epoch: 1128/2000... Training loss: 0.5394\n",
      "Epoch: 1128/2000... Training loss: 0.4958\n",
      "Epoch: 1128/2000... Training loss: 0.5403\n",
      "Epoch: 1128/2000... Training loss: 0.3183\n",
      "Epoch: 1128/2000... Training loss: 0.4213\n",
      "Epoch: 1128/2000... Training loss: 0.5190\n",
      "Epoch: 1129/2000... Training loss: 0.4339\n",
      "Epoch: 1129/2000... Training loss: 0.4775\n",
      "Epoch: 1129/2000... Training loss: 0.3895\n",
      "Epoch: 1129/2000... Training loss: 0.3141\n",
      "Epoch: 1129/2000... Training loss: 0.4071\n",
      "Epoch: 1129/2000... Training loss: 0.4765\n",
      "Epoch: 1129/2000... Training loss: 0.3368\n",
      "Epoch: 1129/2000... Training loss: 0.5294\n",
      "Epoch: 1129/2000... Training loss: 0.4115\n",
      "Epoch: 1129/2000... Training loss: 0.6085\n",
      "Epoch: 1129/2000... Training loss: 0.3199\n",
      "Epoch: 1129/2000... Training loss: 0.3713\n",
      "Epoch: 1129/2000... Training loss: 0.4500\n",
      "Epoch: 1129/2000... Training loss: 0.4797\n",
      "Epoch: 1129/2000... Training loss: 0.3373\n",
      "Epoch: 1129/2000... Training loss: 0.5244\n",
      "Epoch: 1129/2000... Training loss: 0.4784\n",
      "Epoch: 1129/2000... Training loss: 0.3919\n",
      "Epoch: 1129/2000... Training loss: 0.5314\n",
      "Epoch: 1129/2000... Training loss: 0.4752\n",
      "Epoch: 1129/2000... Training loss: 0.5673\n",
      "Epoch: 1129/2000... Training loss: 0.4165\n",
      "Epoch: 1129/2000... Training loss: 0.4358\n",
      "Epoch: 1129/2000... Training loss: 0.4322\n",
      "Epoch: 1129/2000... Training loss: 0.4769\n",
      "Epoch: 1129/2000... Training loss: 0.3629\n",
      "Epoch: 1129/2000... Training loss: 0.4771\n",
      "Epoch: 1129/2000... Training loss: 0.2842\n",
      "Epoch: 1129/2000... Training loss: 0.4461\n",
      "Epoch: 1129/2000... Training loss: 0.4440\n",
      "Epoch: 1129/2000... Training loss: 0.3962\n",
      "Epoch: 1130/2000... Training loss: 0.4461\n",
      "Epoch: 1130/2000... Training loss: 0.3936\n",
      "Epoch: 1130/2000... Training loss: 0.4090\n",
      "Epoch: 1130/2000... Training loss: 0.3333\n",
      "Epoch: 1130/2000... Training loss: 0.4347\n",
      "Epoch: 1130/2000... Training loss: 0.5508\n",
      "Epoch: 1130/2000... Training loss: 0.5437\n",
      "Epoch: 1130/2000... Training loss: 0.5490\n",
      "Epoch: 1130/2000... Training loss: 0.7570\n",
      "Epoch: 1130/2000... Training loss: 0.4586\n",
      "Epoch: 1130/2000... Training loss: 0.4389\n",
      "Epoch: 1130/2000... Training loss: 0.3311\n",
      "Epoch: 1130/2000... Training loss: 0.4945\n",
      "Epoch: 1130/2000... Training loss: 0.5091\n",
      "Epoch: 1130/2000... Training loss: 0.4668\n",
      "Epoch: 1130/2000... Training loss: 0.4998\n",
      "Epoch: 1130/2000... Training loss: 0.6849\n",
      "Epoch: 1130/2000... Training loss: 0.4503\n",
      "Epoch: 1130/2000... Training loss: 0.4778\n",
      "Epoch: 1130/2000... Training loss: 0.4500\n",
      "Epoch: 1130/2000... Training loss: 0.3310\n",
      "Epoch: 1130/2000... Training loss: 0.6441\n",
      "Epoch: 1130/2000... Training loss: 0.4097\n",
      "Epoch: 1130/2000... Training loss: 0.4337\n",
      "Epoch: 1130/2000... Training loss: 0.6153\n",
      "Epoch: 1130/2000... Training loss: 0.6792\n",
      "Epoch: 1130/2000... Training loss: 0.3718\n",
      "Epoch: 1130/2000... Training loss: 0.5240\n",
      "Epoch: 1130/2000... Training loss: 0.4839\n",
      "Epoch: 1130/2000... Training loss: 0.6108\n",
      "Epoch: 1130/2000... Training loss: 0.5084\n",
      "Epoch: 1131/2000... Training loss: 0.4050\n",
      "Epoch: 1131/2000... Training loss: 0.4621\n",
      "Epoch: 1131/2000... Training loss: 0.5165\n",
      "Epoch: 1131/2000... Training loss: 0.6799\n",
      "Epoch: 1131/2000... Training loss: 0.4778\n",
      "Epoch: 1131/2000... Training loss: 0.3974\n",
      "Epoch: 1131/2000... Training loss: 0.4803\n",
      "Epoch: 1131/2000... Training loss: 0.4291\n",
      "Epoch: 1131/2000... Training loss: 0.3937\n",
      "Epoch: 1131/2000... Training loss: 0.4323\n",
      "Epoch: 1131/2000... Training loss: 0.6070\n",
      "Epoch: 1131/2000... Training loss: 0.3569\n",
      "Epoch: 1131/2000... Training loss: 0.3490\n",
      "Epoch: 1131/2000... Training loss: 0.3344\n",
      "Epoch: 1131/2000... Training loss: 0.4242\n",
      "Epoch: 1131/2000... Training loss: 0.4736\n",
      "Epoch: 1131/2000... Training loss: 0.4015\n",
      "Epoch: 1131/2000... Training loss: 0.5278\n",
      "Epoch: 1131/2000... Training loss: 0.4696\n",
      "Epoch: 1131/2000... Training loss: 0.5013\n",
      "Epoch: 1131/2000... Training loss: 0.5313\n",
      "Epoch: 1131/2000... Training loss: 0.3200\n",
      "Epoch: 1131/2000... Training loss: 0.5618\n",
      "Epoch: 1131/2000... Training loss: 0.5281\n",
      "Epoch: 1131/2000... Training loss: 0.6135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1131/2000... Training loss: 0.3723\n",
      "Epoch: 1131/2000... Training loss: 0.4469\n",
      "Epoch: 1131/2000... Training loss: 0.4491\n",
      "Epoch: 1131/2000... Training loss: 0.6055\n",
      "Epoch: 1131/2000... Training loss: 0.4013\n",
      "Epoch: 1131/2000... Training loss: 0.4205\n",
      "Epoch: 1132/2000... Training loss: 0.4194\n",
      "Epoch: 1132/2000... Training loss: 0.4764\n",
      "Epoch: 1132/2000... Training loss: 0.4298\n",
      "Epoch: 1132/2000... Training loss: 0.6381\n",
      "Epoch: 1132/2000... Training loss: 0.6015\n",
      "Epoch: 1132/2000... Training loss: 0.5608\n",
      "Epoch: 1132/2000... Training loss: 0.4583\n",
      "Epoch: 1132/2000... Training loss: 0.5018\n",
      "Epoch: 1132/2000... Training loss: 0.4557\n",
      "Epoch: 1132/2000... Training loss: 0.4159\n",
      "Epoch: 1132/2000... Training loss: 0.4457\n",
      "Epoch: 1132/2000... Training loss: 0.3852\n",
      "Epoch: 1132/2000... Training loss: 0.4259\n",
      "Epoch: 1132/2000... Training loss: 0.3386\n",
      "Epoch: 1132/2000... Training loss: 0.3580\n",
      "Epoch: 1132/2000... Training loss: 0.4466\n",
      "Epoch: 1132/2000... Training loss: 0.5041\n",
      "Epoch: 1132/2000... Training loss: 0.4945\n",
      "Epoch: 1132/2000... Training loss: 0.4910\n",
      "Epoch: 1132/2000... Training loss: 0.3734\n",
      "Epoch: 1132/2000... Training loss: 0.5091\n",
      "Epoch: 1132/2000... Training loss: 0.4903\n",
      "Epoch: 1132/2000... Training loss: 0.5645\n",
      "Epoch: 1132/2000... Training loss: 0.5294\n",
      "Epoch: 1132/2000... Training loss: 0.6216\n",
      "Epoch: 1132/2000... Training loss: 0.5000\n",
      "Epoch: 1132/2000... Training loss: 0.5468\n",
      "Epoch: 1132/2000... Training loss: 0.4358\n",
      "Epoch: 1132/2000... Training loss: 0.3979\n",
      "Epoch: 1132/2000... Training loss: 0.3825\n",
      "Epoch: 1132/2000... Training loss: 0.4285\n",
      "Epoch: 1133/2000... Training loss: 0.7524\n",
      "Epoch: 1133/2000... Training loss: 0.5698\n",
      "Epoch: 1133/2000... Training loss: 0.4610\n",
      "Epoch: 1133/2000... Training loss: 0.4719\n",
      "Epoch: 1133/2000... Training loss: 0.4542\n",
      "Epoch: 1133/2000... Training loss: 0.3327\n",
      "Epoch: 1133/2000... Training loss: 0.2891\n",
      "Epoch: 1133/2000... Training loss: 0.5705\n",
      "Epoch: 1133/2000... Training loss: 0.4722\n",
      "Epoch: 1133/2000... Training loss: 0.5167\n",
      "Epoch: 1133/2000... Training loss: 0.7140\n",
      "Epoch: 1133/2000... Training loss: 0.5417\n",
      "Epoch: 1133/2000... Training loss: 0.6873\n",
      "Epoch: 1133/2000... Training loss: 0.4990\n",
      "Epoch: 1133/2000... Training loss: 0.4645\n",
      "Epoch: 1133/2000... Training loss: 0.4683\n",
      "Epoch: 1133/2000... Training loss: 0.3596\n",
      "Epoch: 1133/2000... Training loss: 0.5409\n",
      "Epoch: 1133/2000... Training loss: 0.5195\n",
      "Epoch: 1133/2000... Training loss: 0.4986\n",
      "Epoch: 1133/2000... Training loss: 0.5259\n",
      "Epoch: 1133/2000... Training loss: 0.5075\n",
      "Epoch: 1133/2000... Training loss: 0.4540\n",
      "Epoch: 1133/2000... Training loss: 0.4183\n",
      "Epoch: 1133/2000... Training loss: 0.4184\n",
      "Epoch: 1133/2000... Training loss: 0.6705\n",
      "Epoch: 1133/2000... Training loss: 0.2929\n",
      "Epoch: 1133/2000... Training loss: 0.4443\n",
      "Epoch: 1133/2000... Training loss: 0.3979\n",
      "Epoch: 1133/2000... Training loss: 0.4450\n",
      "Epoch: 1133/2000... Training loss: 0.6294\n",
      "Epoch: 1134/2000... Training loss: 0.6216\n",
      "Epoch: 1134/2000... Training loss: 0.6934\n",
      "Epoch: 1134/2000... Training loss: 0.3730\n",
      "Epoch: 1134/2000... Training loss: 0.3993\n",
      "Epoch: 1134/2000... Training loss: 0.4843\n",
      "Epoch: 1134/2000... Training loss: 0.6691\n",
      "Epoch: 1134/2000... Training loss: 0.3594\n",
      "Epoch: 1134/2000... Training loss: 0.2899\n",
      "Epoch: 1134/2000... Training loss: 0.5005\n",
      "Epoch: 1134/2000... Training loss: 0.4322\n",
      "Epoch: 1134/2000... Training loss: 0.4248\n",
      "Epoch: 1134/2000... Training loss: 0.4303\n",
      "Epoch: 1134/2000... Training loss: 0.4445\n",
      "Epoch: 1134/2000... Training loss: 0.5086\n",
      "Epoch: 1134/2000... Training loss: 0.4315\n",
      "Epoch: 1134/2000... Training loss: 0.3584\n",
      "Epoch: 1134/2000... Training loss: 0.4677\n",
      "Epoch: 1134/2000... Training loss: 0.5187\n",
      "Epoch: 1134/2000... Training loss: 0.4075\n",
      "Epoch: 1134/2000... Training loss: 0.3504\n",
      "Epoch: 1134/2000... Training loss: 0.6220\n",
      "Epoch: 1134/2000... Training loss: 0.6408\n",
      "Epoch: 1134/2000... Training loss: 0.5191\n",
      "Epoch: 1134/2000... Training loss: 0.3624\n",
      "Epoch: 1134/2000... Training loss: 0.5000\n",
      "Epoch: 1134/2000... Training loss: 0.6174\n",
      "Epoch: 1134/2000... Training loss: 0.3224\n",
      "Epoch: 1134/2000... Training loss: 0.4640\n",
      "Epoch: 1134/2000... Training loss: 0.4109\n",
      "Epoch: 1134/2000... Training loss: 0.5911\n",
      "Epoch: 1134/2000... Training loss: 0.3765\n",
      "Epoch: 1135/2000... Training loss: 0.5100\n",
      "Epoch: 1135/2000... Training loss: 0.3784\n",
      "Epoch: 1135/2000... Training loss: 0.4032\n",
      "Epoch: 1135/2000... Training loss: 0.5484\n",
      "Epoch: 1135/2000... Training loss: 0.4315\n",
      "Epoch: 1135/2000... Training loss: 0.6756\n",
      "Epoch: 1135/2000... Training loss: 0.5792\n",
      "Epoch: 1135/2000... Training loss: 0.5030\n",
      "Epoch: 1135/2000... Training loss: 0.4461\n",
      "Epoch: 1135/2000... Training loss: 0.5165\n",
      "Epoch: 1135/2000... Training loss: 0.4321\n",
      "Epoch: 1135/2000... Training loss: 0.4887\n",
      "Epoch: 1135/2000... Training loss: 0.4871\n",
      "Epoch: 1135/2000... Training loss: 0.5521\n",
      "Epoch: 1135/2000... Training loss: 0.3193\n",
      "Epoch: 1135/2000... Training loss: 0.6822\n",
      "Epoch: 1135/2000... Training loss: 0.3822\n",
      "Epoch: 1135/2000... Training loss: 0.3779\n",
      "Epoch: 1135/2000... Training loss: 0.5350\n",
      "Epoch: 1135/2000... Training loss: 0.4450\n",
      "Epoch: 1135/2000... Training loss: 0.4925\n",
      "Epoch: 1135/2000... Training loss: 0.5477\n",
      "Epoch: 1135/2000... Training loss: 0.4329\n",
      "Epoch: 1135/2000... Training loss: 0.5241\n",
      "Epoch: 1135/2000... Training loss: 0.3264\n",
      "Epoch: 1135/2000... Training loss: 0.5419\n",
      "Epoch: 1135/2000... Training loss: 0.7529\n",
      "Epoch: 1135/2000... Training loss: 0.6043\n",
      "Epoch: 1135/2000... Training loss: 0.6774\n",
      "Epoch: 1135/2000... Training loss: 0.7294\n",
      "Epoch: 1135/2000... Training loss: 0.5573\n",
      "Epoch: 1136/2000... Training loss: 0.3921\n",
      "Epoch: 1136/2000... Training loss: 0.5976\n",
      "Epoch: 1136/2000... Training loss: 0.7825\n",
      "Epoch: 1136/2000... Training loss: 0.5154\n",
      "Epoch: 1136/2000... Training loss: 0.4063\n",
      "Epoch: 1136/2000... Training loss: 0.4793\n",
      "Epoch: 1136/2000... Training loss: 0.3003\n",
      "Epoch: 1136/2000... Training loss: 0.4359\n",
      "Epoch: 1136/2000... Training loss: 0.5241\n",
      "Epoch: 1136/2000... Training loss: 0.4764\n",
      "Epoch: 1136/2000... Training loss: 0.5295\n",
      "Epoch: 1136/2000... Training loss: 0.6265\n",
      "Epoch: 1136/2000... Training loss: 0.5401\n",
      "Epoch: 1136/2000... Training loss: 0.4481\n",
      "Epoch: 1136/2000... Training loss: 0.4102\n",
      "Epoch: 1136/2000... Training loss: 0.4042\n",
      "Epoch: 1136/2000... Training loss: 0.3887\n",
      "Epoch: 1136/2000... Training loss: 0.4114\n",
      "Epoch: 1136/2000... Training loss: 0.3857\n",
      "Epoch: 1136/2000... Training loss: 0.5680\n",
      "Epoch: 1136/2000... Training loss: 0.4882\n",
      "Epoch: 1136/2000... Training loss: 0.5092\n",
      "Epoch: 1136/2000... Training loss: 0.5301\n",
      "Epoch: 1136/2000... Training loss: 0.6209\n",
      "Epoch: 1136/2000... Training loss: 0.5322\n",
      "Epoch: 1136/2000... Training loss: 0.5309\n",
      "Epoch: 1136/2000... Training loss: 0.5772\n",
      "Epoch: 1136/2000... Training loss: 0.5823\n",
      "Epoch: 1136/2000... Training loss: 0.3977\n",
      "Epoch: 1136/2000... Training loss: 0.3990\n",
      "Epoch: 1136/2000... Training loss: 0.5619\n",
      "Epoch: 1137/2000... Training loss: 0.5893\n",
      "Epoch: 1137/2000... Training loss: 0.7150\n",
      "Epoch: 1137/2000... Training loss: 0.4777\n",
      "Epoch: 1137/2000... Training loss: 0.4753\n",
      "Epoch: 1137/2000... Training loss: 0.4366\n",
      "Epoch: 1137/2000... Training loss: 0.5271\n",
      "Epoch: 1137/2000... Training loss: 0.3605\n",
      "Epoch: 1137/2000... Training loss: 0.4805\n",
      "Epoch: 1137/2000... Training loss: 0.2973\n",
      "Epoch: 1137/2000... Training loss: 0.5027\n",
      "Epoch: 1137/2000... Training loss: 0.6444\n",
      "Epoch: 1137/2000... Training loss: 0.5790\n",
      "Epoch: 1137/2000... Training loss: 0.4227\n",
      "Epoch: 1137/2000... Training loss: 0.5440\n",
      "Epoch: 1137/2000... Training loss: 0.5503\n",
      "Epoch: 1137/2000... Training loss: 0.4212\n",
      "Epoch: 1137/2000... Training loss: 0.3728\n",
      "Epoch: 1137/2000... Training loss: 0.7202\n",
      "Epoch: 1137/2000... Training loss: 0.4194\n",
      "Epoch: 1137/2000... Training loss: 0.3948\n",
      "Epoch: 1137/2000... Training loss: 0.4195\n",
      "Epoch: 1137/2000... Training loss: 0.4709\n",
      "Epoch: 1137/2000... Training loss: 0.4813\n",
      "Epoch: 1137/2000... Training loss: 0.4053\n",
      "Epoch: 1137/2000... Training loss: 0.5231\n",
      "Epoch: 1137/2000... Training loss: 0.6363\n",
      "Epoch: 1137/2000... Training loss: 0.4358\n",
      "Epoch: 1137/2000... Training loss: 0.4610\n",
      "Epoch: 1137/2000... Training loss: 0.6535\n",
      "Epoch: 1137/2000... Training loss: 0.4604\n",
      "Epoch: 1137/2000... Training loss: 0.6943\n",
      "Epoch: 1138/2000... Training loss: 0.4883\n",
      "Epoch: 1138/2000... Training loss: 0.5532\n",
      "Epoch: 1138/2000... Training loss: 0.5928\n",
      "Epoch: 1138/2000... Training loss: 0.5334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1138/2000... Training loss: 0.4560\n",
      "Epoch: 1138/2000... Training loss: 0.2469\n",
      "Epoch: 1138/2000... Training loss: 0.3701\n",
      "Epoch: 1138/2000... Training loss: 0.3811\n",
      "Epoch: 1138/2000... Training loss: 0.3793\n",
      "Epoch: 1138/2000... Training loss: 0.5119\n",
      "Epoch: 1138/2000... Training loss: 0.3883\n",
      "Epoch: 1138/2000... Training loss: 0.4344\n",
      "Epoch: 1138/2000... Training loss: 0.3309\n",
      "Epoch: 1138/2000... Training loss: 0.4692\n",
      "Epoch: 1138/2000... Training loss: 0.4216\n",
      "Epoch: 1138/2000... Training loss: 0.3902\n",
      "Epoch: 1138/2000... Training loss: 0.4997\n",
      "Epoch: 1138/2000... Training loss: 0.5429\n",
      "Epoch: 1138/2000... Training loss: 0.5234\n",
      "Epoch: 1138/2000... Training loss: 0.5848\n",
      "Epoch: 1138/2000... Training loss: 0.5458\n",
      "Epoch: 1138/2000... Training loss: 0.6166\n",
      "Epoch: 1138/2000... Training loss: 0.4280\n",
      "Epoch: 1138/2000... Training loss: 0.3899\n",
      "Epoch: 1138/2000... Training loss: 0.6409\n",
      "Epoch: 1138/2000... Training loss: 0.5480\n",
      "Epoch: 1138/2000... Training loss: 0.4231\n",
      "Epoch: 1138/2000... Training loss: 0.5271\n",
      "Epoch: 1138/2000... Training loss: 0.5616\n",
      "Epoch: 1138/2000... Training loss: 0.3451\n",
      "Epoch: 1138/2000... Training loss: 0.6651\n",
      "Epoch: 1139/2000... Training loss: 0.5080\n",
      "Epoch: 1139/2000... Training loss: 0.5131\n",
      "Epoch: 1139/2000... Training loss: 0.3447\n",
      "Epoch: 1139/2000... Training loss: 0.3001\n",
      "Epoch: 1139/2000... Training loss: 0.4769\n",
      "Epoch: 1139/2000... Training loss: 0.3878\n",
      "Epoch: 1139/2000... Training loss: 0.5946\n",
      "Epoch: 1139/2000... Training loss: 0.3823\n",
      "Epoch: 1139/2000... Training loss: 0.4024\n",
      "Epoch: 1139/2000... Training loss: 0.4709\n",
      "Epoch: 1139/2000... Training loss: 0.5415\n",
      "Epoch: 1139/2000... Training loss: 0.3912\n",
      "Epoch: 1139/2000... Training loss: 0.4925\n",
      "Epoch: 1139/2000... Training loss: 0.5042\n",
      "Epoch: 1139/2000... Training loss: 0.2491\n",
      "Epoch: 1139/2000... Training loss: 0.3249\n",
      "Epoch: 1139/2000... Training loss: 0.4581\n",
      "Epoch: 1139/2000... Training loss: 0.3065\n",
      "Epoch: 1139/2000... Training loss: 0.5551\n",
      "Epoch: 1139/2000... Training loss: 0.5655\n",
      "Epoch: 1139/2000... Training loss: 0.4302\n",
      "Epoch: 1139/2000... Training loss: 0.4252\n",
      "Epoch: 1139/2000... Training loss: 0.7477\n",
      "Epoch: 1139/2000... Training loss: 0.4993\n",
      "Epoch: 1139/2000... Training loss: 0.6331\n",
      "Epoch: 1139/2000... Training loss: 0.4219\n",
      "Epoch: 1139/2000... Training loss: 0.3881\n",
      "Epoch: 1139/2000... Training loss: 0.4882\n",
      "Epoch: 1139/2000... Training loss: 0.3674\n",
      "Epoch: 1139/2000... Training loss: 0.4008\n",
      "Epoch: 1139/2000... Training loss: 0.3971\n",
      "Epoch: 1140/2000... Training loss: 0.7014\n",
      "Epoch: 1140/2000... Training loss: 0.5485\n",
      "Epoch: 1140/2000... Training loss: 0.5707\n",
      "Epoch: 1140/2000... Training loss: 0.6879\n",
      "Epoch: 1140/2000... Training loss: 0.4216\n",
      "Epoch: 1140/2000... Training loss: 0.5566\n",
      "Epoch: 1140/2000... Training loss: 0.5998\n",
      "Epoch: 1140/2000... Training loss: 0.5352\n",
      "Epoch: 1140/2000... Training loss: 0.5670\n",
      "Epoch: 1140/2000... Training loss: 0.4499\n",
      "Epoch: 1140/2000... Training loss: 0.6946\n",
      "Epoch: 1140/2000... Training loss: 0.5336\n",
      "Epoch: 1140/2000... Training loss: 0.4026\n",
      "Epoch: 1140/2000... Training loss: 0.5504\n",
      "Epoch: 1140/2000... Training loss: 0.4671\n",
      "Epoch: 1140/2000... Training loss: 0.4463\n",
      "Epoch: 1140/2000... Training loss: 0.4081\n",
      "Epoch: 1140/2000... Training loss: 0.5886\n",
      "Epoch: 1140/2000... Training loss: 0.3629\n",
      "Epoch: 1140/2000... Training loss: 0.4722\n",
      "Epoch: 1140/2000... Training loss: 0.4239\n",
      "Epoch: 1140/2000... Training loss: 0.4227\n",
      "Epoch: 1140/2000... Training loss: 0.3229\n",
      "Epoch: 1140/2000... Training loss: 0.6577\n",
      "Epoch: 1140/2000... Training loss: 0.7511\n",
      "Epoch: 1140/2000... Training loss: 0.3863\n",
      "Epoch: 1140/2000... Training loss: 0.6010\n",
      "Epoch: 1140/2000... Training loss: 0.4385\n",
      "Epoch: 1140/2000... Training loss: 0.6452\n",
      "Epoch: 1140/2000... Training loss: 0.4471\n",
      "Epoch: 1140/2000... Training loss: 0.3172\n",
      "Epoch: 1141/2000... Training loss: 0.4980\n",
      "Epoch: 1141/2000... Training loss: 0.5554\n",
      "Epoch: 1141/2000... Training loss: 0.5833\n",
      "Epoch: 1141/2000... Training loss: 0.3644\n",
      "Epoch: 1141/2000... Training loss: 0.4081\n",
      "Epoch: 1141/2000... Training loss: 0.5842\n",
      "Epoch: 1141/2000... Training loss: 0.6548\n",
      "Epoch: 1141/2000... Training loss: 0.3956\n",
      "Epoch: 1141/2000... Training loss: 0.5547\n",
      "Epoch: 1141/2000... Training loss: 0.5507\n",
      "Epoch: 1141/2000... Training loss: 0.4421\n",
      "Epoch: 1141/2000... Training loss: 0.5342\n",
      "Epoch: 1141/2000... Training loss: 0.3687\n",
      "Epoch: 1141/2000... Training loss: 0.5047\n",
      "Epoch: 1141/2000... Training loss: 0.4438\n",
      "Epoch: 1141/2000... Training loss: 0.4253\n",
      "Epoch: 1141/2000... Training loss: 0.3955\n",
      "Epoch: 1141/2000... Training loss: 0.6626\n",
      "Epoch: 1141/2000... Training loss: 0.5231\n",
      "Epoch: 1141/2000... Training loss: 0.4902\n",
      "Epoch: 1141/2000... Training loss: 0.4815\n",
      "Epoch: 1141/2000... Training loss: 0.4998\n",
      "Epoch: 1141/2000... Training loss: 0.4984\n",
      "Epoch: 1141/2000... Training loss: 0.3940\n",
      "Epoch: 1141/2000... Training loss: 0.5037\n",
      "Epoch: 1141/2000... Training loss: 0.5378\n",
      "Epoch: 1141/2000... Training loss: 0.3346\n",
      "Epoch: 1141/2000... Training loss: 0.2995\n",
      "Epoch: 1141/2000... Training loss: 0.3669\n",
      "Epoch: 1141/2000... Training loss: 0.4746\n",
      "Epoch: 1141/2000... Training loss: 0.5384\n",
      "Epoch: 1142/2000... Training loss: 0.4126\n",
      "Epoch: 1142/2000... Training loss: 0.4225\n",
      "Epoch: 1142/2000... Training loss: 0.3861\n",
      "Epoch: 1142/2000... Training loss: 0.6963\n",
      "Epoch: 1142/2000... Training loss: 0.3605\n",
      "Epoch: 1142/2000... Training loss: 0.4541\n",
      "Epoch: 1142/2000... Training loss: 0.5316\n",
      "Epoch: 1142/2000... Training loss: 0.3735\n",
      "Epoch: 1142/2000... Training loss: 0.5240\n",
      "Epoch: 1142/2000... Training loss: 0.3247\n",
      "Epoch: 1142/2000... Training loss: 0.5592\n",
      "Epoch: 1142/2000... Training loss: 0.4875\n",
      "Epoch: 1142/2000... Training loss: 0.4116\n",
      "Epoch: 1142/2000... Training loss: 0.4821\n",
      "Epoch: 1142/2000... Training loss: 0.5465\n",
      "Epoch: 1142/2000... Training loss: 0.4778\n",
      "Epoch: 1142/2000... Training loss: 0.6036\n",
      "Epoch: 1142/2000... Training loss: 0.6433\n",
      "Epoch: 1142/2000... Training loss: 0.4899\n",
      "Epoch: 1142/2000... Training loss: 0.4047\n",
      "Epoch: 1142/2000... Training loss: 0.3386\n",
      "Epoch: 1142/2000... Training loss: 0.5142\n",
      "Epoch: 1142/2000... Training loss: 0.5544\n",
      "Epoch: 1142/2000... Training loss: 0.5074\n",
      "Epoch: 1142/2000... Training loss: 0.7219\n",
      "Epoch: 1142/2000... Training loss: 0.4975\n",
      "Epoch: 1142/2000... Training loss: 0.3571\n",
      "Epoch: 1142/2000... Training loss: 0.4273\n",
      "Epoch: 1142/2000... Training loss: 0.4304\n",
      "Epoch: 1142/2000... Training loss: 0.3415\n",
      "Epoch: 1142/2000... Training loss: 0.3434\n",
      "Epoch: 1143/2000... Training loss: 0.4487\n",
      "Epoch: 1143/2000... Training loss: 0.4126\n",
      "Epoch: 1143/2000... Training loss: 0.5059\n",
      "Epoch: 1143/2000... Training loss: 0.3944\n",
      "Epoch: 1143/2000... Training loss: 0.4973\n",
      "Epoch: 1143/2000... Training loss: 0.3287\n",
      "Epoch: 1143/2000... Training loss: 0.4443\n",
      "Epoch: 1143/2000... Training loss: 0.6234\n",
      "Epoch: 1143/2000... Training loss: 0.5266\n",
      "Epoch: 1143/2000... Training loss: 0.5408\n",
      "Epoch: 1143/2000... Training loss: 0.4153\n",
      "Epoch: 1143/2000... Training loss: 0.5007\n",
      "Epoch: 1143/2000... Training loss: 0.4586\n",
      "Epoch: 1143/2000... Training loss: 0.5574\n",
      "Epoch: 1143/2000... Training loss: 0.4490\n",
      "Epoch: 1143/2000... Training loss: 0.4717\n",
      "Epoch: 1143/2000... Training loss: 0.4198\n",
      "Epoch: 1143/2000... Training loss: 0.5697\n",
      "Epoch: 1143/2000... Training loss: 0.3365\n",
      "Epoch: 1143/2000... Training loss: 0.3653\n",
      "Epoch: 1143/2000... Training loss: 0.3762\n",
      "Epoch: 1143/2000... Training loss: 0.5321\n",
      "Epoch: 1143/2000... Training loss: 0.4848\n",
      "Epoch: 1143/2000... Training loss: 0.5778\n",
      "Epoch: 1143/2000... Training loss: 0.5100\n",
      "Epoch: 1143/2000... Training loss: 0.6297\n",
      "Epoch: 1143/2000... Training loss: 0.4807\n",
      "Epoch: 1143/2000... Training loss: 0.4521\n",
      "Epoch: 1143/2000... Training loss: 0.5205\n",
      "Epoch: 1143/2000... Training loss: 0.3742\n",
      "Epoch: 1143/2000... Training loss: 0.4746\n",
      "Epoch: 1144/2000... Training loss: 0.4583\n",
      "Epoch: 1144/2000... Training loss: 0.4029\n",
      "Epoch: 1144/2000... Training loss: 0.7252\n",
      "Epoch: 1144/2000... Training loss: 0.6533\n",
      "Epoch: 1144/2000... Training loss: 0.4588\n",
      "Epoch: 1144/2000... Training loss: 0.6431\n",
      "Epoch: 1144/2000... Training loss: 0.5852\n",
      "Epoch: 1144/2000... Training loss: 0.3622\n",
      "Epoch: 1144/2000... Training loss: 0.5551\n",
      "Epoch: 1144/2000... Training loss: 0.4379\n",
      "Epoch: 1144/2000... Training loss: 0.4604\n",
      "Epoch: 1144/2000... Training loss: 0.4893\n",
      "Epoch: 1144/2000... Training loss: 0.6281\n",
      "Epoch: 1144/2000... Training loss: 0.5107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1144/2000... Training loss: 0.4148\n",
      "Epoch: 1144/2000... Training loss: 0.5001\n",
      "Epoch: 1144/2000... Training loss: 0.4058\n",
      "Epoch: 1144/2000... Training loss: 0.2509\n",
      "Epoch: 1144/2000... Training loss: 0.4423\n",
      "Epoch: 1144/2000... Training loss: 0.4389\n",
      "Epoch: 1144/2000... Training loss: 0.5671\n",
      "Epoch: 1144/2000... Training loss: 0.4845\n",
      "Epoch: 1144/2000... Training loss: 0.6022\n",
      "Epoch: 1144/2000... Training loss: 0.6291\n",
      "Epoch: 1144/2000... Training loss: 0.4294\n",
      "Epoch: 1144/2000... Training loss: 0.5080\n",
      "Epoch: 1144/2000... Training loss: 0.4258\n",
      "Epoch: 1144/2000... Training loss: 0.4769\n",
      "Epoch: 1144/2000... Training loss: 0.4865\n",
      "Epoch: 1144/2000... Training loss: 0.4384\n",
      "Epoch: 1144/2000... Training loss: 0.5538\n",
      "Epoch: 1145/2000... Training loss: 0.2762\n",
      "Epoch: 1145/2000... Training loss: 0.6187\n",
      "Epoch: 1145/2000... Training loss: 0.6798\n",
      "Epoch: 1145/2000... Training loss: 0.4458\n",
      "Epoch: 1145/2000... Training loss: 0.3967\n",
      "Epoch: 1145/2000... Training loss: 0.5024\n",
      "Epoch: 1145/2000... Training loss: 0.4128\n",
      "Epoch: 1145/2000... Training loss: 0.3940\n",
      "Epoch: 1145/2000... Training loss: 0.5879\n",
      "Epoch: 1145/2000... Training loss: 0.7596\n",
      "Epoch: 1145/2000... Training loss: 0.5075\n",
      "Epoch: 1145/2000... Training loss: 0.4999\n",
      "Epoch: 1145/2000... Training loss: 0.3965\n",
      "Epoch: 1145/2000... Training loss: 0.3905\n",
      "Epoch: 1145/2000... Training loss: 0.4610\n",
      "Epoch: 1145/2000... Training loss: 0.4254\n",
      "Epoch: 1145/2000... Training loss: 0.3878\n",
      "Epoch: 1145/2000... Training loss: 0.3686\n",
      "Epoch: 1145/2000... Training loss: 0.4337\n",
      "Epoch: 1145/2000... Training loss: 0.4655\n",
      "Epoch: 1145/2000... Training loss: 0.3559\n",
      "Epoch: 1145/2000... Training loss: 0.3866\n",
      "Epoch: 1145/2000... Training loss: 0.4666\n",
      "Epoch: 1145/2000... Training loss: 0.4966\n",
      "Epoch: 1145/2000... Training loss: 0.3885\n",
      "Epoch: 1145/2000... Training loss: 0.5422\n",
      "Epoch: 1145/2000... Training loss: 0.4528\n",
      "Epoch: 1145/2000... Training loss: 0.4461\n",
      "Epoch: 1145/2000... Training loss: 0.5342\n",
      "Epoch: 1145/2000... Training loss: 0.6932\n",
      "Epoch: 1145/2000... Training loss: 0.3738\n",
      "Epoch: 1146/2000... Training loss: 0.4070\n",
      "Epoch: 1146/2000... Training loss: 0.4382\n",
      "Epoch: 1146/2000... Training loss: 0.4272\n",
      "Epoch: 1146/2000... Training loss: 0.5357\n",
      "Epoch: 1146/2000... Training loss: 0.4843\n",
      "Epoch: 1146/2000... Training loss: 0.4782\n",
      "Epoch: 1146/2000... Training loss: 0.5661\n",
      "Epoch: 1146/2000... Training loss: 0.5550\n",
      "Epoch: 1146/2000... Training loss: 0.4401\n",
      "Epoch: 1146/2000... Training loss: 0.5526\n",
      "Epoch: 1146/2000... Training loss: 0.4789\n",
      "Epoch: 1146/2000... Training loss: 0.5388\n",
      "Epoch: 1146/2000... Training loss: 0.4634\n",
      "Epoch: 1146/2000... Training loss: 0.4902\n",
      "Epoch: 1146/2000... Training loss: 0.4080\n",
      "Epoch: 1146/2000... Training loss: 0.4092\n",
      "Epoch: 1146/2000... Training loss: 0.5647\n",
      "Epoch: 1146/2000... Training loss: 0.4449\n",
      "Epoch: 1146/2000... Training loss: 0.4387\n",
      "Epoch: 1146/2000... Training loss: 0.5261\n",
      "Epoch: 1146/2000... Training loss: 0.2757\n",
      "Epoch: 1146/2000... Training loss: 0.2404\n",
      "Epoch: 1146/2000... Training loss: 0.5287\n",
      "Epoch: 1146/2000... Training loss: 0.5146\n",
      "Epoch: 1146/2000... Training loss: 0.4968\n",
      "Epoch: 1146/2000... Training loss: 0.5112\n",
      "Epoch: 1146/2000... Training loss: 0.4726\n",
      "Epoch: 1146/2000... Training loss: 0.6857\n",
      "Epoch: 1146/2000... Training loss: 0.4923\n",
      "Epoch: 1146/2000... Training loss: 0.3442\n",
      "Epoch: 1146/2000... Training loss: 0.3272\n",
      "Epoch: 1147/2000... Training loss: 0.5432\n",
      "Epoch: 1147/2000... Training loss: 0.3925\n",
      "Epoch: 1147/2000... Training loss: 0.3494\n",
      "Epoch: 1147/2000... Training loss: 0.2857\n",
      "Epoch: 1147/2000... Training loss: 0.5727\n",
      "Epoch: 1147/2000... Training loss: 0.3153\n",
      "Epoch: 1147/2000... Training loss: 0.4841\n",
      "Epoch: 1147/2000... Training loss: 0.4348\n",
      "Epoch: 1147/2000... Training loss: 0.3329\n",
      "Epoch: 1147/2000... Training loss: 0.3855\n",
      "Epoch: 1147/2000... Training loss: 0.5326\n",
      "Epoch: 1147/2000... Training loss: 0.2409\n",
      "Epoch: 1147/2000... Training loss: 0.4811\n",
      "Epoch: 1147/2000... Training loss: 0.4560\n",
      "Epoch: 1147/2000... Training loss: 0.5398\n",
      "Epoch: 1147/2000... Training loss: 0.4833\n",
      "Epoch: 1147/2000... Training loss: 0.3295\n",
      "Epoch: 1147/2000... Training loss: 0.5131\n",
      "Epoch: 1147/2000... Training loss: 0.5341\n",
      "Epoch: 1147/2000... Training loss: 0.4045\n",
      "Epoch: 1147/2000... Training loss: 0.3044\n",
      "Epoch: 1147/2000... Training loss: 0.4348\n",
      "Epoch: 1147/2000... Training loss: 0.4778\n",
      "Epoch: 1147/2000... Training loss: 0.4472\n",
      "Epoch: 1147/2000... Training loss: 0.5704\n",
      "Epoch: 1147/2000... Training loss: 0.6579\n",
      "Epoch: 1147/2000... Training loss: 0.4800\n",
      "Epoch: 1147/2000... Training loss: 0.4925\n",
      "Epoch: 1147/2000... Training loss: 0.4075\n",
      "Epoch: 1147/2000... Training loss: 0.4701\n",
      "Epoch: 1147/2000... Training loss: 0.4751\n",
      "Epoch: 1148/2000... Training loss: 0.5535\n",
      "Epoch: 1148/2000... Training loss: 0.3492\n",
      "Epoch: 1148/2000... Training loss: 0.2298\n",
      "Epoch: 1148/2000... Training loss: 0.5516\n",
      "Epoch: 1148/2000... Training loss: 0.4797\n",
      "Epoch: 1148/2000... Training loss: 0.5985\n",
      "Epoch: 1148/2000... Training loss: 0.4200\n",
      "Epoch: 1148/2000... Training loss: 0.4023\n",
      "Epoch: 1148/2000... Training loss: 0.5392\n",
      "Epoch: 1148/2000... Training loss: 0.4149\n",
      "Epoch: 1148/2000... Training loss: 0.2729\n",
      "Epoch: 1148/2000... Training loss: 0.4579\n",
      "Epoch: 1148/2000... Training loss: 0.4735\n",
      "Epoch: 1148/2000... Training loss: 0.5223\n",
      "Epoch: 1148/2000... Training loss: 0.3961\n",
      "Epoch: 1148/2000... Training loss: 0.5713\n",
      "Epoch: 1148/2000... Training loss: 0.5205\n",
      "Epoch: 1148/2000... Training loss: 0.3409\n",
      "Epoch: 1148/2000... Training loss: 0.4251\n",
      "Epoch: 1148/2000... Training loss: 0.5869\n",
      "Epoch: 1148/2000... Training loss: 0.5680\n",
      "Epoch: 1148/2000... Training loss: 0.6333\n",
      "Epoch: 1148/2000... Training loss: 0.3983\n",
      "Epoch: 1148/2000... Training loss: 0.5696\n",
      "Epoch: 1148/2000... Training loss: 0.3786\n",
      "Epoch: 1148/2000... Training loss: 0.4768\n",
      "Epoch: 1148/2000... Training loss: 0.6297\n",
      "Epoch: 1148/2000... Training loss: 0.4763\n",
      "Epoch: 1148/2000... Training loss: 0.5899\n",
      "Epoch: 1148/2000... Training loss: 0.4146\n",
      "Epoch: 1148/2000... Training loss: 0.4957\n",
      "Epoch: 1149/2000... Training loss: 0.3401\n",
      "Epoch: 1149/2000... Training loss: 0.4522\n",
      "Epoch: 1149/2000... Training loss: 0.3172\n",
      "Epoch: 1149/2000... Training loss: 0.4957\n",
      "Epoch: 1149/2000... Training loss: 0.2882\n",
      "Epoch: 1149/2000... Training loss: 0.4041\n",
      "Epoch: 1149/2000... Training loss: 0.4184\n",
      "Epoch: 1149/2000... Training loss: 0.5717\n",
      "Epoch: 1149/2000... Training loss: 0.6056\n",
      "Epoch: 1149/2000... Training loss: 0.6251\n",
      "Epoch: 1149/2000... Training loss: 0.4843\n",
      "Epoch: 1149/2000... Training loss: 0.4371\n",
      "Epoch: 1149/2000... Training loss: 0.4099\n",
      "Epoch: 1149/2000... Training loss: 0.5337\n",
      "Epoch: 1149/2000... Training loss: 0.5659\n",
      "Epoch: 1149/2000... Training loss: 0.3636\n",
      "Epoch: 1149/2000... Training loss: 0.2830\n",
      "Epoch: 1149/2000... Training loss: 0.3807\n",
      "Epoch: 1149/2000... Training loss: 0.4362\n",
      "Epoch: 1149/2000... Training loss: 0.4987\n",
      "Epoch: 1149/2000... Training loss: 0.4696\n",
      "Epoch: 1149/2000... Training loss: 0.4791\n",
      "Epoch: 1149/2000... Training loss: 0.4276\n",
      "Epoch: 1149/2000... Training loss: 0.5392\n",
      "Epoch: 1149/2000... Training loss: 0.6258\n",
      "Epoch: 1149/2000... Training loss: 0.3763\n",
      "Epoch: 1149/2000... Training loss: 0.4796\n",
      "Epoch: 1149/2000... Training loss: 0.3546\n",
      "Epoch: 1149/2000... Training loss: 0.6333\n",
      "Epoch: 1149/2000... Training loss: 0.3240\n",
      "Epoch: 1149/2000... Training loss: 0.5076\n",
      "Epoch: 1150/2000... Training loss: 0.5362\n",
      "Epoch: 1150/2000... Training loss: 0.4754\n",
      "Epoch: 1150/2000... Training loss: 0.3796\n",
      "Epoch: 1150/2000... Training loss: 0.6907\n",
      "Epoch: 1150/2000... Training loss: 0.5582\n",
      "Epoch: 1150/2000... Training loss: 0.5091\n",
      "Epoch: 1150/2000... Training loss: 0.3871\n",
      "Epoch: 1150/2000... Training loss: 0.6158\n",
      "Epoch: 1150/2000... Training loss: 0.5550\n",
      "Epoch: 1150/2000... Training loss: 0.3818\n",
      "Epoch: 1150/2000... Training loss: 0.5017\n",
      "Epoch: 1150/2000... Training loss: 0.5114\n",
      "Epoch: 1150/2000... Training loss: 0.5108\n",
      "Epoch: 1150/2000... Training loss: 0.4315\n",
      "Epoch: 1150/2000... Training loss: 0.4427\n",
      "Epoch: 1150/2000... Training loss: 0.5700\n",
      "Epoch: 1150/2000... Training loss: 0.4793\n",
      "Epoch: 1150/2000... Training loss: 0.4310\n",
      "Epoch: 1150/2000... Training loss: 0.3950\n",
      "Epoch: 1150/2000... Training loss: 0.3677\n",
      "Epoch: 1150/2000... Training loss: 0.5971\n",
      "Epoch: 1150/2000... Training loss: 0.7391\n",
      "Epoch: 1150/2000... Training loss: 0.4477\n",
      "Epoch: 1150/2000... Training loss: 0.3873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1150/2000... Training loss: 0.4904\n",
      "Epoch: 1150/2000... Training loss: 0.5731\n",
      "Epoch: 1150/2000... Training loss: 0.4524\n",
      "Epoch: 1150/2000... Training loss: 0.4794\n",
      "Epoch: 1150/2000... Training loss: 0.4330\n",
      "Epoch: 1150/2000... Training loss: 0.3724\n",
      "Epoch: 1150/2000... Training loss: 0.4116\n",
      "Epoch: 1151/2000... Training loss: 0.4148\n",
      "Epoch: 1151/2000... Training loss: 0.5719\n",
      "Epoch: 1151/2000... Training loss: 0.4887\n",
      "Epoch: 1151/2000... Training loss: 0.4831\n",
      "Epoch: 1151/2000... Training loss: 0.5780\n",
      "Epoch: 1151/2000... Training loss: 0.4599\n",
      "Epoch: 1151/2000... Training loss: 0.3880\n",
      "Epoch: 1151/2000... Training loss: 0.3725\n",
      "Epoch: 1151/2000... Training loss: 0.3850\n",
      "Epoch: 1151/2000... Training loss: 0.4079\n",
      "Epoch: 1151/2000... Training loss: 0.5396\n",
      "Epoch: 1151/2000... Training loss: 0.3155\n",
      "Epoch: 1151/2000... Training loss: 0.5703\n",
      "Epoch: 1151/2000... Training loss: 0.5232\n",
      "Epoch: 1151/2000... Training loss: 0.3742\n",
      "Epoch: 1151/2000... Training loss: 0.3311\n",
      "Epoch: 1151/2000... Training loss: 0.4737\n",
      "Epoch: 1151/2000... Training loss: 0.5375\n",
      "Epoch: 1151/2000... Training loss: 0.5908\n",
      "Epoch: 1151/2000... Training loss: 0.3473\n",
      "Epoch: 1151/2000... Training loss: 0.5675\n",
      "Epoch: 1151/2000... Training loss: 0.4019\n",
      "Epoch: 1151/2000... Training loss: 0.5310\n",
      "Epoch: 1151/2000... Training loss: 0.3333\n",
      "Epoch: 1151/2000... Training loss: 0.6518\n",
      "Epoch: 1151/2000... Training loss: 0.4279\n",
      "Epoch: 1151/2000... Training loss: 0.4615\n",
      "Epoch: 1151/2000... Training loss: 0.6550\n",
      "Epoch: 1151/2000... Training loss: 0.4731\n",
      "Epoch: 1151/2000... Training loss: 0.4896\n",
      "Epoch: 1151/2000... Training loss: 0.3910\n",
      "Epoch: 1152/2000... Training loss: 0.6723\n",
      "Epoch: 1152/2000... Training loss: 0.4810\n",
      "Epoch: 1152/2000... Training loss: 0.5328\n",
      "Epoch: 1152/2000... Training loss: 0.3861\n",
      "Epoch: 1152/2000... Training loss: 0.4253\n",
      "Epoch: 1152/2000... Training loss: 0.3712\n",
      "Epoch: 1152/2000... Training loss: 0.3876\n",
      "Epoch: 1152/2000... Training loss: 0.3644\n",
      "Epoch: 1152/2000... Training loss: 0.5904\n",
      "Epoch: 1152/2000... Training loss: 0.3826\n",
      "Epoch: 1152/2000... Training loss: 0.6060\n",
      "Epoch: 1152/2000... Training loss: 0.5738\n",
      "Epoch: 1152/2000... Training loss: 0.3670\n",
      "Epoch: 1152/2000... Training loss: 0.5531\n",
      "Epoch: 1152/2000... Training loss: 0.5016\n",
      "Epoch: 1152/2000... Training loss: 0.6738\n",
      "Epoch: 1152/2000... Training loss: 0.5351\n",
      "Epoch: 1152/2000... Training loss: 0.4212\n",
      "Epoch: 1152/2000... Training loss: 0.3031\n",
      "Epoch: 1152/2000... Training loss: 0.6071\n",
      "Epoch: 1152/2000... Training loss: 0.3985\n",
      "Epoch: 1152/2000... Training loss: 0.5033\n",
      "Epoch: 1152/2000... Training loss: 0.5153\n",
      "Epoch: 1152/2000... Training loss: 0.3993\n",
      "Epoch: 1152/2000... Training loss: 0.4645\n",
      "Epoch: 1152/2000... Training loss: 0.3490\n",
      "Epoch: 1152/2000... Training loss: 0.3062\n",
      "Epoch: 1152/2000... Training loss: 0.3605\n",
      "Epoch: 1152/2000... Training loss: 0.4450\n",
      "Epoch: 1152/2000... Training loss: 0.5075\n",
      "Epoch: 1152/2000... Training loss: 0.3382\n",
      "Epoch: 1153/2000... Training loss: 0.5287\n",
      "Epoch: 1153/2000... Training loss: 0.4933\n",
      "Epoch: 1153/2000... Training loss: 0.4422\n",
      "Epoch: 1153/2000... Training loss: 0.4443\n",
      "Epoch: 1153/2000... Training loss: 0.3468\n",
      "Epoch: 1153/2000... Training loss: 0.5113\n",
      "Epoch: 1153/2000... Training loss: 0.3245\n",
      "Epoch: 1153/2000... Training loss: 0.4349\n",
      "Epoch: 1153/2000... Training loss: 0.6682\n",
      "Epoch: 1153/2000... Training loss: 0.4774\n",
      "Epoch: 1153/2000... Training loss: 0.6737\n",
      "Epoch: 1153/2000... Training loss: 0.3515\n",
      "Epoch: 1153/2000... Training loss: 0.3839\n",
      "Epoch: 1153/2000... Training loss: 0.4632\n",
      "Epoch: 1153/2000... Training loss: 0.4725\n",
      "Epoch: 1153/2000... Training loss: 0.5366\n",
      "Epoch: 1153/2000... Training loss: 0.5676\n",
      "Epoch: 1153/2000... Training loss: 0.5328\n",
      "Epoch: 1153/2000... Training loss: 0.5437\n",
      "Epoch: 1153/2000... Training loss: 0.4799\n",
      "Epoch: 1153/2000... Training loss: 0.6294\n",
      "Epoch: 1153/2000... Training loss: 0.4835\n",
      "Epoch: 1153/2000... Training loss: 0.4965\n",
      "Epoch: 1153/2000... Training loss: 0.5284\n",
      "Epoch: 1153/2000... Training loss: 0.3513\n",
      "Epoch: 1153/2000... Training loss: 0.6767\n",
      "Epoch: 1153/2000... Training loss: 0.4517\n",
      "Epoch: 1153/2000... Training loss: 0.4329\n",
      "Epoch: 1153/2000... Training loss: 0.5947\n",
      "Epoch: 1153/2000... Training loss: 0.4654\n",
      "Epoch: 1153/2000... Training loss: 0.4933\n",
      "Epoch: 1154/2000... Training loss: 0.4265\n",
      "Epoch: 1154/2000... Training loss: 0.3324\n",
      "Epoch: 1154/2000... Training loss: 0.3853\n",
      "Epoch: 1154/2000... Training loss: 0.6810\n",
      "Epoch: 1154/2000... Training loss: 0.5129\n",
      "Epoch: 1154/2000... Training loss: 0.5257\n",
      "Epoch: 1154/2000... Training loss: 0.2543\n",
      "Epoch: 1154/2000... Training loss: 0.4270\n",
      "Epoch: 1154/2000... Training loss: 0.5433\n",
      "Epoch: 1154/2000... Training loss: 0.6083\n",
      "Epoch: 1154/2000... Training loss: 0.6272\n",
      "Epoch: 1154/2000... Training loss: 0.5240\n",
      "Epoch: 1154/2000... Training loss: 0.5142\n",
      "Epoch: 1154/2000... Training loss: 0.4779\n",
      "Epoch: 1154/2000... Training loss: 0.4882\n",
      "Epoch: 1154/2000... Training loss: 0.3901\n",
      "Epoch: 1154/2000... Training loss: 0.3433\n",
      "Epoch: 1154/2000... Training loss: 0.5821\n",
      "Epoch: 1154/2000... Training loss: 0.5353\n",
      "Epoch: 1154/2000... Training loss: 0.5143\n",
      "Epoch: 1154/2000... Training loss: 0.4060\n",
      "Epoch: 1154/2000... Training loss: 0.4290\n",
      "Epoch: 1154/2000... Training loss: 0.3561\n",
      "Epoch: 1154/2000... Training loss: 0.4813\n",
      "Epoch: 1154/2000... Training loss: 0.5248\n",
      "Epoch: 1154/2000... Training loss: 0.4004\n",
      "Epoch: 1154/2000... Training loss: 0.5377\n",
      "Epoch: 1154/2000... Training loss: 0.6034\n",
      "Epoch: 1154/2000... Training loss: 0.5798\n",
      "Epoch: 1154/2000... Training loss: 0.3445\n",
      "Epoch: 1154/2000... Training loss: 0.4145\n",
      "Epoch: 1155/2000... Training loss: 0.3608\n",
      "Epoch: 1155/2000... Training loss: 0.6279\n",
      "Epoch: 1155/2000... Training loss: 0.4880\n",
      "Epoch: 1155/2000... Training loss: 0.5732\n",
      "Epoch: 1155/2000... Training loss: 0.4116\n",
      "Epoch: 1155/2000... Training loss: 0.3971\n",
      "Epoch: 1155/2000... Training loss: 0.2343\n",
      "Epoch: 1155/2000... Training loss: 0.4664\n",
      "Epoch: 1155/2000... Training loss: 0.4059\n",
      "Epoch: 1155/2000... Training loss: 0.3893\n",
      "Epoch: 1155/2000... Training loss: 0.3557\n",
      "Epoch: 1155/2000... Training loss: 0.6152\n",
      "Epoch: 1155/2000... Training loss: 0.3503\n",
      "Epoch: 1155/2000... Training loss: 0.5609\n",
      "Epoch: 1155/2000... Training loss: 0.3961\n",
      "Epoch: 1155/2000... Training loss: 0.4980\n",
      "Epoch: 1155/2000... Training loss: 0.5156\n",
      "Epoch: 1155/2000... Training loss: 0.4897\n",
      "Epoch: 1155/2000... Training loss: 0.3993\n",
      "Epoch: 1155/2000... Training loss: 0.5405\n",
      "Epoch: 1155/2000... Training loss: 0.4805\n",
      "Epoch: 1155/2000... Training loss: 0.4326\n",
      "Epoch: 1155/2000... Training loss: 0.4397\n",
      "Epoch: 1155/2000... Training loss: 0.4383\n",
      "Epoch: 1155/2000... Training loss: 0.4909\n",
      "Epoch: 1155/2000... Training loss: 0.4245\n",
      "Epoch: 1155/2000... Training loss: 0.5804\n",
      "Epoch: 1155/2000... Training loss: 0.5022\n",
      "Epoch: 1155/2000... Training loss: 0.4522\n",
      "Epoch: 1155/2000... Training loss: 0.5101\n",
      "Epoch: 1155/2000... Training loss: 0.3143\n",
      "Epoch: 1156/2000... Training loss: 0.3360\n",
      "Epoch: 1156/2000... Training loss: 0.4289\n",
      "Epoch: 1156/2000... Training loss: 0.4689\n",
      "Epoch: 1156/2000... Training loss: 0.4834\n",
      "Epoch: 1156/2000... Training loss: 0.3603\n",
      "Epoch: 1156/2000... Training loss: 0.4473\n",
      "Epoch: 1156/2000... Training loss: 0.5379\n",
      "Epoch: 1156/2000... Training loss: 0.5318\n",
      "Epoch: 1156/2000... Training loss: 0.3830\n",
      "Epoch: 1156/2000... Training loss: 0.4756\n",
      "Epoch: 1156/2000... Training loss: 0.3507\n",
      "Epoch: 1156/2000... Training loss: 0.6163\n",
      "Epoch: 1156/2000... Training loss: 0.5756\n",
      "Epoch: 1156/2000... Training loss: 0.4631\n",
      "Epoch: 1156/2000... Training loss: 0.4338\n",
      "Epoch: 1156/2000... Training loss: 0.4323\n",
      "Epoch: 1156/2000... Training loss: 0.6302\n",
      "Epoch: 1156/2000... Training loss: 0.4654\n",
      "Epoch: 1156/2000... Training loss: 0.3574\n",
      "Epoch: 1156/2000... Training loss: 0.3088\n",
      "Epoch: 1156/2000... Training loss: 0.4207\n",
      "Epoch: 1156/2000... Training loss: 0.6063\n",
      "Epoch: 1156/2000... Training loss: 0.4964\n",
      "Epoch: 1156/2000... Training loss: 0.5617\n",
      "Epoch: 1156/2000... Training loss: 0.3581\n",
      "Epoch: 1156/2000... Training loss: 0.5121\n",
      "Epoch: 1156/2000... Training loss: 0.4827\n",
      "Epoch: 1156/2000... Training loss: 0.3910\n",
      "Epoch: 1156/2000... Training loss: 0.3928\n",
      "Epoch: 1156/2000... Training loss: 0.4106\n",
      "Epoch: 1156/2000... Training loss: 0.5577\n",
      "Epoch: 1157/2000... Training loss: 0.4779\n",
      "Epoch: 1157/2000... Training loss: 0.5246\n",
      "Epoch: 1157/2000... Training loss: 0.4231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1157/2000... Training loss: 0.4691\n",
      "Epoch: 1157/2000... Training loss: 0.3302\n",
      "Epoch: 1157/2000... Training loss: 0.3631\n",
      "Epoch: 1157/2000... Training loss: 0.4407\n",
      "Epoch: 1157/2000... Training loss: 0.5188\n",
      "Epoch: 1157/2000... Training loss: 0.3946\n",
      "Epoch: 1157/2000... Training loss: 0.4039\n",
      "Epoch: 1157/2000... Training loss: 0.4347\n",
      "Epoch: 1157/2000... Training loss: 0.4286\n",
      "Epoch: 1157/2000... Training loss: 0.5768\n",
      "Epoch: 1157/2000... Training loss: 0.4756\n",
      "Epoch: 1157/2000... Training loss: 0.5621\n",
      "Epoch: 1157/2000... Training loss: 0.6149\n",
      "Epoch: 1157/2000... Training loss: 0.3662\n",
      "Epoch: 1157/2000... Training loss: 0.5279\n",
      "Epoch: 1157/2000... Training loss: 0.5156\n",
      "Epoch: 1157/2000... Training loss: 0.3903\n",
      "Epoch: 1157/2000... Training loss: 0.6044\n",
      "Epoch: 1157/2000... Training loss: 0.4887\n",
      "Epoch: 1157/2000... Training loss: 0.3741\n",
      "Epoch: 1157/2000... Training loss: 0.4755\n",
      "Epoch: 1157/2000... Training loss: 0.6457\n",
      "Epoch: 1157/2000... Training loss: 0.5480\n",
      "Epoch: 1157/2000... Training loss: 0.4560\n",
      "Epoch: 1157/2000... Training loss: 0.5020\n",
      "Epoch: 1157/2000... Training loss: 0.4778\n",
      "Epoch: 1157/2000... Training loss: 0.5584\n",
      "Epoch: 1157/2000... Training loss: 0.4609\n",
      "Epoch: 1158/2000... Training loss: 0.4742\n",
      "Epoch: 1158/2000... Training loss: 0.4187\n",
      "Epoch: 1158/2000... Training loss: 0.3074\n",
      "Epoch: 1158/2000... Training loss: 0.2680\n",
      "Epoch: 1158/2000... Training loss: 0.4393\n",
      "Epoch: 1158/2000... Training loss: 0.4125\n",
      "Epoch: 1158/2000... Training loss: 0.4912\n",
      "Epoch: 1158/2000... Training loss: 0.4020\n",
      "Epoch: 1158/2000... Training loss: 0.5172\n",
      "Epoch: 1158/2000... Training loss: 0.4823\n",
      "Epoch: 1158/2000... Training loss: 0.4583\n",
      "Epoch: 1158/2000... Training loss: 0.3686\n",
      "Epoch: 1158/2000... Training loss: 0.4990\n",
      "Epoch: 1158/2000... Training loss: 0.3497\n",
      "Epoch: 1158/2000... Training loss: 0.4240\n",
      "Epoch: 1158/2000... Training loss: 0.4707\n",
      "Epoch: 1158/2000... Training loss: 0.2762\n",
      "Epoch: 1158/2000... Training loss: 0.5644\n",
      "Epoch: 1158/2000... Training loss: 0.6418\n",
      "Epoch: 1158/2000... Training loss: 0.4874\n",
      "Epoch: 1158/2000... Training loss: 0.4447\n",
      "Epoch: 1158/2000... Training loss: 0.5203\n",
      "Epoch: 1158/2000... Training loss: 0.5072\n",
      "Epoch: 1158/2000... Training loss: 0.7983\n",
      "Epoch: 1158/2000... Training loss: 0.5127\n",
      "Epoch: 1158/2000... Training loss: 0.4297\n",
      "Epoch: 1158/2000... Training loss: 0.4837\n",
      "Epoch: 1158/2000... Training loss: 0.4905\n",
      "Epoch: 1158/2000... Training loss: 0.4681\n",
      "Epoch: 1158/2000... Training loss: 0.4504\n",
      "Epoch: 1158/2000... Training loss: 0.5354\n",
      "Epoch: 1159/2000... Training loss: 0.3152\n",
      "Epoch: 1159/2000... Training loss: 0.4759\n",
      "Epoch: 1159/2000... Training loss: 0.6944\n",
      "Epoch: 1159/2000... Training loss: 0.4800\n",
      "Epoch: 1159/2000... Training loss: 0.4967\n",
      "Epoch: 1159/2000... Training loss: 0.6481\n",
      "Epoch: 1159/2000... Training loss: 0.6169\n",
      "Epoch: 1159/2000... Training loss: 0.3235\n",
      "Epoch: 1159/2000... Training loss: 0.4409\n",
      "Epoch: 1159/2000... Training loss: 0.4750\n",
      "Epoch: 1159/2000... Training loss: 0.4374\n",
      "Epoch: 1159/2000... Training loss: 0.3536\n",
      "Epoch: 1159/2000... Training loss: 0.4003\n",
      "Epoch: 1159/2000... Training loss: 0.3586\n",
      "Epoch: 1159/2000... Training loss: 0.5840\n",
      "Epoch: 1159/2000... Training loss: 0.5007\n",
      "Epoch: 1159/2000... Training loss: 0.5092\n",
      "Epoch: 1159/2000... Training loss: 0.3354\n",
      "Epoch: 1159/2000... Training loss: 0.4153\n",
      "Epoch: 1159/2000... Training loss: 0.4740\n",
      "Epoch: 1159/2000... Training loss: 0.4660\n",
      "Epoch: 1159/2000... Training loss: 0.4310\n",
      "Epoch: 1159/2000... Training loss: 0.5796\n",
      "Epoch: 1159/2000... Training loss: 0.5965\n",
      "Epoch: 1159/2000... Training loss: 0.4985\n",
      "Epoch: 1159/2000... Training loss: 0.6053\n",
      "Epoch: 1159/2000... Training loss: 0.4137\n",
      "Epoch: 1159/2000... Training loss: 0.4771\n",
      "Epoch: 1159/2000... Training loss: 0.3990\n",
      "Epoch: 1159/2000... Training loss: 0.6842\n",
      "Epoch: 1159/2000... Training loss: 0.5488\n",
      "Epoch: 1160/2000... Training loss: 0.3601\n",
      "Epoch: 1160/2000... Training loss: 0.4270\n",
      "Epoch: 1160/2000... Training loss: 0.3342\n",
      "Epoch: 1160/2000... Training loss: 0.5013\n",
      "Epoch: 1160/2000... Training loss: 0.4959\n",
      "Epoch: 1160/2000... Training loss: 0.5509\n",
      "Epoch: 1160/2000... Training loss: 0.4352\n",
      "Epoch: 1160/2000... Training loss: 0.5801\n",
      "Epoch: 1160/2000... Training loss: 0.4505\n",
      "Epoch: 1160/2000... Training loss: 0.3638\n",
      "Epoch: 1160/2000... Training loss: 0.5150\n",
      "Epoch: 1160/2000... Training loss: 0.3206\n",
      "Epoch: 1160/2000... Training loss: 0.6253\n",
      "Epoch: 1160/2000... Training loss: 0.3944\n",
      "Epoch: 1160/2000... Training loss: 0.4793\n",
      "Epoch: 1160/2000... Training loss: 0.5722\n",
      "Epoch: 1160/2000... Training loss: 0.3404\n",
      "Epoch: 1160/2000... Training loss: 0.5211\n",
      "Epoch: 1160/2000... Training loss: 0.5014\n",
      "Epoch: 1160/2000... Training loss: 0.6150\n",
      "Epoch: 1160/2000... Training loss: 0.5117\n",
      "Epoch: 1160/2000... Training loss: 0.3735\n",
      "Epoch: 1160/2000... Training loss: 0.3560\n",
      "Epoch: 1160/2000... Training loss: 0.4132\n",
      "Epoch: 1160/2000... Training loss: 0.4094\n",
      "Epoch: 1160/2000... Training loss: 0.5213\n",
      "Epoch: 1160/2000... Training loss: 0.5423\n",
      "Epoch: 1160/2000... Training loss: 0.6265\n",
      "Epoch: 1160/2000... Training loss: 0.3531\n",
      "Epoch: 1160/2000... Training loss: 0.3725\n",
      "Epoch: 1160/2000... Training loss: 0.5132\n",
      "Epoch: 1161/2000... Training loss: 0.5523\n",
      "Epoch: 1161/2000... Training loss: 0.6620\n",
      "Epoch: 1161/2000... Training loss: 0.3797\n",
      "Epoch: 1161/2000... Training loss: 0.5322\n",
      "Epoch: 1161/2000... Training loss: 0.4722\n",
      "Epoch: 1161/2000... Training loss: 0.3943\n",
      "Epoch: 1161/2000... Training loss: 0.5429\n",
      "Epoch: 1161/2000... Training loss: 0.3901\n",
      "Epoch: 1161/2000... Training loss: 0.5258\n",
      "Epoch: 1161/2000... Training loss: 0.4978\n",
      "Epoch: 1161/2000... Training loss: 0.5621\n",
      "Epoch: 1161/2000... Training loss: 0.4240\n",
      "Epoch: 1161/2000... Training loss: 0.6757\n",
      "Epoch: 1161/2000... Training loss: 0.6456\n",
      "Epoch: 1161/2000... Training loss: 0.5484\n",
      "Epoch: 1161/2000... Training loss: 0.4126\n",
      "Epoch: 1161/2000... Training loss: 0.4184\n",
      "Epoch: 1161/2000... Training loss: 0.4967\n",
      "Epoch: 1161/2000... Training loss: 0.3239\n",
      "Epoch: 1161/2000... Training loss: 0.4470\n",
      "Epoch: 1161/2000... Training loss: 0.4208\n",
      "Epoch: 1161/2000... Training loss: 0.5687\n",
      "Epoch: 1161/2000... Training loss: 0.4956\n",
      "Epoch: 1161/2000... Training loss: 0.5008\n",
      "Epoch: 1161/2000... Training loss: 0.5744\n",
      "Epoch: 1161/2000... Training loss: 0.5401\n",
      "Epoch: 1161/2000... Training loss: 0.3632\n",
      "Epoch: 1161/2000... Training loss: 0.4605\n",
      "Epoch: 1161/2000... Training loss: 0.4103\n",
      "Epoch: 1161/2000... Training loss: 0.4331\n",
      "Epoch: 1161/2000... Training loss: 0.4417\n",
      "Epoch: 1162/2000... Training loss: 0.6819\n",
      "Epoch: 1162/2000... Training loss: 0.3951\n",
      "Epoch: 1162/2000... Training loss: 0.5904\n",
      "Epoch: 1162/2000... Training loss: 0.4345\n",
      "Epoch: 1162/2000... Training loss: 0.3335\n",
      "Epoch: 1162/2000... Training loss: 0.6595\n",
      "Epoch: 1162/2000... Training loss: 0.4561\n",
      "Epoch: 1162/2000... Training loss: 0.4948\n",
      "Epoch: 1162/2000... Training loss: 0.7887\n",
      "Epoch: 1162/2000... Training loss: 0.3345\n",
      "Epoch: 1162/2000... Training loss: 0.5100\n",
      "Epoch: 1162/2000... Training loss: 0.4544\n",
      "Epoch: 1162/2000... Training loss: 0.3648\n",
      "Epoch: 1162/2000... Training loss: 0.4894\n",
      "Epoch: 1162/2000... Training loss: 0.3937\n",
      "Epoch: 1162/2000... Training loss: 0.4712\n",
      "Epoch: 1162/2000... Training loss: 0.3632\n",
      "Epoch: 1162/2000... Training loss: 0.4424\n",
      "Epoch: 1162/2000... Training loss: 0.4279\n",
      "Epoch: 1162/2000... Training loss: 0.6065\n",
      "Epoch: 1162/2000... Training loss: 0.4221\n",
      "Epoch: 1162/2000... Training loss: 0.4508\n",
      "Epoch: 1162/2000... Training loss: 0.5074\n",
      "Epoch: 1162/2000... Training loss: 0.5920\n",
      "Epoch: 1162/2000... Training loss: 0.6653\n",
      "Epoch: 1162/2000... Training loss: 0.3959\n",
      "Epoch: 1162/2000... Training loss: 0.5774\n",
      "Epoch: 1162/2000... Training loss: 0.4607\n",
      "Epoch: 1162/2000... Training loss: 0.2769\n",
      "Epoch: 1162/2000... Training loss: 0.4923\n",
      "Epoch: 1162/2000... Training loss: 0.5608\n",
      "Epoch: 1163/2000... Training loss: 0.4104\n",
      "Epoch: 1163/2000... Training loss: 0.7191\n",
      "Epoch: 1163/2000... Training loss: 0.3837\n",
      "Epoch: 1163/2000... Training loss: 0.6277\n",
      "Epoch: 1163/2000... Training loss: 0.6250\n",
      "Epoch: 1163/2000... Training loss: 0.4579\n",
      "Epoch: 1163/2000... Training loss: 0.5515\n",
      "Epoch: 1163/2000... Training loss: 0.3387\n",
      "Epoch: 1163/2000... Training loss: 0.4733\n",
      "Epoch: 1163/2000... Training loss: 0.4986\n",
      "Epoch: 1163/2000... Training loss: 0.4741\n",
      "Epoch: 1163/2000... Training loss: 0.3652\n",
      "Epoch: 1163/2000... Training loss: 0.2228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1163/2000... Training loss: 0.3705\n",
      "Epoch: 1163/2000... Training loss: 0.4892\n",
      "Epoch: 1163/2000... Training loss: 0.4719\n",
      "Epoch: 1163/2000... Training loss: 0.4343\n",
      "Epoch: 1163/2000... Training loss: 0.4925\n",
      "Epoch: 1163/2000... Training loss: 0.4035\n",
      "Epoch: 1163/2000... Training loss: 0.4540\n",
      "Epoch: 1163/2000... Training loss: 0.3706\n",
      "Epoch: 1163/2000... Training loss: 0.4374\n",
      "Epoch: 1163/2000... Training loss: 0.5083\n",
      "Epoch: 1163/2000... Training loss: 0.5078\n",
      "Epoch: 1163/2000... Training loss: 0.5293\n",
      "Epoch: 1163/2000... Training loss: 0.5240\n",
      "Epoch: 1163/2000... Training loss: 0.3813\n",
      "Epoch: 1163/2000... Training loss: 0.5593\n",
      "Epoch: 1163/2000... Training loss: 0.4899\n",
      "Epoch: 1163/2000... Training loss: 0.3923\n",
      "Epoch: 1163/2000... Training loss: 0.3552\n",
      "Epoch: 1164/2000... Training loss: 0.5873\n",
      "Epoch: 1164/2000... Training loss: 0.5152\n",
      "Epoch: 1164/2000... Training loss: 0.5806\n",
      "Epoch: 1164/2000... Training loss: 0.5142\n",
      "Epoch: 1164/2000... Training loss: 0.4123\n",
      "Epoch: 1164/2000... Training loss: 0.4924\n",
      "Epoch: 1164/2000... Training loss: 0.4133\n",
      "Epoch: 1164/2000... Training loss: 0.4231\n",
      "Epoch: 1164/2000... Training loss: 0.4092\n",
      "Epoch: 1164/2000... Training loss: 0.4707\n",
      "Epoch: 1164/2000... Training loss: 0.6892\n",
      "Epoch: 1164/2000... Training loss: 0.6768\n",
      "Epoch: 1164/2000... Training loss: 0.3496\n",
      "Epoch: 1164/2000... Training loss: 0.3747\n",
      "Epoch: 1164/2000... Training loss: 0.6043\n",
      "Epoch: 1164/2000... Training loss: 0.5428\n",
      "Epoch: 1164/2000... Training loss: 0.4180\n",
      "Epoch: 1164/2000... Training loss: 0.7343\n",
      "Epoch: 1164/2000... Training loss: 0.5661\n",
      "Epoch: 1164/2000... Training loss: 0.3736\n",
      "Epoch: 1164/2000... Training loss: 0.2417\n",
      "Epoch: 1164/2000... Training loss: 0.4493\n",
      "Epoch: 1164/2000... Training loss: 0.4638\n",
      "Epoch: 1164/2000... Training loss: 0.5211\n",
      "Epoch: 1164/2000... Training loss: 0.4616\n",
      "Epoch: 1164/2000... Training loss: 0.3089\n",
      "Epoch: 1164/2000... Training loss: 0.6266\n",
      "Epoch: 1164/2000... Training loss: 0.5018\n",
      "Epoch: 1164/2000... Training loss: 0.4634\n",
      "Epoch: 1164/2000... Training loss: 0.5443\n",
      "Epoch: 1164/2000... Training loss: 0.5156\n",
      "Epoch: 1165/2000... Training loss: 0.5447\n",
      "Epoch: 1165/2000... Training loss: 0.2951\n",
      "Epoch: 1165/2000... Training loss: 0.5584\n",
      "Epoch: 1165/2000... Training loss: 0.4726\n",
      "Epoch: 1165/2000... Training loss: 0.3777\n",
      "Epoch: 1165/2000... Training loss: 0.6225\n",
      "Epoch: 1165/2000... Training loss: 0.5251\n",
      "Epoch: 1165/2000... Training loss: 0.4149\n",
      "Epoch: 1165/2000... Training loss: 0.5024\n",
      "Epoch: 1165/2000... Training loss: 0.3089\n",
      "Epoch: 1165/2000... Training loss: 0.5298\n",
      "Epoch: 1165/2000... Training loss: 0.4775\n",
      "Epoch: 1165/2000... Training loss: 0.5132\n",
      "Epoch: 1165/2000... Training loss: 0.5444\n",
      "Epoch: 1165/2000... Training loss: 0.3838\n",
      "Epoch: 1165/2000... Training loss: 0.5233\n",
      "Epoch: 1165/2000... Training loss: 0.5430\n",
      "Epoch: 1165/2000... Training loss: 0.5811\n",
      "Epoch: 1165/2000... Training loss: 0.4717\n",
      "Epoch: 1165/2000... Training loss: 0.4685\n",
      "Epoch: 1165/2000... Training loss: 0.2997\n",
      "Epoch: 1165/2000... Training loss: 0.4310\n",
      "Epoch: 1165/2000... Training loss: 0.5619\n",
      "Epoch: 1165/2000... Training loss: 0.4367\n",
      "Epoch: 1165/2000... Training loss: 0.4867\n",
      "Epoch: 1165/2000... Training loss: 0.6444\n",
      "Epoch: 1165/2000... Training loss: 0.6279\n",
      "Epoch: 1165/2000... Training loss: 0.3315\n",
      "Epoch: 1165/2000... Training loss: 0.2836\n",
      "Epoch: 1165/2000... Training loss: 0.5561\n",
      "Epoch: 1165/2000... Training loss: 0.4810\n",
      "Epoch: 1166/2000... Training loss: 0.5110\n",
      "Epoch: 1166/2000... Training loss: 0.4880\n",
      "Epoch: 1166/2000... Training loss: 0.4254\n",
      "Epoch: 1166/2000... Training loss: 0.4943\n",
      "Epoch: 1166/2000... Training loss: 0.4786\n",
      "Epoch: 1166/2000... Training loss: 0.6658\n",
      "Epoch: 1166/2000... Training loss: 0.3562\n",
      "Epoch: 1166/2000... Training loss: 0.4981\n",
      "Epoch: 1166/2000... Training loss: 0.6067\n",
      "Epoch: 1166/2000... Training loss: 0.5867\n",
      "Epoch: 1166/2000... Training loss: 0.4683\n",
      "Epoch: 1166/2000... Training loss: 0.4151\n",
      "Epoch: 1166/2000... Training loss: 0.4875\n",
      "Epoch: 1166/2000... Training loss: 0.5859\n",
      "Epoch: 1166/2000... Training loss: 0.6842\n",
      "Epoch: 1166/2000... Training loss: 0.5344\n",
      "Epoch: 1166/2000... Training loss: 0.5596\n",
      "Epoch: 1166/2000... Training loss: 0.3392\n",
      "Epoch: 1166/2000... Training loss: 0.4416\n",
      "Epoch: 1166/2000... Training loss: 0.6552\n",
      "Epoch: 1166/2000... Training loss: 0.4103\n",
      "Epoch: 1166/2000... Training loss: 0.4951\n",
      "Epoch: 1166/2000... Training loss: 0.6134\n",
      "Epoch: 1166/2000... Training loss: 0.6508\n",
      "Epoch: 1166/2000... Training loss: 0.5416\n",
      "Epoch: 1166/2000... Training loss: 0.6259\n",
      "Epoch: 1166/2000... Training loss: 0.4860\n",
      "Epoch: 1166/2000... Training loss: 0.5494\n",
      "Epoch: 1166/2000... Training loss: 0.5187\n",
      "Epoch: 1166/2000... Training loss: 0.4229\n",
      "Epoch: 1166/2000... Training loss: 0.5633\n",
      "Epoch: 1167/2000... Training loss: 0.6056\n",
      "Epoch: 1167/2000... Training loss: 0.3717\n",
      "Epoch: 1167/2000... Training loss: 0.6064\n",
      "Epoch: 1167/2000... Training loss: 0.5238\n",
      "Epoch: 1167/2000... Training loss: 0.3174\n",
      "Epoch: 1167/2000... Training loss: 0.5258\n",
      "Epoch: 1167/2000... Training loss: 0.4080\n",
      "Epoch: 1167/2000... Training loss: 0.4611\n",
      "Epoch: 1167/2000... Training loss: 0.5896\n",
      "Epoch: 1167/2000... Training loss: 0.3384\n",
      "Epoch: 1167/2000... Training loss: 0.5564\n",
      "Epoch: 1167/2000... Training loss: 0.5676\n",
      "Epoch: 1167/2000... Training loss: 0.4034\n",
      "Epoch: 1167/2000... Training loss: 0.4979\n",
      "Epoch: 1167/2000... Training loss: 0.3531\n",
      "Epoch: 1167/2000... Training loss: 0.2877\n",
      "Epoch: 1167/2000... Training loss: 0.5631\n",
      "Epoch: 1167/2000... Training loss: 0.5767\n",
      "Epoch: 1167/2000... Training loss: 0.5652\n",
      "Epoch: 1167/2000... Training loss: 0.4620\n",
      "Epoch: 1167/2000... Training loss: 0.5393\n",
      "Epoch: 1167/2000... Training loss: 0.4890\n",
      "Epoch: 1167/2000... Training loss: 0.5396\n",
      "Epoch: 1167/2000... Training loss: 0.5159\n",
      "Epoch: 1167/2000... Training loss: 0.3763\n",
      "Epoch: 1167/2000... Training loss: 0.5428\n",
      "Epoch: 1167/2000... Training loss: 0.3370\n",
      "Epoch: 1167/2000... Training loss: 0.5721\n",
      "Epoch: 1167/2000... Training loss: 0.3719\n",
      "Epoch: 1167/2000... Training loss: 0.5916\n",
      "Epoch: 1167/2000... Training loss: 0.4488\n",
      "Epoch: 1168/2000... Training loss: 0.4936\n",
      "Epoch: 1168/2000... Training loss: 0.4825\n",
      "Epoch: 1168/2000... Training loss: 0.5024\n",
      "Epoch: 1168/2000... Training loss: 0.6440\n",
      "Epoch: 1168/2000... Training loss: 0.5049\n",
      "Epoch: 1168/2000... Training loss: 0.5838\n",
      "Epoch: 1168/2000... Training loss: 0.4220\n",
      "Epoch: 1168/2000... Training loss: 0.5431\n",
      "Epoch: 1168/2000... Training loss: 0.6559\n",
      "Epoch: 1168/2000... Training loss: 0.4717\n",
      "Epoch: 1168/2000... Training loss: 0.5906\n",
      "Epoch: 1168/2000... Training loss: 0.3280\n",
      "Epoch: 1168/2000... Training loss: 0.4961\n",
      "Epoch: 1168/2000... Training loss: 0.3544\n",
      "Epoch: 1168/2000... Training loss: 0.3224\n",
      "Epoch: 1168/2000... Training loss: 0.4310\n",
      "Epoch: 1168/2000... Training loss: 0.5526\n",
      "Epoch: 1168/2000... Training loss: 0.5101\n",
      "Epoch: 1168/2000... Training loss: 0.5509\n",
      "Epoch: 1168/2000... Training loss: 0.4962\n",
      "Epoch: 1168/2000... Training loss: 0.6126\n",
      "Epoch: 1168/2000... Training loss: 0.5032\n",
      "Epoch: 1168/2000... Training loss: 0.3721\n",
      "Epoch: 1168/2000... Training loss: 0.3175\n",
      "Epoch: 1168/2000... Training loss: 0.3481\n",
      "Epoch: 1168/2000... Training loss: 0.4499\n",
      "Epoch: 1168/2000... Training loss: 0.4255\n",
      "Epoch: 1168/2000... Training loss: 0.5218\n",
      "Epoch: 1168/2000... Training loss: 0.5305\n",
      "Epoch: 1168/2000... Training loss: 0.4280\n",
      "Epoch: 1168/2000... Training loss: 0.3871\n",
      "Epoch: 1169/2000... Training loss: 0.4338\n",
      "Epoch: 1169/2000... Training loss: 0.3426\n",
      "Epoch: 1169/2000... Training loss: 0.2789\n",
      "Epoch: 1169/2000... Training loss: 0.5951\n",
      "Epoch: 1169/2000... Training loss: 0.5962\n",
      "Epoch: 1169/2000... Training loss: 0.4472\n",
      "Epoch: 1169/2000... Training loss: 0.4042\n",
      "Epoch: 1169/2000... Training loss: 0.5759\n",
      "Epoch: 1169/2000... Training loss: 0.4320\n",
      "Epoch: 1169/2000... Training loss: 0.3193\n",
      "Epoch: 1169/2000... Training loss: 0.3713\n",
      "Epoch: 1169/2000... Training loss: 0.3976\n",
      "Epoch: 1169/2000... Training loss: 0.5282\n",
      "Epoch: 1169/2000... Training loss: 0.4799\n",
      "Epoch: 1169/2000... Training loss: 0.5579\n",
      "Epoch: 1169/2000... Training loss: 0.2734\n",
      "Epoch: 1169/2000... Training loss: 0.5903\n",
      "Epoch: 1169/2000... Training loss: 0.3931\n",
      "Epoch: 1169/2000... Training loss: 0.6052\n",
      "Epoch: 1169/2000... Training loss: 0.3892\n",
      "Epoch: 1169/2000... Training loss: 0.5847\n",
      "Epoch: 1169/2000... Training loss: 0.6582\n",
      "Epoch: 1169/2000... Training loss: 0.4127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1169/2000... Training loss: 0.3632\n",
      "Epoch: 1169/2000... Training loss: 0.4644\n",
      "Epoch: 1169/2000... Training loss: 0.4922\n",
      "Epoch: 1169/2000... Training loss: 0.4357\n",
      "Epoch: 1169/2000... Training loss: 0.3579\n",
      "Epoch: 1169/2000... Training loss: 0.4614\n",
      "Epoch: 1169/2000... Training loss: 0.6481\n",
      "Epoch: 1169/2000... Training loss: 0.5557\n",
      "Epoch: 1170/2000... Training loss: 0.4534\n",
      "Epoch: 1170/2000... Training loss: 0.3728\n",
      "Epoch: 1170/2000... Training loss: 0.4653\n",
      "Epoch: 1170/2000... Training loss: 0.4680\n",
      "Epoch: 1170/2000... Training loss: 0.6522\n",
      "Epoch: 1170/2000... Training loss: 0.3579\n",
      "Epoch: 1170/2000... Training loss: 0.3928\n",
      "Epoch: 1170/2000... Training loss: 0.4939\n",
      "Epoch: 1170/2000... Training loss: 0.5133\n",
      "Epoch: 1170/2000... Training loss: 0.3747\n",
      "Epoch: 1170/2000... Training loss: 0.5242\n",
      "Epoch: 1170/2000... Training loss: 0.4257\n",
      "Epoch: 1170/2000... Training loss: 0.5437\n",
      "Epoch: 1170/2000... Training loss: 0.3769\n",
      "Epoch: 1170/2000... Training loss: 0.3100\n",
      "Epoch: 1170/2000... Training loss: 0.4616\n",
      "Epoch: 1170/2000... Training loss: 0.4062\n",
      "Epoch: 1170/2000... Training loss: 0.4807\n",
      "Epoch: 1170/2000... Training loss: 0.2945\n",
      "Epoch: 1170/2000... Training loss: 0.6621\n",
      "Epoch: 1170/2000... Training loss: 0.4166\n",
      "Epoch: 1170/2000... Training loss: 0.5960\n",
      "Epoch: 1170/2000... Training loss: 0.4216\n",
      "Epoch: 1170/2000... Training loss: 0.4674\n",
      "Epoch: 1170/2000... Training loss: 0.4418\n",
      "Epoch: 1170/2000... Training loss: 0.3491\n",
      "Epoch: 1170/2000... Training loss: 0.3946\n",
      "Epoch: 1170/2000... Training loss: 0.3913\n",
      "Epoch: 1170/2000... Training loss: 0.4033\n",
      "Epoch: 1170/2000... Training loss: 0.4848\n",
      "Epoch: 1170/2000... Training loss: 0.3859\n",
      "Epoch: 1171/2000... Training loss: 0.4264\n",
      "Epoch: 1171/2000... Training loss: 0.4264\n",
      "Epoch: 1171/2000... Training loss: 0.6975\n",
      "Epoch: 1171/2000... Training loss: 0.5362\n",
      "Epoch: 1171/2000... Training loss: 0.3705\n",
      "Epoch: 1171/2000... Training loss: 0.6573\n",
      "Epoch: 1171/2000... Training loss: 0.2790\n",
      "Epoch: 1171/2000... Training loss: 0.4228\n",
      "Epoch: 1171/2000... Training loss: 0.3608\n",
      "Epoch: 1171/2000... Training loss: 0.3980\n",
      "Epoch: 1171/2000... Training loss: 0.4892\n",
      "Epoch: 1171/2000... Training loss: 0.3871\n",
      "Epoch: 1171/2000... Training loss: 0.4726\n",
      "Epoch: 1171/2000... Training loss: 0.4397\n",
      "Epoch: 1171/2000... Training loss: 0.4347\n",
      "Epoch: 1171/2000... Training loss: 0.5280\n",
      "Epoch: 1171/2000... Training loss: 0.4045\n",
      "Epoch: 1171/2000... Training loss: 0.2812\n",
      "Epoch: 1171/2000... Training loss: 0.5255\n",
      "Epoch: 1171/2000... Training loss: 0.4176\n",
      "Epoch: 1171/2000... Training loss: 0.5443\n",
      "Epoch: 1171/2000... Training loss: 0.2619\n",
      "Epoch: 1171/2000... Training loss: 0.5185\n",
      "Epoch: 1171/2000... Training loss: 0.4749\n",
      "Epoch: 1171/2000... Training loss: 0.3399\n",
      "Epoch: 1171/2000... Training loss: 0.6146\n",
      "Epoch: 1171/2000... Training loss: 0.4463\n",
      "Epoch: 1171/2000... Training loss: 0.4652\n",
      "Epoch: 1171/2000... Training loss: 0.4187\n",
      "Epoch: 1171/2000... Training loss: 0.5575\n",
      "Epoch: 1171/2000... Training loss: 0.4931\n",
      "Epoch: 1172/2000... Training loss: 0.4313\n",
      "Epoch: 1172/2000... Training loss: 0.6161\n",
      "Epoch: 1172/2000... Training loss: 0.5442\n",
      "Epoch: 1172/2000... Training loss: 0.5313\n",
      "Epoch: 1172/2000... Training loss: 0.3706\n",
      "Epoch: 1172/2000... Training loss: 0.3175\n",
      "Epoch: 1172/2000... Training loss: 0.3419\n",
      "Epoch: 1172/2000... Training loss: 0.3796\n",
      "Epoch: 1172/2000... Training loss: 0.4380\n",
      "Epoch: 1172/2000... Training loss: 0.5214\n",
      "Epoch: 1172/2000... Training loss: 0.4354\n",
      "Epoch: 1172/2000... Training loss: 0.4405\n",
      "Epoch: 1172/2000... Training loss: 0.4379\n",
      "Epoch: 1172/2000... Training loss: 0.3764\n",
      "Epoch: 1172/2000... Training loss: 0.4430\n",
      "Epoch: 1172/2000... Training loss: 0.2816\n",
      "Epoch: 1172/2000... Training loss: 0.3995\n",
      "Epoch: 1172/2000... Training loss: 0.4613\n",
      "Epoch: 1172/2000... Training loss: 0.4326\n",
      "Epoch: 1172/2000... Training loss: 0.4978\n",
      "Epoch: 1172/2000... Training loss: 0.3767\n",
      "Epoch: 1172/2000... Training loss: 0.3775\n",
      "Epoch: 1172/2000... Training loss: 0.5687\n",
      "Epoch: 1172/2000... Training loss: 0.4308\n",
      "Epoch: 1172/2000... Training loss: 0.6148\n",
      "Epoch: 1172/2000... Training loss: 0.3889\n",
      "Epoch: 1172/2000... Training loss: 0.4186\n",
      "Epoch: 1172/2000... Training loss: 0.3425\n",
      "Epoch: 1172/2000... Training loss: 0.5269\n",
      "Epoch: 1172/2000... Training loss: 0.3945\n",
      "Epoch: 1172/2000... Training loss: 0.4844\n",
      "Epoch: 1173/2000... Training loss: 0.4092\n",
      "Epoch: 1173/2000... Training loss: 0.5842\n",
      "Epoch: 1173/2000... Training loss: 0.5638\n",
      "Epoch: 1173/2000... Training loss: 0.5053\n",
      "Epoch: 1173/2000... Training loss: 0.4689\n",
      "Epoch: 1173/2000... Training loss: 0.5396\n",
      "Epoch: 1173/2000... Training loss: 0.4032\n",
      "Epoch: 1173/2000... Training loss: 0.3841\n",
      "Epoch: 1173/2000... Training loss: 0.4671\n",
      "Epoch: 1173/2000... Training loss: 0.4833\n",
      "Epoch: 1173/2000... Training loss: 0.5019\n",
      "Epoch: 1173/2000... Training loss: 0.5370\n",
      "Epoch: 1173/2000... Training loss: 0.3073\n",
      "Epoch: 1173/2000... Training loss: 0.5481\n",
      "Epoch: 1173/2000... Training loss: 0.4940\n",
      "Epoch: 1173/2000... Training loss: 0.5021\n",
      "Epoch: 1173/2000... Training loss: 0.4541\n",
      "Epoch: 1173/2000... Training loss: 0.4300\n",
      "Epoch: 1173/2000... Training loss: 0.5242\n",
      "Epoch: 1173/2000... Training loss: 0.5966\n",
      "Epoch: 1173/2000... Training loss: 0.4611\n",
      "Epoch: 1173/2000... Training loss: 0.5929\n",
      "Epoch: 1173/2000... Training loss: 0.3868\n",
      "Epoch: 1173/2000... Training loss: 0.3694\n",
      "Epoch: 1173/2000... Training loss: 0.5893\n",
      "Epoch: 1173/2000... Training loss: 0.7782\n",
      "Epoch: 1173/2000... Training loss: 0.4430\n",
      "Epoch: 1173/2000... Training loss: 0.3841\n",
      "Epoch: 1173/2000... Training loss: 0.3007\n",
      "Epoch: 1173/2000... Training loss: 0.7158\n",
      "Epoch: 1173/2000... Training loss: 0.4621\n",
      "Epoch: 1174/2000... Training loss: 0.4180\n",
      "Epoch: 1174/2000... Training loss: 0.5492\n",
      "Epoch: 1174/2000... Training loss: 0.4439\n",
      "Epoch: 1174/2000... Training loss: 0.3515\n",
      "Epoch: 1174/2000... Training loss: 0.6190\n",
      "Epoch: 1174/2000... Training loss: 0.4585\n",
      "Epoch: 1174/2000... Training loss: 0.5551\n",
      "Epoch: 1174/2000... Training loss: 0.3872\n",
      "Epoch: 1174/2000... Training loss: 0.6879\n",
      "Epoch: 1174/2000... Training loss: 0.3977\n",
      "Epoch: 1174/2000... Training loss: 0.5945\n",
      "Epoch: 1174/2000... Training loss: 0.5446\n",
      "Epoch: 1174/2000... Training loss: 0.5517\n",
      "Epoch: 1174/2000... Training loss: 0.4606\n",
      "Epoch: 1174/2000... Training loss: 0.4891\n",
      "Epoch: 1174/2000... Training loss: 0.4737\n",
      "Epoch: 1174/2000... Training loss: 0.4903\n",
      "Epoch: 1174/2000... Training loss: 0.3887\n",
      "Epoch: 1174/2000... Training loss: 0.5674\n",
      "Epoch: 1174/2000... Training loss: 0.4531\n",
      "Epoch: 1174/2000... Training loss: 0.3834\n",
      "Epoch: 1174/2000... Training loss: 0.6768\n",
      "Epoch: 1174/2000... Training loss: 0.4778\n",
      "Epoch: 1174/2000... Training loss: 0.4720\n",
      "Epoch: 1174/2000... Training loss: 0.5471\n",
      "Epoch: 1174/2000... Training loss: 0.4883\n",
      "Epoch: 1174/2000... Training loss: 0.4388\n",
      "Epoch: 1174/2000... Training loss: 0.5889\n",
      "Epoch: 1174/2000... Training loss: 0.4094\n",
      "Epoch: 1174/2000... Training loss: 0.4667\n",
      "Epoch: 1174/2000... Training loss: 0.4501\n",
      "Epoch: 1175/2000... Training loss: 0.3561\n",
      "Epoch: 1175/2000... Training loss: 0.3996\n",
      "Epoch: 1175/2000... Training loss: 0.3494\n",
      "Epoch: 1175/2000... Training loss: 0.5019\n",
      "Epoch: 1175/2000... Training loss: 0.4782\n",
      "Epoch: 1175/2000... Training loss: 0.3288\n",
      "Epoch: 1175/2000... Training loss: 0.5712\n",
      "Epoch: 1175/2000... Training loss: 0.4970\n",
      "Epoch: 1175/2000... Training loss: 0.6075\n",
      "Epoch: 1175/2000... Training loss: 0.3503\n",
      "Epoch: 1175/2000... Training loss: 0.5769\n",
      "Epoch: 1175/2000... Training loss: 0.3401\n",
      "Epoch: 1175/2000... Training loss: 0.3774\n",
      "Epoch: 1175/2000... Training loss: 0.6317\n",
      "Epoch: 1175/2000... Training loss: 0.5385\n",
      "Epoch: 1175/2000... Training loss: 0.3014\n",
      "Epoch: 1175/2000... Training loss: 0.5566\n",
      "Epoch: 1175/2000... Training loss: 0.3699\n",
      "Epoch: 1175/2000... Training loss: 0.2226\n",
      "Epoch: 1175/2000... Training loss: 0.4100\n",
      "Epoch: 1175/2000... Training loss: 0.5384\n",
      "Epoch: 1175/2000... Training loss: 0.3711\n",
      "Epoch: 1175/2000... Training loss: 0.3561\n",
      "Epoch: 1175/2000... Training loss: 0.4866\n",
      "Epoch: 1175/2000... Training loss: 0.3581\n",
      "Epoch: 1175/2000... Training loss: 0.3929\n",
      "Epoch: 1175/2000... Training loss: 0.4040\n",
      "Epoch: 1175/2000... Training loss: 0.4718\n",
      "Epoch: 1175/2000... Training loss: 0.4549\n",
      "Epoch: 1175/2000... Training loss: 0.5409\n",
      "Epoch: 1175/2000... Training loss: 0.5847\n",
      "Epoch: 1176/2000... Training loss: 0.3666\n",
      "Epoch: 1176/2000... Training loss: 0.4843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1176/2000... Training loss: 0.4546\n",
      "Epoch: 1176/2000... Training loss: 0.6119\n",
      "Epoch: 1176/2000... Training loss: 0.5045\n",
      "Epoch: 1176/2000... Training loss: 0.4179\n",
      "Epoch: 1176/2000... Training loss: 0.3172\n",
      "Epoch: 1176/2000... Training loss: 0.3457\n",
      "Epoch: 1176/2000... Training loss: 0.4843\n",
      "Epoch: 1176/2000... Training loss: 0.3439\n",
      "Epoch: 1176/2000... Training loss: 0.3670\n",
      "Epoch: 1176/2000... Training loss: 0.4616\n",
      "Epoch: 1176/2000... Training loss: 0.5259\n",
      "Epoch: 1176/2000... Training loss: 0.3385\n",
      "Epoch: 1176/2000... Training loss: 0.4763\n",
      "Epoch: 1176/2000... Training loss: 0.3888\n",
      "Epoch: 1176/2000... Training loss: 0.4765\n",
      "Epoch: 1176/2000... Training loss: 0.3032\n",
      "Epoch: 1176/2000... Training loss: 0.2858\n",
      "Epoch: 1176/2000... Training loss: 0.3975\n",
      "Epoch: 1176/2000... Training loss: 0.5063\n",
      "Epoch: 1176/2000... Training loss: 0.3191\n",
      "Epoch: 1176/2000... Training loss: 0.3796\n",
      "Epoch: 1176/2000... Training loss: 0.5222\n",
      "Epoch: 1176/2000... Training loss: 0.4132\n",
      "Epoch: 1176/2000... Training loss: 0.4894\n",
      "Epoch: 1176/2000... Training loss: 0.4349\n",
      "Epoch: 1176/2000... Training loss: 0.4316\n",
      "Epoch: 1176/2000... Training loss: 0.7342\n",
      "Epoch: 1176/2000... Training loss: 0.3966\n",
      "Epoch: 1176/2000... Training loss: 0.6304\n",
      "Epoch: 1177/2000... Training loss: 0.4458\n",
      "Epoch: 1177/2000... Training loss: 0.6064\n",
      "Epoch: 1177/2000... Training loss: 0.4475\n",
      "Epoch: 1177/2000... Training loss: 0.3909\n",
      "Epoch: 1177/2000... Training loss: 0.3729\n",
      "Epoch: 1177/2000... Training loss: 0.4837\n",
      "Epoch: 1177/2000... Training loss: 0.2949\n",
      "Epoch: 1177/2000... Training loss: 0.6352\n",
      "Epoch: 1177/2000... Training loss: 0.5166\n",
      "Epoch: 1177/2000... Training loss: 0.4564\n",
      "Epoch: 1177/2000... Training loss: 0.4015\n",
      "Epoch: 1177/2000... Training loss: 0.5528\n",
      "Epoch: 1177/2000... Training loss: 0.4069\n",
      "Epoch: 1177/2000... Training loss: 0.3492\n",
      "Epoch: 1177/2000... Training loss: 0.4053\n",
      "Epoch: 1177/2000... Training loss: 0.4991\n",
      "Epoch: 1177/2000... Training loss: 0.3826\n",
      "Epoch: 1177/2000... Training loss: 0.5487\n",
      "Epoch: 1177/2000... Training loss: 0.6518\n",
      "Epoch: 1177/2000... Training loss: 0.6066\n",
      "Epoch: 1177/2000... Training loss: 0.3648\n",
      "Epoch: 1177/2000... Training loss: 0.4792\n",
      "Epoch: 1177/2000... Training loss: 0.8035\n",
      "Epoch: 1177/2000... Training loss: 0.5853\n",
      "Epoch: 1177/2000... Training loss: 0.4763\n",
      "Epoch: 1177/2000... Training loss: 0.4308\n",
      "Epoch: 1177/2000... Training loss: 0.3487\n",
      "Epoch: 1177/2000... Training loss: 0.5276\n",
      "Epoch: 1177/2000... Training loss: 0.3118\n",
      "Epoch: 1177/2000... Training loss: 0.4175\n",
      "Epoch: 1177/2000... Training loss: 0.5234\n",
      "Epoch: 1178/2000... Training loss: 0.5082\n",
      "Epoch: 1178/2000... Training loss: 0.5461\n",
      "Epoch: 1178/2000... Training loss: 0.4740\n",
      "Epoch: 1178/2000... Training loss: 0.3320\n",
      "Epoch: 1178/2000... Training loss: 0.4232\n",
      "Epoch: 1178/2000... Training loss: 0.4812\n",
      "Epoch: 1178/2000... Training loss: 0.3447\n",
      "Epoch: 1178/2000... Training loss: 0.4856\n",
      "Epoch: 1178/2000... Training loss: 0.5142\n",
      "Epoch: 1178/2000... Training loss: 0.4599\n",
      "Epoch: 1178/2000... Training loss: 0.5823\n",
      "Epoch: 1178/2000... Training loss: 0.3468\n",
      "Epoch: 1178/2000... Training loss: 0.4703\n",
      "Epoch: 1178/2000... Training loss: 0.5852\n",
      "Epoch: 1178/2000... Training loss: 0.4968\n",
      "Epoch: 1178/2000... Training loss: 0.3943\n",
      "Epoch: 1178/2000... Training loss: 0.4948\n",
      "Epoch: 1178/2000... Training loss: 0.5289\n",
      "Epoch: 1178/2000... Training loss: 0.6222\n",
      "Epoch: 1178/2000... Training loss: 0.4883\n",
      "Epoch: 1178/2000... Training loss: 0.4680\n",
      "Epoch: 1178/2000... Training loss: 0.4632\n",
      "Epoch: 1178/2000... Training loss: 0.4322\n",
      "Epoch: 1178/2000... Training loss: 0.3956\n",
      "Epoch: 1178/2000... Training loss: 0.4386\n",
      "Epoch: 1178/2000... Training loss: 0.4303\n",
      "Epoch: 1178/2000... Training loss: 0.4399\n",
      "Epoch: 1178/2000... Training loss: 0.4214\n",
      "Epoch: 1178/2000... Training loss: 0.5260\n",
      "Epoch: 1178/2000... Training loss: 0.5013\n",
      "Epoch: 1178/2000... Training loss: 0.5103\n",
      "Epoch: 1179/2000... Training loss: 0.3992\n",
      "Epoch: 1179/2000... Training loss: 0.5920\n",
      "Epoch: 1179/2000... Training loss: 0.5526\n",
      "Epoch: 1179/2000... Training loss: 0.5293\n",
      "Epoch: 1179/2000... Training loss: 0.3660\n",
      "Epoch: 1179/2000... Training loss: 0.6083\n",
      "Epoch: 1179/2000... Training loss: 0.3754\n",
      "Epoch: 1179/2000... Training loss: 0.5141\n",
      "Epoch: 1179/2000... Training loss: 0.4135\n",
      "Epoch: 1179/2000... Training loss: 0.3487\n",
      "Epoch: 1179/2000... Training loss: 0.5291\n",
      "Epoch: 1179/2000... Training loss: 0.4366\n",
      "Epoch: 1179/2000... Training loss: 0.5133\n",
      "Epoch: 1179/2000... Training loss: 0.4853\n",
      "Epoch: 1179/2000... Training loss: 0.3207\n",
      "Epoch: 1179/2000... Training loss: 0.4965\n",
      "Epoch: 1179/2000... Training loss: 0.3263\n",
      "Epoch: 1179/2000... Training loss: 0.4965\n",
      "Epoch: 1179/2000... Training loss: 0.3929\n",
      "Epoch: 1179/2000... Training loss: 0.5197\n",
      "Epoch: 1179/2000... Training loss: 0.3951\n",
      "Epoch: 1179/2000... Training loss: 0.6248\n",
      "Epoch: 1179/2000... Training loss: 0.4679\n",
      "Epoch: 1179/2000... Training loss: 0.6213\n",
      "Epoch: 1179/2000... Training loss: 0.6127\n",
      "Epoch: 1179/2000... Training loss: 0.4285\n",
      "Epoch: 1179/2000... Training loss: 0.6835\n",
      "Epoch: 1179/2000... Training loss: 0.4544\n",
      "Epoch: 1179/2000... Training loss: 0.3682\n",
      "Epoch: 1179/2000... Training loss: 0.6155\n",
      "Epoch: 1179/2000... Training loss: 0.3717\n",
      "Epoch: 1180/2000... Training loss: 0.4773\n",
      "Epoch: 1180/2000... Training loss: 0.5739\n",
      "Epoch: 1180/2000... Training loss: 0.4103\n",
      "Epoch: 1180/2000... Training loss: 0.4489\n",
      "Epoch: 1180/2000... Training loss: 0.3981\n",
      "Epoch: 1180/2000... Training loss: 0.3662\n",
      "Epoch: 1180/2000... Training loss: 0.4623\n",
      "Epoch: 1180/2000... Training loss: 0.6806\n",
      "Epoch: 1180/2000... Training loss: 0.4778\n",
      "Epoch: 1180/2000... Training loss: 0.6008\n",
      "Epoch: 1180/2000... Training loss: 0.4505\n",
      "Epoch: 1180/2000... Training loss: 0.3898\n",
      "Epoch: 1180/2000... Training loss: 0.3815\n",
      "Epoch: 1180/2000... Training loss: 0.6432\n",
      "Epoch: 1180/2000... Training loss: 0.5054\n",
      "Epoch: 1180/2000... Training loss: 0.5815\n",
      "Epoch: 1180/2000... Training loss: 0.5776\n",
      "Epoch: 1180/2000... Training loss: 0.4769\n",
      "Epoch: 1180/2000... Training loss: 0.4581\n",
      "Epoch: 1180/2000... Training loss: 0.2918\n",
      "Epoch: 1180/2000... Training loss: 0.3929\n",
      "Epoch: 1180/2000... Training loss: 0.3503\n",
      "Epoch: 1180/2000... Training loss: 0.2781\n",
      "Epoch: 1180/2000... Training loss: 0.6191\n",
      "Epoch: 1180/2000... Training loss: 0.4124\n",
      "Epoch: 1180/2000... Training loss: 0.4596\n",
      "Epoch: 1180/2000... Training loss: 0.5325\n",
      "Epoch: 1180/2000... Training loss: 0.5255\n",
      "Epoch: 1180/2000... Training loss: 0.3449\n",
      "Epoch: 1180/2000... Training loss: 0.5456\n",
      "Epoch: 1180/2000... Training loss: 0.3527\n",
      "Epoch: 1181/2000... Training loss: 0.4406\n",
      "Epoch: 1181/2000... Training loss: 0.4636\n",
      "Epoch: 1181/2000... Training loss: 0.5541\n",
      "Epoch: 1181/2000... Training loss: 0.4692\n",
      "Epoch: 1181/2000... Training loss: 0.3172\n",
      "Epoch: 1181/2000... Training loss: 0.6486\n",
      "Epoch: 1181/2000... Training loss: 0.3358\n",
      "Epoch: 1181/2000... Training loss: 0.4482\n",
      "Epoch: 1181/2000... Training loss: 0.5143\n",
      "Epoch: 1181/2000... Training loss: 0.4276\n",
      "Epoch: 1181/2000... Training loss: 0.3539\n",
      "Epoch: 1181/2000... Training loss: 0.4342\n",
      "Epoch: 1181/2000... Training loss: 0.3850\n",
      "Epoch: 1181/2000... Training loss: 0.4638\n",
      "Epoch: 1181/2000... Training loss: 0.4463\n",
      "Epoch: 1181/2000... Training loss: 0.6070\n",
      "Epoch: 1181/2000... Training loss: 0.3122\n",
      "Epoch: 1181/2000... Training loss: 0.5641\n",
      "Epoch: 1181/2000... Training loss: 0.6665\n",
      "Epoch: 1181/2000... Training loss: 0.4483\n",
      "Epoch: 1181/2000... Training loss: 0.4245\n",
      "Epoch: 1181/2000... Training loss: 0.4507\n",
      "Epoch: 1181/2000... Training loss: 0.3871\n",
      "Epoch: 1181/2000... Training loss: 0.6094\n",
      "Epoch: 1181/2000... Training loss: 0.4656\n",
      "Epoch: 1181/2000... Training loss: 0.5006\n",
      "Epoch: 1181/2000... Training loss: 0.3505\n",
      "Epoch: 1181/2000... Training loss: 0.6243\n",
      "Epoch: 1181/2000... Training loss: 0.5800\n",
      "Epoch: 1181/2000... Training loss: 0.5746\n",
      "Epoch: 1181/2000... Training loss: 0.4959\n",
      "Epoch: 1182/2000... Training loss: 0.4984\n",
      "Epoch: 1182/2000... Training loss: 0.4625\n",
      "Epoch: 1182/2000... Training loss: 0.6619\n",
      "Epoch: 1182/2000... Training loss: 0.4258\n",
      "Epoch: 1182/2000... Training loss: 0.4114\n",
      "Epoch: 1182/2000... Training loss: 0.4237\n",
      "Epoch: 1182/2000... Training loss: 0.3598\n",
      "Epoch: 1182/2000... Training loss: 0.6349\n",
      "Epoch: 1182/2000... Training loss: 0.6165\n",
      "Epoch: 1182/2000... Training loss: 0.5890\n",
      "Epoch: 1182/2000... Training loss: 0.6226\n",
      "Epoch: 1182/2000... Training loss: 0.5073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1182/2000... Training loss: 0.5782\n",
      "Epoch: 1182/2000... Training loss: 0.4771\n",
      "Epoch: 1182/2000... Training loss: 0.5662\n",
      "Epoch: 1182/2000... Training loss: 0.3611\n",
      "Epoch: 1182/2000... Training loss: 0.4885\n",
      "Epoch: 1182/2000... Training loss: 0.3793\n",
      "Epoch: 1182/2000... Training loss: 0.4418\n",
      "Epoch: 1182/2000... Training loss: 0.4647\n",
      "Epoch: 1182/2000... Training loss: 0.4860\n",
      "Epoch: 1182/2000... Training loss: 0.4807\n",
      "Epoch: 1182/2000... Training loss: 0.4654\n",
      "Epoch: 1182/2000... Training loss: 0.4189\n",
      "Epoch: 1182/2000... Training loss: 0.5849\n",
      "Epoch: 1182/2000... Training loss: 0.5204\n",
      "Epoch: 1182/2000... Training loss: 0.5578\n",
      "Epoch: 1182/2000... Training loss: 0.3948\n",
      "Epoch: 1182/2000... Training loss: 0.4526\n",
      "Epoch: 1182/2000... Training loss: 0.3899\n",
      "Epoch: 1182/2000... Training loss: 0.4730\n",
      "Epoch: 1183/2000... Training loss: 0.4819\n",
      "Epoch: 1183/2000... Training loss: 0.4122\n",
      "Epoch: 1183/2000... Training loss: 0.5514\n",
      "Epoch: 1183/2000... Training loss: 0.4088\n",
      "Epoch: 1183/2000... Training loss: 0.5488\n",
      "Epoch: 1183/2000... Training loss: 0.4077\n",
      "Epoch: 1183/2000... Training loss: 0.3747\n",
      "Epoch: 1183/2000... Training loss: 0.5915\n",
      "Epoch: 1183/2000... Training loss: 0.4990\n",
      "Epoch: 1183/2000... Training loss: 0.4126\n",
      "Epoch: 1183/2000... Training loss: 0.4748\n",
      "Epoch: 1183/2000... Training loss: 0.3773\n",
      "Epoch: 1183/2000... Training loss: 0.4257\n",
      "Epoch: 1183/2000... Training loss: 0.4979\n",
      "Epoch: 1183/2000... Training loss: 0.4545\n",
      "Epoch: 1183/2000... Training loss: 0.6653\n",
      "Epoch: 1183/2000... Training loss: 0.5463\n",
      "Epoch: 1183/2000... Training loss: 0.3694\n",
      "Epoch: 1183/2000... Training loss: 0.5244\n",
      "Epoch: 1183/2000... Training loss: 0.3581\n",
      "Epoch: 1183/2000... Training loss: 0.6213\n",
      "Epoch: 1183/2000... Training loss: 0.5048\n",
      "Epoch: 1183/2000... Training loss: 0.5327\n",
      "Epoch: 1183/2000... Training loss: 0.5635\n",
      "Epoch: 1183/2000... Training loss: 0.6441\n",
      "Epoch: 1183/2000... Training loss: 0.5975\n",
      "Epoch: 1183/2000... Training loss: 0.3651\n",
      "Epoch: 1183/2000... Training loss: 0.5057\n",
      "Epoch: 1183/2000... Training loss: 0.5912\n",
      "Epoch: 1183/2000... Training loss: 0.4244\n",
      "Epoch: 1183/2000... Training loss: 0.5823\n",
      "Epoch: 1184/2000... Training loss: 0.4717\n",
      "Epoch: 1184/2000... Training loss: 0.5555\n",
      "Epoch: 1184/2000... Training loss: 0.6700\n",
      "Epoch: 1184/2000... Training loss: 0.6037\n",
      "Epoch: 1184/2000... Training loss: 0.4005\n",
      "Epoch: 1184/2000... Training loss: 0.6241\n",
      "Epoch: 1184/2000... Training loss: 0.4307\n",
      "Epoch: 1184/2000... Training loss: 0.5161\n",
      "Epoch: 1184/2000... Training loss: 0.2985\n",
      "Epoch: 1184/2000... Training loss: 0.5609\n",
      "Epoch: 1184/2000... Training loss: 0.3640\n",
      "Epoch: 1184/2000... Training loss: 0.5323\n",
      "Epoch: 1184/2000... Training loss: 0.5338\n",
      "Epoch: 1184/2000... Training loss: 0.4020\n",
      "Epoch: 1184/2000... Training loss: 0.5857\n",
      "Epoch: 1184/2000... Training loss: 0.6748\n",
      "Epoch: 1184/2000... Training loss: 0.4497\n",
      "Epoch: 1184/2000... Training loss: 0.5705\n",
      "Epoch: 1184/2000... Training loss: 0.5045\n",
      "Epoch: 1184/2000... Training loss: 0.5589\n",
      "Epoch: 1184/2000... Training loss: 0.5988\n",
      "Epoch: 1184/2000... Training loss: 0.5163\n",
      "Epoch: 1184/2000... Training loss: 0.3768\n",
      "Epoch: 1184/2000... Training loss: 0.5051\n",
      "Epoch: 1184/2000... Training loss: 0.4161\n",
      "Epoch: 1184/2000... Training loss: 0.5196\n",
      "Epoch: 1184/2000... Training loss: 0.4688\n",
      "Epoch: 1184/2000... Training loss: 0.5246\n",
      "Epoch: 1184/2000... Training loss: 0.4732\n",
      "Epoch: 1184/2000... Training loss: 0.3054\n",
      "Epoch: 1184/2000... Training loss: 0.3530\n",
      "Epoch: 1185/2000... Training loss: 0.4685\n",
      "Epoch: 1185/2000... Training loss: 0.6564\n",
      "Epoch: 1185/2000... Training loss: 0.5424\n",
      "Epoch: 1185/2000... Training loss: 0.5000\n",
      "Epoch: 1185/2000... Training loss: 0.3537\n",
      "Epoch: 1185/2000... Training loss: 0.4095\n",
      "Epoch: 1185/2000... Training loss: 0.4455\n",
      "Epoch: 1185/2000... Training loss: 0.6356\n",
      "Epoch: 1185/2000... Training loss: 0.5935\n",
      "Epoch: 1185/2000... Training loss: 0.4290\n",
      "Epoch: 1185/2000... Training loss: 0.5138\n",
      "Epoch: 1185/2000... Training loss: 0.5109\n",
      "Epoch: 1185/2000... Training loss: 0.5843\n",
      "Epoch: 1185/2000... Training loss: 0.4059\n",
      "Epoch: 1185/2000... Training loss: 0.3562\n",
      "Epoch: 1185/2000... Training loss: 0.3122\n",
      "Epoch: 1185/2000... Training loss: 0.3856\n",
      "Epoch: 1185/2000... Training loss: 0.3937\n",
      "Epoch: 1185/2000... Training loss: 0.3295\n",
      "Epoch: 1185/2000... Training loss: 0.5032\n",
      "Epoch: 1185/2000... Training loss: 0.4921\n",
      "Epoch: 1185/2000... Training loss: 0.5144\n",
      "Epoch: 1185/2000... Training loss: 0.5506\n",
      "Epoch: 1185/2000... Training loss: 0.5249\n",
      "Epoch: 1185/2000... Training loss: 0.5172\n",
      "Epoch: 1185/2000... Training loss: 0.5310\n",
      "Epoch: 1185/2000... Training loss: 0.4484\n",
      "Epoch: 1185/2000... Training loss: 0.2224\n",
      "Epoch: 1185/2000... Training loss: 0.5420\n",
      "Epoch: 1185/2000... Training loss: 0.4832\n",
      "Epoch: 1185/2000... Training loss: 0.4646\n",
      "Epoch: 1186/2000... Training loss: 0.3112\n",
      "Epoch: 1186/2000... Training loss: 0.6057\n",
      "Epoch: 1186/2000... Training loss: 0.4158\n",
      "Epoch: 1186/2000... Training loss: 0.4043\n",
      "Epoch: 1186/2000... Training loss: 0.7105\n",
      "Epoch: 1186/2000... Training loss: 0.3481\n",
      "Epoch: 1186/2000... Training loss: 0.4124\n",
      "Epoch: 1186/2000... Training loss: 0.5086\n",
      "Epoch: 1186/2000... Training loss: 0.5986\n",
      "Epoch: 1186/2000... Training loss: 0.3847\n",
      "Epoch: 1186/2000... Training loss: 0.5487\n",
      "Epoch: 1186/2000... Training loss: 0.3832\n",
      "Epoch: 1186/2000... Training loss: 0.4323\n",
      "Epoch: 1186/2000... Training loss: 0.5672\n",
      "Epoch: 1186/2000... Training loss: 0.4196\n",
      "Epoch: 1186/2000... Training loss: 0.4573\n",
      "Epoch: 1186/2000... Training loss: 0.4525\n",
      "Epoch: 1186/2000... Training loss: 0.6571\n",
      "Epoch: 1186/2000... Training loss: 0.6757\n",
      "Epoch: 1186/2000... Training loss: 0.5163\n",
      "Epoch: 1186/2000... Training loss: 0.4434\n",
      "Epoch: 1186/2000... Training loss: 0.4066\n",
      "Epoch: 1186/2000... Training loss: 0.2480\n",
      "Epoch: 1186/2000... Training loss: 0.5001\n",
      "Epoch: 1186/2000... Training loss: 0.4877\n",
      "Epoch: 1186/2000... Training loss: 0.6373\n",
      "Epoch: 1186/2000... Training loss: 0.4026\n",
      "Epoch: 1186/2000... Training loss: 0.2683\n",
      "Epoch: 1186/2000... Training loss: 0.4176\n",
      "Epoch: 1186/2000... Training loss: 0.5945\n",
      "Epoch: 1186/2000... Training loss: 0.6341\n",
      "Epoch: 1187/2000... Training loss: 0.3933\n",
      "Epoch: 1187/2000... Training loss: 0.5508\n",
      "Epoch: 1187/2000... Training loss: 0.5109\n",
      "Epoch: 1187/2000... Training loss: 0.4603\n",
      "Epoch: 1187/2000... Training loss: 0.5034\n",
      "Epoch: 1187/2000... Training loss: 0.3536\n",
      "Epoch: 1187/2000... Training loss: 0.5408\n",
      "Epoch: 1187/2000... Training loss: 0.3787\n",
      "Epoch: 1187/2000... Training loss: 0.3839\n",
      "Epoch: 1187/2000... Training loss: 0.5035\n",
      "Epoch: 1187/2000... Training loss: 0.3835\n",
      "Epoch: 1187/2000... Training loss: 0.4592\n",
      "Epoch: 1187/2000... Training loss: 0.4771\n",
      "Epoch: 1187/2000... Training loss: 0.4922\n",
      "Epoch: 1187/2000... Training loss: 0.4037\n",
      "Epoch: 1187/2000... Training loss: 0.3206\n",
      "Epoch: 1187/2000... Training loss: 0.5505\n",
      "Epoch: 1187/2000... Training loss: 0.5736\n",
      "Epoch: 1187/2000... Training loss: 0.3405\n",
      "Epoch: 1187/2000... Training loss: 0.2296\n",
      "Epoch: 1187/2000... Training loss: 0.5032\n",
      "Epoch: 1187/2000... Training loss: 0.4529\n",
      "Epoch: 1187/2000... Training loss: 0.6157\n",
      "Epoch: 1187/2000... Training loss: 0.4088\n",
      "Epoch: 1187/2000... Training loss: 0.3817\n",
      "Epoch: 1187/2000... Training loss: 0.5461\n",
      "Epoch: 1187/2000... Training loss: 0.4394\n",
      "Epoch: 1187/2000... Training loss: 0.4622\n",
      "Epoch: 1187/2000... Training loss: 0.4350\n",
      "Epoch: 1187/2000... Training loss: 0.5813\n",
      "Epoch: 1187/2000... Training loss: 0.3894\n",
      "Epoch: 1188/2000... Training loss: 0.4165\n",
      "Epoch: 1188/2000... Training loss: 0.4271\n",
      "Epoch: 1188/2000... Training loss: 0.5529\n",
      "Epoch: 1188/2000... Training loss: 0.5047\n",
      "Epoch: 1188/2000... Training loss: 0.3537\n",
      "Epoch: 1188/2000... Training loss: 0.3210\n",
      "Epoch: 1188/2000... Training loss: 0.5160\n",
      "Epoch: 1188/2000... Training loss: 0.5952\n",
      "Epoch: 1188/2000... Training loss: 0.5894\n",
      "Epoch: 1188/2000... Training loss: 0.4961\n",
      "Epoch: 1188/2000... Training loss: 0.4422\n",
      "Epoch: 1188/2000... Training loss: 0.2942\n",
      "Epoch: 1188/2000... Training loss: 0.4804\n",
      "Epoch: 1188/2000... Training loss: 0.3532\n",
      "Epoch: 1188/2000... Training loss: 0.4966\n",
      "Epoch: 1188/2000... Training loss: 0.5446\n",
      "Epoch: 1188/2000... Training loss: 0.4051\n",
      "Epoch: 1188/2000... Training loss: 0.3901\n",
      "Epoch: 1188/2000... Training loss: 0.4947\n",
      "Epoch: 1188/2000... Training loss: 0.6327\n",
      "Epoch: 1188/2000... Training loss: 0.3503\n",
      "Epoch: 1188/2000... Training loss: 0.4621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1188/2000... Training loss: 0.4606\n",
      "Epoch: 1188/2000... Training loss: 0.6378\n",
      "Epoch: 1188/2000... Training loss: 0.4088\n",
      "Epoch: 1188/2000... Training loss: 0.4793\n",
      "Epoch: 1188/2000... Training loss: 0.4997\n",
      "Epoch: 1188/2000... Training loss: 0.5815\n",
      "Epoch: 1188/2000... Training loss: 0.4223\n",
      "Epoch: 1188/2000... Training loss: 0.5007\n",
      "Epoch: 1188/2000... Training loss: 0.3638\n",
      "Epoch: 1189/2000... Training loss: 0.3828\n",
      "Epoch: 1189/2000... Training loss: 0.4295\n",
      "Epoch: 1189/2000... Training loss: 0.4587\n",
      "Epoch: 1189/2000... Training loss: 0.3731\n",
      "Epoch: 1189/2000... Training loss: 0.2805\n",
      "Epoch: 1189/2000... Training loss: 0.4962\n",
      "Epoch: 1189/2000... Training loss: 0.4343\n",
      "Epoch: 1189/2000... Training loss: 0.3705\n",
      "Epoch: 1189/2000... Training loss: 0.3290\n",
      "Epoch: 1189/2000... Training loss: 0.4472\n",
      "Epoch: 1189/2000... Training loss: 0.3983\n",
      "Epoch: 1189/2000... Training loss: 0.5228\n",
      "Epoch: 1189/2000... Training loss: 0.3258\n",
      "Epoch: 1189/2000... Training loss: 0.4418\n",
      "Epoch: 1189/2000... Training loss: 0.4389\n",
      "Epoch: 1189/2000... Training loss: 0.4503\n",
      "Epoch: 1189/2000... Training loss: 0.4978\n",
      "Epoch: 1189/2000... Training loss: 0.5360\n",
      "Epoch: 1189/2000... Training loss: 0.3544\n",
      "Epoch: 1189/2000... Training loss: 0.4680\n",
      "Epoch: 1189/2000... Training loss: 0.5035\n",
      "Epoch: 1189/2000... Training loss: 0.4745\n",
      "Epoch: 1189/2000... Training loss: 0.6106\n",
      "Epoch: 1189/2000... Training loss: 0.4171\n",
      "Epoch: 1189/2000... Training loss: 0.6371\n",
      "Epoch: 1189/2000... Training loss: 0.5430\n",
      "Epoch: 1189/2000... Training loss: 0.5081\n",
      "Epoch: 1189/2000... Training loss: 0.4519\n",
      "Epoch: 1189/2000... Training loss: 0.4545\n",
      "Epoch: 1189/2000... Training loss: 0.3932\n",
      "Epoch: 1189/2000... Training loss: 0.3737\n",
      "Epoch: 1190/2000... Training loss: 0.4981\n",
      "Epoch: 1190/2000... Training loss: 0.4140\n",
      "Epoch: 1190/2000... Training loss: 0.3752\n",
      "Epoch: 1190/2000... Training loss: 0.5207\n",
      "Epoch: 1190/2000... Training loss: 0.4887\n",
      "Epoch: 1190/2000... Training loss: 0.6229\n",
      "Epoch: 1190/2000... Training loss: 0.6158\n",
      "Epoch: 1190/2000... Training loss: 0.4580\n",
      "Epoch: 1190/2000... Training loss: 0.6136\n",
      "Epoch: 1190/2000... Training loss: 0.4323\n",
      "Epoch: 1190/2000... Training loss: 0.4861\n",
      "Epoch: 1190/2000... Training loss: 0.2105\n",
      "Epoch: 1190/2000... Training loss: 0.5376\n",
      "Epoch: 1190/2000... Training loss: 0.4652\n",
      "Epoch: 1190/2000... Training loss: 0.5491\n",
      "Epoch: 1190/2000... Training loss: 0.3149\n",
      "Epoch: 1190/2000... Training loss: 0.4854\n",
      "Epoch: 1190/2000... Training loss: 0.6564\n",
      "Epoch: 1190/2000... Training loss: 0.4856\n",
      "Epoch: 1190/2000... Training loss: 0.3751\n",
      "Epoch: 1190/2000... Training loss: 0.3660\n",
      "Epoch: 1190/2000... Training loss: 0.3578\n",
      "Epoch: 1190/2000... Training loss: 0.4279\n",
      "Epoch: 1190/2000... Training loss: 0.5353\n",
      "Epoch: 1190/2000... Training loss: 0.5147\n",
      "Epoch: 1190/2000... Training loss: 0.3541\n",
      "Epoch: 1190/2000... Training loss: 0.4703\n",
      "Epoch: 1190/2000... Training loss: 0.5432\n",
      "Epoch: 1190/2000... Training loss: 0.4686\n",
      "Epoch: 1190/2000... Training loss: 0.5598\n",
      "Epoch: 1190/2000... Training loss: 0.3248\n",
      "Epoch: 1191/2000... Training loss: 0.3519\n",
      "Epoch: 1191/2000... Training loss: 0.6181\n",
      "Epoch: 1191/2000... Training loss: 0.4442\n",
      "Epoch: 1191/2000... Training loss: 0.4806\n",
      "Epoch: 1191/2000... Training loss: 0.5823\n",
      "Epoch: 1191/2000... Training loss: 0.4251\n",
      "Epoch: 1191/2000... Training loss: 0.4428\n",
      "Epoch: 1191/2000... Training loss: 0.5210\n",
      "Epoch: 1191/2000... Training loss: 0.4973\n",
      "Epoch: 1191/2000... Training loss: 0.4083\n",
      "Epoch: 1191/2000... Training loss: 0.5052\n",
      "Epoch: 1191/2000... Training loss: 0.4890\n",
      "Epoch: 1191/2000... Training loss: 0.4746\n",
      "Epoch: 1191/2000... Training loss: 0.4996\n",
      "Epoch: 1191/2000... Training loss: 0.4421\n",
      "Epoch: 1191/2000... Training loss: 0.4426\n",
      "Epoch: 1191/2000... Training loss: 0.5710\n",
      "Epoch: 1191/2000... Training loss: 0.4182\n",
      "Epoch: 1191/2000... Training loss: 0.5358\n",
      "Epoch: 1191/2000... Training loss: 0.5017\n",
      "Epoch: 1191/2000... Training loss: 0.3838\n",
      "Epoch: 1191/2000... Training loss: 0.6035\n",
      "Epoch: 1191/2000... Training loss: 0.4221\n",
      "Epoch: 1191/2000... Training loss: 0.7015\n",
      "Epoch: 1191/2000... Training loss: 0.6175\n",
      "Epoch: 1191/2000... Training loss: 0.5676\n",
      "Epoch: 1191/2000... Training loss: 0.4555\n",
      "Epoch: 1191/2000... Training loss: 0.4096\n",
      "Epoch: 1191/2000... Training loss: 0.3866\n",
      "Epoch: 1191/2000... Training loss: 0.4283\n",
      "Epoch: 1191/2000... Training loss: 0.3927\n",
      "Epoch: 1192/2000... Training loss: 0.4489\n",
      "Epoch: 1192/2000... Training loss: 0.3502\n",
      "Epoch: 1192/2000... Training loss: 0.3893\n",
      "Epoch: 1192/2000... Training loss: 0.5339\n",
      "Epoch: 1192/2000... Training loss: 0.4939\n",
      "Epoch: 1192/2000... Training loss: 0.3809\n",
      "Epoch: 1192/2000... Training loss: 0.3734\n",
      "Epoch: 1192/2000... Training loss: 0.2679\n",
      "Epoch: 1192/2000... Training loss: 0.3772\n",
      "Epoch: 1192/2000... Training loss: 0.5633\n",
      "Epoch: 1192/2000... Training loss: 0.5093\n",
      "Epoch: 1192/2000... Training loss: 0.5201\n",
      "Epoch: 1192/2000... Training loss: 0.4594\n",
      "Epoch: 1192/2000... Training loss: 0.3999\n",
      "Epoch: 1192/2000... Training loss: 0.3530\n",
      "Epoch: 1192/2000... Training loss: 0.6320\n",
      "Epoch: 1192/2000... Training loss: 0.4342\n",
      "Epoch: 1192/2000... Training loss: 0.4303\n",
      "Epoch: 1192/2000... Training loss: 0.3853\n",
      "Epoch: 1192/2000... Training loss: 0.4260\n",
      "Epoch: 1192/2000... Training loss: 0.2595\n",
      "Epoch: 1192/2000... Training loss: 0.6124\n",
      "Epoch: 1192/2000... Training loss: 0.4777\n",
      "Epoch: 1192/2000... Training loss: 0.4487\n",
      "Epoch: 1192/2000... Training loss: 0.5315\n",
      "Epoch: 1192/2000... Training loss: 0.4218\n",
      "Epoch: 1192/2000... Training loss: 0.3551\n",
      "Epoch: 1192/2000... Training loss: 0.3981\n",
      "Epoch: 1192/2000... Training loss: 0.3868\n",
      "Epoch: 1192/2000... Training loss: 0.4654\n",
      "Epoch: 1192/2000... Training loss: 0.5317\n",
      "Epoch: 1193/2000... Training loss: 0.3971\n",
      "Epoch: 1193/2000... Training loss: 0.2872\n",
      "Epoch: 1193/2000... Training loss: 0.3224\n",
      "Epoch: 1193/2000... Training loss: 0.5230\n",
      "Epoch: 1193/2000... Training loss: 0.4282\n",
      "Epoch: 1193/2000... Training loss: 0.3892\n",
      "Epoch: 1193/2000... Training loss: 0.4145\n",
      "Epoch: 1193/2000... Training loss: 0.3923\n",
      "Epoch: 1193/2000... Training loss: 0.5668\n",
      "Epoch: 1193/2000... Training loss: 0.3115\n",
      "Epoch: 1193/2000... Training loss: 0.4639\n",
      "Epoch: 1193/2000... Training loss: 0.2967\n",
      "Epoch: 1193/2000... Training loss: 0.4044\n",
      "Epoch: 1193/2000... Training loss: 0.4368\n",
      "Epoch: 1193/2000... Training loss: 0.3958\n",
      "Epoch: 1193/2000... Training loss: 0.3544\n",
      "Epoch: 1193/2000... Training loss: 0.5244\n",
      "Epoch: 1193/2000... Training loss: 0.7550\n",
      "Epoch: 1193/2000... Training loss: 0.3475\n",
      "Epoch: 1193/2000... Training loss: 0.3584\n",
      "Epoch: 1193/2000... Training loss: 0.4942\n",
      "Epoch: 1193/2000... Training loss: 0.4071\n",
      "Epoch: 1193/2000... Training loss: 0.4385\n",
      "Epoch: 1193/2000... Training loss: 0.5929\n",
      "Epoch: 1193/2000... Training loss: 0.4431\n",
      "Epoch: 1193/2000... Training loss: 0.4919\n",
      "Epoch: 1193/2000... Training loss: 0.3964\n",
      "Epoch: 1193/2000... Training loss: 0.4011\n",
      "Epoch: 1193/2000... Training loss: 0.5121\n",
      "Epoch: 1193/2000... Training loss: 0.4285\n",
      "Epoch: 1193/2000... Training loss: 0.3073\n",
      "Epoch: 1194/2000... Training loss: 0.5127\n",
      "Epoch: 1194/2000... Training loss: 0.4219\n",
      "Epoch: 1194/2000... Training loss: 0.4176\n",
      "Epoch: 1194/2000... Training loss: 0.5464\n",
      "Epoch: 1194/2000... Training loss: 0.4132\n",
      "Epoch: 1194/2000... Training loss: 0.4190\n",
      "Epoch: 1194/2000... Training loss: 0.3938\n",
      "Epoch: 1194/2000... Training loss: 0.4005\n",
      "Epoch: 1194/2000... Training loss: 0.4692\n",
      "Epoch: 1194/2000... Training loss: 0.4305\n",
      "Epoch: 1194/2000... Training loss: 0.4750\n",
      "Epoch: 1194/2000... Training loss: 0.3680\n",
      "Epoch: 1194/2000... Training loss: 0.5414\n",
      "Epoch: 1194/2000... Training loss: 0.5226\n",
      "Epoch: 1194/2000... Training loss: 0.4425\n",
      "Epoch: 1194/2000... Training loss: 0.4942\n",
      "Epoch: 1194/2000... Training loss: 0.3775\n",
      "Epoch: 1194/2000... Training loss: 0.4957\n",
      "Epoch: 1194/2000... Training loss: 0.3746\n",
      "Epoch: 1194/2000... Training loss: 0.3300\n",
      "Epoch: 1194/2000... Training loss: 0.5049\n",
      "Epoch: 1194/2000... Training loss: 0.4598\n",
      "Epoch: 1194/2000... Training loss: 0.3949\n",
      "Epoch: 1194/2000... Training loss: 0.6859\n",
      "Epoch: 1194/2000... Training loss: 0.4523\n",
      "Epoch: 1194/2000... Training loss: 0.5001\n",
      "Epoch: 1194/2000... Training loss: 0.4737\n",
      "Epoch: 1194/2000... Training loss: 0.6212\n",
      "Epoch: 1194/2000... Training loss: 0.5113\n",
      "Epoch: 1194/2000... Training loss: 0.5955\n",
      "Epoch: 1194/2000... Training loss: 0.4567\n",
      "Epoch: 1195/2000... Training loss: 0.4922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1195/2000... Training loss: 0.3440\n",
      "Epoch: 1195/2000... Training loss: 0.3104\n",
      "Epoch: 1195/2000... Training loss: 0.6278\n",
      "Epoch: 1195/2000... Training loss: 0.5357\n",
      "Epoch: 1195/2000... Training loss: 0.5116\n",
      "Epoch: 1195/2000... Training loss: 0.3113\n",
      "Epoch: 1195/2000... Training loss: 0.4232\n",
      "Epoch: 1195/2000... Training loss: 0.6197\n",
      "Epoch: 1195/2000... Training loss: 0.4068\n",
      "Epoch: 1195/2000... Training loss: 0.4568\n",
      "Epoch: 1195/2000... Training loss: 0.3305\n",
      "Epoch: 1195/2000... Training loss: 0.4270\n",
      "Epoch: 1195/2000... Training loss: 0.4107\n",
      "Epoch: 1195/2000... Training loss: 0.5006\n",
      "Epoch: 1195/2000... Training loss: 0.2901\n",
      "Epoch: 1195/2000... Training loss: 0.3732\n",
      "Epoch: 1195/2000... Training loss: 0.4266\n",
      "Epoch: 1195/2000... Training loss: 0.4144\n",
      "Epoch: 1195/2000... Training loss: 0.3794\n",
      "Epoch: 1195/2000... Training loss: 0.5243\n",
      "Epoch: 1195/2000... Training loss: 0.3217\n",
      "Epoch: 1195/2000... Training loss: 0.6190\n",
      "Epoch: 1195/2000... Training loss: 0.3978\n",
      "Epoch: 1195/2000... Training loss: 0.5538\n",
      "Epoch: 1195/2000... Training loss: 0.6323\n",
      "Epoch: 1195/2000... Training loss: 0.5561\n",
      "Epoch: 1195/2000... Training loss: 0.5470\n",
      "Epoch: 1195/2000... Training loss: 0.4503\n",
      "Epoch: 1195/2000... Training loss: 0.5330\n",
      "Epoch: 1195/2000... Training loss: 0.5173\n",
      "Epoch: 1196/2000... Training loss: 0.6214\n",
      "Epoch: 1196/2000... Training loss: 0.6605\n",
      "Epoch: 1196/2000... Training loss: 0.5594\n",
      "Epoch: 1196/2000... Training loss: 0.3700\n",
      "Epoch: 1196/2000... Training loss: 0.5954\n",
      "Epoch: 1196/2000... Training loss: 0.4356\n",
      "Epoch: 1196/2000... Training loss: 0.4084\n",
      "Epoch: 1196/2000... Training loss: 0.4608\n",
      "Epoch: 1196/2000... Training loss: 0.3557\n",
      "Epoch: 1196/2000... Training loss: 0.3994\n",
      "Epoch: 1196/2000... Training loss: 0.4505\n",
      "Epoch: 1196/2000... Training loss: 0.4810\n",
      "Epoch: 1196/2000... Training loss: 0.4655\n",
      "Epoch: 1196/2000... Training loss: 0.3575\n",
      "Epoch: 1196/2000... Training loss: 0.4587\n",
      "Epoch: 1196/2000... Training loss: 0.6594\n",
      "Epoch: 1196/2000... Training loss: 0.4448\n",
      "Epoch: 1196/2000... Training loss: 0.5650\n",
      "Epoch: 1196/2000... Training loss: 0.3691\n",
      "Epoch: 1196/2000... Training loss: 0.5369\n",
      "Epoch: 1196/2000... Training loss: 0.3139\n",
      "Epoch: 1196/2000... Training loss: 0.6088\n",
      "Epoch: 1196/2000... Training loss: 0.4243\n",
      "Epoch: 1196/2000... Training loss: 0.5384\n",
      "Epoch: 1196/2000... Training loss: 0.4406\n",
      "Epoch: 1196/2000... Training loss: 0.5081\n",
      "Epoch: 1196/2000... Training loss: 0.4505\n",
      "Epoch: 1196/2000... Training loss: 0.4745\n",
      "Epoch: 1196/2000... Training loss: 0.4682\n",
      "Epoch: 1196/2000... Training loss: 0.5059\n",
      "Epoch: 1196/2000... Training loss: 0.3593\n",
      "Epoch: 1197/2000... Training loss: 0.3459\n",
      "Epoch: 1197/2000... Training loss: 0.4532\n",
      "Epoch: 1197/2000... Training loss: 0.4062\n",
      "Epoch: 1197/2000... Training loss: 0.4737\n",
      "Epoch: 1197/2000... Training loss: 0.3627\n",
      "Epoch: 1197/2000... Training loss: 0.5237\n",
      "Epoch: 1197/2000... Training loss: 0.5778\n",
      "Epoch: 1197/2000... Training loss: 0.3760\n",
      "Epoch: 1197/2000... Training loss: 0.5728\n",
      "Epoch: 1197/2000... Training loss: 0.4954\n",
      "Epoch: 1197/2000... Training loss: 0.5428\n",
      "Epoch: 1197/2000... Training loss: 0.3836\n",
      "Epoch: 1197/2000... Training loss: 0.4138\n",
      "Epoch: 1197/2000... Training loss: 0.3999\n",
      "Epoch: 1197/2000... Training loss: 0.4776\n",
      "Epoch: 1197/2000... Training loss: 0.4357\n",
      "Epoch: 1197/2000... Training loss: 0.4294\n",
      "Epoch: 1197/2000... Training loss: 0.2641\n",
      "Epoch: 1197/2000... Training loss: 0.3482\n",
      "Epoch: 1197/2000... Training loss: 0.4379\n",
      "Epoch: 1197/2000... Training loss: 0.5105\n",
      "Epoch: 1197/2000... Training loss: 0.4838\n",
      "Epoch: 1197/2000... Training loss: 0.3471\n",
      "Epoch: 1197/2000... Training loss: 0.3871\n",
      "Epoch: 1197/2000... Training loss: 0.5098\n",
      "Epoch: 1197/2000... Training loss: 0.2234\n",
      "Epoch: 1197/2000... Training loss: 0.3803\n",
      "Epoch: 1197/2000... Training loss: 0.3177\n",
      "Epoch: 1197/2000... Training loss: 0.2803\n",
      "Epoch: 1197/2000... Training loss: 0.3869\n",
      "Epoch: 1197/2000... Training loss: 0.3065\n",
      "Epoch: 1198/2000... Training loss: 0.5311\n",
      "Epoch: 1198/2000... Training loss: 0.3728\n",
      "Epoch: 1198/2000... Training loss: 0.4187\n",
      "Epoch: 1198/2000... Training loss: 0.4936\n",
      "Epoch: 1198/2000... Training loss: 0.4472\n",
      "Epoch: 1198/2000... Training loss: 0.4604\n",
      "Epoch: 1198/2000... Training loss: 0.5194\n",
      "Epoch: 1198/2000... Training loss: 0.4564\n",
      "Epoch: 1198/2000... Training loss: 0.4238\n",
      "Epoch: 1198/2000... Training loss: 0.2929\n",
      "Epoch: 1198/2000... Training loss: 0.6074\n",
      "Epoch: 1198/2000... Training loss: 0.4309\n",
      "Epoch: 1198/2000... Training loss: 0.4334\n",
      "Epoch: 1198/2000... Training loss: 0.6287\n",
      "Epoch: 1198/2000... Training loss: 0.5005\n",
      "Epoch: 1198/2000... Training loss: 0.5029\n",
      "Epoch: 1198/2000... Training loss: 0.4870\n",
      "Epoch: 1198/2000... Training loss: 0.3962\n",
      "Epoch: 1198/2000... Training loss: 0.4573\n",
      "Epoch: 1198/2000... Training loss: 0.5487\n",
      "Epoch: 1198/2000... Training loss: 0.3171\n",
      "Epoch: 1198/2000... Training loss: 0.4252\n",
      "Epoch: 1198/2000... Training loss: 0.4128\n",
      "Epoch: 1198/2000... Training loss: 0.5530\n",
      "Epoch: 1198/2000... Training loss: 0.3346\n",
      "Epoch: 1198/2000... Training loss: 0.4028\n",
      "Epoch: 1198/2000... Training loss: 0.5754\n",
      "Epoch: 1198/2000... Training loss: 0.4551\n",
      "Epoch: 1198/2000... Training loss: 0.4985\n",
      "Epoch: 1198/2000... Training loss: 0.3382\n",
      "Epoch: 1198/2000... Training loss: 0.5421\n",
      "Epoch: 1199/2000... Training loss: 0.7493\n",
      "Epoch: 1199/2000... Training loss: 0.4950\n",
      "Epoch: 1199/2000... Training loss: 0.5428\n",
      "Epoch: 1199/2000... Training loss: 0.3752\n",
      "Epoch: 1199/2000... Training loss: 0.3950\n",
      "Epoch: 1199/2000... Training loss: 0.4165\n",
      "Epoch: 1199/2000... Training loss: 0.3168\n",
      "Epoch: 1199/2000... Training loss: 0.3728\n",
      "Epoch: 1199/2000... Training loss: 0.5130\n",
      "Epoch: 1199/2000... Training loss: 0.2512\n",
      "Epoch: 1199/2000... Training loss: 0.5035\n",
      "Epoch: 1199/2000... Training loss: 0.3636\n",
      "Epoch: 1199/2000... Training loss: 0.3828\n",
      "Epoch: 1199/2000... Training loss: 0.4590\n",
      "Epoch: 1199/2000... Training loss: 0.6369\n",
      "Epoch: 1199/2000... Training loss: 0.3519\n",
      "Epoch: 1199/2000... Training loss: 0.4116\n",
      "Epoch: 1199/2000... Training loss: 0.6505\n",
      "Epoch: 1199/2000... Training loss: 0.4481\n",
      "Epoch: 1199/2000... Training loss: 0.3597\n",
      "Epoch: 1199/2000... Training loss: 0.5111\n",
      "Epoch: 1199/2000... Training loss: 0.7840\n",
      "Epoch: 1199/2000... Training loss: 0.4862\n",
      "Epoch: 1199/2000... Training loss: 0.6493\n",
      "Epoch: 1199/2000... Training loss: 0.4439\n",
      "Epoch: 1199/2000... Training loss: 0.5389\n",
      "Epoch: 1199/2000... Training loss: 0.3641\n",
      "Epoch: 1199/2000... Training loss: 0.4423\n",
      "Epoch: 1199/2000... Training loss: 0.5791\n",
      "Epoch: 1199/2000... Training loss: 0.3473\n",
      "Epoch: 1199/2000... Training loss: 0.5767\n",
      "Epoch: 1200/2000... Training loss: 0.4674\n",
      "Epoch: 1200/2000... Training loss: 0.5524\n",
      "Epoch: 1200/2000... Training loss: 0.4868\n",
      "Epoch: 1200/2000... Training loss: 0.3365\n",
      "Epoch: 1200/2000... Training loss: 0.5524\n",
      "Epoch: 1200/2000... Training loss: 0.4916\n",
      "Epoch: 1200/2000... Training loss: 0.5821\n",
      "Epoch: 1200/2000... Training loss: 0.4513\n",
      "Epoch: 1200/2000... Training loss: 0.4245\n",
      "Epoch: 1200/2000... Training loss: 0.4326\n",
      "Epoch: 1200/2000... Training loss: 0.5748\n",
      "Epoch: 1200/2000... Training loss: 0.4531\n",
      "Epoch: 1200/2000... Training loss: 0.3576\n",
      "Epoch: 1200/2000... Training loss: 0.3552\n",
      "Epoch: 1200/2000... Training loss: 0.5385\n",
      "Epoch: 1200/2000... Training loss: 0.4599\n",
      "Epoch: 1200/2000... Training loss: 0.3471\n",
      "Epoch: 1200/2000... Training loss: 0.4855\n",
      "Epoch: 1200/2000... Training loss: 0.6435\n",
      "Epoch: 1200/2000... Training loss: 0.6663\n",
      "Epoch: 1200/2000... Training loss: 0.3764\n",
      "Epoch: 1200/2000... Training loss: 0.3759\n",
      "Epoch: 1200/2000... Training loss: 0.5890\n",
      "Epoch: 1200/2000... Training loss: 0.6280\n",
      "Epoch: 1200/2000... Training loss: 0.3835\n",
      "Epoch: 1200/2000... Training loss: 0.6760\n",
      "Epoch: 1200/2000... Training loss: 0.5055\n",
      "Epoch: 1200/2000... Training loss: 0.6046\n",
      "Epoch: 1200/2000... Training loss: 0.3863\n",
      "Epoch: 1200/2000... Training loss: 0.4858\n",
      "Epoch: 1200/2000... Training loss: 0.5290\n",
      "Epoch: 1201/2000... Training loss: 0.3693\n",
      "Epoch: 1201/2000... Training loss: 0.5911\n",
      "Epoch: 1201/2000... Training loss: 0.3152\n",
      "Epoch: 1201/2000... Training loss: 0.5117\n",
      "Epoch: 1201/2000... Training loss: 0.4375\n",
      "Epoch: 1201/2000... Training loss: 0.4755\n",
      "Epoch: 1201/2000... Training loss: 0.4659\n",
      "Epoch: 1201/2000... Training loss: 0.4745\n",
      "Epoch: 1201/2000... Training loss: 0.5804\n",
      "Epoch: 1201/2000... Training loss: 0.6056\n",
      "Epoch: 1201/2000... Training loss: 0.4405\n",
      "Epoch: 1201/2000... Training loss: 0.5866\n",
      "Epoch: 1201/2000... Training loss: 0.4304\n",
      "Epoch: 1201/2000... Training loss: 0.3126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1201/2000... Training loss: 0.3959\n",
      "Epoch: 1201/2000... Training loss: 0.4346\n",
      "Epoch: 1201/2000... Training loss: 0.5259\n",
      "Epoch: 1201/2000... Training loss: 0.5734\n",
      "Epoch: 1201/2000... Training loss: 0.4674\n",
      "Epoch: 1201/2000... Training loss: 0.4417\n",
      "Epoch: 1201/2000... Training loss: 0.4146\n",
      "Epoch: 1201/2000... Training loss: 0.4505\n",
      "Epoch: 1201/2000... Training loss: 0.3880\n",
      "Epoch: 1201/2000... Training loss: 0.4572\n",
      "Epoch: 1201/2000... Training loss: 0.3904\n",
      "Epoch: 1201/2000... Training loss: 0.4886\n",
      "Epoch: 1201/2000... Training loss: 0.4785\n",
      "Epoch: 1201/2000... Training loss: 0.3823\n",
      "Epoch: 1201/2000... Training loss: 0.4316\n",
      "Epoch: 1201/2000... Training loss: 0.4172\n",
      "Epoch: 1201/2000... Training loss: 0.4381\n",
      "Epoch: 1202/2000... Training loss: 0.3327\n",
      "Epoch: 1202/2000... Training loss: 0.5425\n",
      "Epoch: 1202/2000... Training loss: 0.4032\n",
      "Epoch: 1202/2000... Training loss: 0.4457\n",
      "Epoch: 1202/2000... Training loss: 0.5241\n",
      "Epoch: 1202/2000... Training loss: 0.5073\n",
      "Epoch: 1202/2000... Training loss: 0.3615\n",
      "Epoch: 1202/2000... Training loss: 0.3588\n",
      "Epoch: 1202/2000... Training loss: 0.5019\n",
      "Epoch: 1202/2000... Training loss: 0.4944\n",
      "Epoch: 1202/2000... Training loss: 0.4887\n",
      "Epoch: 1202/2000... Training loss: 0.4413\n",
      "Epoch: 1202/2000... Training loss: 0.5786\n",
      "Epoch: 1202/2000... Training loss: 0.5574\n",
      "Epoch: 1202/2000... Training loss: 0.3775\n",
      "Epoch: 1202/2000... Training loss: 0.4772\n",
      "Epoch: 1202/2000... Training loss: 0.5461\n",
      "Epoch: 1202/2000... Training loss: 0.3759\n",
      "Epoch: 1202/2000... Training loss: 0.5014\n",
      "Epoch: 1202/2000... Training loss: 0.4375\n",
      "Epoch: 1202/2000... Training loss: 0.5827\n",
      "Epoch: 1202/2000... Training loss: 0.5973\n",
      "Epoch: 1202/2000... Training loss: 0.4120\n",
      "Epoch: 1202/2000... Training loss: 0.3926\n",
      "Epoch: 1202/2000... Training loss: 0.6559\n",
      "Epoch: 1202/2000... Training loss: 0.3710\n",
      "Epoch: 1202/2000... Training loss: 0.4973\n",
      "Epoch: 1202/2000... Training loss: 0.4445\n",
      "Epoch: 1202/2000... Training loss: 0.6085\n",
      "Epoch: 1202/2000... Training loss: 0.5008\n",
      "Epoch: 1202/2000... Training loss: 0.3169\n",
      "Epoch: 1203/2000... Training loss: 0.6731\n",
      "Epoch: 1203/2000... Training loss: 0.4449\n",
      "Epoch: 1203/2000... Training loss: 0.5786\n",
      "Epoch: 1203/2000... Training loss: 0.3580\n",
      "Epoch: 1203/2000... Training loss: 0.2557\n",
      "Epoch: 1203/2000... Training loss: 0.3774\n",
      "Epoch: 1203/2000... Training loss: 0.5251\n",
      "Epoch: 1203/2000... Training loss: 0.5420\n",
      "Epoch: 1203/2000... Training loss: 0.6035\n",
      "Epoch: 1203/2000... Training loss: 0.5805\n",
      "Epoch: 1203/2000... Training loss: 0.4012\n",
      "Epoch: 1203/2000... Training loss: 0.4416\n",
      "Epoch: 1203/2000... Training loss: 0.3902\n",
      "Epoch: 1203/2000... Training loss: 0.3191\n",
      "Epoch: 1203/2000... Training loss: 0.4802\n",
      "Epoch: 1203/2000... Training loss: 0.3717\n",
      "Epoch: 1203/2000... Training loss: 0.4339\n",
      "Epoch: 1203/2000... Training loss: 0.4778\n",
      "Epoch: 1203/2000... Training loss: 0.5971\n",
      "Epoch: 1203/2000... Training loss: 0.4878\n",
      "Epoch: 1203/2000... Training loss: 0.3233\n",
      "Epoch: 1203/2000... Training loss: 0.3092\n",
      "Epoch: 1203/2000... Training loss: 0.3881\n",
      "Epoch: 1203/2000... Training loss: 0.3844\n",
      "Epoch: 1203/2000... Training loss: 0.4635\n",
      "Epoch: 1203/2000... Training loss: 0.5450\n",
      "Epoch: 1203/2000... Training loss: 0.3516\n",
      "Epoch: 1203/2000... Training loss: 0.3567\n",
      "Epoch: 1203/2000... Training loss: 0.5674\n",
      "Epoch: 1203/2000... Training loss: 0.4732\n",
      "Epoch: 1203/2000... Training loss: 0.3649\n",
      "Epoch: 1204/2000... Training loss: 0.5052\n",
      "Epoch: 1204/2000... Training loss: 0.2526\n",
      "Epoch: 1204/2000... Training loss: 0.2953\n",
      "Epoch: 1204/2000... Training loss: 0.6306\n",
      "Epoch: 1204/2000... Training loss: 0.2818\n",
      "Epoch: 1204/2000... Training loss: 0.5436\n",
      "Epoch: 1204/2000... Training loss: 0.5568\n",
      "Epoch: 1204/2000... Training loss: 0.4277\n",
      "Epoch: 1204/2000... Training loss: 0.5633\n",
      "Epoch: 1204/2000... Training loss: 0.4515\n",
      "Epoch: 1204/2000... Training loss: 0.4201\n",
      "Epoch: 1204/2000... Training loss: 0.3915\n",
      "Epoch: 1204/2000... Training loss: 0.3522\n",
      "Epoch: 1204/2000... Training loss: 0.5584\n",
      "Epoch: 1204/2000... Training loss: 0.5476\n",
      "Epoch: 1204/2000... Training loss: 0.4334\n",
      "Epoch: 1204/2000... Training loss: 0.4836\n",
      "Epoch: 1204/2000... Training loss: 0.4017\n",
      "Epoch: 1204/2000... Training loss: 0.4551\n",
      "Epoch: 1204/2000... Training loss: 0.4286\n",
      "Epoch: 1204/2000... Training loss: 0.4249\n",
      "Epoch: 1204/2000... Training loss: 0.4655\n",
      "Epoch: 1204/2000... Training loss: 0.5462\n",
      "Epoch: 1204/2000... Training loss: 0.4585\n",
      "Epoch: 1204/2000... Training loss: 0.5245\n",
      "Epoch: 1204/2000... Training loss: 0.3599\n",
      "Epoch: 1204/2000... Training loss: 0.3678\n",
      "Epoch: 1204/2000... Training loss: 0.5430\n",
      "Epoch: 1204/2000... Training loss: 0.4998\n",
      "Epoch: 1204/2000... Training loss: 0.4341\n",
      "Epoch: 1204/2000... Training loss: 0.4220\n",
      "Epoch: 1205/2000... Training loss: 0.3747\n",
      "Epoch: 1205/2000... Training loss: 0.4955\n",
      "Epoch: 1205/2000... Training loss: 0.3015\n",
      "Epoch: 1205/2000... Training loss: 0.4764\n",
      "Epoch: 1205/2000... Training loss: 0.3307\n",
      "Epoch: 1205/2000... Training loss: 0.3891\n",
      "Epoch: 1205/2000... Training loss: 0.5394\n",
      "Epoch: 1205/2000... Training loss: 0.3009\n",
      "Epoch: 1205/2000... Training loss: 0.3661\n",
      "Epoch: 1205/2000... Training loss: 0.3788\n",
      "Epoch: 1205/2000... Training loss: 0.4335\n",
      "Epoch: 1205/2000... Training loss: 0.4965\n",
      "Epoch: 1205/2000... Training loss: 0.3736\n",
      "Epoch: 1205/2000... Training loss: 0.4277\n",
      "Epoch: 1205/2000... Training loss: 0.4556\n",
      "Epoch: 1205/2000... Training loss: 0.3793\n",
      "Epoch: 1205/2000... Training loss: 0.5701\n",
      "Epoch: 1205/2000... Training loss: 0.4020\n",
      "Epoch: 1205/2000... Training loss: 0.5045\n",
      "Epoch: 1205/2000... Training loss: 0.3129\n",
      "Epoch: 1205/2000... Training loss: 0.4337\n",
      "Epoch: 1205/2000... Training loss: 0.2747\n",
      "Epoch: 1205/2000... Training loss: 0.5118\n",
      "Epoch: 1205/2000... Training loss: 0.4649\n",
      "Epoch: 1205/2000... Training loss: 0.4626\n",
      "Epoch: 1205/2000... Training loss: 0.3075\n",
      "Epoch: 1205/2000... Training loss: 0.3817\n",
      "Epoch: 1205/2000... Training loss: 0.3103\n",
      "Epoch: 1205/2000... Training loss: 0.4809\n",
      "Epoch: 1205/2000... Training loss: 0.6409\n",
      "Epoch: 1205/2000... Training loss: 0.3754\n",
      "Epoch: 1206/2000... Training loss: 0.3349\n",
      "Epoch: 1206/2000... Training loss: 0.4476\n",
      "Epoch: 1206/2000... Training loss: 0.4272\n",
      "Epoch: 1206/2000... Training loss: 0.4683\n",
      "Epoch: 1206/2000... Training loss: 0.4940\n",
      "Epoch: 1206/2000... Training loss: 0.5218\n",
      "Epoch: 1206/2000... Training loss: 0.4023\n",
      "Epoch: 1206/2000... Training loss: 0.5398\n",
      "Epoch: 1206/2000... Training loss: 0.5404\n",
      "Epoch: 1206/2000... Training loss: 0.4244\n",
      "Epoch: 1206/2000... Training loss: 0.5881\n",
      "Epoch: 1206/2000... Training loss: 0.3897\n",
      "Epoch: 1206/2000... Training loss: 0.5603\n",
      "Epoch: 1206/2000... Training loss: 0.2798\n",
      "Epoch: 1206/2000... Training loss: 0.5667\n",
      "Epoch: 1206/2000... Training loss: 0.3906\n",
      "Epoch: 1206/2000... Training loss: 0.4604\n",
      "Epoch: 1206/2000... Training loss: 0.4816\n",
      "Epoch: 1206/2000... Training loss: 0.4554\n",
      "Epoch: 1206/2000... Training loss: 0.4424\n",
      "Epoch: 1206/2000... Training loss: 0.4346\n",
      "Epoch: 1206/2000... Training loss: 0.2726\n",
      "Epoch: 1206/2000... Training loss: 0.3790\n",
      "Epoch: 1206/2000... Training loss: 0.5422\n",
      "Epoch: 1206/2000... Training loss: 0.6197\n",
      "Epoch: 1206/2000... Training loss: 0.3984\n",
      "Epoch: 1206/2000... Training loss: 0.6039\n",
      "Epoch: 1206/2000... Training loss: 0.8505\n",
      "Epoch: 1206/2000... Training loss: 0.6245\n",
      "Epoch: 1206/2000... Training loss: 0.3768\n",
      "Epoch: 1206/2000... Training loss: 0.4601\n",
      "Epoch: 1207/2000... Training loss: 0.6303\n",
      "Epoch: 1207/2000... Training loss: 0.4845\n",
      "Epoch: 1207/2000... Training loss: 0.4749\n",
      "Epoch: 1207/2000... Training loss: 0.5734\n",
      "Epoch: 1207/2000... Training loss: 0.5890\n",
      "Epoch: 1207/2000... Training loss: 0.5512\n",
      "Epoch: 1207/2000... Training loss: 0.4957\n",
      "Epoch: 1207/2000... Training loss: 0.4392\n",
      "Epoch: 1207/2000... Training loss: 0.5285\n",
      "Epoch: 1207/2000... Training loss: 0.4778\n",
      "Epoch: 1207/2000... Training loss: 0.3913\n",
      "Epoch: 1207/2000... Training loss: 0.3533\n",
      "Epoch: 1207/2000... Training loss: 0.4255\n",
      "Epoch: 1207/2000... Training loss: 0.4682\n",
      "Epoch: 1207/2000... Training loss: 0.4300\n",
      "Epoch: 1207/2000... Training loss: 0.3916\n",
      "Epoch: 1207/2000... Training loss: 0.5176\n",
      "Epoch: 1207/2000... Training loss: 0.3600\n",
      "Epoch: 1207/2000... Training loss: 0.3999\n",
      "Epoch: 1207/2000... Training loss: 0.3015\n",
      "Epoch: 1207/2000... Training loss: 0.3928\n",
      "Epoch: 1207/2000... Training loss: 0.4720\n",
      "Epoch: 1207/2000... Training loss: 0.5101\n",
      "Epoch: 1207/2000... Training loss: 0.5144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1207/2000... Training loss: 0.6609\n",
      "Epoch: 1207/2000... Training loss: 0.4258\n",
      "Epoch: 1207/2000... Training loss: 0.4455\n",
      "Epoch: 1207/2000... Training loss: 0.4239\n",
      "Epoch: 1207/2000... Training loss: 0.6064\n",
      "Epoch: 1207/2000... Training loss: 0.5304\n",
      "Epoch: 1207/2000... Training loss: 0.5464\n",
      "Epoch: 1208/2000... Training loss: 0.7236\n",
      "Epoch: 1208/2000... Training loss: 0.5043\n",
      "Epoch: 1208/2000... Training loss: 0.4146\n",
      "Epoch: 1208/2000... Training loss: 0.4222\n",
      "Epoch: 1208/2000... Training loss: 0.4669\n",
      "Epoch: 1208/2000... Training loss: 0.4167\n",
      "Epoch: 1208/2000... Training loss: 0.5294\n",
      "Epoch: 1208/2000... Training loss: 0.4278\n",
      "Epoch: 1208/2000... Training loss: 0.5018\n",
      "Epoch: 1208/2000... Training loss: 0.6098\n",
      "Epoch: 1208/2000... Training loss: 0.4862\n",
      "Epoch: 1208/2000... Training loss: 0.4224\n",
      "Epoch: 1208/2000... Training loss: 0.5008\n",
      "Epoch: 1208/2000... Training loss: 0.7098\n",
      "Epoch: 1208/2000... Training loss: 0.4753\n",
      "Epoch: 1208/2000... Training loss: 0.4372\n",
      "Epoch: 1208/2000... Training loss: 0.4386\n",
      "Epoch: 1208/2000... Training loss: 0.5877\n",
      "Epoch: 1208/2000... Training loss: 0.3906\n",
      "Epoch: 1208/2000... Training loss: 0.4420\n",
      "Epoch: 1208/2000... Training loss: 0.6505\n",
      "Epoch: 1208/2000... Training loss: 0.4158\n",
      "Epoch: 1208/2000... Training loss: 0.5083\n",
      "Epoch: 1208/2000... Training loss: 0.3906\n",
      "Epoch: 1208/2000... Training loss: 0.5859\n",
      "Epoch: 1208/2000... Training loss: 0.4590\n",
      "Epoch: 1208/2000... Training loss: 0.3024\n",
      "Epoch: 1208/2000... Training loss: 0.4853\n",
      "Epoch: 1208/2000... Training loss: 0.3475\n",
      "Epoch: 1208/2000... Training loss: 0.4787\n",
      "Epoch: 1208/2000... Training loss: 0.4771\n",
      "Epoch: 1209/2000... Training loss: 0.4171\n",
      "Epoch: 1209/2000... Training loss: 0.6349\n",
      "Epoch: 1209/2000... Training loss: 0.3273\n",
      "Epoch: 1209/2000... Training loss: 0.3575\n",
      "Epoch: 1209/2000... Training loss: 0.3174\n",
      "Epoch: 1209/2000... Training loss: 0.4500\n",
      "Epoch: 1209/2000... Training loss: 0.4777\n",
      "Epoch: 1209/2000... Training loss: 0.5880\n",
      "Epoch: 1209/2000... Training loss: 0.5473\n",
      "Epoch: 1209/2000... Training loss: 0.4238\n",
      "Epoch: 1209/2000... Training loss: 0.3447\n",
      "Epoch: 1209/2000... Training loss: 0.4483\n",
      "Epoch: 1209/2000... Training loss: 0.5088\n",
      "Epoch: 1209/2000... Training loss: 0.3492\n",
      "Epoch: 1209/2000... Training loss: 0.3827\n",
      "Epoch: 1209/2000... Training loss: 0.4969\n",
      "Epoch: 1209/2000... Training loss: 0.4120\n",
      "Epoch: 1209/2000... Training loss: 0.3680\n",
      "Epoch: 1209/2000... Training loss: 0.2852\n",
      "Epoch: 1209/2000... Training loss: 0.5685\n",
      "Epoch: 1209/2000... Training loss: 0.5518\n",
      "Epoch: 1209/2000... Training loss: 0.3450\n",
      "Epoch: 1209/2000... Training loss: 0.5175\n",
      "Epoch: 1209/2000... Training loss: 0.8073\n",
      "Epoch: 1209/2000... Training loss: 0.5073\n",
      "Epoch: 1209/2000... Training loss: 0.4141\n",
      "Epoch: 1209/2000... Training loss: 0.4041\n",
      "Epoch: 1209/2000... Training loss: 0.5787\n",
      "Epoch: 1209/2000... Training loss: 0.4968\n",
      "Epoch: 1209/2000... Training loss: 0.5736\n",
      "Epoch: 1209/2000... Training loss: 0.3151\n",
      "Epoch: 1210/2000... Training loss: 0.5086\n",
      "Epoch: 1210/2000... Training loss: 0.3049\n",
      "Epoch: 1210/2000... Training loss: 0.4086\n",
      "Epoch: 1210/2000... Training loss: 0.5882\n",
      "Epoch: 1210/2000... Training loss: 0.6404\n",
      "Epoch: 1210/2000... Training loss: 0.5393\n",
      "Epoch: 1210/2000... Training loss: 0.3903\n",
      "Epoch: 1210/2000... Training loss: 0.4145\n",
      "Epoch: 1210/2000... Training loss: 0.4102\n",
      "Epoch: 1210/2000... Training loss: 0.5210\n",
      "Epoch: 1210/2000... Training loss: 0.3630\n",
      "Epoch: 1210/2000... Training loss: 0.5846\n",
      "Epoch: 1210/2000... Training loss: 0.4238\n",
      "Epoch: 1210/2000... Training loss: 0.5646\n",
      "Epoch: 1210/2000... Training loss: 0.4557\n",
      "Epoch: 1210/2000... Training loss: 0.5648\n",
      "Epoch: 1210/2000... Training loss: 0.4975\n",
      "Epoch: 1210/2000... Training loss: 0.3288\n",
      "Epoch: 1210/2000... Training loss: 0.5057\n",
      "Epoch: 1210/2000... Training loss: 0.5811\n",
      "Epoch: 1210/2000... Training loss: 0.3791\n",
      "Epoch: 1210/2000... Training loss: 0.3454\n",
      "Epoch: 1210/2000... Training loss: 0.4924\n",
      "Epoch: 1210/2000... Training loss: 0.7217\n",
      "Epoch: 1210/2000... Training loss: 0.4706\n",
      "Epoch: 1210/2000... Training loss: 0.4919\n",
      "Epoch: 1210/2000... Training loss: 0.4764\n",
      "Epoch: 1210/2000... Training loss: 0.4351\n",
      "Epoch: 1210/2000... Training loss: 0.3754\n",
      "Epoch: 1210/2000... Training loss: 0.4436\n",
      "Epoch: 1210/2000... Training loss: 0.3537\n",
      "Epoch: 1211/2000... Training loss: 0.6715\n",
      "Epoch: 1211/2000... Training loss: 0.4721\n",
      "Epoch: 1211/2000... Training loss: 0.5993\n",
      "Epoch: 1211/2000... Training loss: 0.5769\n",
      "Epoch: 1211/2000... Training loss: 0.3694\n",
      "Epoch: 1211/2000... Training loss: 0.3506\n",
      "Epoch: 1211/2000... Training loss: 0.4204\n",
      "Epoch: 1211/2000... Training loss: 0.4955\n",
      "Epoch: 1211/2000... Training loss: 0.6574\n",
      "Epoch: 1211/2000... Training loss: 0.4127\n",
      "Epoch: 1211/2000... Training loss: 0.5576\n",
      "Epoch: 1211/2000... Training loss: 0.4076\n",
      "Epoch: 1211/2000... Training loss: 0.4860\n",
      "Epoch: 1211/2000... Training loss: 0.4161\n",
      "Epoch: 1211/2000... Training loss: 0.4632\n",
      "Epoch: 1211/2000... Training loss: 0.4444\n",
      "Epoch: 1211/2000... Training loss: 0.4845\n",
      "Epoch: 1211/2000... Training loss: 0.4717\n",
      "Epoch: 1211/2000... Training loss: 0.5662\n",
      "Epoch: 1211/2000... Training loss: 0.3950\n",
      "Epoch: 1211/2000... Training loss: 0.3798\n",
      "Epoch: 1211/2000... Training loss: 0.6894\n",
      "Epoch: 1211/2000... Training loss: 0.4279\n",
      "Epoch: 1211/2000... Training loss: 0.3941\n",
      "Epoch: 1211/2000... Training loss: 0.4981\n",
      "Epoch: 1211/2000... Training loss: 0.4098\n",
      "Epoch: 1211/2000... Training loss: 0.5691\n",
      "Epoch: 1211/2000... Training loss: 0.3354\n",
      "Epoch: 1211/2000... Training loss: 0.4379\n",
      "Epoch: 1211/2000... Training loss: 0.4296\n",
      "Epoch: 1211/2000... Training loss: 0.4289\n",
      "Epoch: 1212/2000... Training loss: 0.4776\n",
      "Epoch: 1212/2000... Training loss: 0.4307\n",
      "Epoch: 1212/2000... Training loss: 0.5230\n",
      "Epoch: 1212/2000... Training loss: 0.3309\n",
      "Epoch: 1212/2000... Training loss: 0.4192\n",
      "Epoch: 1212/2000... Training loss: 0.4145\n",
      "Epoch: 1212/2000... Training loss: 0.3069\n",
      "Epoch: 1212/2000... Training loss: 0.4492\n",
      "Epoch: 1212/2000... Training loss: 0.5036\n",
      "Epoch: 1212/2000... Training loss: 0.5735\n",
      "Epoch: 1212/2000... Training loss: 0.5134\n",
      "Epoch: 1212/2000... Training loss: 0.6804\n",
      "Epoch: 1212/2000... Training loss: 0.6836\n",
      "Epoch: 1212/2000... Training loss: 0.2877\n",
      "Epoch: 1212/2000... Training loss: 0.3278\n",
      "Epoch: 1212/2000... Training loss: 0.4313\n",
      "Epoch: 1212/2000... Training loss: 0.4653\n",
      "Epoch: 1212/2000... Training loss: 0.2996\n",
      "Epoch: 1212/2000... Training loss: 0.3453\n",
      "Epoch: 1212/2000... Training loss: 0.4599\n",
      "Epoch: 1212/2000... Training loss: 0.3343\n",
      "Epoch: 1212/2000... Training loss: 0.4754\n",
      "Epoch: 1212/2000... Training loss: 0.5160\n",
      "Epoch: 1212/2000... Training loss: 0.4940\n",
      "Epoch: 1212/2000... Training loss: 0.6264\n",
      "Epoch: 1212/2000... Training loss: 0.5667\n",
      "Epoch: 1212/2000... Training loss: 0.4763\n",
      "Epoch: 1212/2000... Training loss: 0.4180\n",
      "Epoch: 1212/2000... Training loss: 0.3409\n",
      "Epoch: 1212/2000... Training loss: 0.3544\n",
      "Epoch: 1212/2000... Training loss: 0.4978\n",
      "Epoch: 1213/2000... Training loss: 0.5898\n",
      "Epoch: 1213/2000... Training loss: 0.6290\n",
      "Epoch: 1213/2000... Training loss: 0.5108\n",
      "Epoch: 1213/2000... Training loss: 0.4789\n",
      "Epoch: 1213/2000... Training loss: 0.3356\n",
      "Epoch: 1213/2000... Training loss: 0.5235\n",
      "Epoch: 1213/2000... Training loss: 0.4329\n",
      "Epoch: 1213/2000... Training loss: 0.5336\n",
      "Epoch: 1213/2000... Training loss: 0.6684\n",
      "Epoch: 1213/2000... Training loss: 0.3248\n",
      "Epoch: 1213/2000... Training loss: 0.4273\n",
      "Epoch: 1213/2000... Training loss: 0.5674\n",
      "Epoch: 1213/2000... Training loss: 0.3892\n",
      "Epoch: 1213/2000... Training loss: 0.4204\n",
      "Epoch: 1213/2000... Training loss: 0.2973\n",
      "Epoch: 1213/2000... Training loss: 0.4525\n",
      "Epoch: 1213/2000... Training loss: 0.4515\n",
      "Epoch: 1213/2000... Training loss: 0.4786\n",
      "Epoch: 1213/2000... Training loss: 0.4355\n",
      "Epoch: 1213/2000... Training loss: 0.5814\n",
      "Epoch: 1213/2000... Training loss: 0.6052\n",
      "Epoch: 1213/2000... Training loss: 0.4293\n",
      "Epoch: 1213/2000... Training loss: 0.2886\n",
      "Epoch: 1213/2000... Training loss: 0.3011\n",
      "Epoch: 1213/2000... Training loss: 0.5544\n",
      "Epoch: 1213/2000... Training loss: 0.3875\n",
      "Epoch: 1213/2000... Training loss: 0.5992\n",
      "Epoch: 1213/2000... Training loss: 0.3704\n",
      "Epoch: 1213/2000... Training loss: 0.4621\n",
      "Epoch: 1213/2000... Training loss: 0.5091\n",
      "Epoch: 1213/2000... Training loss: 0.4783\n",
      "Epoch: 1214/2000... Training loss: 0.3456\n",
      "Epoch: 1214/2000... Training loss: 0.3940\n",
      "Epoch: 1214/2000... Training loss: 0.3911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1214/2000... Training loss: 0.4874\n",
      "Epoch: 1214/2000... Training loss: 0.4544\n",
      "Epoch: 1214/2000... Training loss: 0.6857\n",
      "Epoch: 1214/2000... Training loss: 0.4080\n",
      "Epoch: 1214/2000... Training loss: 0.3171\n",
      "Epoch: 1214/2000... Training loss: 0.5912\n",
      "Epoch: 1214/2000... Training loss: 0.5248\n",
      "Epoch: 1214/2000... Training loss: 0.4965\n",
      "Epoch: 1214/2000... Training loss: 0.4503\n",
      "Epoch: 1214/2000... Training loss: 0.4695\n",
      "Epoch: 1214/2000... Training loss: 0.5300\n",
      "Epoch: 1214/2000... Training loss: 0.4446\n",
      "Epoch: 1214/2000... Training loss: 0.4790\n",
      "Epoch: 1214/2000... Training loss: 0.3715\n",
      "Epoch: 1214/2000... Training loss: 0.5364\n",
      "Epoch: 1214/2000... Training loss: 0.5709\n",
      "Epoch: 1214/2000... Training loss: 0.4339\n",
      "Epoch: 1214/2000... Training loss: 0.5234\n",
      "Epoch: 1214/2000... Training loss: 0.5414\n",
      "Epoch: 1214/2000... Training loss: 0.5230\n",
      "Epoch: 1214/2000... Training loss: 0.4955\n",
      "Epoch: 1214/2000... Training loss: 0.4014\n",
      "Epoch: 1214/2000... Training loss: 0.4707\n",
      "Epoch: 1214/2000... Training loss: 0.4474\n",
      "Epoch: 1214/2000... Training loss: 0.5540\n",
      "Epoch: 1214/2000... Training loss: 0.4146\n",
      "Epoch: 1214/2000... Training loss: 0.4837\n",
      "Epoch: 1214/2000... Training loss: 0.3588\n",
      "Epoch: 1215/2000... Training loss: 0.3568\n",
      "Epoch: 1215/2000... Training loss: 0.4588\n",
      "Epoch: 1215/2000... Training loss: 0.4458\n",
      "Epoch: 1215/2000... Training loss: 0.5861\n",
      "Epoch: 1215/2000... Training loss: 0.4924\n",
      "Epoch: 1215/2000... Training loss: 0.4602\n",
      "Epoch: 1215/2000... Training loss: 0.3873\n",
      "Epoch: 1215/2000... Training loss: 0.4369\n",
      "Epoch: 1215/2000... Training loss: 0.6202\n",
      "Epoch: 1215/2000... Training loss: 0.5793\n",
      "Epoch: 1215/2000... Training loss: 0.3141\n",
      "Epoch: 1215/2000... Training loss: 0.4116\n",
      "Epoch: 1215/2000... Training loss: 0.4407\n",
      "Epoch: 1215/2000... Training loss: 0.3387\n",
      "Epoch: 1215/2000... Training loss: 0.6274\n",
      "Epoch: 1215/2000... Training loss: 0.4749\n",
      "Epoch: 1215/2000... Training loss: 0.5471\n",
      "Epoch: 1215/2000... Training loss: 0.3568\n",
      "Epoch: 1215/2000... Training loss: 0.3961\n",
      "Epoch: 1215/2000... Training loss: 0.5059\n",
      "Epoch: 1215/2000... Training loss: 0.4309\n",
      "Epoch: 1215/2000... Training loss: 0.3822\n",
      "Epoch: 1215/2000... Training loss: 0.4705\n",
      "Epoch: 1215/2000... Training loss: 0.3531\n",
      "Epoch: 1215/2000... Training loss: 0.5317\n",
      "Epoch: 1215/2000... Training loss: 0.4191\n",
      "Epoch: 1215/2000... Training loss: 0.5517\n",
      "Epoch: 1215/2000... Training loss: 0.3983\n",
      "Epoch: 1215/2000... Training loss: 0.4954\n",
      "Epoch: 1215/2000... Training loss: 0.3631\n",
      "Epoch: 1215/2000... Training loss: 0.3870\n",
      "Epoch: 1216/2000... Training loss: 0.4549\n",
      "Epoch: 1216/2000... Training loss: 0.4456\n",
      "Epoch: 1216/2000... Training loss: 0.4726\n",
      "Epoch: 1216/2000... Training loss: 0.4826\n",
      "Epoch: 1216/2000... Training loss: 0.4683\n",
      "Epoch: 1216/2000... Training loss: 0.3135\n",
      "Epoch: 1216/2000... Training loss: 0.3380\n",
      "Epoch: 1216/2000... Training loss: 0.3177\n",
      "Epoch: 1216/2000... Training loss: 0.4895\n",
      "Epoch: 1216/2000... Training loss: 0.5174\n",
      "Epoch: 1216/2000... Training loss: 0.4840\n",
      "Epoch: 1216/2000... Training loss: 0.3452\n",
      "Epoch: 1216/2000... Training loss: 0.4417\n",
      "Epoch: 1216/2000... Training loss: 0.5583\n",
      "Epoch: 1216/2000... Training loss: 0.5525\n",
      "Epoch: 1216/2000... Training loss: 0.6853\n",
      "Epoch: 1216/2000... Training loss: 0.5460\n",
      "Epoch: 1216/2000... Training loss: 0.5083\n",
      "Epoch: 1216/2000... Training loss: 0.5089\n",
      "Epoch: 1216/2000... Training loss: 0.5010\n",
      "Epoch: 1216/2000... Training loss: 0.5721\n",
      "Epoch: 1216/2000... Training loss: 0.6086\n",
      "Epoch: 1216/2000... Training loss: 0.4309\n",
      "Epoch: 1216/2000... Training loss: 0.4628\n",
      "Epoch: 1216/2000... Training loss: 0.3732\n",
      "Epoch: 1216/2000... Training loss: 0.3664\n",
      "Epoch: 1216/2000... Training loss: 0.5380\n",
      "Epoch: 1216/2000... Training loss: 0.4794\n",
      "Epoch: 1216/2000... Training loss: 0.3154\n",
      "Epoch: 1216/2000... Training loss: 0.3156\n",
      "Epoch: 1216/2000... Training loss: 0.3360\n",
      "Epoch: 1217/2000... Training loss: 0.3764\n",
      "Epoch: 1217/2000... Training loss: 0.6705\n",
      "Epoch: 1217/2000... Training loss: 0.4323\n",
      "Epoch: 1217/2000... Training loss: 0.3937\n",
      "Epoch: 1217/2000... Training loss: 0.4308\n",
      "Epoch: 1217/2000... Training loss: 0.4057\n",
      "Epoch: 1217/2000... Training loss: 0.5494\n",
      "Epoch: 1217/2000... Training loss: 0.5784\n",
      "Epoch: 1217/2000... Training loss: 0.5697\n",
      "Epoch: 1217/2000... Training loss: 0.4384\n",
      "Epoch: 1217/2000... Training loss: 0.5555\n",
      "Epoch: 1217/2000... Training loss: 0.5109\n",
      "Epoch: 1217/2000... Training loss: 0.5444\n",
      "Epoch: 1217/2000... Training loss: 0.3983\n",
      "Epoch: 1217/2000... Training loss: 0.2681\n",
      "Epoch: 1217/2000... Training loss: 0.4350\n",
      "Epoch: 1217/2000... Training loss: 0.4878\n",
      "Epoch: 1217/2000... Training loss: 0.4871\n",
      "Epoch: 1217/2000... Training loss: 0.3406\n",
      "Epoch: 1217/2000... Training loss: 0.5177\n",
      "Epoch: 1217/2000... Training loss: 0.3064\n",
      "Epoch: 1217/2000... Training loss: 0.4511\n",
      "Epoch: 1217/2000... Training loss: 0.4871\n",
      "Epoch: 1217/2000... Training loss: 0.3777\n",
      "Epoch: 1217/2000... Training loss: 0.4703\n",
      "Epoch: 1217/2000... Training loss: 0.4658\n",
      "Epoch: 1217/2000... Training loss: 0.2928\n",
      "Epoch: 1217/2000... Training loss: 0.6361\n",
      "Epoch: 1217/2000... Training loss: 0.7115\n",
      "Epoch: 1217/2000... Training loss: 0.5037\n",
      "Epoch: 1217/2000... Training loss: 0.3211\n",
      "Epoch: 1218/2000... Training loss: 0.5348\n",
      "Epoch: 1218/2000... Training loss: 0.4133\n",
      "Epoch: 1218/2000... Training loss: 0.3175\n",
      "Epoch: 1218/2000... Training loss: 0.5348\n",
      "Epoch: 1218/2000... Training loss: 0.4327\n",
      "Epoch: 1218/2000... Training loss: 0.4138\n",
      "Epoch: 1218/2000... Training loss: 0.3922\n",
      "Epoch: 1218/2000... Training loss: 0.5174\n",
      "Epoch: 1218/2000... Training loss: 0.4047\n",
      "Epoch: 1218/2000... Training loss: 0.4122\n",
      "Epoch: 1218/2000... Training loss: 0.4912\n",
      "Epoch: 1218/2000... Training loss: 0.5052\n",
      "Epoch: 1218/2000... Training loss: 0.3380\n",
      "Epoch: 1218/2000... Training loss: 0.4770\n",
      "Epoch: 1218/2000... Training loss: 0.3226\n",
      "Epoch: 1218/2000... Training loss: 0.4139\n",
      "Epoch: 1218/2000... Training loss: 0.4615\n",
      "Epoch: 1218/2000... Training loss: 0.2974\n",
      "Epoch: 1218/2000... Training loss: 0.3803\n",
      "Epoch: 1218/2000... Training loss: 0.5620\n",
      "Epoch: 1218/2000... Training loss: 0.5220\n",
      "Epoch: 1218/2000... Training loss: 0.4417\n",
      "Epoch: 1218/2000... Training loss: 0.5595\n",
      "Epoch: 1218/2000... Training loss: 0.5434\n",
      "Epoch: 1218/2000... Training loss: 0.4762\n",
      "Epoch: 1218/2000... Training loss: 0.5919\n",
      "Epoch: 1218/2000... Training loss: 0.3276\n",
      "Epoch: 1218/2000... Training loss: 0.5689\n",
      "Epoch: 1218/2000... Training loss: 0.4467\n",
      "Epoch: 1218/2000... Training loss: 0.4857\n",
      "Epoch: 1218/2000... Training loss: 0.4755\n",
      "Epoch: 1219/2000... Training loss: 0.5502\n",
      "Epoch: 1219/2000... Training loss: 0.2583\n",
      "Epoch: 1219/2000... Training loss: 0.6782\n",
      "Epoch: 1219/2000... Training loss: 0.4037\n",
      "Epoch: 1219/2000... Training loss: 0.6602\n",
      "Epoch: 1219/2000... Training loss: 0.4721\n",
      "Epoch: 1219/2000... Training loss: 0.3554\n",
      "Epoch: 1219/2000... Training loss: 0.4468\n",
      "Epoch: 1219/2000... Training loss: 0.3924\n",
      "Epoch: 1219/2000... Training loss: 0.4376\n",
      "Epoch: 1219/2000... Training loss: 0.5017\n",
      "Epoch: 1219/2000... Training loss: 0.3780\n",
      "Epoch: 1219/2000... Training loss: 0.2691\n",
      "Epoch: 1219/2000... Training loss: 0.3426\n",
      "Epoch: 1219/2000... Training loss: 0.3451\n",
      "Epoch: 1219/2000... Training loss: 0.5062\n",
      "Epoch: 1219/2000... Training loss: 0.4888\n",
      "Epoch: 1219/2000... Training loss: 0.4110\n",
      "Epoch: 1219/2000... Training loss: 0.5151\n",
      "Epoch: 1219/2000... Training loss: 0.5615\n",
      "Epoch: 1219/2000... Training loss: 0.3748\n",
      "Epoch: 1219/2000... Training loss: 0.4171\n",
      "Epoch: 1219/2000... Training loss: 0.4158\n",
      "Epoch: 1219/2000... Training loss: 0.6127\n",
      "Epoch: 1219/2000... Training loss: 0.5976\n",
      "Epoch: 1219/2000... Training loss: 0.5375\n",
      "Epoch: 1219/2000... Training loss: 0.4378\n",
      "Epoch: 1219/2000... Training loss: 0.3659\n",
      "Epoch: 1219/2000... Training loss: 0.3989\n",
      "Epoch: 1219/2000... Training loss: 0.4563\n",
      "Epoch: 1219/2000... Training loss: 0.5612\n",
      "Epoch: 1220/2000... Training loss: 0.3696\n",
      "Epoch: 1220/2000... Training loss: 0.3253\n",
      "Epoch: 1220/2000... Training loss: 0.3353\n",
      "Epoch: 1220/2000... Training loss: 0.5435\n",
      "Epoch: 1220/2000... Training loss: 0.4723\n",
      "Epoch: 1220/2000... Training loss: 0.4932\n",
      "Epoch: 1220/2000... Training loss: 0.3866\n",
      "Epoch: 1220/2000... Training loss: 0.4332\n",
      "Epoch: 1220/2000... Training loss: 0.3485\n",
      "Epoch: 1220/2000... Training loss: 0.5964\n",
      "Epoch: 1220/2000... Training loss: 0.4575\n",
      "Epoch: 1220/2000... Training loss: 0.5862\n",
      "Epoch: 1220/2000... Training loss: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1220/2000... Training loss: 0.5049\n",
      "Epoch: 1220/2000... Training loss: 0.4730\n",
      "Epoch: 1220/2000... Training loss: 0.3477\n",
      "Epoch: 1220/2000... Training loss: 0.4455\n",
      "Epoch: 1220/2000... Training loss: 0.5770\n",
      "Epoch: 1220/2000... Training loss: 0.5145\n",
      "Epoch: 1220/2000... Training loss: 0.5542\n",
      "Epoch: 1220/2000... Training loss: 0.4882\n",
      "Epoch: 1220/2000... Training loss: 0.4408\n",
      "Epoch: 1220/2000... Training loss: 0.5572\n",
      "Epoch: 1220/2000... Training loss: 0.4775\n",
      "Epoch: 1220/2000... Training loss: 0.3279\n",
      "Epoch: 1220/2000... Training loss: 0.5042\n",
      "Epoch: 1220/2000... Training loss: 0.5265\n",
      "Epoch: 1220/2000... Training loss: 0.3797\n",
      "Epoch: 1220/2000... Training loss: 0.3520\n",
      "Epoch: 1220/2000... Training loss: 0.5058\n",
      "Epoch: 1220/2000... Training loss: 0.3782\n",
      "Epoch: 1221/2000... Training loss: 0.4236\n",
      "Epoch: 1221/2000... Training loss: 0.2572\n",
      "Epoch: 1221/2000... Training loss: 0.4734\n",
      "Epoch: 1221/2000... Training loss: 0.4790\n",
      "Epoch: 1221/2000... Training loss: 0.3913\n",
      "Epoch: 1221/2000... Training loss: 0.4821\n",
      "Epoch: 1221/2000... Training loss: 0.3578\n",
      "Epoch: 1221/2000... Training loss: 0.5981\n",
      "Epoch: 1221/2000... Training loss: 0.3822\n",
      "Epoch: 1221/2000... Training loss: 0.4781\n",
      "Epoch: 1221/2000... Training loss: 0.6005\n",
      "Epoch: 1221/2000... Training loss: 0.4109\n",
      "Epoch: 1221/2000... Training loss: 0.4765\n",
      "Epoch: 1221/2000... Training loss: 0.6505\n",
      "Epoch: 1221/2000... Training loss: 0.6550\n",
      "Epoch: 1221/2000... Training loss: 0.4756\n",
      "Epoch: 1221/2000... Training loss: 0.4121\n",
      "Epoch: 1221/2000... Training loss: 0.4191\n",
      "Epoch: 1221/2000... Training loss: 0.3703\n",
      "Epoch: 1221/2000... Training loss: 0.6407\n",
      "Epoch: 1221/2000... Training loss: 0.6250\n",
      "Epoch: 1221/2000... Training loss: 0.6023\n",
      "Epoch: 1221/2000... Training loss: 0.3412\n",
      "Epoch: 1221/2000... Training loss: 0.6467\n",
      "Epoch: 1221/2000... Training loss: 0.3689\n",
      "Epoch: 1221/2000... Training loss: 0.4252\n",
      "Epoch: 1221/2000... Training loss: 0.4411\n",
      "Epoch: 1221/2000... Training loss: 0.4209\n",
      "Epoch: 1221/2000... Training loss: 0.5782\n",
      "Epoch: 1221/2000... Training loss: 0.4189\n",
      "Epoch: 1221/2000... Training loss: 0.5357\n",
      "Epoch: 1222/2000... Training loss: 0.6875\n",
      "Epoch: 1222/2000... Training loss: 0.4168\n",
      "Epoch: 1222/2000... Training loss: 0.6352\n",
      "Epoch: 1222/2000... Training loss: 0.5197\n",
      "Epoch: 1222/2000... Training loss: 0.4417\n",
      "Epoch: 1222/2000... Training loss: 0.4448\n",
      "Epoch: 1222/2000... Training loss: 0.3654\n",
      "Epoch: 1222/2000... Training loss: 0.4351\n",
      "Epoch: 1222/2000... Training loss: 0.4547\n",
      "Epoch: 1222/2000... Training loss: 0.4766\n",
      "Epoch: 1222/2000... Training loss: 0.5109\n",
      "Epoch: 1222/2000... Training loss: 0.4592\n",
      "Epoch: 1222/2000... Training loss: 0.4663\n",
      "Epoch: 1222/2000... Training loss: 0.3459\n",
      "Epoch: 1222/2000... Training loss: 0.5354\n",
      "Epoch: 1222/2000... Training loss: 0.3461\n",
      "Epoch: 1222/2000... Training loss: 0.5705\n",
      "Epoch: 1222/2000... Training loss: 0.5321\n",
      "Epoch: 1222/2000... Training loss: 0.3672\n",
      "Epoch: 1222/2000... Training loss: 0.5015\n",
      "Epoch: 1222/2000... Training loss: 0.5841\n",
      "Epoch: 1222/2000... Training loss: 0.5102\n",
      "Epoch: 1222/2000... Training loss: 0.3383\n",
      "Epoch: 1222/2000... Training loss: 0.5820\n",
      "Epoch: 1222/2000... Training loss: 0.3488\n",
      "Epoch: 1222/2000... Training loss: 0.4480\n",
      "Epoch: 1222/2000... Training loss: 0.5853\n",
      "Epoch: 1222/2000... Training loss: 0.4734\n",
      "Epoch: 1222/2000... Training loss: 0.5088\n",
      "Epoch: 1222/2000... Training loss: 0.3321\n",
      "Epoch: 1222/2000... Training loss: 0.3332\n",
      "Epoch: 1223/2000... Training loss: 0.5386\n",
      "Epoch: 1223/2000... Training loss: 0.2991\n",
      "Epoch: 1223/2000... Training loss: 0.5259\n",
      "Epoch: 1223/2000... Training loss: 0.3834\n",
      "Epoch: 1223/2000... Training loss: 0.4557\n",
      "Epoch: 1223/2000... Training loss: 0.4243\n",
      "Epoch: 1223/2000... Training loss: 0.4469\n",
      "Epoch: 1223/2000... Training loss: 0.3250\n",
      "Epoch: 1223/2000... Training loss: 0.7124\n",
      "Epoch: 1223/2000... Training loss: 0.4140\n",
      "Epoch: 1223/2000... Training loss: 0.3560\n",
      "Epoch: 1223/2000... Training loss: 0.4810\n",
      "Epoch: 1223/2000... Training loss: 0.5205\n",
      "Epoch: 1223/2000... Training loss: 0.4370\n",
      "Epoch: 1223/2000... Training loss: 0.4156\n",
      "Epoch: 1223/2000... Training loss: 0.4502\n",
      "Epoch: 1223/2000... Training loss: 0.4548\n",
      "Epoch: 1223/2000... Training loss: 0.3887\n",
      "Epoch: 1223/2000... Training loss: 0.5645\n",
      "Epoch: 1223/2000... Training loss: 0.6021\n",
      "Epoch: 1223/2000... Training loss: 0.4423\n",
      "Epoch: 1223/2000... Training loss: 0.6725\n",
      "Epoch: 1223/2000... Training loss: 0.4772\n",
      "Epoch: 1223/2000... Training loss: 0.6098\n",
      "Epoch: 1223/2000... Training loss: 0.4838\n",
      "Epoch: 1223/2000... Training loss: 0.5320\n",
      "Epoch: 1223/2000... Training loss: 0.4122\n",
      "Epoch: 1223/2000... Training loss: 0.4133\n",
      "Epoch: 1223/2000... Training loss: 0.5824\n",
      "Epoch: 1223/2000... Training loss: 0.4405\n",
      "Epoch: 1223/2000... Training loss: 0.4960\n",
      "Epoch: 1224/2000... Training loss: 0.8022\n",
      "Epoch: 1224/2000... Training loss: 0.6681\n",
      "Epoch: 1224/2000... Training loss: 0.4125\n",
      "Epoch: 1224/2000... Training loss: 0.4224\n",
      "Epoch: 1224/2000... Training loss: 0.4227\n",
      "Epoch: 1224/2000... Training loss: 0.3468\n",
      "Epoch: 1224/2000... Training loss: 0.4753\n",
      "Epoch: 1224/2000... Training loss: 0.6105\n",
      "Epoch: 1224/2000... Training loss: 0.4593\n",
      "Epoch: 1224/2000... Training loss: 0.4202\n",
      "Epoch: 1224/2000... Training loss: 0.4469\n",
      "Epoch: 1224/2000... Training loss: 0.5122\n",
      "Epoch: 1224/2000... Training loss: 0.3596\n",
      "Epoch: 1224/2000... Training loss: 0.4392\n",
      "Epoch: 1224/2000... Training loss: 0.5055\n",
      "Epoch: 1224/2000... Training loss: 0.5096\n",
      "Epoch: 1224/2000... Training loss: 0.4485\n",
      "Epoch: 1224/2000... Training loss: 0.5370\n",
      "Epoch: 1224/2000... Training loss: 0.7731\n",
      "Epoch: 1224/2000... Training loss: 0.4743\n",
      "Epoch: 1224/2000... Training loss: 0.4809\n",
      "Epoch: 1224/2000... Training loss: 0.3727\n",
      "Epoch: 1224/2000... Training loss: 0.4422\n",
      "Epoch: 1224/2000... Training loss: 0.3904\n",
      "Epoch: 1224/2000... Training loss: 0.4392\n",
      "Epoch: 1224/2000... Training loss: 0.6044\n",
      "Epoch: 1224/2000... Training loss: 0.3072\n",
      "Epoch: 1224/2000... Training loss: 0.5861\n",
      "Epoch: 1224/2000... Training loss: 0.4759\n",
      "Epoch: 1224/2000... Training loss: 0.4516\n",
      "Epoch: 1224/2000... Training loss: 0.4775\n",
      "Epoch: 1225/2000... Training loss: 0.3843\n",
      "Epoch: 1225/2000... Training loss: 0.4459\n",
      "Epoch: 1225/2000... Training loss: 0.4204\n",
      "Epoch: 1225/2000... Training loss: 0.5731\n",
      "Epoch: 1225/2000... Training loss: 0.3484\n",
      "Epoch: 1225/2000... Training loss: 0.4503\n",
      "Epoch: 1225/2000... Training loss: 0.4685\n",
      "Epoch: 1225/2000... Training loss: 0.3925\n",
      "Epoch: 1225/2000... Training loss: 0.6389\n",
      "Epoch: 1225/2000... Training loss: 0.4927\n",
      "Epoch: 1225/2000... Training loss: 0.4274\n",
      "Epoch: 1225/2000... Training loss: 0.3409\n",
      "Epoch: 1225/2000... Training loss: 0.5946\n",
      "Epoch: 1225/2000... Training loss: 0.4666\n",
      "Epoch: 1225/2000... Training loss: 0.4404\n",
      "Epoch: 1225/2000... Training loss: 0.3786\n",
      "Epoch: 1225/2000... Training loss: 0.4689\n",
      "Epoch: 1225/2000... Training loss: 0.3664\n",
      "Epoch: 1225/2000... Training loss: 0.5503\n",
      "Epoch: 1225/2000... Training loss: 0.3747\n",
      "Epoch: 1225/2000... Training loss: 0.4005\n",
      "Epoch: 1225/2000... Training loss: 0.4985\n",
      "Epoch: 1225/2000... Training loss: 0.6855\n",
      "Epoch: 1225/2000... Training loss: 0.6634\n",
      "Epoch: 1225/2000... Training loss: 0.2910\n",
      "Epoch: 1225/2000... Training loss: 0.4626\n",
      "Epoch: 1225/2000... Training loss: 0.6056\n",
      "Epoch: 1225/2000... Training loss: 0.4787\n",
      "Epoch: 1225/2000... Training loss: 0.4356\n",
      "Epoch: 1225/2000... Training loss: 0.6344\n",
      "Epoch: 1225/2000... Training loss: 0.4584\n",
      "Epoch: 1226/2000... Training loss: 0.4771\n",
      "Epoch: 1226/2000... Training loss: 0.3595\n",
      "Epoch: 1226/2000... Training loss: 0.3668\n",
      "Epoch: 1226/2000... Training loss: 0.4507\n",
      "Epoch: 1226/2000... Training loss: 0.4117\n",
      "Epoch: 1226/2000... Training loss: 0.6016\n",
      "Epoch: 1226/2000... Training loss: 0.4035\n",
      "Epoch: 1226/2000... Training loss: 0.3941\n",
      "Epoch: 1226/2000... Training loss: 0.4749\n",
      "Epoch: 1226/2000... Training loss: 0.6662\n",
      "Epoch: 1226/2000... Training loss: 0.6434\n",
      "Epoch: 1226/2000... Training loss: 0.4512\n",
      "Epoch: 1226/2000... Training loss: 0.4279\n",
      "Epoch: 1226/2000... Training loss: 0.4523\n",
      "Epoch: 1226/2000... Training loss: 0.4330\n",
      "Epoch: 1226/2000... Training loss: 0.6367\n",
      "Epoch: 1226/2000... Training loss: 0.5555\n",
      "Epoch: 1226/2000... Training loss: 0.2937\n",
      "Epoch: 1226/2000... Training loss: 0.5223\n",
      "Epoch: 1226/2000... Training loss: 0.3275\n",
      "Epoch: 1226/2000... Training loss: 0.4681\n",
      "Epoch: 1226/2000... Training loss: 0.5037\n",
      "Epoch: 1226/2000... Training loss: 0.5514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1226/2000... Training loss: 0.3755\n",
      "Epoch: 1226/2000... Training loss: 0.3488\n",
      "Epoch: 1226/2000... Training loss: 0.4585\n",
      "Epoch: 1226/2000... Training loss: 0.3701\n",
      "Epoch: 1226/2000... Training loss: 0.5352\n",
      "Epoch: 1226/2000... Training loss: 0.5925\n",
      "Epoch: 1226/2000... Training loss: 0.2553\n",
      "Epoch: 1226/2000... Training loss: 0.3525\n",
      "Epoch: 1227/2000... Training loss: 0.3501\n",
      "Epoch: 1227/2000... Training loss: 0.4352\n",
      "Epoch: 1227/2000... Training loss: 0.4928\n",
      "Epoch: 1227/2000... Training loss: 0.5675\n",
      "Epoch: 1227/2000... Training loss: 0.5907\n",
      "Epoch: 1227/2000... Training loss: 0.4814\n",
      "Epoch: 1227/2000... Training loss: 0.3081\n",
      "Epoch: 1227/2000... Training loss: 0.4418\n",
      "Epoch: 1227/2000... Training loss: 0.3858\n",
      "Epoch: 1227/2000... Training loss: 0.4405\n",
      "Epoch: 1227/2000... Training loss: 0.4016\n",
      "Epoch: 1227/2000... Training loss: 0.3952\n",
      "Epoch: 1227/2000... Training loss: 0.4394\n",
      "Epoch: 1227/2000... Training loss: 0.3963\n",
      "Epoch: 1227/2000... Training loss: 0.5153\n",
      "Epoch: 1227/2000... Training loss: 0.5181\n",
      "Epoch: 1227/2000... Training loss: 0.4755\n",
      "Epoch: 1227/2000... Training loss: 0.7431\n",
      "Epoch: 1227/2000... Training loss: 0.4941\n",
      "Epoch: 1227/2000... Training loss: 0.4132\n",
      "Epoch: 1227/2000... Training loss: 0.4350\n",
      "Epoch: 1227/2000... Training loss: 0.4215\n",
      "Epoch: 1227/2000... Training loss: 0.6175\n",
      "Epoch: 1227/2000... Training loss: 0.6180\n",
      "Epoch: 1227/2000... Training loss: 0.4805\n",
      "Epoch: 1227/2000... Training loss: 0.4573\n",
      "Epoch: 1227/2000... Training loss: 0.4044\n",
      "Epoch: 1227/2000... Training loss: 0.5615\n",
      "Epoch: 1227/2000... Training loss: 0.4165\n",
      "Epoch: 1227/2000... Training loss: 0.4667\n",
      "Epoch: 1227/2000... Training loss: 0.4198\n",
      "Epoch: 1228/2000... Training loss: 0.5186\n",
      "Epoch: 1228/2000... Training loss: 0.4510\n",
      "Epoch: 1228/2000... Training loss: 0.4107\n",
      "Epoch: 1228/2000... Training loss: 0.4011\n",
      "Epoch: 1228/2000... Training loss: 0.3294\n",
      "Epoch: 1228/2000... Training loss: 0.4406\n",
      "Epoch: 1228/2000... Training loss: 0.4977\n",
      "Epoch: 1228/2000... Training loss: 0.4273\n",
      "Epoch: 1228/2000... Training loss: 0.7940\n",
      "Epoch: 1228/2000... Training loss: 0.3571\n",
      "Epoch: 1228/2000... Training loss: 0.4847\n",
      "Epoch: 1228/2000... Training loss: 0.4728\n",
      "Epoch: 1228/2000... Training loss: 0.4737\n",
      "Epoch: 1228/2000... Training loss: 0.7437\n",
      "Epoch: 1228/2000... Training loss: 0.4585\n",
      "Epoch: 1228/2000... Training loss: 0.3848\n",
      "Epoch: 1228/2000... Training loss: 0.4729\n",
      "Epoch: 1228/2000... Training loss: 0.5165\n",
      "Epoch: 1228/2000... Training loss: 0.3574\n",
      "Epoch: 1228/2000... Training loss: 0.5302\n",
      "Epoch: 1228/2000... Training loss: 0.4910\n",
      "Epoch: 1228/2000... Training loss: 0.3499\n",
      "Epoch: 1228/2000... Training loss: 0.5514\n",
      "Epoch: 1228/2000... Training loss: 0.4942\n",
      "Epoch: 1228/2000... Training loss: 0.4711\n",
      "Epoch: 1228/2000... Training loss: 0.4407\n",
      "Epoch: 1228/2000... Training loss: 0.5202\n",
      "Epoch: 1228/2000... Training loss: 0.4063\n",
      "Epoch: 1228/2000... Training loss: 0.2794\n",
      "Epoch: 1228/2000... Training loss: 0.4446\n",
      "Epoch: 1228/2000... Training loss: 0.3143\n",
      "Epoch: 1229/2000... Training loss: 0.4419\n",
      "Epoch: 1229/2000... Training loss: 0.5707\n",
      "Epoch: 1229/2000... Training loss: 0.3503\n",
      "Epoch: 1229/2000... Training loss: 0.3079\n",
      "Epoch: 1229/2000... Training loss: 0.5087\n",
      "Epoch: 1229/2000... Training loss: 0.5397\n",
      "Epoch: 1229/2000... Training loss: 0.5333\n",
      "Epoch: 1229/2000... Training loss: 0.4817\n",
      "Epoch: 1229/2000... Training loss: 0.3317\n",
      "Epoch: 1229/2000... Training loss: 0.5184\n",
      "Epoch: 1229/2000... Training loss: 0.5830\n",
      "Epoch: 1229/2000... Training loss: 0.3516\n",
      "Epoch: 1229/2000... Training loss: 0.3688\n",
      "Epoch: 1229/2000... Training loss: 0.3152\n",
      "Epoch: 1229/2000... Training loss: 0.4547\n",
      "Epoch: 1229/2000... Training loss: 0.3762\n",
      "Epoch: 1229/2000... Training loss: 0.3883\n",
      "Epoch: 1229/2000... Training loss: 0.6113\n",
      "Epoch: 1229/2000... Training loss: 0.3431\n",
      "Epoch: 1229/2000... Training loss: 0.3665\n",
      "Epoch: 1229/2000... Training loss: 0.4160\n",
      "Epoch: 1229/2000... Training loss: 0.3646\n",
      "Epoch: 1229/2000... Training loss: 0.5261\n",
      "Epoch: 1229/2000... Training loss: 0.5015\n",
      "Epoch: 1229/2000... Training loss: 0.4265\n",
      "Epoch: 1229/2000... Training loss: 0.3750\n",
      "Epoch: 1229/2000... Training loss: 0.3224\n",
      "Epoch: 1229/2000... Training loss: 0.4109\n",
      "Epoch: 1229/2000... Training loss: 0.4675\n",
      "Epoch: 1229/2000... Training loss: 0.4304\n",
      "Epoch: 1229/2000... Training loss: 0.4087\n",
      "Epoch: 1230/2000... Training loss: 0.4699\n",
      "Epoch: 1230/2000... Training loss: 0.6179\n",
      "Epoch: 1230/2000... Training loss: 0.4697\n",
      "Epoch: 1230/2000... Training loss: 0.3985\n",
      "Epoch: 1230/2000... Training loss: 0.3553\n",
      "Epoch: 1230/2000... Training loss: 0.6331\n",
      "Epoch: 1230/2000... Training loss: 0.4327\n",
      "Epoch: 1230/2000... Training loss: 0.4443\n",
      "Epoch: 1230/2000... Training loss: 0.5604\n",
      "Epoch: 1230/2000... Training loss: 0.5065\n",
      "Epoch: 1230/2000... Training loss: 0.6202\n",
      "Epoch: 1230/2000... Training loss: 0.4157\n",
      "Epoch: 1230/2000... Training loss: 0.5462\n",
      "Epoch: 1230/2000... Training loss: 0.5014\n",
      "Epoch: 1230/2000... Training loss: 0.2857\n",
      "Epoch: 1230/2000... Training loss: 0.3692\n",
      "Epoch: 1230/2000... Training loss: 0.4136\n",
      "Epoch: 1230/2000... Training loss: 0.3760\n",
      "Epoch: 1230/2000... Training loss: 0.2928\n",
      "Epoch: 1230/2000... Training loss: 0.3350\n",
      "Epoch: 1230/2000... Training loss: 0.5205\n",
      "Epoch: 1230/2000... Training loss: 0.4871\n",
      "Epoch: 1230/2000... Training loss: 0.4864\n",
      "Epoch: 1230/2000... Training loss: 0.5395\n",
      "Epoch: 1230/2000... Training loss: 0.5146\n",
      "Epoch: 1230/2000... Training loss: 0.4771\n",
      "Epoch: 1230/2000... Training loss: 0.3404\n",
      "Epoch: 1230/2000... Training loss: 0.4812\n",
      "Epoch: 1230/2000... Training loss: 0.4603\n",
      "Epoch: 1230/2000... Training loss: 0.3789\n",
      "Epoch: 1230/2000... Training loss: 0.5255\n",
      "Epoch: 1231/2000... Training loss: 0.4146\n",
      "Epoch: 1231/2000... Training loss: 0.5219\n",
      "Epoch: 1231/2000... Training loss: 0.3275\n",
      "Epoch: 1231/2000... Training loss: 0.5348\n",
      "Epoch: 1231/2000... Training loss: 0.2813\n",
      "Epoch: 1231/2000... Training loss: 0.4310\n",
      "Epoch: 1231/2000... Training loss: 0.2378\n",
      "Epoch: 1231/2000... Training loss: 0.2645\n",
      "Epoch: 1231/2000... Training loss: 0.5016\n",
      "Epoch: 1231/2000... Training loss: 0.3308\n",
      "Epoch: 1231/2000... Training loss: 0.5691\n",
      "Epoch: 1231/2000... Training loss: 0.2005\n",
      "Epoch: 1231/2000... Training loss: 0.4117\n",
      "Epoch: 1231/2000... Training loss: 0.3787\n",
      "Epoch: 1231/2000... Training loss: 0.4148\n",
      "Epoch: 1231/2000... Training loss: 0.5081\n",
      "Epoch: 1231/2000... Training loss: 0.6038\n",
      "Epoch: 1231/2000... Training loss: 0.3395\n",
      "Epoch: 1231/2000... Training loss: 0.2867\n",
      "Epoch: 1231/2000... Training loss: 0.4796\n",
      "Epoch: 1231/2000... Training loss: 0.4824\n",
      "Epoch: 1231/2000... Training loss: 0.3418\n",
      "Epoch: 1231/2000... Training loss: 0.4215\n",
      "Epoch: 1231/2000... Training loss: 0.4549\n",
      "Epoch: 1231/2000... Training loss: 0.5959\n",
      "Epoch: 1231/2000... Training loss: 0.5042\n",
      "Epoch: 1231/2000... Training loss: 0.4037\n",
      "Epoch: 1231/2000... Training loss: 0.5405\n",
      "Epoch: 1231/2000... Training loss: 0.4140\n",
      "Epoch: 1231/2000... Training loss: 0.3461\n",
      "Epoch: 1231/2000... Training loss: 0.3802\n",
      "Epoch: 1232/2000... Training loss: 0.4182\n",
      "Epoch: 1232/2000... Training loss: 0.3875\n",
      "Epoch: 1232/2000... Training loss: 0.4318\n",
      "Epoch: 1232/2000... Training loss: 0.4759\n",
      "Epoch: 1232/2000... Training loss: 0.3980\n",
      "Epoch: 1232/2000... Training loss: 0.3614\n",
      "Epoch: 1232/2000... Training loss: 0.4223\n",
      "Epoch: 1232/2000... Training loss: 0.3095\n",
      "Epoch: 1232/2000... Training loss: 0.4119\n",
      "Epoch: 1232/2000... Training loss: 0.3987\n",
      "Epoch: 1232/2000... Training loss: 0.3781\n",
      "Epoch: 1232/2000... Training loss: 0.5384\n",
      "Epoch: 1232/2000... Training loss: 0.4257\n",
      "Epoch: 1232/2000... Training loss: 0.3019\n",
      "Epoch: 1232/2000... Training loss: 0.3300\n",
      "Epoch: 1232/2000... Training loss: 0.2728\n",
      "Epoch: 1232/2000... Training loss: 0.6210\n",
      "Epoch: 1232/2000... Training loss: 0.3947\n",
      "Epoch: 1232/2000... Training loss: 0.4670\n",
      "Epoch: 1232/2000... Training loss: 0.5077\n",
      "Epoch: 1232/2000... Training loss: 0.4766\n",
      "Epoch: 1232/2000... Training loss: 0.5541\n",
      "Epoch: 1232/2000... Training loss: 0.3722\n",
      "Epoch: 1232/2000... Training loss: 0.5860\n",
      "Epoch: 1232/2000... Training loss: 0.3708\n",
      "Epoch: 1232/2000... Training loss: 0.3961\n",
      "Epoch: 1232/2000... Training loss: 0.5202\n",
      "Epoch: 1232/2000... Training loss: 0.5813\n",
      "Epoch: 1232/2000... Training loss: 0.3298\n",
      "Epoch: 1232/2000... Training loss: 0.5044\n",
      "Epoch: 1232/2000... Training loss: 0.5657\n",
      "Epoch: 1233/2000... Training loss: 0.3999\n",
      "Epoch: 1233/2000... Training loss: 0.4377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1233/2000... Training loss: 0.5860\n",
      "Epoch: 1233/2000... Training loss: 0.3643\n",
      "Epoch: 1233/2000... Training loss: 0.3294\n",
      "Epoch: 1233/2000... Training loss: 0.3971\n",
      "Epoch: 1233/2000... Training loss: 0.4396\n",
      "Epoch: 1233/2000... Training loss: 0.3041\n",
      "Epoch: 1233/2000... Training loss: 0.3838\n",
      "Epoch: 1233/2000... Training loss: 0.3324\n",
      "Epoch: 1233/2000... Training loss: 0.4695\n",
      "Epoch: 1233/2000... Training loss: 0.5005\n",
      "Epoch: 1233/2000... Training loss: 0.3513\n",
      "Epoch: 1233/2000... Training loss: 0.4260\n",
      "Epoch: 1233/2000... Training loss: 0.3377\n",
      "Epoch: 1233/2000... Training loss: 0.3722\n",
      "Epoch: 1233/2000... Training loss: 0.4754\n",
      "Epoch: 1233/2000... Training loss: 0.4732\n",
      "Epoch: 1233/2000... Training loss: 0.4618\n",
      "Epoch: 1233/2000... Training loss: 0.4527\n",
      "Epoch: 1233/2000... Training loss: 0.4706\n",
      "Epoch: 1233/2000... Training loss: 0.4986\n",
      "Epoch: 1233/2000... Training loss: 0.5082\n",
      "Epoch: 1233/2000... Training loss: 0.5588\n",
      "Epoch: 1233/2000... Training loss: 0.4980\n",
      "Epoch: 1233/2000... Training loss: 0.3412\n",
      "Epoch: 1233/2000... Training loss: 0.5248\n",
      "Epoch: 1233/2000... Training loss: 0.4281\n",
      "Epoch: 1233/2000... Training loss: 0.7928\n",
      "Epoch: 1233/2000... Training loss: 0.4356\n",
      "Epoch: 1233/2000... Training loss: 0.6855\n",
      "Epoch: 1234/2000... Training loss: 0.5739\n",
      "Epoch: 1234/2000... Training loss: 0.3547\n",
      "Epoch: 1234/2000... Training loss: 0.4896\n",
      "Epoch: 1234/2000... Training loss: 0.2231\n",
      "Epoch: 1234/2000... Training loss: 0.4539\n",
      "Epoch: 1234/2000... Training loss: 0.5270\n",
      "Epoch: 1234/2000... Training loss: 0.5831\n",
      "Epoch: 1234/2000... Training loss: 0.5145\n",
      "Epoch: 1234/2000... Training loss: 0.4729\n",
      "Epoch: 1234/2000... Training loss: 0.4994\n",
      "Epoch: 1234/2000... Training loss: 0.3834\n",
      "Epoch: 1234/2000... Training loss: 0.5775\n",
      "Epoch: 1234/2000... Training loss: 0.4685\n",
      "Epoch: 1234/2000... Training loss: 0.5182\n",
      "Epoch: 1234/2000... Training loss: 0.5762\n",
      "Epoch: 1234/2000... Training loss: 0.3276\n",
      "Epoch: 1234/2000... Training loss: 0.4565\n",
      "Epoch: 1234/2000... Training loss: 0.4170\n",
      "Epoch: 1234/2000... Training loss: 0.4216\n",
      "Epoch: 1234/2000... Training loss: 0.4863\n",
      "Epoch: 1234/2000... Training loss: 0.2924\n",
      "Epoch: 1234/2000... Training loss: 0.4256\n",
      "Epoch: 1234/2000... Training loss: 0.5462\n",
      "Epoch: 1234/2000... Training loss: 0.5893\n",
      "Epoch: 1234/2000... Training loss: 0.5363\n",
      "Epoch: 1234/2000... Training loss: 0.3189\n",
      "Epoch: 1234/2000... Training loss: 0.4849\n",
      "Epoch: 1234/2000... Training loss: 0.4513\n",
      "Epoch: 1234/2000... Training loss: 0.2291\n",
      "Epoch: 1234/2000... Training loss: 0.6336\n",
      "Epoch: 1234/2000... Training loss: 0.4590\n",
      "Epoch: 1235/2000... Training loss: 0.5000\n",
      "Epoch: 1235/2000... Training loss: 0.4971\n",
      "Epoch: 1235/2000... Training loss: 0.4668\n",
      "Epoch: 1235/2000... Training loss: 0.3158\n",
      "Epoch: 1235/2000... Training loss: 0.3096\n",
      "Epoch: 1235/2000... Training loss: 0.5436\n",
      "Epoch: 1235/2000... Training loss: 0.4724\n",
      "Epoch: 1235/2000... Training loss: 0.3202\n",
      "Epoch: 1235/2000... Training loss: 0.5771\n",
      "Epoch: 1235/2000... Training loss: 0.3608\n",
      "Epoch: 1235/2000... Training loss: 0.4449\n",
      "Epoch: 1235/2000... Training loss: 0.4569\n",
      "Epoch: 1235/2000... Training loss: 0.6022\n",
      "Epoch: 1235/2000... Training loss: 0.3636\n",
      "Epoch: 1235/2000... Training loss: 0.4073\n",
      "Epoch: 1235/2000... Training loss: 0.4120\n",
      "Epoch: 1235/2000... Training loss: 0.4527\n",
      "Epoch: 1235/2000... Training loss: 0.4946\n",
      "Epoch: 1235/2000... Training loss: 0.5200\n",
      "Epoch: 1235/2000... Training loss: 0.5405\n",
      "Epoch: 1235/2000... Training loss: 0.3070\n",
      "Epoch: 1235/2000... Training loss: 0.3998\n",
      "Epoch: 1235/2000... Training loss: 0.3863\n",
      "Epoch: 1235/2000... Training loss: 0.4497\n",
      "Epoch: 1235/2000... Training loss: 0.4177\n",
      "Epoch: 1235/2000... Training loss: 0.4832\n",
      "Epoch: 1235/2000... Training loss: 0.3510\n",
      "Epoch: 1235/2000... Training loss: 0.4928\n",
      "Epoch: 1235/2000... Training loss: 0.3327\n",
      "Epoch: 1235/2000... Training loss: 0.3192\n",
      "Epoch: 1235/2000... Training loss: 0.3331\n",
      "Epoch: 1236/2000... Training loss: 0.3776\n",
      "Epoch: 1236/2000... Training loss: 0.3659\n",
      "Epoch: 1236/2000... Training loss: 0.5566\n",
      "Epoch: 1236/2000... Training loss: 0.3864\n",
      "Epoch: 1236/2000... Training loss: 0.3201\n",
      "Epoch: 1236/2000... Training loss: 0.3620\n",
      "Epoch: 1236/2000... Training loss: 0.4736\n",
      "Epoch: 1236/2000... Training loss: 0.3998\n",
      "Epoch: 1236/2000... Training loss: 0.3069\n",
      "Epoch: 1236/2000... Training loss: 0.3339\n",
      "Epoch: 1236/2000... Training loss: 0.3656\n",
      "Epoch: 1236/2000... Training loss: 0.4060\n",
      "Epoch: 1236/2000... Training loss: 0.4458\n",
      "Epoch: 1236/2000... Training loss: 0.5355\n",
      "Epoch: 1236/2000... Training loss: 0.3027\n",
      "Epoch: 1236/2000... Training loss: 0.3007\n",
      "Epoch: 1236/2000... Training loss: 0.4140\n",
      "Epoch: 1236/2000... Training loss: 0.3702\n",
      "Epoch: 1236/2000... Training loss: 0.4222\n",
      "Epoch: 1236/2000... Training loss: 0.4307\n",
      "Epoch: 1236/2000... Training loss: 0.4584\n",
      "Epoch: 1236/2000... Training loss: 0.4973\n",
      "Epoch: 1236/2000... Training loss: 0.4434\n",
      "Epoch: 1236/2000... Training loss: 0.3544\n",
      "Epoch: 1236/2000... Training loss: 0.4465\n",
      "Epoch: 1236/2000... Training loss: 0.3568\n",
      "Epoch: 1236/2000... Training loss: 0.5656\n",
      "Epoch: 1236/2000... Training loss: 0.3862\n",
      "Epoch: 1236/2000... Training loss: 0.4198\n",
      "Epoch: 1236/2000... Training loss: 0.3593\n",
      "Epoch: 1236/2000... Training loss: 0.5178\n",
      "Epoch: 1237/2000... Training loss: 0.3721\n",
      "Epoch: 1237/2000... Training loss: 0.4951\n",
      "Epoch: 1237/2000... Training loss: 0.3756\n",
      "Epoch: 1237/2000... Training loss: 0.3876\n",
      "Epoch: 1237/2000... Training loss: 0.3943\n",
      "Epoch: 1237/2000... Training loss: 0.4050\n",
      "Epoch: 1237/2000... Training loss: 0.4274\n",
      "Epoch: 1237/2000... Training loss: 0.4591\n",
      "Epoch: 1237/2000... Training loss: 0.3618\n",
      "Epoch: 1237/2000... Training loss: 0.4381\n",
      "Epoch: 1237/2000... Training loss: 0.4761\n",
      "Epoch: 1237/2000... Training loss: 0.3522\n",
      "Epoch: 1237/2000... Training loss: 0.3997\n",
      "Epoch: 1237/2000... Training loss: 0.3069\n",
      "Epoch: 1237/2000... Training loss: 0.3246\n",
      "Epoch: 1237/2000... Training loss: 0.3716\n",
      "Epoch: 1237/2000... Training loss: 0.2590\n",
      "Epoch: 1237/2000... Training loss: 0.5332\n",
      "Epoch: 1237/2000... Training loss: 0.3356\n",
      "Epoch: 1237/2000... Training loss: 0.3745\n",
      "Epoch: 1237/2000... Training loss: 0.5867\n",
      "Epoch: 1237/2000... Training loss: 0.4915\n",
      "Epoch: 1237/2000... Training loss: 0.5685\n",
      "Epoch: 1237/2000... Training loss: 0.4008\n",
      "Epoch: 1237/2000... Training loss: 0.3771\n",
      "Epoch: 1237/2000... Training loss: 0.3619\n",
      "Epoch: 1237/2000... Training loss: 0.5600\n",
      "Epoch: 1237/2000... Training loss: 0.4486\n",
      "Epoch: 1237/2000... Training loss: 0.4072\n",
      "Epoch: 1237/2000... Training loss: 0.3300\n",
      "Epoch: 1237/2000... Training loss: 0.4520\n",
      "Epoch: 1238/2000... Training loss: 0.4337\n",
      "Epoch: 1238/2000... Training loss: 0.4738\n",
      "Epoch: 1238/2000... Training loss: 0.3648\n",
      "Epoch: 1238/2000... Training loss: 0.5524\n",
      "Epoch: 1238/2000... Training loss: 0.5669\n",
      "Epoch: 1238/2000... Training loss: 0.4054\n",
      "Epoch: 1238/2000... Training loss: 0.3547\n",
      "Epoch: 1238/2000... Training loss: 0.4408\n",
      "Epoch: 1238/2000... Training loss: 0.4017\n",
      "Epoch: 1238/2000... Training loss: 0.5214\n",
      "Epoch: 1238/2000... Training loss: 0.4712\n",
      "Epoch: 1238/2000... Training loss: 0.4877\n",
      "Epoch: 1238/2000... Training loss: 0.3779\n",
      "Epoch: 1238/2000... Training loss: 0.4804\n",
      "Epoch: 1238/2000... Training loss: 0.3517\n",
      "Epoch: 1238/2000... Training loss: 0.5247\n",
      "Epoch: 1238/2000... Training loss: 0.4849\n",
      "Epoch: 1238/2000... Training loss: 0.5313\n",
      "Epoch: 1238/2000... Training loss: 0.4023\n",
      "Epoch: 1238/2000... Training loss: 0.4606\n",
      "Epoch: 1238/2000... Training loss: 0.3862\n",
      "Epoch: 1238/2000... Training loss: 0.4053\n",
      "Epoch: 1238/2000... Training loss: 0.4314\n",
      "Epoch: 1238/2000... Training loss: 0.5233\n",
      "Epoch: 1238/2000... Training loss: 0.4630\n",
      "Epoch: 1238/2000... Training loss: 0.5981\n",
      "Epoch: 1238/2000... Training loss: 0.4619\n",
      "Epoch: 1238/2000... Training loss: 0.4411\n",
      "Epoch: 1238/2000... Training loss: 0.3545\n",
      "Epoch: 1238/2000... Training loss: 0.5477\n",
      "Epoch: 1238/2000... Training loss: 0.6619\n",
      "Epoch: 1239/2000... Training loss: 0.3657\n",
      "Epoch: 1239/2000... Training loss: 0.3965\n",
      "Epoch: 1239/2000... Training loss: 0.4133\n",
      "Epoch: 1239/2000... Training loss: 0.3787\n",
      "Epoch: 1239/2000... Training loss: 0.4135\n",
      "Epoch: 1239/2000... Training loss: 0.5359\n",
      "Epoch: 1239/2000... Training loss: 0.4638\n",
      "Epoch: 1239/2000... Training loss: 0.3163\n",
      "Epoch: 1239/2000... Training loss: 0.3954\n",
      "Epoch: 1239/2000... Training loss: 0.4581\n",
      "Epoch: 1239/2000... Training loss: 0.4272\n",
      "Epoch: 1239/2000... Training loss: 0.3189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1239/2000... Training loss: 0.4800\n",
      "Epoch: 1239/2000... Training loss: 0.4423\n",
      "Epoch: 1239/2000... Training loss: 0.4340\n",
      "Epoch: 1239/2000... Training loss: 0.7269\n",
      "Epoch: 1239/2000... Training loss: 0.3563\n",
      "Epoch: 1239/2000... Training loss: 0.4235\n",
      "Epoch: 1239/2000... Training loss: 0.3675\n",
      "Epoch: 1239/2000... Training loss: 0.4570\n",
      "Epoch: 1239/2000... Training loss: 0.4151\n",
      "Epoch: 1239/2000... Training loss: 0.4965\n",
      "Epoch: 1239/2000... Training loss: 0.4740\n",
      "Epoch: 1239/2000... Training loss: 0.5201\n",
      "Epoch: 1239/2000... Training loss: 0.4832\n",
      "Epoch: 1239/2000... Training loss: 0.4808\n",
      "Epoch: 1239/2000... Training loss: 0.5192\n",
      "Epoch: 1239/2000... Training loss: 0.3633\n",
      "Epoch: 1239/2000... Training loss: 0.5009\n",
      "Epoch: 1239/2000... Training loss: 0.4561\n",
      "Epoch: 1239/2000... Training loss: 0.4501\n",
      "Epoch: 1240/2000... Training loss: 0.6057\n",
      "Epoch: 1240/2000... Training loss: 0.3730\n",
      "Epoch: 1240/2000... Training loss: 0.4160\n",
      "Epoch: 1240/2000... Training loss: 0.5961\n",
      "Epoch: 1240/2000... Training loss: 0.3756\n",
      "Epoch: 1240/2000... Training loss: 0.2997\n",
      "Epoch: 1240/2000... Training loss: 0.4013\n",
      "Epoch: 1240/2000... Training loss: 0.6843\n",
      "Epoch: 1240/2000... Training loss: 0.5442\n",
      "Epoch: 1240/2000... Training loss: 0.4177\n",
      "Epoch: 1240/2000... Training loss: 0.4630\n",
      "Epoch: 1240/2000... Training loss: 0.3682\n",
      "Epoch: 1240/2000... Training loss: 0.6169\n",
      "Epoch: 1240/2000... Training loss: 0.4557\n",
      "Epoch: 1240/2000... Training loss: 0.3259\n",
      "Epoch: 1240/2000... Training loss: 0.5318\n",
      "Epoch: 1240/2000... Training loss: 0.5224\n",
      "Epoch: 1240/2000... Training loss: 0.4727\n",
      "Epoch: 1240/2000... Training loss: 0.6268\n",
      "Epoch: 1240/2000... Training loss: 0.3978\n",
      "Epoch: 1240/2000... Training loss: 0.4437\n",
      "Epoch: 1240/2000... Training loss: 0.3591\n",
      "Epoch: 1240/2000... Training loss: 0.4385\n",
      "Epoch: 1240/2000... Training loss: 0.5070\n",
      "Epoch: 1240/2000... Training loss: 0.4763\n",
      "Epoch: 1240/2000... Training loss: 0.4250\n",
      "Epoch: 1240/2000... Training loss: 0.4243\n",
      "Epoch: 1240/2000... Training loss: 0.3902\n",
      "Epoch: 1240/2000... Training loss: 0.5630\n",
      "Epoch: 1240/2000... Training loss: 0.4185\n",
      "Epoch: 1240/2000... Training loss: 0.4082\n",
      "Epoch: 1241/2000... Training loss: 0.5911\n",
      "Epoch: 1241/2000... Training loss: 0.5945\n",
      "Epoch: 1241/2000... Training loss: 0.3434\n",
      "Epoch: 1241/2000... Training loss: 0.5474\n",
      "Epoch: 1241/2000... Training loss: 0.3451\n",
      "Epoch: 1241/2000... Training loss: 0.3653\n",
      "Epoch: 1241/2000... Training loss: 0.4781\n",
      "Epoch: 1241/2000... Training loss: 0.4280\n",
      "Epoch: 1241/2000... Training loss: 0.3618\n",
      "Epoch: 1241/2000... Training loss: 0.3387\n",
      "Epoch: 1241/2000... Training loss: 0.4437\n",
      "Epoch: 1241/2000... Training loss: 0.4183\n",
      "Epoch: 1241/2000... Training loss: 0.4445\n",
      "Epoch: 1241/2000... Training loss: 0.3954\n",
      "Epoch: 1241/2000... Training loss: 0.3327\n",
      "Epoch: 1241/2000... Training loss: 0.4102\n",
      "Epoch: 1241/2000... Training loss: 0.4533\n",
      "Epoch: 1241/2000... Training loss: 0.4519\n",
      "Epoch: 1241/2000... Training loss: 0.3346\n",
      "Epoch: 1241/2000... Training loss: 0.4511\n",
      "Epoch: 1241/2000... Training loss: 0.5238\n",
      "Epoch: 1241/2000... Training loss: 0.5180\n",
      "Epoch: 1241/2000... Training loss: 0.4387\n",
      "Epoch: 1241/2000... Training loss: 0.4163\n",
      "Epoch: 1241/2000... Training loss: 0.5114\n",
      "Epoch: 1241/2000... Training loss: 0.6173\n",
      "Epoch: 1241/2000... Training loss: 0.4884\n",
      "Epoch: 1241/2000... Training loss: 0.3760\n",
      "Epoch: 1241/2000... Training loss: 0.5555\n",
      "Epoch: 1241/2000... Training loss: 0.4366\n",
      "Epoch: 1241/2000... Training loss: 0.4678\n",
      "Epoch: 1242/2000... Training loss: 0.4406\n",
      "Epoch: 1242/2000... Training loss: 0.5337\n",
      "Epoch: 1242/2000... Training loss: 0.5319\n",
      "Epoch: 1242/2000... Training loss: 0.3536\n",
      "Epoch: 1242/2000... Training loss: 0.4449\n",
      "Epoch: 1242/2000... Training loss: 0.4619\n",
      "Epoch: 1242/2000... Training loss: 0.4551\n",
      "Epoch: 1242/2000... Training loss: 0.2945\n",
      "Epoch: 1242/2000... Training loss: 0.4470\n",
      "Epoch: 1242/2000... Training loss: 0.4763\n",
      "Epoch: 1242/2000... Training loss: 0.3764\n",
      "Epoch: 1242/2000... Training loss: 0.4904\n",
      "Epoch: 1242/2000... Training loss: 0.4965\n",
      "Epoch: 1242/2000... Training loss: 0.4106\n",
      "Epoch: 1242/2000... Training loss: 0.3817\n",
      "Epoch: 1242/2000... Training loss: 0.5089\n",
      "Epoch: 1242/2000... Training loss: 0.4260\n",
      "Epoch: 1242/2000... Training loss: 0.2937\n",
      "Epoch: 1242/2000... Training loss: 0.4136\n",
      "Epoch: 1242/2000... Training loss: 0.6304\n",
      "Epoch: 1242/2000... Training loss: 0.5280\n",
      "Epoch: 1242/2000... Training loss: 0.6111\n",
      "Epoch: 1242/2000... Training loss: 0.3780\n",
      "Epoch: 1242/2000... Training loss: 0.5860\n",
      "Epoch: 1242/2000... Training loss: 0.4906\n",
      "Epoch: 1242/2000... Training loss: 0.5359\n",
      "Epoch: 1242/2000... Training loss: 0.4744\n",
      "Epoch: 1242/2000... Training loss: 0.3876\n",
      "Epoch: 1242/2000... Training loss: 0.3707\n",
      "Epoch: 1242/2000... Training loss: 0.4363\n",
      "Epoch: 1242/2000... Training loss: 0.6456\n",
      "Epoch: 1243/2000... Training loss: 0.4503\n",
      "Epoch: 1243/2000... Training loss: 0.3555\n",
      "Epoch: 1243/2000... Training loss: 0.2778\n",
      "Epoch: 1243/2000... Training loss: 0.6030\n",
      "Epoch: 1243/2000... Training loss: 0.4571\n",
      "Epoch: 1243/2000... Training loss: 0.5434\n",
      "Epoch: 1243/2000... Training loss: 0.4688\n",
      "Epoch: 1243/2000... Training loss: 0.3238\n",
      "Epoch: 1243/2000... Training loss: 0.6466\n",
      "Epoch: 1243/2000... Training loss: 0.3893\n",
      "Epoch: 1243/2000... Training loss: 0.5500\n",
      "Epoch: 1243/2000... Training loss: 0.3099\n",
      "Epoch: 1243/2000... Training loss: 0.5320\n",
      "Epoch: 1243/2000... Training loss: 0.3905\n",
      "Epoch: 1243/2000... Training loss: 0.5490\n",
      "Epoch: 1243/2000... Training loss: 0.4206\n",
      "Epoch: 1243/2000... Training loss: 0.5882\n",
      "Epoch: 1243/2000... Training loss: 0.4011\n",
      "Epoch: 1243/2000... Training loss: 0.4048\n",
      "Epoch: 1243/2000... Training loss: 0.3755\n",
      "Epoch: 1243/2000... Training loss: 0.3222\n",
      "Epoch: 1243/2000... Training loss: 0.4710\n",
      "Epoch: 1243/2000... Training loss: 0.6577\n",
      "Epoch: 1243/2000... Training loss: 0.3473\n",
      "Epoch: 1243/2000... Training loss: 0.4107\n",
      "Epoch: 1243/2000... Training loss: 0.3925\n",
      "Epoch: 1243/2000... Training loss: 0.4392\n",
      "Epoch: 1243/2000... Training loss: 0.6712\n",
      "Epoch: 1243/2000... Training loss: 0.2693\n",
      "Epoch: 1243/2000... Training loss: 0.6912\n",
      "Epoch: 1243/2000... Training loss: 0.4753\n",
      "Epoch: 1244/2000... Training loss: 0.5415\n",
      "Epoch: 1244/2000... Training loss: 0.3756\n",
      "Epoch: 1244/2000... Training loss: 0.4717\n",
      "Epoch: 1244/2000... Training loss: 0.3589\n",
      "Epoch: 1244/2000... Training loss: 0.6444\n",
      "Epoch: 1244/2000... Training loss: 0.3698\n",
      "Epoch: 1244/2000... Training loss: 0.4656\n",
      "Epoch: 1244/2000... Training loss: 0.5412\n",
      "Epoch: 1244/2000... Training loss: 0.4236\n",
      "Epoch: 1244/2000... Training loss: 0.3747\n",
      "Epoch: 1244/2000... Training loss: 0.4635\n",
      "Epoch: 1244/2000... Training loss: 0.3394\n",
      "Epoch: 1244/2000... Training loss: 0.3738\n",
      "Epoch: 1244/2000... Training loss: 0.6173\n",
      "Epoch: 1244/2000... Training loss: 0.4310\n",
      "Epoch: 1244/2000... Training loss: 0.4848\n",
      "Epoch: 1244/2000... Training loss: 0.2890\n",
      "Epoch: 1244/2000... Training loss: 0.4449\n",
      "Epoch: 1244/2000... Training loss: 0.5411\n",
      "Epoch: 1244/2000... Training loss: 0.4709\n",
      "Epoch: 1244/2000... Training loss: 0.5249\n",
      "Epoch: 1244/2000... Training loss: 0.5019\n",
      "Epoch: 1244/2000... Training loss: 0.3612\n",
      "Epoch: 1244/2000... Training loss: 0.6030\n",
      "Epoch: 1244/2000... Training loss: 0.4584\n",
      "Epoch: 1244/2000... Training loss: 0.3842\n",
      "Epoch: 1244/2000... Training loss: 0.5622\n",
      "Epoch: 1244/2000... Training loss: 0.5978\n",
      "Epoch: 1244/2000... Training loss: 0.4653\n",
      "Epoch: 1244/2000... Training loss: 0.4413\n",
      "Epoch: 1244/2000... Training loss: 0.3093\n",
      "Epoch: 1245/2000... Training loss: 0.4228\n",
      "Epoch: 1245/2000... Training loss: 0.4115\n",
      "Epoch: 1245/2000... Training loss: 0.4442\n",
      "Epoch: 1245/2000... Training loss: 0.3681\n",
      "Epoch: 1245/2000... Training loss: 0.6118\n",
      "Epoch: 1245/2000... Training loss: 0.4296\n",
      "Epoch: 1245/2000... Training loss: 0.2900\n",
      "Epoch: 1245/2000... Training loss: 0.3767\n",
      "Epoch: 1245/2000... Training loss: 0.5671\n",
      "Epoch: 1245/2000... Training loss: 0.5508\n",
      "Epoch: 1245/2000... Training loss: 0.4391\n",
      "Epoch: 1245/2000... Training loss: 0.4671\n",
      "Epoch: 1245/2000... Training loss: 0.5782\n",
      "Epoch: 1245/2000... Training loss: 0.4217\n",
      "Epoch: 1245/2000... Training loss: 0.4399\n",
      "Epoch: 1245/2000... Training loss: 0.5731\n",
      "Epoch: 1245/2000... Training loss: 0.2603\n",
      "Epoch: 1245/2000... Training loss: 0.6801\n",
      "Epoch: 1245/2000... Training loss: 0.4552\n",
      "Epoch: 1245/2000... Training loss: 0.3945\n",
      "Epoch: 1245/2000... Training loss: 0.4162\n",
      "Epoch: 1245/2000... Training loss: 0.5466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1245/2000... Training loss: 0.5468\n",
      "Epoch: 1245/2000... Training loss: 0.4971\n",
      "Epoch: 1245/2000... Training loss: 0.2684\n",
      "Epoch: 1245/2000... Training loss: 0.5220\n",
      "Epoch: 1245/2000... Training loss: 0.4308\n",
      "Epoch: 1245/2000... Training loss: 0.4549\n",
      "Epoch: 1245/2000... Training loss: 0.3348\n",
      "Epoch: 1245/2000... Training loss: 0.3744\n",
      "Epoch: 1245/2000... Training loss: 0.4457\n",
      "Epoch: 1246/2000... Training loss: 0.4732\n",
      "Epoch: 1246/2000... Training loss: 0.6857\n",
      "Epoch: 1246/2000... Training loss: 0.3437\n",
      "Epoch: 1246/2000... Training loss: 0.7362\n",
      "Epoch: 1246/2000... Training loss: 0.5480\n",
      "Epoch: 1246/2000... Training loss: 0.3893\n",
      "Epoch: 1246/2000... Training loss: 0.4489\n",
      "Epoch: 1246/2000... Training loss: 0.3866\n",
      "Epoch: 1246/2000... Training loss: 0.4015\n",
      "Epoch: 1246/2000... Training loss: 0.5597\n",
      "Epoch: 1246/2000... Training loss: 0.3315\n",
      "Epoch: 1246/2000... Training loss: 0.6052\n",
      "Epoch: 1246/2000... Training loss: 0.3667\n",
      "Epoch: 1246/2000... Training loss: 0.4796\n",
      "Epoch: 1246/2000... Training loss: 0.5361\n",
      "Epoch: 1246/2000... Training loss: 0.4153\n",
      "Epoch: 1246/2000... Training loss: 0.4987\n",
      "Epoch: 1246/2000... Training loss: 0.3449\n",
      "Epoch: 1246/2000... Training loss: 0.3913\n",
      "Epoch: 1246/2000... Training loss: 0.3599\n",
      "Epoch: 1246/2000... Training loss: 0.4048\n",
      "Epoch: 1246/2000... Training loss: 0.3637\n",
      "Epoch: 1246/2000... Training loss: 0.4129\n",
      "Epoch: 1246/2000... Training loss: 0.4722\n",
      "Epoch: 1246/2000... Training loss: 0.3781\n",
      "Epoch: 1246/2000... Training loss: 0.5550\n",
      "Epoch: 1246/2000... Training loss: 0.2675\n",
      "Epoch: 1246/2000... Training loss: 0.3905\n",
      "Epoch: 1246/2000... Training loss: 0.3352\n",
      "Epoch: 1246/2000... Training loss: 0.6185\n",
      "Epoch: 1246/2000... Training loss: 0.3090\n",
      "Epoch: 1247/2000... Training loss: 0.3003\n",
      "Epoch: 1247/2000... Training loss: 0.6234\n",
      "Epoch: 1247/2000... Training loss: 0.4508\n",
      "Epoch: 1247/2000... Training loss: 0.4146\n",
      "Epoch: 1247/2000... Training loss: 0.6263\n",
      "Epoch: 1247/2000... Training loss: 0.4257\n",
      "Epoch: 1247/2000... Training loss: 0.4328\n",
      "Epoch: 1247/2000... Training loss: 0.4624\n",
      "Epoch: 1247/2000... Training loss: 0.4775\n",
      "Epoch: 1247/2000... Training loss: 0.6011\n",
      "Epoch: 1247/2000... Training loss: 0.4853\n",
      "Epoch: 1247/2000... Training loss: 0.4705\n",
      "Epoch: 1247/2000... Training loss: 0.3357\n",
      "Epoch: 1247/2000... Training loss: 0.4489\n",
      "Epoch: 1247/2000... Training loss: 0.3519\n",
      "Epoch: 1247/2000... Training loss: 0.4806\n",
      "Epoch: 1247/2000... Training loss: 0.5905\n",
      "Epoch: 1247/2000... Training loss: 0.3280\n",
      "Epoch: 1247/2000... Training loss: 0.2495\n",
      "Epoch: 1247/2000... Training loss: 0.4943\n",
      "Epoch: 1247/2000... Training loss: 0.4855\n",
      "Epoch: 1247/2000... Training loss: 0.6051\n",
      "Epoch: 1247/2000... Training loss: 0.6138\n",
      "Epoch: 1247/2000... Training loss: 0.4239\n",
      "Epoch: 1247/2000... Training loss: 0.4206\n",
      "Epoch: 1247/2000... Training loss: 0.4371\n",
      "Epoch: 1247/2000... Training loss: 0.3645\n",
      "Epoch: 1247/2000... Training loss: 0.5416\n",
      "Epoch: 1247/2000... Training loss: 0.4606\n",
      "Epoch: 1247/2000... Training loss: 0.5742\n",
      "Epoch: 1247/2000... Training loss: 0.3773\n",
      "Epoch: 1248/2000... Training loss: 0.4908\n",
      "Epoch: 1248/2000... Training loss: 0.4426\n",
      "Epoch: 1248/2000... Training loss: 0.4710\n",
      "Epoch: 1248/2000... Training loss: 0.3739\n",
      "Epoch: 1248/2000... Training loss: 0.4401\n",
      "Epoch: 1248/2000... Training loss: 0.2651\n",
      "Epoch: 1248/2000... Training loss: 0.5198\n",
      "Epoch: 1248/2000... Training loss: 0.4719\n",
      "Epoch: 1248/2000... Training loss: 0.6260\n",
      "Epoch: 1248/2000... Training loss: 0.3236\n",
      "Epoch: 1248/2000... Training loss: 0.5862\n",
      "Epoch: 1248/2000... Training loss: 0.3391\n",
      "Epoch: 1248/2000... Training loss: 0.5017\n",
      "Epoch: 1248/2000... Training loss: 0.3042\n",
      "Epoch: 1248/2000... Training loss: 0.5199\n",
      "Epoch: 1248/2000... Training loss: 0.4253\n",
      "Epoch: 1248/2000... Training loss: 0.3771\n",
      "Epoch: 1248/2000... Training loss: 0.4573\n",
      "Epoch: 1248/2000... Training loss: 0.3819\n",
      "Epoch: 1248/2000... Training loss: 0.5467\n",
      "Epoch: 1248/2000... Training loss: 0.4670\n",
      "Epoch: 1248/2000... Training loss: 0.5137\n",
      "Epoch: 1248/2000... Training loss: 0.4423\n",
      "Epoch: 1248/2000... Training loss: 0.5509\n",
      "Epoch: 1248/2000... Training loss: 0.4000\n",
      "Epoch: 1248/2000... Training loss: 0.4286\n",
      "Epoch: 1248/2000... Training loss: 0.5462\n",
      "Epoch: 1248/2000... Training loss: 0.6071\n",
      "Epoch: 1248/2000... Training loss: 0.4287\n",
      "Epoch: 1248/2000... Training loss: 0.4997\n",
      "Epoch: 1248/2000... Training loss: 0.3368\n",
      "Epoch: 1249/2000... Training loss: 0.2897\n",
      "Epoch: 1249/2000... Training loss: 0.5465\n",
      "Epoch: 1249/2000... Training loss: 0.4947\n",
      "Epoch: 1249/2000... Training loss: 0.5023\n",
      "Epoch: 1249/2000... Training loss: 0.3257\n",
      "Epoch: 1249/2000... Training loss: 0.5278\n",
      "Epoch: 1249/2000... Training loss: 0.3200\n",
      "Epoch: 1249/2000... Training loss: 0.5394\n",
      "Epoch: 1249/2000... Training loss: 0.5271\n",
      "Epoch: 1249/2000... Training loss: 0.4242\n",
      "Epoch: 1249/2000... Training loss: 0.3489\n",
      "Epoch: 1249/2000... Training loss: 0.6702\n",
      "Epoch: 1249/2000... Training loss: 0.4691\n",
      "Epoch: 1249/2000... Training loss: 0.2912\n",
      "Epoch: 1249/2000... Training loss: 0.4917\n",
      "Epoch: 1249/2000... Training loss: 0.4164\n",
      "Epoch: 1249/2000... Training loss: 0.2815\n",
      "Epoch: 1249/2000... Training loss: 0.4935\n",
      "Epoch: 1249/2000... Training loss: 0.5497\n",
      "Epoch: 1249/2000... Training loss: 0.7964\n",
      "Epoch: 1249/2000... Training loss: 0.4327\n",
      "Epoch: 1249/2000... Training loss: 0.3885\n",
      "Epoch: 1249/2000... Training loss: 0.4715\n",
      "Epoch: 1249/2000... Training loss: 0.3337\n",
      "Epoch: 1249/2000... Training loss: 0.4924\n",
      "Epoch: 1249/2000... Training loss: 0.5022\n",
      "Epoch: 1249/2000... Training loss: 0.6772\n",
      "Epoch: 1249/2000... Training loss: 0.5314\n",
      "Epoch: 1249/2000... Training loss: 0.6217\n",
      "Epoch: 1249/2000... Training loss: 0.3302\n",
      "Epoch: 1249/2000... Training loss: 0.4108\n",
      "Epoch: 1250/2000... Training loss: 0.3681\n",
      "Epoch: 1250/2000... Training loss: 0.3949\n",
      "Epoch: 1250/2000... Training loss: 0.4109\n",
      "Epoch: 1250/2000... Training loss: 0.3653\n",
      "Epoch: 1250/2000... Training loss: 0.4052\n",
      "Epoch: 1250/2000... Training loss: 0.4930\n",
      "Epoch: 1250/2000... Training loss: 0.4827\n",
      "Epoch: 1250/2000... Training loss: 0.4417\n",
      "Epoch: 1250/2000... Training loss: 0.7041\n",
      "Epoch: 1250/2000... Training loss: 0.5233\n",
      "Epoch: 1250/2000... Training loss: 0.4580\n",
      "Epoch: 1250/2000... Training loss: 0.3186\n",
      "Epoch: 1250/2000... Training loss: 0.5756\n",
      "Epoch: 1250/2000... Training loss: 0.4151\n",
      "Epoch: 1250/2000... Training loss: 0.4880\n",
      "Epoch: 1250/2000... Training loss: 0.4835\n",
      "Epoch: 1250/2000... Training loss: 0.6354\n",
      "Epoch: 1250/2000... Training loss: 0.3124\n",
      "Epoch: 1250/2000... Training loss: 0.5715\n",
      "Epoch: 1250/2000... Training loss: 0.4793\n",
      "Epoch: 1250/2000... Training loss: 0.4332\n",
      "Epoch: 1250/2000... Training loss: 0.6029\n",
      "Epoch: 1250/2000... Training loss: 0.3375\n",
      "Epoch: 1250/2000... Training loss: 0.4617\n",
      "Epoch: 1250/2000... Training loss: 0.4718\n",
      "Epoch: 1250/2000... Training loss: 0.4622\n",
      "Epoch: 1250/2000... Training loss: 0.5102\n",
      "Epoch: 1250/2000... Training loss: 0.4792\n",
      "Epoch: 1250/2000... Training loss: 0.2397\n",
      "Epoch: 1250/2000... Training loss: 0.4879\n",
      "Epoch: 1250/2000... Training loss: 0.5358\n",
      "Epoch: 1251/2000... Training loss: 0.4511\n",
      "Epoch: 1251/2000... Training loss: 0.4607\n",
      "Epoch: 1251/2000... Training loss: 0.5641\n",
      "Epoch: 1251/2000... Training loss: 0.4929\n",
      "Epoch: 1251/2000... Training loss: 0.5337\n",
      "Epoch: 1251/2000... Training loss: 0.6222\n",
      "Epoch: 1251/2000... Training loss: 0.5467\n",
      "Epoch: 1251/2000... Training loss: 0.5883\n",
      "Epoch: 1251/2000... Training loss: 0.4038\n",
      "Epoch: 1251/2000... Training loss: 0.4750\n",
      "Epoch: 1251/2000... Training loss: 0.4001\n",
      "Epoch: 1251/2000... Training loss: 0.4788\n",
      "Epoch: 1251/2000... Training loss: 0.4605\n",
      "Epoch: 1251/2000... Training loss: 0.4703\n",
      "Epoch: 1251/2000... Training loss: 0.5200\n",
      "Epoch: 1251/2000... Training loss: 0.5443\n",
      "Epoch: 1251/2000... Training loss: 0.4232\n",
      "Epoch: 1251/2000... Training loss: 0.5216\n",
      "Epoch: 1251/2000... Training loss: 0.4576\n",
      "Epoch: 1251/2000... Training loss: 0.5690\n",
      "Epoch: 1251/2000... Training loss: 0.5575\n",
      "Epoch: 1251/2000... Training loss: 0.4383\n",
      "Epoch: 1251/2000... Training loss: 0.4520\n",
      "Epoch: 1251/2000... Training loss: 0.5027\n",
      "Epoch: 1251/2000... Training loss: 0.4955\n",
      "Epoch: 1251/2000... Training loss: 0.5860\n",
      "Epoch: 1251/2000... Training loss: 0.2670\n",
      "Epoch: 1251/2000... Training loss: 0.4901\n",
      "Epoch: 1251/2000... Training loss: 0.5343\n",
      "Epoch: 1251/2000... Training loss: 0.6017\n",
      "Epoch: 1251/2000... Training loss: 0.5091\n",
      "Epoch: 1252/2000... Training loss: 0.4925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1252/2000... Training loss: 0.3248\n",
      "Epoch: 1252/2000... Training loss: 0.2811\n",
      "Epoch: 1252/2000... Training loss: 0.4863\n",
      "Epoch: 1252/2000... Training loss: 0.3874\n",
      "Epoch: 1252/2000... Training loss: 0.6442\n",
      "Epoch: 1252/2000... Training loss: 0.5856\n",
      "Epoch: 1252/2000... Training loss: 0.4625\n",
      "Epoch: 1252/2000... Training loss: 0.6506\n",
      "Epoch: 1252/2000... Training loss: 0.4522\n",
      "Epoch: 1252/2000... Training loss: 0.3818\n",
      "Epoch: 1252/2000... Training loss: 0.4385\n",
      "Epoch: 1252/2000... Training loss: 0.5491\n",
      "Epoch: 1252/2000... Training loss: 0.3372\n",
      "Epoch: 1252/2000... Training loss: 0.4282\n",
      "Epoch: 1252/2000... Training loss: 0.4269\n",
      "Epoch: 1252/2000... Training loss: 0.4512\n",
      "Epoch: 1252/2000... Training loss: 0.4710\n",
      "Epoch: 1252/2000... Training loss: 0.4570\n",
      "Epoch: 1252/2000... Training loss: 0.4299\n",
      "Epoch: 1252/2000... Training loss: 0.6188\n",
      "Epoch: 1252/2000... Training loss: 0.4622\n",
      "Epoch: 1252/2000... Training loss: 0.5528\n",
      "Epoch: 1252/2000... Training loss: 0.4226\n",
      "Epoch: 1252/2000... Training loss: 0.5675\n",
      "Epoch: 1252/2000... Training loss: 0.4376\n",
      "Epoch: 1252/2000... Training loss: 0.3518\n",
      "Epoch: 1252/2000... Training loss: 0.4818\n",
      "Epoch: 1252/2000... Training loss: 0.3869\n",
      "Epoch: 1252/2000... Training loss: 0.4426\n",
      "Epoch: 1252/2000... Training loss: 0.3578\n",
      "Epoch: 1253/2000... Training loss: 0.5917\n",
      "Epoch: 1253/2000... Training loss: 0.6979\n",
      "Epoch: 1253/2000... Training loss: 0.4069\n",
      "Epoch: 1253/2000... Training loss: 0.4487\n",
      "Epoch: 1253/2000... Training loss: 0.5381\n",
      "Epoch: 1253/2000... Training loss: 0.3039\n",
      "Epoch: 1253/2000... Training loss: 0.3702\n",
      "Epoch: 1253/2000... Training loss: 0.4654\n",
      "Epoch: 1253/2000... Training loss: 0.4871\n",
      "Epoch: 1253/2000... Training loss: 0.3514\n",
      "Epoch: 1253/2000... Training loss: 0.5584\n",
      "Epoch: 1253/2000... Training loss: 0.4548\n",
      "Epoch: 1253/2000... Training loss: 0.3607\n",
      "Epoch: 1253/2000... Training loss: 0.4312\n",
      "Epoch: 1253/2000... Training loss: 0.4382\n",
      "Epoch: 1253/2000... Training loss: 0.5421\n",
      "Epoch: 1253/2000... Training loss: 0.5692\n",
      "Epoch: 1253/2000... Training loss: 0.5951\n",
      "Epoch: 1253/2000... Training loss: 0.4398\n",
      "Epoch: 1253/2000... Training loss: 0.4225\n",
      "Epoch: 1253/2000... Training loss: 0.2992\n",
      "Epoch: 1253/2000... Training loss: 0.5649\n",
      "Epoch: 1253/2000... Training loss: 0.4918\n",
      "Epoch: 1253/2000... Training loss: 0.4597\n",
      "Epoch: 1253/2000... Training loss: 0.4657\n",
      "Epoch: 1253/2000... Training loss: 0.5318\n",
      "Epoch: 1253/2000... Training loss: 0.2943\n",
      "Epoch: 1253/2000... Training loss: 0.4335\n",
      "Epoch: 1253/2000... Training loss: 0.4702\n",
      "Epoch: 1253/2000... Training loss: 0.3820\n",
      "Epoch: 1253/2000... Training loss: 0.3155\n",
      "Epoch: 1254/2000... Training loss: 0.4421\n",
      "Epoch: 1254/2000... Training loss: 0.1806\n",
      "Epoch: 1254/2000... Training loss: 0.4752\n",
      "Epoch: 1254/2000... Training loss: 0.4910\n",
      "Epoch: 1254/2000... Training loss: 0.6083\n",
      "Epoch: 1254/2000... Training loss: 0.4507\n",
      "Epoch: 1254/2000... Training loss: 0.5321\n",
      "Epoch: 1254/2000... Training loss: 0.4741\n",
      "Epoch: 1254/2000... Training loss: 0.5195\n",
      "Epoch: 1254/2000... Training loss: 0.4630\n",
      "Epoch: 1254/2000... Training loss: 0.5378\n",
      "Epoch: 1254/2000... Training loss: 0.2889\n",
      "Epoch: 1254/2000... Training loss: 0.3676\n",
      "Epoch: 1254/2000... Training loss: 0.3656\n",
      "Epoch: 1254/2000... Training loss: 0.5247\n",
      "Epoch: 1254/2000... Training loss: 0.5369\n",
      "Epoch: 1254/2000... Training loss: 0.5384\n",
      "Epoch: 1254/2000... Training loss: 0.4277\n",
      "Epoch: 1254/2000... Training loss: 0.3155\n",
      "Epoch: 1254/2000... Training loss: 0.5083\n",
      "Epoch: 1254/2000... Training loss: 0.4485\n",
      "Epoch: 1254/2000... Training loss: 0.3214\n",
      "Epoch: 1254/2000... Training loss: 0.4539\n",
      "Epoch: 1254/2000... Training loss: 0.4676\n",
      "Epoch: 1254/2000... Training loss: 0.3245\n",
      "Epoch: 1254/2000... Training loss: 0.4553\n",
      "Epoch: 1254/2000... Training loss: 0.3997\n",
      "Epoch: 1254/2000... Training loss: 0.3787\n",
      "Epoch: 1254/2000... Training loss: 0.4674\n",
      "Epoch: 1254/2000... Training loss: 0.4996\n",
      "Epoch: 1254/2000... Training loss: 0.4569\n",
      "Epoch: 1255/2000... Training loss: 0.4920\n",
      "Epoch: 1255/2000... Training loss: 0.4049\n",
      "Epoch: 1255/2000... Training loss: 0.4933\n",
      "Epoch: 1255/2000... Training loss: 0.2359\n",
      "Epoch: 1255/2000... Training loss: 0.2933\n",
      "Epoch: 1255/2000... Training loss: 0.4228\n",
      "Epoch: 1255/2000... Training loss: 0.3564\n",
      "Epoch: 1255/2000... Training loss: 0.6474\n",
      "Epoch: 1255/2000... Training loss: 0.3611\n",
      "Epoch: 1255/2000... Training loss: 0.4200\n",
      "Epoch: 1255/2000... Training loss: 0.2993\n",
      "Epoch: 1255/2000... Training loss: 0.4058\n",
      "Epoch: 1255/2000... Training loss: 0.4064\n",
      "Epoch: 1255/2000... Training loss: 0.4388\n",
      "Epoch: 1255/2000... Training loss: 0.4139\n",
      "Epoch: 1255/2000... Training loss: 0.5811\n",
      "Epoch: 1255/2000... Training loss: 0.4946\n",
      "Epoch: 1255/2000... Training loss: 0.4786\n",
      "Epoch: 1255/2000... Training loss: 0.4391\n",
      "Epoch: 1255/2000... Training loss: 0.4275\n",
      "Epoch: 1255/2000... Training loss: 0.3185\n",
      "Epoch: 1255/2000... Training loss: 0.4791\n",
      "Epoch: 1255/2000... Training loss: 0.3737\n",
      "Epoch: 1255/2000... Training loss: 0.6422\n",
      "Epoch: 1255/2000... Training loss: 0.4933\n",
      "Epoch: 1255/2000... Training loss: 0.5840\n",
      "Epoch: 1255/2000... Training loss: 0.3355\n",
      "Epoch: 1255/2000... Training loss: 0.4284\n",
      "Epoch: 1255/2000... Training loss: 0.4403\n",
      "Epoch: 1255/2000... Training loss: 0.7018\n",
      "Epoch: 1255/2000... Training loss: 0.5592\n",
      "Epoch: 1256/2000... Training loss: 0.4273\n",
      "Epoch: 1256/2000... Training loss: 0.4555\n",
      "Epoch: 1256/2000... Training loss: 0.5819\n",
      "Epoch: 1256/2000... Training loss: 0.4251\n",
      "Epoch: 1256/2000... Training loss: 0.4572\n",
      "Epoch: 1256/2000... Training loss: 0.5388\n",
      "Epoch: 1256/2000... Training loss: 0.2066\n",
      "Epoch: 1256/2000... Training loss: 0.4607\n",
      "Epoch: 1256/2000... Training loss: 0.3061\n",
      "Epoch: 1256/2000... Training loss: 0.3501\n",
      "Epoch: 1256/2000... Training loss: 0.6388\n",
      "Epoch: 1256/2000... Training loss: 0.5307\n",
      "Epoch: 1256/2000... Training loss: 0.3312\n",
      "Epoch: 1256/2000... Training loss: 0.5650\n",
      "Epoch: 1256/2000... Training loss: 0.4408\n",
      "Epoch: 1256/2000... Training loss: 0.5886\n",
      "Epoch: 1256/2000... Training loss: 0.5442\n",
      "Epoch: 1256/2000... Training loss: 0.5764\n",
      "Epoch: 1256/2000... Training loss: 0.6000\n",
      "Epoch: 1256/2000... Training loss: 0.2259\n",
      "Epoch: 1256/2000... Training loss: 0.3945\n",
      "Epoch: 1256/2000... Training loss: 0.5487\n",
      "Epoch: 1256/2000... Training loss: 0.4304\n",
      "Epoch: 1256/2000... Training loss: 0.3721\n",
      "Epoch: 1256/2000... Training loss: 0.3563\n",
      "Epoch: 1256/2000... Training loss: 0.3710\n",
      "Epoch: 1256/2000... Training loss: 0.6152\n",
      "Epoch: 1256/2000... Training loss: 0.4762\n",
      "Epoch: 1256/2000... Training loss: 0.3485\n",
      "Epoch: 1256/2000... Training loss: 0.3641\n",
      "Epoch: 1256/2000... Training loss: 0.3586\n",
      "Epoch: 1257/2000... Training loss: 0.5181\n",
      "Epoch: 1257/2000... Training loss: 0.3620\n",
      "Epoch: 1257/2000... Training loss: 0.4282\n",
      "Epoch: 1257/2000... Training loss: 0.4317\n",
      "Epoch: 1257/2000... Training loss: 0.3360\n",
      "Epoch: 1257/2000... Training loss: 0.6829\n",
      "Epoch: 1257/2000... Training loss: 0.3807\n",
      "Epoch: 1257/2000... Training loss: 0.3072\n",
      "Epoch: 1257/2000... Training loss: 0.4462\n",
      "Epoch: 1257/2000... Training loss: 0.3507\n",
      "Epoch: 1257/2000... Training loss: 0.3823\n",
      "Epoch: 1257/2000... Training loss: 0.3324\n",
      "Epoch: 1257/2000... Training loss: 0.3595\n",
      "Epoch: 1257/2000... Training loss: 0.4373\n",
      "Epoch: 1257/2000... Training loss: 0.3946\n",
      "Epoch: 1257/2000... Training loss: 0.4865\n",
      "Epoch: 1257/2000... Training loss: 0.4705\n",
      "Epoch: 1257/2000... Training loss: 0.4662\n",
      "Epoch: 1257/2000... Training loss: 0.3244\n",
      "Epoch: 1257/2000... Training loss: 0.4617\n",
      "Epoch: 1257/2000... Training loss: 0.5269\n",
      "Epoch: 1257/2000... Training loss: 0.3212\n",
      "Epoch: 1257/2000... Training loss: 0.4725\n",
      "Epoch: 1257/2000... Training loss: 0.4866\n",
      "Epoch: 1257/2000... Training loss: 0.3591\n",
      "Epoch: 1257/2000... Training loss: 0.3790\n",
      "Epoch: 1257/2000... Training loss: 0.3466\n",
      "Epoch: 1257/2000... Training loss: 0.4770\n",
      "Epoch: 1257/2000... Training loss: 0.4288\n",
      "Epoch: 1257/2000... Training loss: 0.5460\n",
      "Epoch: 1257/2000... Training loss: 0.2856\n",
      "Epoch: 1258/2000... Training loss: 0.4870\n",
      "Epoch: 1258/2000... Training loss: 0.5889\n",
      "Epoch: 1258/2000... Training loss: 0.3151\n",
      "Epoch: 1258/2000... Training loss: 0.5536\n",
      "Epoch: 1258/2000... Training loss: 0.4506\n",
      "Epoch: 1258/2000... Training loss: 0.6786\n",
      "Epoch: 1258/2000... Training loss: 0.4685\n",
      "Epoch: 1258/2000... Training loss: 0.5151\n",
      "Epoch: 1258/2000... Training loss: 0.5414\n",
      "Epoch: 1258/2000... Training loss: 0.4777\n",
      "Epoch: 1258/2000... Training loss: 0.2735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1258/2000... Training loss: 0.5345\n",
      "Epoch: 1258/2000... Training loss: 0.4411\n",
      "Epoch: 1258/2000... Training loss: 0.3889\n",
      "Epoch: 1258/2000... Training loss: 0.6349\n",
      "Epoch: 1258/2000... Training loss: 0.3775\n",
      "Epoch: 1258/2000... Training loss: 0.5043\n",
      "Epoch: 1258/2000... Training loss: 0.3690\n",
      "Epoch: 1258/2000... Training loss: 0.6014\n",
      "Epoch: 1258/2000... Training loss: 0.6814\n",
      "Epoch: 1258/2000... Training loss: 0.3538\n",
      "Epoch: 1258/2000... Training loss: 0.5454\n",
      "Epoch: 1258/2000... Training loss: 0.6496\n",
      "Epoch: 1258/2000... Training loss: 0.4376\n",
      "Epoch: 1258/2000... Training loss: 0.4889\n",
      "Epoch: 1258/2000... Training loss: 0.3954\n",
      "Epoch: 1258/2000... Training loss: 0.4303\n",
      "Epoch: 1258/2000... Training loss: 0.5541\n",
      "Epoch: 1258/2000... Training loss: 0.4133\n",
      "Epoch: 1258/2000... Training loss: 0.3868\n",
      "Epoch: 1258/2000... Training loss: 0.5828\n",
      "Epoch: 1259/2000... Training loss: 0.6096\n",
      "Epoch: 1259/2000... Training loss: 0.4384\n",
      "Epoch: 1259/2000... Training loss: 0.5627\n",
      "Epoch: 1259/2000... Training loss: 0.5047\n",
      "Epoch: 1259/2000... Training loss: 0.3550\n",
      "Epoch: 1259/2000... Training loss: 0.3521\n",
      "Epoch: 1259/2000... Training loss: 0.4696\n",
      "Epoch: 1259/2000... Training loss: 0.3930\n",
      "Epoch: 1259/2000... Training loss: 0.5071\n",
      "Epoch: 1259/2000... Training loss: 0.5015\n",
      "Epoch: 1259/2000... Training loss: 0.3995\n",
      "Epoch: 1259/2000... Training loss: 0.4546\n",
      "Epoch: 1259/2000... Training loss: 0.6349\n",
      "Epoch: 1259/2000... Training loss: 0.4111\n",
      "Epoch: 1259/2000... Training loss: 0.3545\n",
      "Epoch: 1259/2000... Training loss: 0.4130\n",
      "Epoch: 1259/2000... Training loss: 0.1857\n",
      "Epoch: 1259/2000... Training loss: 0.3340\n",
      "Epoch: 1259/2000... Training loss: 0.4487\n",
      "Epoch: 1259/2000... Training loss: 0.5045\n",
      "Epoch: 1259/2000... Training loss: 0.4374\n",
      "Epoch: 1259/2000... Training loss: 0.4252\n",
      "Epoch: 1259/2000... Training loss: 0.4446\n",
      "Epoch: 1259/2000... Training loss: 0.5142\n",
      "Epoch: 1259/2000... Training loss: 0.6690\n",
      "Epoch: 1259/2000... Training loss: 0.4973\n",
      "Epoch: 1259/2000... Training loss: 0.6070\n",
      "Epoch: 1259/2000... Training loss: 0.4796\n",
      "Epoch: 1259/2000... Training loss: 0.4952\n",
      "Epoch: 1259/2000... Training loss: 0.3489\n",
      "Epoch: 1259/2000... Training loss: 0.3949\n",
      "Epoch: 1260/2000... Training loss: 0.4995\n",
      "Epoch: 1260/2000... Training loss: 0.3845\n",
      "Epoch: 1260/2000... Training loss: 0.4040\n",
      "Epoch: 1260/2000... Training loss: 0.4919\n",
      "Epoch: 1260/2000... Training loss: 0.4041\n",
      "Epoch: 1260/2000... Training loss: 0.5529\n",
      "Epoch: 1260/2000... Training loss: 0.6635\n",
      "Epoch: 1260/2000... Training loss: 0.4412\n",
      "Epoch: 1260/2000... Training loss: 0.3456\n",
      "Epoch: 1260/2000... Training loss: 0.4924\n",
      "Epoch: 1260/2000... Training loss: 0.4888\n",
      "Epoch: 1260/2000... Training loss: 0.4341\n",
      "Epoch: 1260/2000... Training loss: 0.3694\n",
      "Epoch: 1260/2000... Training loss: 0.4485\n",
      "Epoch: 1260/2000... Training loss: 0.4691\n",
      "Epoch: 1260/2000... Training loss: 0.4378\n",
      "Epoch: 1260/2000... Training loss: 0.4528\n",
      "Epoch: 1260/2000... Training loss: 0.5683\n",
      "Epoch: 1260/2000... Training loss: 0.3585\n",
      "Epoch: 1260/2000... Training loss: 0.4074\n",
      "Epoch: 1260/2000... Training loss: 0.5893\n",
      "Epoch: 1260/2000... Training loss: 0.5443\n",
      "Epoch: 1260/2000... Training loss: 0.3902\n",
      "Epoch: 1260/2000... Training loss: 0.5304\n",
      "Epoch: 1260/2000... Training loss: 0.5476\n",
      "Epoch: 1260/2000... Training loss: 0.4336\n",
      "Epoch: 1260/2000... Training loss: 0.6604\n",
      "Epoch: 1260/2000... Training loss: 0.4675\n",
      "Epoch: 1260/2000... Training loss: 0.4196\n",
      "Epoch: 1260/2000... Training loss: 0.4415\n",
      "Epoch: 1260/2000... Training loss: 0.2956\n",
      "Epoch: 1261/2000... Training loss: 0.3420\n",
      "Epoch: 1261/2000... Training loss: 0.3192\n",
      "Epoch: 1261/2000... Training loss: 0.4495\n",
      "Epoch: 1261/2000... Training loss: 0.4764\n",
      "Epoch: 1261/2000... Training loss: 0.4382\n",
      "Epoch: 1261/2000... Training loss: 0.4514\n",
      "Epoch: 1261/2000... Training loss: 0.3144\n",
      "Epoch: 1261/2000... Training loss: 0.3710\n",
      "Epoch: 1261/2000... Training loss: 0.5147\n",
      "Epoch: 1261/2000... Training loss: 0.3409\n",
      "Epoch: 1261/2000... Training loss: 0.5025\n",
      "Epoch: 1261/2000... Training loss: 0.4831\n",
      "Epoch: 1261/2000... Training loss: 0.4232\n",
      "Epoch: 1261/2000... Training loss: 0.6378\n",
      "Epoch: 1261/2000... Training loss: 0.4159\n",
      "Epoch: 1261/2000... Training loss: 0.4136\n",
      "Epoch: 1261/2000... Training loss: 0.4325\n",
      "Epoch: 1261/2000... Training loss: 0.4158\n",
      "Epoch: 1261/2000... Training loss: 0.4484\n",
      "Epoch: 1261/2000... Training loss: 0.4975\n",
      "Epoch: 1261/2000... Training loss: 0.3463\n",
      "Epoch: 1261/2000... Training loss: 0.4681\n",
      "Epoch: 1261/2000... Training loss: 0.4507\n",
      "Epoch: 1261/2000... Training loss: 0.3500\n",
      "Epoch: 1261/2000... Training loss: 0.4881\n",
      "Epoch: 1261/2000... Training loss: 0.7490\n",
      "Epoch: 1261/2000... Training loss: 0.5059\n",
      "Epoch: 1261/2000... Training loss: 0.6760\n",
      "Epoch: 1261/2000... Training loss: 0.4992\n",
      "Epoch: 1261/2000... Training loss: 0.4150\n",
      "Epoch: 1261/2000... Training loss: 0.4149\n",
      "Epoch: 1262/2000... Training loss: 0.3858\n",
      "Epoch: 1262/2000... Training loss: 0.2757\n",
      "Epoch: 1262/2000... Training loss: 0.5202\n",
      "Epoch: 1262/2000... Training loss: 0.4818\n",
      "Epoch: 1262/2000... Training loss: 0.3170\n",
      "Epoch: 1262/2000... Training loss: 0.3871\n",
      "Epoch: 1262/2000... Training loss: 0.5368\n",
      "Epoch: 1262/2000... Training loss: 0.4029\n",
      "Epoch: 1262/2000... Training loss: 0.3619\n",
      "Epoch: 1262/2000... Training loss: 0.5221\n",
      "Epoch: 1262/2000... Training loss: 0.4867\n",
      "Epoch: 1262/2000... Training loss: 0.3987\n",
      "Epoch: 1262/2000... Training loss: 0.5698\n",
      "Epoch: 1262/2000... Training loss: 0.4690\n",
      "Epoch: 1262/2000... Training loss: 0.4423\n",
      "Epoch: 1262/2000... Training loss: 0.4237\n",
      "Epoch: 1262/2000... Training loss: 0.3327\n",
      "Epoch: 1262/2000... Training loss: 0.3372\n",
      "Epoch: 1262/2000... Training loss: 0.5380\n",
      "Epoch: 1262/2000... Training loss: 0.3523\n",
      "Epoch: 1262/2000... Training loss: 0.3740\n",
      "Epoch: 1262/2000... Training loss: 0.5204\n",
      "Epoch: 1262/2000... Training loss: 0.4388\n",
      "Epoch: 1262/2000... Training loss: 0.3871\n",
      "Epoch: 1262/2000... Training loss: 0.3382\n",
      "Epoch: 1262/2000... Training loss: 0.3406\n",
      "Epoch: 1262/2000... Training loss: 0.4903\n",
      "Epoch: 1262/2000... Training loss: 0.5016\n",
      "Epoch: 1262/2000... Training loss: 0.4530\n",
      "Epoch: 1262/2000... Training loss: 0.4878\n",
      "Epoch: 1262/2000... Training loss: 0.6264\n",
      "Epoch: 1263/2000... Training loss: 0.4035\n",
      "Epoch: 1263/2000... Training loss: 0.3739\n",
      "Epoch: 1263/2000... Training loss: 0.4719\n",
      "Epoch: 1263/2000... Training loss: 0.5327\n",
      "Epoch: 1263/2000... Training loss: 0.5241\n",
      "Epoch: 1263/2000... Training loss: 0.5207\n",
      "Epoch: 1263/2000... Training loss: 0.3289\n",
      "Epoch: 1263/2000... Training loss: 0.3634\n",
      "Epoch: 1263/2000... Training loss: 0.4257\n",
      "Epoch: 1263/2000... Training loss: 0.4288\n",
      "Epoch: 1263/2000... Training loss: 0.3183\n",
      "Epoch: 1263/2000... Training loss: 0.6464\n",
      "Epoch: 1263/2000... Training loss: 0.5072\n",
      "Epoch: 1263/2000... Training loss: 0.2700\n",
      "Epoch: 1263/2000... Training loss: 0.3774\n",
      "Epoch: 1263/2000... Training loss: 0.2603\n",
      "Epoch: 1263/2000... Training loss: 0.5219\n",
      "Epoch: 1263/2000... Training loss: 0.4559\n",
      "Epoch: 1263/2000... Training loss: 0.4432\n",
      "Epoch: 1263/2000... Training loss: 0.4618\n",
      "Epoch: 1263/2000... Training loss: 0.4476\n",
      "Epoch: 1263/2000... Training loss: 0.4363\n",
      "Epoch: 1263/2000... Training loss: 0.4648\n",
      "Epoch: 1263/2000... Training loss: 0.5891\n",
      "Epoch: 1263/2000... Training loss: 0.5219\n",
      "Epoch: 1263/2000... Training loss: 0.5843\n",
      "Epoch: 1263/2000... Training loss: 0.5803\n",
      "Epoch: 1263/2000... Training loss: 0.3680\n",
      "Epoch: 1263/2000... Training loss: 0.4762\n",
      "Epoch: 1263/2000... Training loss: 0.6376\n",
      "Epoch: 1263/2000... Training loss: 0.3381\n",
      "Epoch: 1264/2000... Training loss: 0.5318\n",
      "Epoch: 1264/2000... Training loss: 0.5093\n",
      "Epoch: 1264/2000... Training loss: 0.3964\n",
      "Epoch: 1264/2000... Training loss: 0.4321\n",
      "Epoch: 1264/2000... Training loss: 0.3267\n",
      "Epoch: 1264/2000... Training loss: 0.4594\n",
      "Epoch: 1264/2000... Training loss: 0.3957\n",
      "Epoch: 1264/2000... Training loss: 0.4593\n",
      "Epoch: 1264/2000... Training loss: 0.4907\n",
      "Epoch: 1264/2000... Training loss: 0.4001\n",
      "Epoch: 1264/2000... Training loss: 0.4764\n",
      "Epoch: 1264/2000... Training loss: 0.5075\n",
      "Epoch: 1264/2000... Training loss: 0.6248\n",
      "Epoch: 1264/2000... Training loss: 0.3589\n",
      "Epoch: 1264/2000... Training loss: 0.4232\n",
      "Epoch: 1264/2000... Training loss: 0.5358\n",
      "Epoch: 1264/2000... Training loss: 0.5602\n",
      "Epoch: 1264/2000... Training loss: 0.3277\n",
      "Epoch: 1264/2000... Training loss: 0.5701\n",
      "Epoch: 1264/2000... Training loss: 0.4113\n",
      "Epoch: 1264/2000... Training loss: 0.4865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1264/2000... Training loss: 0.3800\n",
      "Epoch: 1264/2000... Training loss: 0.5062\n",
      "Epoch: 1264/2000... Training loss: 0.3302\n",
      "Epoch: 1264/2000... Training loss: 0.4973\n",
      "Epoch: 1264/2000... Training loss: 0.5692\n",
      "Epoch: 1264/2000... Training loss: 0.3487\n",
      "Epoch: 1264/2000... Training loss: 0.3779\n",
      "Epoch: 1264/2000... Training loss: 0.5353\n",
      "Epoch: 1264/2000... Training loss: 0.4666\n",
      "Epoch: 1264/2000... Training loss: 0.4935\n",
      "Epoch: 1265/2000... Training loss: 0.4288\n",
      "Epoch: 1265/2000... Training loss: 0.4374\n",
      "Epoch: 1265/2000... Training loss: 0.4113\n",
      "Epoch: 1265/2000... Training loss: 0.4147\n",
      "Epoch: 1265/2000... Training loss: 0.3931\n",
      "Epoch: 1265/2000... Training loss: 0.4695\n",
      "Epoch: 1265/2000... Training loss: 0.5150\n",
      "Epoch: 1265/2000... Training loss: 0.4374\n",
      "Epoch: 1265/2000... Training loss: 0.3237\n",
      "Epoch: 1265/2000... Training loss: 0.6299\n",
      "Epoch: 1265/2000... Training loss: 0.5385\n",
      "Epoch: 1265/2000... Training loss: 0.5335\n",
      "Epoch: 1265/2000... Training loss: 0.4975\n",
      "Epoch: 1265/2000... Training loss: 0.5638\n",
      "Epoch: 1265/2000... Training loss: 0.3766\n",
      "Epoch: 1265/2000... Training loss: 0.4464\n",
      "Epoch: 1265/2000... Training loss: 0.5156\n",
      "Epoch: 1265/2000... Training loss: 0.4676\n",
      "Epoch: 1265/2000... Training loss: 0.3785\n",
      "Epoch: 1265/2000... Training loss: 0.6528\n",
      "Epoch: 1265/2000... Training loss: 0.6659\n",
      "Epoch: 1265/2000... Training loss: 0.5991\n",
      "Epoch: 1265/2000... Training loss: 0.3394\n",
      "Epoch: 1265/2000... Training loss: 0.5708\n",
      "Epoch: 1265/2000... Training loss: 0.4982\n",
      "Epoch: 1265/2000... Training loss: 0.3715\n",
      "Epoch: 1265/2000... Training loss: 0.3904\n",
      "Epoch: 1265/2000... Training loss: 0.3522\n",
      "Epoch: 1265/2000... Training loss: 0.6498\n",
      "Epoch: 1265/2000... Training loss: 0.3944\n",
      "Epoch: 1265/2000... Training loss: 0.3490\n",
      "Epoch: 1266/2000... Training loss: 0.3611\n",
      "Epoch: 1266/2000... Training loss: 0.5260\n",
      "Epoch: 1266/2000... Training loss: 0.3782\n",
      "Epoch: 1266/2000... Training loss: 0.6589\n",
      "Epoch: 1266/2000... Training loss: 0.4472\n",
      "Epoch: 1266/2000... Training loss: 0.3816\n",
      "Epoch: 1266/2000... Training loss: 0.4689\n",
      "Epoch: 1266/2000... Training loss: 0.4439\n",
      "Epoch: 1266/2000... Training loss: 0.3592\n",
      "Epoch: 1266/2000... Training loss: 0.3933\n",
      "Epoch: 1266/2000... Training loss: 0.3507\n",
      "Epoch: 1266/2000... Training loss: 0.5464\n",
      "Epoch: 1266/2000... Training loss: 0.4320\n",
      "Epoch: 1266/2000... Training loss: 0.5076\n",
      "Epoch: 1266/2000... Training loss: 0.5450\n",
      "Epoch: 1266/2000... Training loss: 0.3631\n",
      "Epoch: 1266/2000... Training loss: 0.4497\n",
      "Epoch: 1266/2000... Training loss: 0.3447\n",
      "Epoch: 1266/2000... Training loss: 0.4643\n",
      "Epoch: 1266/2000... Training loss: 0.4578\n",
      "Epoch: 1266/2000... Training loss: 0.3216\n",
      "Epoch: 1266/2000... Training loss: 0.4087\n",
      "Epoch: 1266/2000... Training loss: 0.5473\n",
      "Epoch: 1266/2000... Training loss: 0.4490\n",
      "Epoch: 1266/2000... Training loss: 0.3675\n",
      "Epoch: 1266/2000... Training loss: 0.4231\n",
      "Epoch: 1266/2000... Training loss: 0.5661\n",
      "Epoch: 1266/2000... Training loss: 0.3098\n",
      "Epoch: 1266/2000... Training loss: 0.3322\n",
      "Epoch: 1266/2000... Training loss: 0.4236\n",
      "Epoch: 1266/2000... Training loss: 0.6384\n",
      "Epoch: 1267/2000... Training loss: 0.4472\n",
      "Epoch: 1267/2000... Training loss: 0.5035\n",
      "Epoch: 1267/2000... Training loss: 0.4155\n",
      "Epoch: 1267/2000... Training loss: 0.5127\n",
      "Epoch: 1267/2000... Training loss: 0.4214\n",
      "Epoch: 1267/2000... Training loss: 0.3863\n",
      "Epoch: 1267/2000... Training loss: 0.5106\n",
      "Epoch: 1267/2000... Training loss: 0.3158\n",
      "Epoch: 1267/2000... Training loss: 0.4355\n",
      "Epoch: 1267/2000... Training loss: 0.7537\n",
      "Epoch: 1267/2000... Training loss: 0.2605\n",
      "Epoch: 1267/2000... Training loss: 0.4236\n",
      "Epoch: 1267/2000... Training loss: 0.3651\n",
      "Epoch: 1267/2000... Training loss: 0.4495\n",
      "Epoch: 1267/2000... Training loss: 0.4484\n",
      "Epoch: 1267/2000... Training loss: 0.4626\n",
      "Epoch: 1267/2000... Training loss: 0.6492\n",
      "Epoch: 1267/2000... Training loss: 0.6614\n",
      "Epoch: 1267/2000... Training loss: 0.4575\n",
      "Epoch: 1267/2000... Training loss: 0.5189\n",
      "Epoch: 1267/2000... Training loss: 0.4627\n",
      "Epoch: 1267/2000... Training loss: 0.4037\n",
      "Epoch: 1267/2000... Training loss: 0.4979\n",
      "Epoch: 1267/2000... Training loss: 0.4425\n",
      "Epoch: 1267/2000... Training loss: 0.6030\n",
      "Epoch: 1267/2000... Training loss: 0.2365\n",
      "Epoch: 1267/2000... Training loss: 0.4236\n",
      "Epoch: 1267/2000... Training loss: 0.3191\n",
      "Epoch: 1267/2000... Training loss: 0.5296\n",
      "Epoch: 1267/2000... Training loss: 0.4743\n",
      "Epoch: 1267/2000... Training loss: 0.4219\n",
      "Epoch: 1268/2000... Training loss: 0.5132\n",
      "Epoch: 1268/2000... Training loss: 0.5502\n",
      "Epoch: 1268/2000... Training loss: 0.4025\n",
      "Epoch: 1268/2000... Training loss: 0.3319\n",
      "Epoch: 1268/2000... Training loss: 0.4622\n",
      "Epoch: 1268/2000... Training loss: 0.4241\n",
      "Epoch: 1268/2000... Training loss: 0.4881\n",
      "Epoch: 1268/2000... Training loss: 0.3995\n",
      "Epoch: 1268/2000... Training loss: 0.4949\n",
      "Epoch: 1268/2000... Training loss: 0.4208\n",
      "Epoch: 1268/2000... Training loss: 0.2988\n",
      "Epoch: 1268/2000... Training loss: 0.5076\n",
      "Epoch: 1268/2000... Training loss: 0.3049\n",
      "Epoch: 1268/2000... Training loss: 0.5499\n",
      "Epoch: 1268/2000... Training loss: 0.3183\n",
      "Epoch: 1268/2000... Training loss: 0.3450\n",
      "Epoch: 1268/2000... Training loss: 0.5682\n",
      "Epoch: 1268/2000... Training loss: 0.4734\n",
      "Epoch: 1268/2000... Training loss: 0.6628\n",
      "Epoch: 1268/2000... Training loss: 0.5442\n",
      "Epoch: 1268/2000... Training loss: 0.4179\n",
      "Epoch: 1268/2000... Training loss: 0.6082\n",
      "Epoch: 1268/2000... Training loss: 0.2771\n",
      "Epoch: 1268/2000... Training loss: 0.3470\n",
      "Epoch: 1268/2000... Training loss: 0.4324\n",
      "Epoch: 1268/2000... Training loss: 0.5181\n",
      "Epoch: 1268/2000... Training loss: 0.4254\n",
      "Epoch: 1268/2000... Training loss: 0.4805\n",
      "Epoch: 1268/2000... Training loss: 0.2402\n",
      "Epoch: 1268/2000... Training loss: 0.4695\n",
      "Epoch: 1268/2000... Training loss: 0.3215\n",
      "Epoch: 1269/2000... Training loss: 0.4109\n",
      "Epoch: 1269/2000... Training loss: 0.3631\n",
      "Epoch: 1269/2000... Training loss: 0.6013\n",
      "Epoch: 1269/2000... Training loss: 0.5545\n",
      "Epoch: 1269/2000... Training loss: 0.7111\n",
      "Epoch: 1269/2000... Training loss: 0.5605\n",
      "Epoch: 1269/2000... Training loss: 0.4601\n",
      "Epoch: 1269/2000... Training loss: 0.3665\n",
      "Epoch: 1269/2000... Training loss: 0.3665\n",
      "Epoch: 1269/2000... Training loss: 0.3822\n",
      "Epoch: 1269/2000... Training loss: 0.3807\n",
      "Epoch: 1269/2000... Training loss: 0.4088\n",
      "Epoch: 1269/2000... Training loss: 0.2568\n",
      "Epoch: 1269/2000... Training loss: 0.4565\n",
      "Epoch: 1269/2000... Training loss: 0.4648\n",
      "Epoch: 1269/2000... Training loss: 0.2781\n",
      "Epoch: 1269/2000... Training loss: 0.5439\n",
      "Epoch: 1269/2000... Training loss: 0.3513\n",
      "Epoch: 1269/2000... Training loss: 0.5759\n",
      "Epoch: 1269/2000... Training loss: 0.5395\n",
      "Epoch: 1269/2000... Training loss: 0.5919\n",
      "Epoch: 1269/2000... Training loss: 0.3020\n",
      "Epoch: 1269/2000... Training loss: 0.4164\n",
      "Epoch: 1269/2000... Training loss: 0.5959\n",
      "Epoch: 1269/2000... Training loss: 0.6077\n",
      "Epoch: 1269/2000... Training loss: 0.6304\n",
      "Epoch: 1269/2000... Training loss: 0.3334\n",
      "Epoch: 1269/2000... Training loss: 0.4980\n",
      "Epoch: 1269/2000... Training loss: 0.5492\n",
      "Epoch: 1269/2000... Training loss: 0.4820\n",
      "Epoch: 1269/2000... Training loss: 0.5149\n",
      "Epoch: 1270/2000... Training loss: 0.4371\n",
      "Epoch: 1270/2000... Training loss: 0.5297\n",
      "Epoch: 1270/2000... Training loss: 0.4413\n",
      "Epoch: 1270/2000... Training loss: 0.5030\n",
      "Epoch: 1270/2000... Training loss: 0.4439\n",
      "Epoch: 1270/2000... Training loss: 0.4520\n",
      "Epoch: 1270/2000... Training loss: 0.3395\n",
      "Epoch: 1270/2000... Training loss: 0.3046\n",
      "Epoch: 1270/2000... Training loss: 0.3335\n",
      "Epoch: 1270/2000... Training loss: 0.5068\n",
      "Epoch: 1270/2000... Training loss: 0.4216\n",
      "Epoch: 1270/2000... Training loss: 0.4972\n",
      "Epoch: 1270/2000... Training loss: 0.3821\n",
      "Epoch: 1270/2000... Training loss: 0.4694\n",
      "Epoch: 1270/2000... Training loss: 0.3367\n",
      "Epoch: 1270/2000... Training loss: 0.4179\n",
      "Epoch: 1270/2000... Training loss: 0.3934\n",
      "Epoch: 1270/2000... Training loss: 0.5080\n",
      "Epoch: 1270/2000... Training loss: 0.7574\n",
      "Epoch: 1270/2000... Training loss: 0.6297\n",
      "Epoch: 1270/2000... Training loss: 0.3873\n",
      "Epoch: 1270/2000... Training loss: 0.4373\n",
      "Epoch: 1270/2000... Training loss: 0.4615\n",
      "Epoch: 1270/2000... Training loss: 0.3675\n",
      "Epoch: 1270/2000... Training loss: 0.5063\n",
      "Epoch: 1270/2000... Training loss: 0.4045\n",
      "Epoch: 1270/2000... Training loss: 0.4920\n",
      "Epoch: 1270/2000... Training loss: 0.4946\n",
      "Epoch: 1270/2000... Training loss: 0.3932\n",
      "Epoch: 1270/2000... Training loss: 0.4579\n",
      "Epoch: 1270/2000... Training loss: 0.4512\n",
      "Epoch: 1271/2000... Training loss: 0.6193\n",
      "Epoch: 1271/2000... Training loss: 0.3963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1271/2000... Training loss: 0.4381\n",
      "Epoch: 1271/2000... Training loss: 0.4451\n",
      "Epoch: 1271/2000... Training loss: 0.5837\n",
      "Epoch: 1271/2000... Training loss: 0.3306\n",
      "Epoch: 1271/2000... Training loss: 0.2891\n",
      "Epoch: 1271/2000... Training loss: 0.5361\n",
      "Epoch: 1271/2000... Training loss: 0.4418\n",
      "Epoch: 1271/2000... Training loss: 0.2986\n",
      "Epoch: 1271/2000... Training loss: 0.4160\n",
      "Epoch: 1271/2000... Training loss: 0.3989\n",
      "Epoch: 1271/2000... Training loss: 0.3710\n",
      "Epoch: 1271/2000... Training loss: 0.3180\n",
      "Epoch: 1271/2000... Training loss: 0.3153\n",
      "Epoch: 1271/2000... Training loss: 0.3312\n",
      "Epoch: 1271/2000... Training loss: 0.5343\n",
      "Epoch: 1271/2000... Training loss: 0.5619\n",
      "Epoch: 1271/2000... Training loss: 0.5653\n",
      "Epoch: 1271/2000... Training loss: 0.5047\n",
      "Epoch: 1271/2000... Training loss: 0.5101\n",
      "Epoch: 1271/2000... Training loss: 0.3566\n",
      "Epoch: 1271/2000... Training loss: 0.4689\n",
      "Epoch: 1271/2000... Training loss: 0.4181\n",
      "Epoch: 1271/2000... Training loss: 0.7425\n",
      "Epoch: 1271/2000... Training loss: 0.3006\n",
      "Epoch: 1271/2000... Training loss: 0.4180\n",
      "Epoch: 1271/2000... Training loss: 0.4672\n",
      "Epoch: 1271/2000... Training loss: 0.5343\n",
      "Epoch: 1271/2000... Training loss: 0.3183\n",
      "Epoch: 1271/2000... Training loss: 0.3606\n",
      "Epoch: 1272/2000... Training loss: 0.4840\n",
      "Epoch: 1272/2000... Training loss: 0.5244\n",
      "Epoch: 1272/2000... Training loss: 0.5874\n",
      "Epoch: 1272/2000... Training loss: 0.5304\n",
      "Epoch: 1272/2000... Training loss: 0.2928\n",
      "Epoch: 1272/2000... Training loss: 0.3419\n",
      "Epoch: 1272/2000... Training loss: 0.4929\n",
      "Epoch: 1272/2000... Training loss: 0.3855\n",
      "Epoch: 1272/2000... Training loss: 0.3918\n",
      "Epoch: 1272/2000... Training loss: 0.4125\n",
      "Epoch: 1272/2000... Training loss: 0.6047\n",
      "Epoch: 1272/2000... Training loss: 0.5904\n",
      "Epoch: 1272/2000... Training loss: 0.4752\n",
      "Epoch: 1272/2000... Training loss: 0.5781\n",
      "Epoch: 1272/2000... Training loss: 0.4158\n",
      "Epoch: 1272/2000... Training loss: 0.4060\n",
      "Epoch: 1272/2000... Training loss: 0.5907\n",
      "Epoch: 1272/2000... Training loss: 0.4200\n",
      "Epoch: 1272/2000... Training loss: 0.3811\n",
      "Epoch: 1272/2000... Training loss: 0.4616\n",
      "Epoch: 1272/2000... Training loss: 0.3977\n",
      "Epoch: 1272/2000... Training loss: 0.3655\n",
      "Epoch: 1272/2000... Training loss: 0.4397\n",
      "Epoch: 1272/2000... Training loss: 0.4218\n",
      "Epoch: 1272/2000... Training loss: 0.3807\n",
      "Epoch: 1272/2000... Training loss: 0.4170\n",
      "Epoch: 1272/2000... Training loss: 0.5037\n",
      "Epoch: 1272/2000... Training loss: 0.4390\n",
      "Epoch: 1272/2000... Training loss: 0.4615\n",
      "Epoch: 1272/2000... Training loss: 0.7342\n",
      "Epoch: 1272/2000... Training loss: 0.5971\n",
      "Epoch: 1273/2000... Training loss: 0.5925\n",
      "Epoch: 1273/2000... Training loss: 0.5225\n",
      "Epoch: 1273/2000... Training loss: 0.4345\n",
      "Epoch: 1273/2000... Training loss: 0.5279\n",
      "Epoch: 1273/2000... Training loss: 0.5632\n",
      "Epoch: 1273/2000... Training loss: 0.4340\n",
      "Epoch: 1273/2000... Training loss: 0.4001\n",
      "Epoch: 1273/2000... Training loss: 0.4471\n",
      "Epoch: 1273/2000... Training loss: 0.5857\n",
      "Epoch: 1273/2000... Training loss: 0.4620\n",
      "Epoch: 1273/2000... Training loss: 0.4893\n",
      "Epoch: 1273/2000... Training loss: 0.5597\n",
      "Epoch: 1273/2000... Training loss: 0.3705\n",
      "Epoch: 1273/2000... Training loss: 0.3289\n",
      "Epoch: 1273/2000... Training loss: 0.4412\n",
      "Epoch: 1273/2000... Training loss: 0.2733\n",
      "Epoch: 1273/2000... Training loss: 0.2896\n",
      "Epoch: 1273/2000... Training loss: 0.3407\n",
      "Epoch: 1273/2000... Training loss: 0.5163\n",
      "Epoch: 1273/2000... Training loss: 0.3967\n",
      "Epoch: 1273/2000... Training loss: 0.3782\n",
      "Epoch: 1273/2000... Training loss: 0.5656\n",
      "Epoch: 1273/2000... Training loss: 0.4928\n",
      "Epoch: 1273/2000... Training loss: 0.4593\n",
      "Epoch: 1273/2000... Training loss: 0.4389\n",
      "Epoch: 1273/2000... Training loss: 0.3586\n",
      "Epoch: 1273/2000... Training loss: 0.5115\n",
      "Epoch: 1273/2000... Training loss: 0.5421\n",
      "Epoch: 1273/2000... Training loss: 0.6092\n",
      "Epoch: 1273/2000... Training loss: 0.4146\n",
      "Epoch: 1273/2000... Training loss: 0.5184\n",
      "Epoch: 1274/2000... Training loss: 0.4017\n",
      "Epoch: 1274/2000... Training loss: 0.2745\n",
      "Epoch: 1274/2000... Training loss: 0.4589\n",
      "Epoch: 1274/2000... Training loss: 0.4318\n",
      "Epoch: 1274/2000... Training loss: 0.5210\n",
      "Epoch: 1274/2000... Training loss: 0.5384\n",
      "Epoch: 1274/2000... Training loss: 0.4425\n",
      "Epoch: 1274/2000... Training loss: 0.4992\n",
      "Epoch: 1274/2000... Training loss: 0.4471\n",
      "Epoch: 1274/2000... Training loss: 0.4160\n",
      "Epoch: 1274/2000... Training loss: 0.4990\n",
      "Epoch: 1274/2000... Training loss: 0.4813\n",
      "Epoch: 1274/2000... Training loss: 0.3834\n",
      "Epoch: 1274/2000... Training loss: 0.6058\n",
      "Epoch: 1274/2000... Training loss: 0.3828\n",
      "Epoch: 1274/2000... Training loss: 0.5215\n",
      "Epoch: 1274/2000... Training loss: 0.4166\n",
      "Epoch: 1274/2000... Training loss: 0.4250\n",
      "Epoch: 1274/2000... Training loss: 0.5217\n",
      "Epoch: 1274/2000... Training loss: 0.4307\n",
      "Epoch: 1274/2000... Training loss: 0.3760\n",
      "Epoch: 1274/2000... Training loss: 0.4321\n",
      "Epoch: 1274/2000... Training loss: 0.4509\n",
      "Epoch: 1274/2000... Training loss: 0.2786\n",
      "Epoch: 1274/2000... Training loss: 0.4279\n",
      "Epoch: 1274/2000... Training loss: 0.5674\n",
      "Epoch: 1274/2000... Training loss: 0.3783\n",
      "Epoch: 1274/2000... Training loss: 0.4802\n",
      "Epoch: 1274/2000... Training loss: 0.4175\n",
      "Epoch: 1274/2000... Training loss: 0.4814\n",
      "Epoch: 1274/2000... Training loss: 0.4719\n",
      "Epoch: 1275/2000... Training loss: 0.4115\n",
      "Epoch: 1275/2000... Training loss: 0.4466\n",
      "Epoch: 1275/2000... Training loss: 0.4526\n",
      "Epoch: 1275/2000... Training loss: 0.4866\n",
      "Epoch: 1275/2000... Training loss: 0.4267\n",
      "Epoch: 1275/2000... Training loss: 0.4976\n",
      "Epoch: 1275/2000... Training loss: 0.2867\n",
      "Epoch: 1275/2000... Training loss: 0.4732\n",
      "Epoch: 1275/2000... Training loss: 0.2375\n",
      "Epoch: 1275/2000... Training loss: 0.5211\n",
      "Epoch: 1275/2000... Training loss: 0.4862\n",
      "Epoch: 1275/2000... Training loss: 0.4900\n",
      "Epoch: 1275/2000... Training loss: 0.3039\n",
      "Epoch: 1275/2000... Training loss: 0.2227\n",
      "Epoch: 1275/2000... Training loss: 0.5888\n",
      "Epoch: 1275/2000... Training loss: 0.5004\n",
      "Epoch: 1275/2000... Training loss: 0.4961\n",
      "Epoch: 1275/2000... Training loss: 0.4114\n",
      "Epoch: 1275/2000... Training loss: 0.3268\n",
      "Epoch: 1275/2000... Training loss: 0.3271\n",
      "Epoch: 1275/2000... Training loss: 0.4585\n",
      "Epoch: 1275/2000... Training loss: 0.2423\n",
      "Epoch: 1275/2000... Training loss: 0.2657\n",
      "Epoch: 1275/2000... Training loss: 0.3369\n",
      "Epoch: 1275/2000... Training loss: 0.4604\n",
      "Epoch: 1275/2000... Training loss: 0.2797\n",
      "Epoch: 1275/2000... Training loss: 0.4590\n",
      "Epoch: 1275/2000... Training loss: 0.5207\n",
      "Epoch: 1275/2000... Training loss: 0.5283\n",
      "Epoch: 1275/2000... Training loss: 0.6441\n",
      "Epoch: 1275/2000... Training loss: 0.4062\n",
      "Epoch: 1276/2000... Training loss: 0.2963\n",
      "Epoch: 1276/2000... Training loss: 0.5493\n",
      "Epoch: 1276/2000... Training loss: 0.5932\n",
      "Epoch: 1276/2000... Training loss: 0.4877\n",
      "Epoch: 1276/2000... Training loss: 0.3983\n",
      "Epoch: 1276/2000... Training loss: 0.4094\n",
      "Epoch: 1276/2000... Training loss: 0.3953\n",
      "Epoch: 1276/2000... Training loss: 0.3544\n",
      "Epoch: 1276/2000... Training loss: 0.5460\n",
      "Epoch: 1276/2000... Training loss: 0.3770\n",
      "Epoch: 1276/2000... Training loss: 0.3880\n",
      "Epoch: 1276/2000... Training loss: 0.5334\n",
      "Epoch: 1276/2000... Training loss: 0.3800\n",
      "Epoch: 1276/2000... Training loss: 0.4511\n",
      "Epoch: 1276/2000... Training loss: 0.4670\n",
      "Epoch: 1276/2000... Training loss: 0.3844\n",
      "Epoch: 1276/2000... Training loss: 0.3281\n",
      "Epoch: 1276/2000... Training loss: 0.5708\n",
      "Epoch: 1276/2000... Training loss: 0.5378\n",
      "Epoch: 1276/2000... Training loss: 0.4320\n",
      "Epoch: 1276/2000... Training loss: 0.5031\n",
      "Epoch: 1276/2000... Training loss: 0.4541\n",
      "Epoch: 1276/2000... Training loss: 0.6361\n",
      "Epoch: 1276/2000... Training loss: 0.4242\n",
      "Epoch: 1276/2000... Training loss: 0.5998\n",
      "Epoch: 1276/2000... Training loss: 0.3886\n",
      "Epoch: 1276/2000... Training loss: 0.4231\n",
      "Epoch: 1276/2000... Training loss: 0.3194\n",
      "Epoch: 1276/2000... Training loss: 0.5865\n",
      "Epoch: 1276/2000... Training loss: 0.5738\n",
      "Epoch: 1276/2000... Training loss: 0.4774\n",
      "Epoch: 1277/2000... Training loss: 0.4820\n",
      "Epoch: 1277/2000... Training loss: 0.3773\n",
      "Epoch: 1277/2000... Training loss: 0.5410\n",
      "Epoch: 1277/2000... Training loss: 0.4605\n",
      "Epoch: 1277/2000... Training loss: 0.2957\n",
      "Epoch: 1277/2000... Training loss: 0.4102\n",
      "Epoch: 1277/2000... Training loss: 0.3406\n",
      "Epoch: 1277/2000... Training loss: 0.5309\n",
      "Epoch: 1277/2000... Training loss: 0.5684\n",
      "Epoch: 1277/2000... Training loss: 0.4956\n",
      "Epoch: 1277/2000... Training loss: 0.5651\n",
      "Epoch: 1277/2000... Training loss: 0.3534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1277/2000... Training loss: 0.4718\n",
      "Epoch: 1277/2000... Training loss: 0.4963\n",
      "Epoch: 1277/2000... Training loss: 0.5175\n",
      "Epoch: 1277/2000... Training loss: 0.5008\n",
      "Epoch: 1277/2000... Training loss: 0.4689\n",
      "Epoch: 1277/2000... Training loss: 0.3155\n",
      "Epoch: 1277/2000... Training loss: 0.5160\n",
      "Epoch: 1277/2000... Training loss: 0.3635\n",
      "Epoch: 1277/2000... Training loss: 0.3450\n",
      "Epoch: 1277/2000... Training loss: 0.4889\n",
      "Epoch: 1277/2000... Training loss: 0.4484\n",
      "Epoch: 1277/2000... Training loss: 0.4303\n",
      "Epoch: 1277/2000... Training loss: 0.5025\n",
      "Epoch: 1277/2000... Training loss: 0.5132\n",
      "Epoch: 1277/2000... Training loss: 0.5695\n",
      "Epoch: 1277/2000... Training loss: 0.3789\n",
      "Epoch: 1277/2000... Training loss: 0.4874\n",
      "Epoch: 1277/2000... Training loss: 0.4217\n",
      "Epoch: 1277/2000... Training loss: 0.4258\n",
      "Epoch: 1278/2000... Training loss: 0.3738\n",
      "Epoch: 1278/2000... Training loss: 0.5113\n",
      "Epoch: 1278/2000... Training loss: 0.4892\n",
      "Epoch: 1278/2000... Training loss: 0.2960\n",
      "Epoch: 1278/2000... Training loss: 0.3778\n",
      "Epoch: 1278/2000... Training loss: 0.4648\n",
      "Epoch: 1278/2000... Training loss: 0.4564\n",
      "Epoch: 1278/2000... Training loss: 0.5371\n",
      "Epoch: 1278/2000... Training loss: 0.4435\n",
      "Epoch: 1278/2000... Training loss: 0.4070\n",
      "Epoch: 1278/2000... Training loss: 0.3754\n",
      "Epoch: 1278/2000... Training loss: 0.3545\n",
      "Epoch: 1278/2000... Training loss: 0.4531\n",
      "Epoch: 1278/2000... Training loss: 0.4397\n",
      "Epoch: 1278/2000... Training loss: 0.4530\n",
      "Epoch: 1278/2000... Training loss: 0.4178\n",
      "Epoch: 1278/2000... Training loss: 0.5046\n",
      "Epoch: 1278/2000... Training loss: 0.4706\n",
      "Epoch: 1278/2000... Training loss: 0.4751\n",
      "Epoch: 1278/2000... Training loss: 0.3862\n",
      "Epoch: 1278/2000... Training loss: 0.5692\n",
      "Epoch: 1278/2000... Training loss: 0.4649\n",
      "Epoch: 1278/2000... Training loss: 0.4226\n",
      "Epoch: 1278/2000... Training loss: 0.4614\n",
      "Epoch: 1278/2000... Training loss: 0.4911\n",
      "Epoch: 1278/2000... Training loss: 0.5309\n",
      "Epoch: 1278/2000... Training loss: 0.3290\n",
      "Epoch: 1278/2000... Training loss: 0.4736\n",
      "Epoch: 1278/2000... Training loss: 0.2529\n",
      "Epoch: 1278/2000... Training loss: 0.5169\n",
      "Epoch: 1278/2000... Training loss: 0.3024\n",
      "Epoch: 1279/2000... Training loss: 0.4243\n",
      "Epoch: 1279/2000... Training loss: 0.3603\n",
      "Epoch: 1279/2000... Training loss: 0.4598\n",
      "Epoch: 1279/2000... Training loss: 0.3774\n",
      "Epoch: 1279/2000... Training loss: 0.4346\n",
      "Epoch: 1279/2000... Training loss: 0.4553\n",
      "Epoch: 1279/2000... Training loss: 0.4872\n",
      "Epoch: 1279/2000... Training loss: 0.4376\n",
      "Epoch: 1279/2000... Training loss: 0.4385\n",
      "Epoch: 1279/2000... Training loss: 0.3581\n",
      "Epoch: 1279/2000... Training loss: 0.3378\n",
      "Epoch: 1279/2000... Training loss: 0.3569\n",
      "Epoch: 1279/2000... Training loss: 0.3569\n",
      "Epoch: 1279/2000... Training loss: 0.4514\n",
      "Epoch: 1279/2000... Training loss: 0.4316\n",
      "Epoch: 1279/2000... Training loss: 0.6393\n",
      "Epoch: 1279/2000... Training loss: 0.3600\n",
      "Epoch: 1279/2000... Training loss: 0.2601\n",
      "Epoch: 1279/2000... Training loss: 0.4367\n",
      "Epoch: 1279/2000... Training loss: 0.4382\n",
      "Epoch: 1279/2000... Training loss: 0.4862\n",
      "Epoch: 1279/2000... Training loss: 0.6163\n",
      "Epoch: 1279/2000... Training loss: 0.3187\n",
      "Epoch: 1279/2000... Training loss: 0.4618\n",
      "Epoch: 1279/2000... Training loss: 0.4155\n",
      "Epoch: 1279/2000... Training loss: 0.4766\n",
      "Epoch: 1279/2000... Training loss: 0.3406\n",
      "Epoch: 1279/2000... Training loss: 0.4065\n",
      "Epoch: 1279/2000... Training loss: 0.6148\n",
      "Epoch: 1279/2000... Training loss: 0.4719\n",
      "Epoch: 1279/2000... Training loss: 0.4482\n",
      "Epoch: 1280/2000... Training loss: 0.3889\n",
      "Epoch: 1280/2000... Training loss: 0.3463\n",
      "Epoch: 1280/2000... Training loss: 0.5125\n",
      "Epoch: 1280/2000... Training loss: 0.2925\n",
      "Epoch: 1280/2000... Training loss: 0.4289\n",
      "Epoch: 1280/2000... Training loss: 0.6010\n",
      "Epoch: 1280/2000... Training loss: 0.3759\n",
      "Epoch: 1280/2000... Training loss: 0.3374\n",
      "Epoch: 1280/2000... Training loss: 0.2957\n",
      "Epoch: 1280/2000... Training loss: 0.4704\n",
      "Epoch: 1280/2000... Training loss: 0.4836\n",
      "Epoch: 1280/2000... Training loss: 0.5326\n",
      "Epoch: 1280/2000... Training loss: 0.3066\n",
      "Epoch: 1280/2000... Training loss: 0.3372\n",
      "Epoch: 1280/2000... Training loss: 0.5705\n",
      "Epoch: 1280/2000... Training loss: 0.4981\n",
      "Epoch: 1280/2000... Training loss: 0.6396\n",
      "Epoch: 1280/2000... Training loss: 0.3553\n",
      "Epoch: 1280/2000... Training loss: 0.5109\n",
      "Epoch: 1280/2000... Training loss: 0.7287\n",
      "Epoch: 1280/2000... Training loss: 0.4095\n",
      "Epoch: 1280/2000... Training loss: 0.4670\n",
      "Epoch: 1280/2000... Training loss: 0.4866\n",
      "Epoch: 1280/2000... Training loss: 0.4273\n",
      "Epoch: 1280/2000... Training loss: 0.4528\n",
      "Epoch: 1280/2000... Training loss: 0.4364\n",
      "Epoch: 1280/2000... Training loss: 0.3026\n",
      "Epoch: 1280/2000... Training loss: 0.5571\n",
      "Epoch: 1280/2000... Training loss: 0.4933\n",
      "Epoch: 1280/2000... Training loss: 0.3279\n",
      "Epoch: 1280/2000... Training loss: 0.4288\n",
      "Epoch: 1281/2000... Training loss: 0.4683\n",
      "Epoch: 1281/2000... Training loss: 0.4310\n",
      "Epoch: 1281/2000... Training loss: 0.5260\n",
      "Epoch: 1281/2000... Training loss: 0.4378\n",
      "Epoch: 1281/2000... Training loss: 0.3362\n",
      "Epoch: 1281/2000... Training loss: 0.5332\n",
      "Epoch: 1281/2000... Training loss: 0.4638\n",
      "Epoch: 1281/2000... Training loss: 0.4043\n",
      "Epoch: 1281/2000... Training loss: 0.4086\n",
      "Epoch: 1281/2000... Training loss: 0.4469\n",
      "Epoch: 1281/2000... Training loss: 0.5871\n",
      "Epoch: 1281/2000... Training loss: 0.4236\n",
      "Epoch: 1281/2000... Training loss: 0.6019\n",
      "Epoch: 1281/2000... Training loss: 0.5997\n",
      "Epoch: 1281/2000... Training loss: 0.4537\n",
      "Epoch: 1281/2000... Training loss: 0.3083\n",
      "Epoch: 1281/2000... Training loss: 0.3590\n",
      "Epoch: 1281/2000... Training loss: 0.6114\n",
      "Epoch: 1281/2000... Training loss: 0.4839\n",
      "Epoch: 1281/2000... Training loss: 0.5798\n",
      "Epoch: 1281/2000... Training loss: 0.3610\n",
      "Epoch: 1281/2000... Training loss: 0.4772\n",
      "Epoch: 1281/2000... Training loss: 0.2897\n",
      "Epoch: 1281/2000... Training loss: 0.3966\n",
      "Epoch: 1281/2000... Training loss: 0.3517\n",
      "Epoch: 1281/2000... Training loss: 0.7899\n",
      "Epoch: 1281/2000... Training loss: 0.5551\n",
      "Epoch: 1281/2000... Training loss: 0.3987\n",
      "Epoch: 1281/2000... Training loss: 0.4026\n",
      "Epoch: 1281/2000... Training loss: 0.4370\n",
      "Epoch: 1281/2000... Training loss: 0.4413\n",
      "Epoch: 1282/2000... Training loss: 0.6125\n",
      "Epoch: 1282/2000... Training loss: 0.5339\n",
      "Epoch: 1282/2000... Training loss: 0.3646\n",
      "Epoch: 1282/2000... Training loss: 0.4380\n",
      "Epoch: 1282/2000... Training loss: 0.4994\n",
      "Epoch: 1282/2000... Training loss: 0.5815\n",
      "Epoch: 1282/2000... Training loss: 0.3179\n",
      "Epoch: 1282/2000... Training loss: 0.3581\n",
      "Epoch: 1282/2000... Training loss: 0.3336\n",
      "Epoch: 1282/2000... Training loss: 0.5145\n",
      "Epoch: 1282/2000... Training loss: 0.5938\n",
      "Epoch: 1282/2000... Training loss: 0.4191\n",
      "Epoch: 1282/2000... Training loss: 0.3947\n",
      "Epoch: 1282/2000... Training loss: 0.4505\n",
      "Epoch: 1282/2000... Training loss: 0.6189\n",
      "Epoch: 1282/2000... Training loss: 0.7215\n",
      "Epoch: 1282/2000... Training loss: 0.6716\n",
      "Epoch: 1282/2000... Training loss: 0.6043\n",
      "Epoch: 1282/2000... Training loss: 0.3795\n",
      "Epoch: 1282/2000... Training loss: 0.5460\n",
      "Epoch: 1282/2000... Training loss: 0.4104\n",
      "Epoch: 1282/2000... Training loss: 0.6053\n",
      "Epoch: 1282/2000... Training loss: 0.5476\n",
      "Epoch: 1282/2000... Training loss: 0.6815\n",
      "Epoch: 1282/2000... Training loss: 0.5988\n",
      "Epoch: 1282/2000... Training loss: 0.5240\n",
      "Epoch: 1282/2000... Training loss: 0.4101\n",
      "Epoch: 1282/2000... Training loss: 0.5332\n",
      "Epoch: 1282/2000... Training loss: 0.3491\n",
      "Epoch: 1282/2000... Training loss: 0.6117\n",
      "Epoch: 1282/2000... Training loss: 0.3974\n",
      "Epoch: 1283/2000... Training loss: 0.3756\n",
      "Epoch: 1283/2000... Training loss: 0.3041\n",
      "Epoch: 1283/2000... Training loss: 0.4125\n",
      "Epoch: 1283/2000... Training loss: 0.4075\n",
      "Epoch: 1283/2000... Training loss: 0.6066\n",
      "Epoch: 1283/2000... Training loss: 0.4310\n",
      "Epoch: 1283/2000... Training loss: 0.2840\n",
      "Epoch: 1283/2000... Training loss: 0.4621\n",
      "Epoch: 1283/2000... Training loss: 0.5665\n",
      "Epoch: 1283/2000... Training loss: 0.4141\n",
      "Epoch: 1283/2000... Training loss: 0.3672\n",
      "Epoch: 1283/2000... Training loss: 0.3189\n",
      "Epoch: 1283/2000... Training loss: 0.3994\n",
      "Epoch: 1283/2000... Training loss: 0.5070\n",
      "Epoch: 1283/2000... Training loss: 0.5111\n",
      "Epoch: 1283/2000... Training loss: 0.7061\n",
      "Epoch: 1283/2000... Training loss: 0.3097\n",
      "Epoch: 1283/2000... Training loss: 0.4012\n",
      "Epoch: 1283/2000... Training loss: 0.5182\n",
      "Epoch: 1283/2000... Training loss: 0.5957\n",
      "Epoch: 1283/2000... Training loss: 0.5248\n",
      "Epoch: 1283/2000... Training loss: 0.6238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1283/2000... Training loss: 0.3075\n",
      "Epoch: 1283/2000... Training loss: 0.3999\n",
      "Epoch: 1283/2000... Training loss: 0.2436\n",
      "Epoch: 1283/2000... Training loss: 0.2772\n",
      "Epoch: 1283/2000... Training loss: 0.2913\n",
      "Epoch: 1283/2000... Training loss: 0.4768\n",
      "Epoch: 1283/2000... Training loss: 0.5229\n",
      "Epoch: 1283/2000... Training loss: 0.5125\n",
      "Epoch: 1283/2000... Training loss: 0.4763\n",
      "Epoch: 1284/2000... Training loss: 0.4943\n",
      "Epoch: 1284/2000... Training loss: 0.3316\n",
      "Epoch: 1284/2000... Training loss: 0.3700\n",
      "Epoch: 1284/2000... Training loss: 0.4154\n",
      "Epoch: 1284/2000... Training loss: 0.4572\n",
      "Epoch: 1284/2000... Training loss: 0.6366\n",
      "Epoch: 1284/2000... Training loss: 0.4581\n",
      "Epoch: 1284/2000... Training loss: 0.3597\n",
      "Epoch: 1284/2000... Training loss: 0.3209\n",
      "Epoch: 1284/2000... Training loss: 0.6442\n",
      "Epoch: 1284/2000... Training loss: 0.3842\n",
      "Epoch: 1284/2000... Training loss: 0.5546\n",
      "Epoch: 1284/2000... Training loss: 0.3929\n",
      "Epoch: 1284/2000... Training loss: 0.3887\n",
      "Epoch: 1284/2000... Training loss: 0.4539\n",
      "Epoch: 1284/2000... Training loss: 0.3243\n",
      "Epoch: 1284/2000... Training loss: 0.4743\n",
      "Epoch: 1284/2000... Training loss: 0.4546\n",
      "Epoch: 1284/2000... Training loss: 0.5502\n",
      "Epoch: 1284/2000... Training loss: 0.4613\n",
      "Epoch: 1284/2000... Training loss: 0.4983\n",
      "Epoch: 1284/2000... Training loss: 0.3822\n",
      "Epoch: 1284/2000... Training loss: 0.4243\n",
      "Epoch: 1284/2000... Training loss: 0.5420\n",
      "Epoch: 1284/2000... Training loss: 0.4557\n",
      "Epoch: 1284/2000... Training loss: 0.3408\n",
      "Epoch: 1284/2000... Training loss: 0.4435\n",
      "Epoch: 1284/2000... Training loss: 0.4645\n",
      "Epoch: 1284/2000... Training loss: 0.4408\n",
      "Epoch: 1284/2000... Training loss: 0.3951\n",
      "Epoch: 1284/2000... Training loss: 0.3263\n",
      "Epoch: 1285/2000... Training loss: 0.5106\n",
      "Epoch: 1285/2000... Training loss: 0.3984\n",
      "Epoch: 1285/2000... Training loss: 0.5270\n",
      "Epoch: 1285/2000... Training loss: 0.3793\n",
      "Epoch: 1285/2000... Training loss: 0.4251\n",
      "Epoch: 1285/2000... Training loss: 0.5580\n",
      "Epoch: 1285/2000... Training loss: 0.3764\n",
      "Epoch: 1285/2000... Training loss: 0.3873\n",
      "Epoch: 1285/2000... Training loss: 0.5315\n",
      "Epoch: 1285/2000... Training loss: 0.5722\n",
      "Epoch: 1285/2000... Training loss: 0.5959\n",
      "Epoch: 1285/2000... Training loss: 0.4842\n",
      "Epoch: 1285/2000... Training loss: 0.2603\n",
      "Epoch: 1285/2000... Training loss: 0.4188\n",
      "Epoch: 1285/2000... Training loss: 0.3705\n",
      "Epoch: 1285/2000... Training loss: 0.3796\n",
      "Epoch: 1285/2000... Training loss: 0.5913\n",
      "Epoch: 1285/2000... Training loss: 0.3764\n",
      "Epoch: 1285/2000... Training loss: 0.4296\n",
      "Epoch: 1285/2000... Training loss: 0.3965\n",
      "Epoch: 1285/2000... Training loss: 0.7126\n",
      "Epoch: 1285/2000... Training loss: 0.3913\n",
      "Epoch: 1285/2000... Training loss: 0.5265\n",
      "Epoch: 1285/2000... Training loss: 0.3203\n",
      "Epoch: 1285/2000... Training loss: 0.4004\n",
      "Epoch: 1285/2000... Training loss: 0.5241\n",
      "Epoch: 1285/2000... Training loss: 0.3878\n",
      "Epoch: 1285/2000... Training loss: 0.3431\n",
      "Epoch: 1285/2000... Training loss: 0.5594\n",
      "Epoch: 1285/2000... Training loss: 0.6433\n",
      "Epoch: 1285/2000... Training loss: 0.2623\n",
      "Epoch: 1286/2000... Training loss: 0.2927\n",
      "Epoch: 1286/2000... Training loss: 0.2895\n",
      "Epoch: 1286/2000... Training loss: 0.4782\n",
      "Epoch: 1286/2000... Training loss: 0.5231\n",
      "Epoch: 1286/2000... Training loss: 0.4078\n",
      "Epoch: 1286/2000... Training loss: 0.3151\n",
      "Epoch: 1286/2000... Training loss: 0.4628\n",
      "Epoch: 1286/2000... Training loss: 0.4489\n",
      "Epoch: 1286/2000... Training loss: 0.3417\n",
      "Epoch: 1286/2000... Training loss: 0.5502\n",
      "Epoch: 1286/2000... Training loss: 0.4036\n",
      "Epoch: 1286/2000... Training loss: 0.3726\n",
      "Epoch: 1286/2000... Training loss: 0.4327\n",
      "Epoch: 1286/2000... Training loss: 0.3227\n",
      "Epoch: 1286/2000... Training loss: 0.4339\n",
      "Epoch: 1286/2000... Training loss: 0.4772\n",
      "Epoch: 1286/2000... Training loss: 0.5072\n",
      "Epoch: 1286/2000... Training loss: 0.3345\n",
      "Epoch: 1286/2000... Training loss: 0.4662\n",
      "Epoch: 1286/2000... Training loss: 0.5178\n",
      "Epoch: 1286/2000... Training loss: 0.5169\n",
      "Epoch: 1286/2000... Training loss: 0.3811\n",
      "Epoch: 1286/2000... Training loss: 0.3309\n",
      "Epoch: 1286/2000... Training loss: 0.4595\n",
      "Epoch: 1286/2000... Training loss: 0.4782\n",
      "Epoch: 1286/2000... Training loss: 0.4403\n",
      "Epoch: 1286/2000... Training loss: 0.6738\n",
      "Epoch: 1286/2000... Training loss: 0.4800\n",
      "Epoch: 1286/2000... Training loss: 0.2659\n",
      "Epoch: 1286/2000... Training loss: 0.3882\n",
      "Epoch: 1286/2000... Training loss: 0.3762\n",
      "Epoch: 1287/2000... Training loss: 0.5113\n",
      "Epoch: 1287/2000... Training loss: 0.4638\n",
      "Epoch: 1287/2000... Training loss: 0.4363\n",
      "Epoch: 1287/2000... Training loss: 0.4299\n",
      "Epoch: 1287/2000... Training loss: 0.6060\n",
      "Epoch: 1287/2000... Training loss: 0.4941\n",
      "Epoch: 1287/2000... Training loss: 0.4975\n",
      "Epoch: 1287/2000... Training loss: 0.4248\n",
      "Epoch: 1287/2000... Training loss: 0.6622\n",
      "Epoch: 1287/2000... Training loss: 0.4056\n",
      "Epoch: 1287/2000... Training loss: 0.5069\n",
      "Epoch: 1287/2000... Training loss: 0.4761\n",
      "Epoch: 1287/2000... Training loss: 0.5506\n",
      "Epoch: 1287/2000... Training loss: 0.3095\n",
      "Epoch: 1287/2000... Training loss: 0.5049\n",
      "Epoch: 1287/2000... Training loss: 0.3946\n",
      "Epoch: 1287/2000... Training loss: 0.4513\n",
      "Epoch: 1287/2000... Training loss: 0.4703\n",
      "Epoch: 1287/2000... Training loss: 0.4977\n",
      "Epoch: 1287/2000... Training loss: 0.3540\n",
      "Epoch: 1287/2000... Training loss: 0.4179\n",
      "Epoch: 1287/2000... Training loss: 0.4390\n",
      "Epoch: 1287/2000... Training loss: 0.3111\n",
      "Epoch: 1287/2000... Training loss: 0.4525\n",
      "Epoch: 1287/2000... Training loss: 0.5417\n",
      "Epoch: 1287/2000... Training loss: 0.4186\n",
      "Epoch: 1287/2000... Training loss: 0.6267\n",
      "Epoch: 1287/2000... Training loss: 0.3560\n",
      "Epoch: 1287/2000... Training loss: 0.5299\n",
      "Epoch: 1287/2000... Training loss: 0.4599\n",
      "Epoch: 1287/2000... Training loss: 0.5766\n",
      "Epoch: 1288/2000... Training loss: 0.5029\n",
      "Epoch: 1288/2000... Training loss: 0.6413\n",
      "Epoch: 1288/2000... Training loss: 0.3673\n",
      "Epoch: 1288/2000... Training loss: 0.3879\n",
      "Epoch: 1288/2000... Training loss: 0.4479\n",
      "Epoch: 1288/2000... Training loss: 0.6686\n",
      "Epoch: 1288/2000... Training loss: 0.3892\n",
      "Epoch: 1288/2000... Training loss: 0.4278\n",
      "Epoch: 1288/2000... Training loss: 0.6403\n",
      "Epoch: 1288/2000... Training loss: 0.5322\n",
      "Epoch: 1288/2000... Training loss: 0.3813\n",
      "Epoch: 1288/2000... Training loss: 0.3524\n",
      "Epoch: 1288/2000... Training loss: 0.4018\n",
      "Epoch: 1288/2000... Training loss: 0.3712\n",
      "Epoch: 1288/2000... Training loss: 0.3576\n",
      "Epoch: 1288/2000... Training loss: 0.4628\n",
      "Epoch: 1288/2000... Training loss: 0.4959\n",
      "Epoch: 1288/2000... Training loss: 0.2927\n",
      "Epoch: 1288/2000... Training loss: 0.4348\n",
      "Epoch: 1288/2000... Training loss: 0.4455\n",
      "Epoch: 1288/2000... Training loss: 0.5085\n",
      "Epoch: 1288/2000... Training loss: 0.3999\n",
      "Epoch: 1288/2000... Training loss: 0.6066\n",
      "Epoch: 1288/2000... Training loss: 0.5338\n",
      "Epoch: 1288/2000... Training loss: 0.2317\n",
      "Epoch: 1288/2000... Training loss: 0.3866\n",
      "Epoch: 1288/2000... Training loss: 0.3168\n",
      "Epoch: 1288/2000... Training loss: 0.4502\n",
      "Epoch: 1288/2000... Training loss: 0.6956\n",
      "Epoch: 1288/2000... Training loss: 0.2720\n",
      "Epoch: 1288/2000... Training loss: 0.4846\n",
      "Epoch: 1289/2000... Training loss: 0.4962\n",
      "Epoch: 1289/2000... Training loss: 0.4818\n",
      "Epoch: 1289/2000... Training loss: 0.6394\n",
      "Epoch: 1289/2000... Training loss: 0.4596\n",
      "Epoch: 1289/2000... Training loss: 0.3816\n",
      "Epoch: 1289/2000... Training loss: 0.4291\n",
      "Epoch: 1289/2000... Training loss: 0.3799\n",
      "Epoch: 1289/2000... Training loss: 0.7224\n",
      "Epoch: 1289/2000... Training loss: 0.4753\n",
      "Epoch: 1289/2000... Training loss: 0.3491\n",
      "Epoch: 1289/2000... Training loss: 0.6392\n",
      "Epoch: 1289/2000... Training loss: 0.4033\n",
      "Epoch: 1289/2000... Training loss: 0.5126\n",
      "Epoch: 1289/2000... Training loss: 0.3761\n",
      "Epoch: 1289/2000... Training loss: 0.3239\n",
      "Epoch: 1289/2000... Training loss: 0.3272\n",
      "Epoch: 1289/2000... Training loss: 0.3849\n",
      "Epoch: 1289/2000... Training loss: 0.4016\n",
      "Epoch: 1289/2000... Training loss: 0.4686\n",
      "Epoch: 1289/2000... Training loss: 0.3027\n",
      "Epoch: 1289/2000... Training loss: 0.3232\n",
      "Epoch: 1289/2000... Training loss: 0.4873\n",
      "Epoch: 1289/2000... Training loss: 0.4306\n",
      "Epoch: 1289/2000... Training loss: 0.4253\n",
      "Epoch: 1289/2000... Training loss: 0.3204\n",
      "Epoch: 1289/2000... Training loss: 0.4253\n",
      "Epoch: 1289/2000... Training loss: 0.3654\n",
      "Epoch: 1289/2000... Training loss: 0.4034\n",
      "Epoch: 1289/2000... Training loss: 0.2381\n",
      "Epoch: 1289/2000... Training loss: 0.5016\n",
      "Epoch: 1289/2000... Training loss: 0.2806\n",
      "Epoch: 1290/2000... Training loss: 0.3140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1290/2000... Training loss: 0.3883\n",
      "Epoch: 1290/2000... Training loss: 0.3343\n",
      "Epoch: 1290/2000... Training loss: 0.4385\n",
      "Epoch: 1290/2000... Training loss: 0.3984\n",
      "Epoch: 1290/2000... Training loss: 0.4666\n",
      "Epoch: 1290/2000... Training loss: 0.2479\n",
      "Epoch: 1290/2000... Training loss: 0.3224\n",
      "Epoch: 1290/2000... Training loss: 0.4982\n",
      "Epoch: 1290/2000... Training loss: 0.4500\n",
      "Epoch: 1290/2000... Training loss: 0.3410\n",
      "Epoch: 1290/2000... Training loss: 0.4592\n",
      "Epoch: 1290/2000... Training loss: 0.3890\n",
      "Epoch: 1290/2000... Training loss: 0.3776\n",
      "Epoch: 1290/2000... Training loss: 0.2805\n",
      "Epoch: 1290/2000... Training loss: 0.2506\n",
      "Epoch: 1290/2000... Training loss: 0.3463\n",
      "Epoch: 1290/2000... Training loss: 0.4244\n",
      "Epoch: 1290/2000... Training loss: 0.6198\n",
      "Epoch: 1290/2000... Training loss: 0.4349\n",
      "Epoch: 1290/2000... Training loss: 0.4696\n",
      "Epoch: 1290/2000... Training loss: 0.2933\n",
      "Epoch: 1290/2000... Training loss: 0.5083\n",
      "Epoch: 1290/2000... Training loss: 0.5620\n",
      "Epoch: 1290/2000... Training loss: 0.3113\n",
      "Epoch: 1290/2000... Training loss: 0.5651\n",
      "Epoch: 1290/2000... Training loss: 0.3610\n",
      "Epoch: 1290/2000... Training loss: 0.5429\n",
      "Epoch: 1290/2000... Training loss: 0.5941\n",
      "Epoch: 1290/2000... Training loss: 0.4142\n",
      "Epoch: 1290/2000... Training loss: 0.3972\n",
      "Epoch: 1291/2000... Training loss: 0.4804\n",
      "Epoch: 1291/2000... Training loss: 0.3988\n",
      "Epoch: 1291/2000... Training loss: 0.4837\n",
      "Epoch: 1291/2000... Training loss: 0.5252\n",
      "Epoch: 1291/2000... Training loss: 0.5046\n",
      "Epoch: 1291/2000... Training loss: 0.4006\n",
      "Epoch: 1291/2000... Training loss: 0.3189\n",
      "Epoch: 1291/2000... Training loss: 0.5301\n",
      "Epoch: 1291/2000... Training loss: 0.4524\n",
      "Epoch: 1291/2000... Training loss: 0.5329\n",
      "Epoch: 1291/2000... Training loss: 0.4724\n",
      "Epoch: 1291/2000... Training loss: 0.5441\n",
      "Epoch: 1291/2000... Training loss: 0.3396\n",
      "Epoch: 1291/2000... Training loss: 0.5340\n",
      "Epoch: 1291/2000... Training loss: 0.3231\n",
      "Epoch: 1291/2000... Training loss: 0.3007\n",
      "Epoch: 1291/2000... Training loss: 0.3566\n",
      "Epoch: 1291/2000... Training loss: 0.5904\n",
      "Epoch: 1291/2000... Training loss: 0.4938\n",
      "Epoch: 1291/2000... Training loss: 0.4471\n",
      "Epoch: 1291/2000... Training loss: 0.2329\n",
      "Epoch: 1291/2000... Training loss: 0.4574\n",
      "Epoch: 1291/2000... Training loss: 0.3352\n",
      "Epoch: 1291/2000... Training loss: 0.4197\n",
      "Epoch: 1291/2000... Training loss: 0.6766\n",
      "Epoch: 1291/2000... Training loss: 0.3328\n",
      "Epoch: 1291/2000... Training loss: 0.5115\n",
      "Epoch: 1291/2000... Training loss: 0.3932\n",
      "Epoch: 1291/2000... Training loss: 0.4808\n",
      "Epoch: 1291/2000... Training loss: 0.4274\n",
      "Epoch: 1291/2000... Training loss: 0.4057\n",
      "Epoch: 1292/2000... Training loss: 0.3703\n",
      "Epoch: 1292/2000... Training loss: 0.5565\n",
      "Epoch: 1292/2000... Training loss: 0.5569\n",
      "Epoch: 1292/2000... Training loss: 0.4601\n",
      "Epoch: 1292/2000... Training loss: 0.5245\n",
      "Epoch: 1292/2000... Training loss: 0.2980\n",
      "Epoch: 1292/2000... Training loss: 0.7762\n",
      "Epoch: 1292/2000... Training loss: 0.4552\n",
      "Epoch: 1292/2000... Training loss: 0.2704\n",
      "Epoch: 1292/2000... Training loss: 0.4323\n",
      "Epoch: 1292/2000... Training loss: 0.5708\n",
      "Epoch: 1292/2000... Training loss: 0.2528\n",
      "Epoch: 1292/2000... Training loss: 0.4314\n",
      "Epoch: 1292/2000... Training loss: 0.3198\n",
      "Epoch: 1292/2000... Training loss: 0.3467\n",
      "Epoch: 1292/2000... Training loss: 0.4750\n",
      "Epoch: 1292/2000... Training loss: 0.5399\n",
      "Epoch: 1292/2000... Training loss: 0.3975\n",
      "Epoch: 1292/2000... Training loss: 0.4494\n",
      "Epoch: 1292/2000... Training loss: 0.3823\n",
      "Epoch: 1292/2000... Training loss: 0.3653\n",
      "Epoch: 1292/2000... Training loss: 0.5534\n",
      "Epoch: 1292/2000... Training loss: 0.6361\n",
      "Epoch: 1292/2000... Training loss: 0.4455\n",
      "Epoch: 1292/2000... Training loss: 0.4640\n",
      "Epoch: 1292/2000... Training loss: 0.4123\n",
      "Epoch: 1292/2000... Training loss: 0.4512\n",
      "Epoch: 1292/2000... Training loss: 0.4981\n",
      "Epoch: 1292/2000... Training loss: 0.5607\n",
      "Epoch: 1292/2000... Training loss: 0.5153\n",
      "Epoch: 1292/2000... Training loss: 0.3426\n",
      "Epoch: 1293/2000... Training loss: 0.5529\n",
      "Epoch: 1293/2000... Training loss: 0.4491\n",
      "Epoch: 1293/2000... Training loss: 0.4106\n",
      "Epoch: 1293/2000... Training loss: 0.4238\n",
      "Epoch: 1293/2000... Training loss: 0.4940\n",
      "Epoch: 1293/2000... Training loss: 0.5261\n",
      "Epoch: 1293/2000... Training loss: 0.4305\n",
      "Epoch: 1293/2000... Training loss: 0.4005\n",
      "Epoch: 1293/2000... Training loss: 0.4380\n",
      "Epoch: 1293/2000... Training loss: 0.5129\n",
      "Epoch: 1293/2000... Training loss: 0.3200\n",
      "Epoch: 1293/2000... Training loss: 0.3111\n",
      "Epoch: 1293/2000... Training loss: 0.6184\n",
      "Epoch: 1293/2000... Training loss: 0.3649\n",
      "Epoch: 1293/2000... Training loss: 0.4881\n",
      "Epoch: 1293/2000... Training loss: 0.3139\n",
      "Epoch: 1293/2000... Training loss: 0.3944\n",
      "Epoch: 1293/2000... Training loss: 0.4414\n",
      "Epoch: 1293/2000... Training loss: 0.3352\n",
      "Epoch: 1293/2000... Training loss: 0.3632\n",
      "Epoch: 1293/2000... Training loss: 0.4706\n",
      "Epoch: 1293/2000... Training loss: 0.4580\n",
      "Epoch: 1293/2000... Training loss: 0.2439\n",
      "Epoch: 1293/2000... Training loss: 0.5507\n",
      "Epoch: 1293/2000... Training loss: 0.4229\n",
      "Epoch: 1293/2000... Training loss: 0.3747\n",
      "Epoch: 1293/2000... Training loss: 0.6427\n",
      "Epoch: 1293/2000... Training loss: 0.7453\n",
      "Epoch: 1293/2000... Training loss: 0.3977\n",
      "Epoch: 1293/2000... Training loss: 0.3667\n",
      "Epoch: 1293/2000... Training loss: 0.4690\n",
      "Epoch: 1294/2000... Training loss: 0.4285\n",
      "Epoch: 1294/2000... Training loss: 0.4242\n",
      "Epoch: 1294/2000... Training loss: 0.3583\n",
      "Epoch: 1294/2000... Training loss: 0.5665\n",
      "Epoch: 1294/2000... Training loss: 0.5100\n",
      "Epoch: 1294/2000... Training loss: 0.4069\n",
      "Epoch: 1294/2000... Training loss: 0.3943\n",
      "Epoch: 1294/2000... Training loss: 0.3608\n",
      "Epoch: 1294/2000... Training loss: 0.4329\n",
      "Epoch: 1294/2000... Training loss: 0.5265\n",
      "Epoch: 1294/2000... Training loss: 0.3767\n",
      "Epoch: 1294/2000... Training loss: 0.4064\n",
      "Epoch: 1294/2000... Training loss: 0.5762\n",
      "Epoch: 1294/2000... Training loss: 0.3349\n",
      "Epoch: 1294/2000... Training loss: 0.4876\n",
      "Epoch: 1294/2000... Training loss: 0.5584\n",
      "Epoch: 1294/2000... Training loss: 0.5206\n",
      "Epoch: 1294/2000... Training loss: 0.7134\n",
      "Epoch: 1294/2000... Training loss: 0.5040\n",
      "Epoch: 1294/2000... Training loss: 0.4466\n",
      "Epoch: 1294/2000... Training loss: 0.4097\n",
      "Epoch: 1294/2000... Training loss: 0.3482\n",
      "Epoch: 1294/2000... Training loss: 0.6031\n",
      "Epoch: 1294/2000... Training loss: 0.4611\n",
      "Epoch: 1294/2000... Training loss: 0.4074\n",
      "Epoch: 1294/2000... Training loss: 0.2698\n",
      "Epoch: 1294/2000... Training loss: 0.5741\n",
      "Epoch: 1294/2000... Training loss: 0.2806\n",
      "Epoch: 1294/2000... Training loss: 0.2893\n",
      "Epoch: 1294/2000... Training loss: 0.5127\n",
      "Epoch: 1294/2000... Training loss: 0.4770\n",
      "Epoch: 1295/2000... Training loss: 0.4791\n",
      "Epoch: 1295/2000... Training loss: 0.4121\n",
      "Epoch: 1295/2000... Training loss: 0.3728\n",
      "Epoch: 1295/2000... Training loss: 0.4623\n",
      "Epoch: 1295/2000... Training loss: 0.4224\n",
      "Epoch: 1295/2000... Training loss: 0.4960\n",
      "Epoch: 1295/2000... Training loss: 0.2974\n",
      "Epoch: 1295/2000... Training loss: 0.6552\n",
      "Epoch: 1295/2000... Training loss: 0.3846\n",
      "Epoch: 1295/2000... Training loss: 0.3719\n",
      "Epoch: 1295/2000... Training loss: 0.5730\n",
      "Epoch: 1295/2000... Training loss: 0.4833\n",
      "Epoch: 1295/2000... Training loss: 0.4550\n",
      "Epoch: 1295/2000... Training loss: 0.8312\n",
      "Epoch: 1295/2000... Training loss: 0.3151\n",
      "Epoch: 1295/2000... Training loss: 0.4141\n",
      "Epoch: 1295/2000... Training loss: 0.2702\n",
      "Epoch: 1295/2000... Training loss: 0.3181\n",
      "Epoch: 1295/2000... Training loss: 0.4423\n",
      "Epoch: 1295/2000... Training loss: 0.5310\n",
      "Epoch: 1295/2000... Training loss: 0.4314\n",
      "Epoch: 1295/2000... Training loss: 0.3703\n",
      "Epoch: 1295/2000... Training loss: 0.5004\n",
      "Epoch: 1295/2000... Training loss: 0.4348\n",
      "Epoch: 1295/2000... Training loss: 0.2262\n",
      "Epoch: 1295/2000... Training loss: 0.3734\n",
      "Epoch: 1295/2000... Training loss: 0.3425\n",
      "Epoch: 1295/2000... Training loss: 0.3395\n",
      "Epoch: 1295/2000... Training loss: 0.3880\n",
      "Epoch: 1295/2000... Training loss: 0.5559\n",
      "Epoch: 1295/2000... Training loss: 0.4011\n",
      "Epoch: 1296/2000... Training loss: 0.3346\n",
      "Epoch: 1296/2000... Training loss: 0.3593\n",
      "Epoch: 1296/2000... Training loss: 0.3109\n",
      "Epoch: 1296/2000... Training loss: 0.5618\n",
      "Epoch: 1296/2000... Training loss: 0.3831\n",
      "Epoch: 1296/2000... Training loss: 0.3908\n",
      "Epoch: 1296/2000... Training loss: 0.5056\n",
      "Epoch: 1296/2000... Training loss: 0.4603\n",
      "Epoch: 1296/2000... Training loss: 0.4805\n",
      "Epoch: 1296/2000... Training loss: 0.5614\n",
      "Epoch: 1296/2000... Training loss: 0.3992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1296/2000... Training loss: 0.2554\n",
      "Epoch: 1296/2000... Training loss: 0.3633\n",
      "Epoch: 1296/2000... Training loss: 0.3565\n",
      "Epoch: 1296/2000... Training loss: 0.2282\n",
      "Epoch: 1296/2000... Training loss: 0.5586\n",
      "Epoch: 1296/2000... Training loss: 0.3758\n",
      "Epoch: 1296/2000... Training loss: 0.4685\n",
      "Epoch: 1296/2000... Training loss: 0.3168\n",
      "Epoch: 1296/2000... Training loss: 0.4696\n",
      "Epoch: 1296/2000... Training loss: 0.4405\n",
      "Epoch: 1296/2000... Training loss: 0.4565\n",
      "Epoch: 1296/2000... Training loss: 0.4688\n",
      "Epoch: 1296/2000... Training loss: 0.3650\n",
      "Epoch: 1296/2000... Training loss: 0.5792\n",
      "Epoch: 1296/2000... Training loss: 0.5140\n",
      "Epoch: 1296/2000... Training loss: 0.3867\n",
      "Epoch: 1296/2000... Training loss: 0.5106\n",
      "Epoch: 1296/2000... Training loss: 0.2647\n",
      "Epoch: 1296/2000... Training loss: 0.5712\n",
      "Epoch: 1296/2000... Training loss: 0.3052\n",
      "Epoch: 1297/2000... Training loss: 0.3230\n",
      "Epoch: 1297/2000... Training loss: 0.3210\n",
      "Epoch: 1297/2000... Training loss: 0.2701\n",
      "Epoch: 1297/2000... Training loss: 0.2992\n",
      "Epoch: 1297/2000... Training loss: 0.4935\n",
      "Epoch: 1297/2000... Training loss: 0.4221\n",
      "Epoch: 1297/2000... Training loss: 0.5131\n",
      "Epoch: 1297/2000... Training loss: 0.3289\n",
      "Epoch: 1297/2000... Training loss: 0.3607\n",
      "Epoch: 1297/2000... Training loss: 0.4357\n",
      "Epoch: 1297/2000... Training loss: 0.4713\n",
      "Epoch: 1297/2000... Training loss: 0.4681\n",
      "Epoch: 1297/2000... Training loss: 0.3350\n",
      "Epoch: 1297/2000... Training loss: 0.3299\n",
      "Epoch: 1297/2000... Training loss: 0.4318\n",
      "Epoch: 1297/2000... Training loss: 0.4808\n",
      "Epoch: 1297/2000... Training loss: 0.5683\n",
      "Epoch: 1297/2000... Training loss: 0.3434\n",
      "Epoch: 1297/2000... Training loss: 0.5316\n",
      "Epoch: 1297/2000... Training loss: 0.4584\n",
      "Epoch: 1297/2000... Training loss: 0.3753\n",
      "Epoch: 1297/2000... Training loss: 0.3727\n",
      "Epoch: 1297/2000... Training loss: 0.3388\n",
      "Epoch: 1297/2000... Training loss: 0.3660\n",
      "Epoch: 1297/2000... Training loss: 0.3570\n",
      "Epoch: 1297/2000... Training loss: 0.4283\n",
      "Epoch: 1297/2000... Training loss: 0.4148\n",
      "Epoch: 1297/2000... Training loss: 0.3484\n",
      "Epoch: 1297/2000... Training loss: 0.4790\n",
      "Epoch: 1297/2000... Training loss: 0.4573\n",
      "Epoch: 1297/2000... Training loss: 0.3228\n",
      "Epoch: 1298/2000... Training loss: 0.4893\n",
      "Epoch: 1298/2000... Training loss: 0.5686\n",
      "Epoch: 1298/2000... Training loss: 0.4738\n",
      "Epoch: 1298/2000... Training loss: 0.4181\n",
      "Epoch: 1298/2000... Training loss: 0.3775\n",
      "Epoch: 1298/2000... Training loss: 0.3860\n",
      "Epoch: 1298/2000... Training loss: 0.5804\n",
      "Epoch: 1298/2000... Training loss: 0.4428\n",
      "Epoch: 1298/2000... Training loss: 0.5290\n",
      "Epoch: 1298/2000... Training loss: 0.3531\n",
      "Epoch: 1298/2000... Training loss: 0.4432\n",
      "Epoch: 1298/2000... Training loss: 0.5184\n",
      "Epoch: 1298/2000... Training loss: 0.4717\n",
      "Epoch: 1298/2000... Training loss: 0.3420\n",
      "Epoch: 1298/2000... Training loss: 0.5464\n",
      "Epoch: 1298/2000... Training loss: 0.3478\n",
      "Epoch: 1298/2000... Training loss: 0.4156\n",
      "Epoch: 1298/2000... Training loss: 0.3219\n",
      "Epoch: 1298/2000... Training loss: 0.2531\n",
      "Epoch: 1298/2000... Training loss: 0.5264\n",
      "Epoch: 1298/2000... Training loss: 0.4928\n",
      "Epoch: 1298/2000... Training loss: 0.5415\n",
      "Epoch: 1298/2000... Training loss: 0.4490\n",
      "Epoch: 1298/2000... Training loss: 0.5252\n",
      "Epoch: 1298/2000... Training loss: 0.3883\n",
      "Epoch: 1298/2000... Training loss: 0.5303\n",
      "Epoch: 1298/2000... Training loss: 0.4552\n",
      "Epoch: 1298/2000... Training loss: 0.4576\n",
      "Epoch: 1298/2000... Training loss: 0.2579\n",
      "Epoch: 1298/2000... Training loss: 0.5345\n",
      "Epoch: 1298/2000... Training loss: 0.5709\n",
      "Epoch: 1299/2000... Training loss: 0.4076\n",
      "Epoch: 1299/2000... Training loss: 0.4337\n",
      "Epoch: 1299/2000... Training loss: 0.3337\n",
      "Epoch: 1299/2000... Training loss: 0.3326\n",
      "Epoch: 1299/2000... Training loss: 0.4539\n",
      "Epoch: 1299/2000... Training loss: 0.5188\n",
      "Epoch: 1299/2000... Training loss: 0.4155\n",
      "Epoch: 1299/2000... Training loss: 0.3757\n",
      "Epoch: 1299/2000... Training loss: 0.3477\n",
      "Epoch: 1299/2000... Training loss: 0.6085\n",
      "Epoch: 1299/2000... Training loss: 0.5891\n",
      "Epoch: 1299/2000... Training loss: 0.5283\n",
      "Epoch: 1299/2000... Training loss: 0.5224\n",
      "Epoch: 1299/2000... Training loss: 0.5262\n",
      "Epoch: 1299/2000... Training loss: 0.3381\n",
      "Epoch: 1299/2000... Training loss: 0.2755\n",
      "Epoch: 1299/2000... Training loss: 0.3917\n",
      "Epoch: 1299/2000... Training loss: 0.3677\n",
      "Epoch: 1299/2000... Training loss: 0.4363\n",
      "Epoch: 1299/2000... Training loss: 0.4463\n",
      "Epoch: 1299/2000... Training loss: 0.4273\n",
      "Epoch: 1299/2000... Training loss: 0.4605\n",
      "Epoch: 1299/2000... Training loss: 0.2877\n",
      "Epoch: 1299/2000... Training loss: 0.4134\n",
      "Epoch: 1299/2000... Training loss: 0.3422\n",
      "Epoch: 1299/2000... Training loss: 0.3920\n",
      "Epoch: 1299/2000... Training loss: 0.3176\n",
      "Epoch: 1299/2000... Training loss: 0.5193\n",
      "Epoch: 1299/2000... Training loss: 0.5150\n",
      "Epoch: 1299/2000... Training loss: 0.4076\n",
      "Epoch: 1299/2000... Training loss: 0.4188\n",
      "Epoch: 1300/2000... Training loss: 0.4041\n",
      "Epoch: 1300/2000... Training loss: 0.6436\n",
      "Epoch: 1300/2000... Training loss: 0.4020\n",
      "Epoch: 1300/2000... Training loss: 0.3663\n",
      "Epoch: 1300/2000... Training loss: 0.4830\n",
      "Epoch: 1300/2000... Training loss: 0.3345\n",
      "Epoch: 1300/2000... Training loss: 0.2635\n",
      "Epoch: 1300/2000... Training loss: 0.3774\n",
      "Epoch: 1300/2000... Training loss: 0.5269\n",
      "Epoch: 1300/2000... Training loss: 0.5327\n",
      "Epoch: 1300/2000... Training loss: 0.3402\n",
      "Epoch: 1300/2000... Training loss: 0.5118\n",
      "Epoch: 1300/2000... Training loss: 0.5268\n",
      "Epoch: 1300/2000... Training loss: 0.4083\n",
      "Epoch: 1300/2000... Training loss: 0.3098\n",
      "Epoch: 1300/2000... Training loss: 0.4421\n",
      "Epoch: 1300/2000... Training loss: 0.3277\n",
      "Epoch: 1300/2000... Training loss: 0.5149\n",
      "Epoch: 1300/2000... Training loss: 0.4278\n",
      "Epoch: 1300/2000... Training loss: 0.4906\n",
      "Epoch: 1300/2000... Training loss: 0.5986\n",
      "Epoch: 1300/2000... Training loss: 0.5166\n",
      "Epoch: 1300/2000... Training loss: 0.3148\n",
      "Epoch: 1300/2000... Training loss: 0.4757\n",
      "Epoch: 1300/2000... Training loss: 0.4322\n",
      "Epoch: 1300/2000... Training loss: 0.4644\n",
      "Epoch: 1300/2000... Training loss: 0.3951\n",
      "Epoch: 1300/2000... Training loss: 0.4852\n",
      "Epoch: 1300/2000... Training loss: 0.4863\n",
      "Epoch: 1300/2000... Training loss: 0.2966\n",
      "Epoch: 1300/2000... Training loss: 0.5114\n",
      "Epoch: 1301/2000... Training loss: 0.5428\n",
      "Epoch: 1301/2000... Training loss: 0.2617\n",
      "Epoch: 1301/2000... Training loss: 0.4614\n",
      "Epoch: 1301/2000... Training loss: 0.4138\n",
      "Epoch: 1301/2000... Training loss: 0.4662\n",
      "Epoch: 1301/2000... Training loss: 0.4314\n",
      "Epoch: 1301/2000... Training loss: 0.3578\n",
      "Epoch: 1301/2000... Training loss: 0.5685\n",
      "Epoch: 1301/2000... Training loss: 0.7948\n",
      "Epoch: 1301/2000... Training loss: 0.4549\n",
      "Epoch: 1301/2000... Training loss: 0.4672\n",
      "Epoch: 1301/2000... Training loss: 0.4026\n",
      "Epoch: 1301/2000... Training loss: 0.5228\n",
      "Epoch: 1301/2000... Training loss: 0.4825\n",
      "Epoch: 1301/2000... Training loss: 0.4327\n",
      "Epoch: 1301/2000... Training loss: 0.4503\n",
      "Epoch: 1301/2000... Training loss: 0.3344\n",
      "Epoch: 1301/2000... Training loss: 0.4139\n",
      "Epoch: 1301/2000... Training loss: 0.4973\n",
      "Epoch: 1301/2000... Training loss: 0.4635\n",
      "Epoch: 1301/2000... Training loss: 0.3323\n",
      "Epoch: 1301/2000... Training loss: 0.5000\n",
      "Epoch: 1301/2000... Training loss: 0.3629\n",
      "Epoch: 1301/2000... Training loss: 0.4579\n",
      "Epoch: 1301/2000... Training loss: 0.5045\n",
      "Epoch: 1301/2000... Training loss: 0.3594\n",
      "Epoch: 1301/2000... Training loss: 0.4713\n",
      "Epoch: 1301/2000... Training loss: 0.5061\n",
      "Epoch: 1301/2000... Training loss: 0.4586\n",
      "Epoch: 1301/2000... Training loss: 0.4803\n",
      "Epoch: 1301/2000... Training loss: 0.3689\n",
      "Epoch: 1302/2000... Training loss: 0.4976\n",
      "Epoch: 1302/2000... Training loss: 0.4185\n",
      "Epoch: 1302/2000... Training loss: 0.3947\n",
      "Epoch: 1302/2000... Training loss: 0.3482\n",
      "Epoch: 1302/2000... Training loss: 0.3980\n",
      "Epoch: 1302/2000... Training loss: 0.4641\n",
      "Epoch: 1302/2000... Training loss: 0.3776\n",
      "Epoch: 1302/2000... Training loss: 0.2879\n",
      "Epoch: 1302/2000... Training loss: 0.3691\n",
      "Epoch: 1302/2000... Training loss: 0.4643\n",
      "Epoch: 1302/2000... Training loss: 0.4336\n",
      "Epoch: 1302/2000... Training loss: 0.4618\n",
      "Epoch: 1302/2000... Training loss: 0.4483\n",
      "Epoch: 1302/2000... Training loss: 0.3667\n",
      "Epoch: 1302/2000... Training loss: 0.5763\n",
      "Epoch: 1302/2000... Training loss: 0.3809\n",
      "Epoch: 1302/2000... Training loss: 0.3367\n",
      "Epoch: 1302/2000... Training loss: 0.4166\n",
      "Epoch: 1302/2000... Training loss: 0.6504\n",
      "Epoch: 1302/2000... Training loss: 0.4650\n",
      "Epoch: 1302/2000... Training loss: 0.3704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1302/2000... Training loss: 0.4246\n",
      "Epoch: 1302/2000... Training loss: 0.5157\n",
      "Epoch: 1302/2000... Training loss: 0.3513\n",
      "Epoch: 1302/2000... Training loss: 0.3212\n",
      "Epoch: 1302/2000... Training loss: 0.4066\n",
      "Epoch: 1302/2000... Training loss: 0.5532\n",
      "Epoch: 1302/2000... Training loss: 0.3634\n",
      "Epoch: 1302/2000... Training loss: 0.4293\n",
      "Epoch: 1302/2000... Training loss: 0.5975\n",
      "Epoch: 1302/2000... Training loss: 0.4311\n",
      "Epoch: 1303/2000... Training loss: 0.5431\n",
      "Epoch: 1303/2000... Training loss: 0.3741\n",
      "Epoch: 1303/2000... Training loss: 0.4408\n",
      "Epoch: 1303/2000... Training loss: 0.3658\n",
      "Epoch: 1303/2000... Training loss: 0.5094\n",
      "Epoch: 1303/2000... Training loss: 0.4835\n",
      "Epoch: 1303/2000... Training loss: 0.3846\n",
      "Epoch: 1303/2000... Training loss: 0.3689\n",
      "Epoch: 1303/2000... Training loss: 0.3998\n",
      "Epoch: 1303/2000... Training loss: 0.4377\n",
      "Epoch: 1303/2000... Training loss: 0.3280\n",
      "Epoch: 1303/2000... Training loss: 0.3811\n",
      "Epoch: 1303/2000... Training loss: 0.3570\n",
      "Epoch: 1303/2000... Training loss: 0.3602\n",
      "Epoch: 1303/2000... Training loss: 0.4443\n",
      "Epoch: 1303/2000... Training loss: 0.3777\n",
      "Epoch: 1303/2000... Training loss: 0.4257\n",
      "Epoch: 1303/2000... Training loss: 0.3245\n",
      "Epoch: 1303/2000... Training loss: 0.4635\n",
      "Epoch: 1303/2000... Training loss: 0.2809\n",
      "Epoch: 1303/2000... Training loss: 0.5039\n",
      "Epoch: 1303/2000... Training loss: 0.3784\n",
      "Epoch: 1303/2000... Training loss: 0.4573\n",
      "Epoch: 1303/2000... Training loss: 0.4205\n",
      "Epoch: 1303/2000... Training loss: 0.7387\n",
      "Epoch: 1303/2000... Training loss: 0.4415\n",
      "Epoch: 1303/2000... Training loss: 0.4512\n",
      "Epoch: 1303/2000... Training loss: 0.4195\n",
      "Epoch: 1303/2000... Training loss: 0.4875\n",
      "Epoch: 1303/2000... Training loss: 0.3383\n",
      "Epoch: 1303/2000... Training loss: 0.3820\n",
      "Epoch: 1304/2000... Training loss: 0.5055\n",
      "Epoch: 1304/2000... Training loss: 0.4756\n",
      "Epoch: 1304/2000... Training loss: 0.4621\n",
      "Epoch: 1304/2000... Training loss: 0.3922\n",
      "Epoch: 1304/2000... Training loss: 0.5545\n",
      "Epoch: 1304/2000... Training loss: 0.3905\n",
      "Epoch: 1304/2000... Training loss: 0.3672\n",
      "Epoch: 1304/2000... Training loss: 0.3898\n",
      "Epoch: 1304/2000... Training loss: 0.3575\n",
      "Epoch: 1304/2000... Training loss: 0.4435\n",
      "Epoch: 1304/2000... Training loss: 0.5280\n",
      "Epoch: 1304/2000... Training loss: 0.5736\n",
      "Epoch: 1304/2000... Training loss: 0.3963\n",
      "Epoch: 1304/2000... Training loss: 0.3574\n",
      "Epoch: 1304/2000... Training loss: 0.4813\n",
      "Epoch: 1304/2000... Training loss: 0.3448\n",
      "Epoch: 1304/2000... Training loss: 0.4016\n",
      "Epoch: 1304/2000... Training loss: 0.6354\n",
      "Epoch: 1304/2000... Training loss: 0.3668\n",
      "Epoch: 1304/2000... Training loss: 0.3977\n",
      "Epoch: 1304/2000... Training loss: 0.3474\n",
      "Epoch: 1304/2000... Training loss: 0.4473\n",
      "Epoch: 1304/2000... Training loss: 0.4874\n",
      "Epoch: 1304/2000... Training loss: 0.4898\n",
      "Epoch: 1304/2000... Training loss: 0.4728\n",
      "Epoch: 1304/2000... Training loss: 0.5234\n",
      "Epoch: 1304/2000... Training loss: 0.4253\n",
      "Epoch: 1304/2000... Training loss: 0.4275\n",
      "Epoch: 1304/2000... Training loss: 0.4535\n",
      "Epoch: 1304/2000... Training loss: 0.4407\n",
      "Epoch: 1304/2000... Training loss: 0.5027\n",
      "Epoch: 1305/2000... Training loss: 0.2810\n",
      "Epoch: 1305/2000... Training loss: 0.4367\n",
      "Epoch: 1305/2000... Training loss: 0.3013\n",
      "Epoch: 1305/2000... Training loss: 0.4728\n",
      "Epoch: 1305/2000... Training loss: 0.3763\n",
      "Epoch: 1305/2000... Training loss: 0.5160\n",
      "Epoch: 1305/2000... Training loss: 0.4110\n",
      "Epoch: 1305/2000... Training loss: 0.6137\n",
      "Epoch: 1305/2000... Training loss: 0.3594\n",
      "Epoch: 1305/2000... Training loss: 0.6103\n",
      "Epoch: 1305/2000... Training loss: 0.4858\n",
      "Epoch: 1305/2000... Training loss: 0.4760\n",
      "Epoch: 1305/2000... Training loss: 0.5250\n",
      "Epoch: 1305/2000... Training loss: 0.5660\n",
      "Epoch: 1305/2000... Training loss: 0.4012\n",
      "Epoch: 1305/2000... Training loss: 0.4214\n",
      "Epoch: 1305/2000... Training loss: 0.3093\n",
      "Epoch: 1305/2000... Training loss: 0.3389\n",
      "Epoch: 1305/2000... Training loss: 0.5383\n",
      "Epoch: 1305/2000... Training loss: 0.4616\n",
      "Epoch: 1305/2000... Training loss: 0.4514\n",
      "Epoch: 1305/2000... Training loss: 0.4660\n",
      "Epoch: 1305/2000... Training loss: 0.3741\n",
      "Epoch: 1305/2000... Training loss: 0.4698\n",
      "Epoch: 1305/2000... Training loss: 0.3457\n",
      "Epoch: 1305/2000... Training loss: 0.4754\n",
      "Epoch: 1305/2000... Training loss: 0.4473\n",
      "Epoch: 1305/2000... Training loss: 0.6217\n",
      "Epoch: 1305/2000... Training loss: 0.4524\n",
      "Epoch: 1305/2000... Training loss: 0.4030\n",
      "Epoch: 1305/2000... Training loss: 0.4629\n",
      "Epoch: 1306/2000... Training loss: 0.3958\n",
      "Epoch: 1306/2000... Training loss: 0.4192\n",
      "Epoch: 1306/2000... Training loss: 0.6787\n",
      "Epoch: 1306/2000... Training loss: 0.3685\n",
      "Epoch: 1306/2000... Training loss: 0.3199\n",
      "Epoch: 1306/2000... Training loss: 0.3861\n",
      "Epoch: 1306/2000... Training loss: 0.3538\n",
      "Epoch: 1306/2000... Training loss: 0.3703\n",
      "Epoch: 1306/2000... Training loss: 0.3606\n",
      "Epoch: 1306/2000... Training loss: 0.5755\n",
      "Epoch: 1306/2000... Training loss: 0.3038\n",
      "Epoch: 1306/2000... Training loss: 0.4835\n",
      "Epoch: 1306/2000... Training loss: 0.4638\n",
      "Epoch: 1306/2000... Training loss: 0.3999\n",
      "Epoch: 1306/2000... Training loss: 0.5375\n",
      "Epoch: 1306/2000... Training loss: 0.3737\n",
      "Epoch: 1306/2000... Training loss: 0.3304\n",
      "Epoch: 1306/2000... Training loss: 0.4533\n",
      "Epoch: 1306/2000... Training loss: 0.4441\n",
      "Epoch: 1306/2000... Training loss: 0.4145\n",
      "Epoch: 1306/2000... Training loss: 0.5114\n",
      "Epoch: 1306/2000... Training loss: 0.3313\n",
      "Epoch: 1306/2000... Training loss: 0.3870\n",
      "Epoch: 1306/2000... Training loss: 0.2873\n",
      "Epoch: 1306/2000... Training loss: 0.4342\n",
      "Epoch: 1306/2000... Training loss: 0.3889\n",
      "Epoch: 1306/2000... Training loss: 0.3349\n",
      "Epoch: 1306/2000... Training loss: 0.4104\n",
      "Epoch: 1306/2000... Training loss: 0.3033\n",
      "Epoch: 1306/2000... Training loss: 0.4595\n",
      "Epoch: 1306/2000... Training loss: 0.3615\n",
      "Epoch: 1307/2000... Training loss: 0.4998\n",
      "Epoch: 1307/2000... Training loss: 0.4354\n",
      "Epoch: 1307/2000... Training loss: 0.4600\n",
      "Epoch: 1307/2000... Training loss: 0.3968\n",
      "Epoch: 1307/2000... Training loss: 0.4700\n",
      "Epoch: 1307/2000... Training loss: 0.3589\n",
      "Epoch: 1307/2000... Training loss: 0.4542\n",
      "Epoch: 1307/2000... Training loss: 0.4685\n",
      "Epoch: 1307/2000... Training loss: 0.4646\n",
      "Epoch: 1307/2000... Training loss: 0.4866\n",
      "Epoch: 1307/2000... Training loss: 0.3191\n",
      "Epoch: 1307/2000... Training loss: 0.3741\n",
      "Epoch: 1307/2000... Training loss: 0.3996\n",
      "Epoch: 1307/2000... Training loss: 0.4595\n",
      "Epoch: 1307/2000... Training loss: 0.4909\n",
      "Epoch: 1307/2000... Training loss: 0.3474\n",
      "Epoch: 1307/2000... Training loss: 0.4526\n",
      "Epoch: 1307/2000... Training loss: 0.3412\n",
      "Epoch: 1307/2000... Training loss: 0.4122\n",
      "Epoch: 1307/2000... Training loss: 0.5186\n",
      "Epoch: 1307/2000... Training loss: 0.4020\n",
      "Epoch: 1307/2000... Training loss: 0.3893\n",
      "Epoch: 1307/2000... Training loss: 0.4443\n",
      "Epoch: 1307/2000... Training loss: 0.4256\n",
      "Epoch: 1307/2000... Training loss: 0.5147\n",
      "Epoch: 1307/2000... Training loss: 0.4978\n",
      "Epoch: 1307/2000... Training loss: 0.6107\n",
      "Epoch: 1307/2000... Training loss: 0.3873\n",
      "Epoch: 1307/2000... Training loss: 0.4778\n",
      "Epoch: 1307/2000... Training loss: 0.3101\n",
      "Epoch: 1307/2000... Training loss: 0.3276\n",
      "Epoch: 1308/2000... Training loss: 0.4838\n",
      "Epoch: 1308/2000... Training loss: 0.4233\n",
      "Epoch: 1308/2000... Training loss: 0.5472\n",
      "Epoch: 1308/2000... Training loss: 0.4937\n",
      "Epoch: 1308/2000... Training loss: 0.3749\n",
      "Epoch: 1308/2000... Training loss: 0.4937\n",
      "Epoch: 1308/2000... Training loss: 0.5816\n",
      "Epoch: 1308/2000... Training loss: 0.4366\n",
      "Epoch: 1308/2000... Training loss: 0.4190\n",
      "Epoch: 1308/2000... Training loss: 0.2945\n",
      "Epoch: 1308/2000... Training loss: 0.3098\n",
      "Epoch: 1308/2000... Training loss: 0.5036\n",
      "Epoch: 1308/2000... Training loss: 0.4523\n",
      "Epoch: 1308/2000... Training loss: 0.4438\n",
      "Epoch: 1308/2000... Training loss: 0.3756\n",
      "Epoch: 1308/2000... Training loss: 0.3678\n",
      "Epoch: 1308/2000... Training loss: 0.4408\n",
      "Epoch: 1308/2000... Training loss: 0.4255\n",
      "Epoch: 1308/2000... Training loss: 0.2997\n",
      "Epoch: 1308/2000... Training loss: 0.4008\n",
      "Epoch: 1308/2000... Training loss: 0.4967\n",
      "Epoch: 1308/2000... Training loss: 0.4853\n",
      "Epoch: 1308/2000... Training loss: 0.4788\n",
      "Epoch: 1308/2000... Training loss: 0.3940\n",
      "Epoch: 1308/2000... Training loss: 0.3443\n",
      "Epoch: 1308/2000... Training loss: 0.4327\n",
      "Epoch: 1308/2000... Training loss: 0.4563\n",
      "Epoch: 1308/2000... Training loss: 0.4664\n",
      "Epoch: 1308/2000... Training loss: 0.4558\n",
      "Epoch: 1308/2000... Training loss: 0.6103\n",
      "Epoch: 1308/2000... Training loss: 0.5448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1309/2000... Training loss: 0.5062\n",
      "Epoch: 1309/2000... Training loss: 0.3775\n",
      "Epoch: 1309/2000... Training loss: 0.5220\n",
      "Epoch: 1309/2000... Training loss: 0.6879\n",
      "Epoch: 1309/2000... Training loss: 0.3399\n",
      "Epoch: 1309/2000... Training loss: 0.6026\n",
      "Epoch: 1309/2000... Training loss: 0.3241\n",
      "Epoch: 1309/2000... Training loss: 0.3780\n",
      "Epoch: 1309/2000... Training loss: 0.5265\n",
      "Epoch: 1309/2000... Training loss: 0.3437\n",
      "Epoch: 1309/2000... Training loss: 0.5671\n",
      "Epoch: 1309/2000... Training loss: 0.4896\n",
      "Epoch: 1309/2000... Training loss: 0.3488\n",
      "Epoch: 1309/2000... Training loss: 0.3718\n",
      "Epoch: 1309/2000... Training loss: 0.3697\n",
      "Epoch: 1309/2000... Training loss: 0.2868\n",
      "Epoch: 1309/2000... Training loss: 0.5541\n",
      "Epoch: 1309/2000... Training loss: 0.3784\n",
      "Epoch: 1309/2000... Training loss: 0.3924\n",
      "Epoch: 1309/2000... Training loss: 0.5632\n",
      "Epoch: 1309/2000... Training loss: 0.4744\n",
      "Epoch: 1309/2000... Training loss: 0.5327\n",
      "Epoch: 1309/2000... Training loss: 0.3695\n",
      "Epoch: 1309/2000... Training loss: 0.4781\n",
      "Epoch: 1309/2000... Training loss: 0.4750\n",
      "Epoch: 1309/2000... Training loss: 0.3799\n",
      "Epoch: 1309/2000... Training loss: 0.2485\n",
      "Epoch: 1309/2000... Training loss: 0.6545\n",
      "Epoch: 1309/2000... Training loss: 0.2408\n",
      "Epoch: 1309/2000... Training loss: 0.3442\n",
      "Epoch: 1309/2000... Training loss: 0.3174\n",
      "Epoch: 1310/2000... Training loss: 0.4689\n",
      "Epoch: 1310/2000... Training loss: 0.3856\n",
      "Epoch: 1310/2000... Training loss: 0.5499\n",
      "Epoch: 1310/2000... Training loss: 0.4721\n",
      "Epoch: 1310/2000... Training loss: 0.5916\n",
      "Epoch: 1310/2000... Training loss: 0.3659\n",
      "Epoch: 1310/2000... Training loss: 0.3148\n",
      "Epoch: 1310/2000... Training loss: 0.4518\n",
      "Epoch: 1310/2000... Training loss: 0.2941\n",
      "Epoch: 1310/2000... Training loss: 0.5795\n",
      "Epoch: 1310/2000... Training loss: 0.6225\n",
      "Epoch: 1310/2000... Training loss: 0.5868\n",
      "Epoch: 1310/2000... Training loss: 0.3236\n",
      "Epoch: 1310/2000... Training loss: 0.5444\n",
      "Epoch: 1310/2000... Training loss: 0.3808\n",
      "Epoch: 1310/2000... Training loss: 0.2419\n",
      "Epoch: 1310/2000... Training loss: 0.5221\n",
      "Epoch: 1310/2000... Training loss: 0.4727\n",
      "Epoch: 1310/2000... Training loss: 0.4364\n",
      "Epoch: 1310/2000... Training loss: 0.2846\n",
      "Epoch: 1310/2000... Training loss: 0.3444\n",
      "Epoch: 1310/2000... Training loss: 0.4062\n",
      "Epoch: 1310/2000... Training loss: 0.5983\n",
      "Epoch: 1310/2000... Training loss: 0.4861\n",
      "Epoch: 1310/2000... Training loss: 0.6335\n",
      "Epoch: 1310/2000... Training loss: 0.3805\n",
      "Epoch: 1310/2000... Training loss: 0.5697\n",
      "Epoch: 1310/2000... Training loss: 0.5475\n",
      "Epoch: 1310/2000... Training loss: 0.5769\n",
      "Epoch: 1310/2000... Training loss: 0.4481\n",
      "Epoch: 1310/2000... Training loss: 0.3419\n",
      "Epoch: 1311/2000... Training loss: 0.4206\n",
      "Epoch: 1311/2000... Training loss: 0.5676\n",
      "Epoch: 1311/2000... Training loss: 0.3445\n",
      "Epoch: 1311/2000... Training loss: 0.4308\n",
      "Epoch: 1311/2000... Training loss: 0.5550\n",
      "Epoch: 1311/2000... Training loss: 0.4623\n",
      "Epoch: 1311/2000... Training loss: 0.4135\n",
      "Epoch: 1311/2000... Training loss: 0.3736\n",
      "Epoch: 1311/2000... Training loss: 0.5529\n",
      "Epoch: 1311/2000... Training loss: 0.6842\n",
      "Epoch: 1311/2000... Training loss: 0.4153\n",
      "Epoch: 1311/2000... Training loss: 0.4301\n",
      "Epoch: 1311/2000... Training loss: 0.4048\n",
      "Epoch: 1311/2000... Training loss: 0.6680\n",
      "Epoch: 1311/2000... Training loss: 0.5092\n",
      "Epoch: 1311/2000... Training loss: 0.4454\n",
      "Epoch: 1311/2000... Training loss: 0.4842\n",
      "Epoch: 1311/2000... Training loss: 0.4272\n",
      "Epoch: 1311/2000... Training loss: 0.4079\n",
      "Epoch: 1311/2000... Training loss: 0.3983\n",
      "Epoch: 1311/2000... Training loss: 0.3542\n",
      "Epoch: 1311/2000... Training loss: 0.4356\n",
      "Epoch: 1311/2000... Training loss: 0.6393\n",
      "Epoch: 1311/2000... Training loss: 0.3699\n",
      "Epoch: 1311/2000... Training loss: 0.3746\n",
      "Epoch: 1311/2000... Training loss: 0.4536\n",
      "Epoch: 1311/2000... Training loss: 0.4540\n",
      "Epoch: 1311/2000... Training loss: 0.3297\n",
      "Epoch: 1311/2000... Training loss: 0.4998\n",
      "Epoch: 1311/2000... Training loss: 0.3826\n",
      "Epoch: 1311/2000... Training loss: 0.4379\n",
      "Epoch: 1312/2000... Training loss: 0.2453\n",
      "Epoch: 1312/2000... Training loss: 0.3134\n",
      "Epoch: 1312/2000... Training loss: 0.5355\n",
      "Epoch: 1312/2000... Training loss: 0.3109\n",
      "Epoch: 1312/2000... Training loss: 0.4406\n",
      "Epoch: 1312/2000... Training loss: 0.3973\n",
      "Epoch: 1312/2000... Training loss: 0.3033\n",
      "Epoch: 1312/2000... Training loss: 0.4991\n",
      "Epoch: 1312/2000... Training loss: 0.3603\n",
      "Epoch: 1312/2000... Training loss: 0.4717\n",
      "Epoch: 1312/2000... Training loss: 0.4292\n",
      "Epoch: 1312/2000... Training loss: 0.6273\n",
      "Epoch: 1312/2000... Training loss: 0.4176\n",
      "Epoch: 1312/2000... Training loss: 0.4461\n",
      "Epoch: 1312/2000... Training loss: 0.4162\n",
      "Epoch: 1312/2000... Training loss: 0.5014\n",
      "Epoch: 1312/2000... Training loss: 0.4055\n",
      "Epoch: 1312/2000... Training loss: 0.4574\n",
      "Epoch: 1312/2000... Training loss: 0.7041\n",
      "Epoch: 1312/2000... Training loss: 0.4985\n",
      "Epoch: 1312/2000... Training loss: 0.6310\n",
      "Epoch: 1312/2000... Training loss: 0.3985\n",
      "Epoch: 1312/2000... Training loss: 0.3430\n",
      "Epoch: 1312/2000... Training loss: 0.3342\n",
      "Epoch: 1312/2000... Training loss: 0.3782\n",
      "Epoch: 1312/2000... Training loss: 0.2776\n",
      "Epoch: 1312/2000... Training loss: 0.3640\n",
      "Epoch: 1312/2000... Training loss: 0.3291\n",
      "Epoch: 1312/2000... Training loss: 0.4710\n",
      "Epoch: 1312/2000... Training loss: 0.3582\n",
      "Epoch: 1312/2000... Training loss: 0.2986\n",
      "Epoch: 1313/2000... Training loss: 0.5362\n",
      "Epoch: 1313/2000... Training loss: 0.4217\n",
      "Epoch: 1313/2000... Training loss: 0.5084\n",
      "Epoch: 1313/2000... Training loss: 0.4000\n",
      "Epoch: 1313/2000... Training loss: 0.3393\n",
      "Epoch: 1313/2000... Training loss: 0.5611\n",
      "Epoch: 1313/2000... Training loss: 0.3404\n",
      "Epoch: 1313/2000... Training loss: 0.3912\n",
      "Epoch: 1313/2000... Training loss: 0.4019\n",
      "Epoch: 1313/2000... Training loss: 0.4313\n",
      "Epoch: 1313/2000... Training loss: 0.4937\n",
      "Epoch: 1313/2000... Training loss: 0.4777\n",
      "Epoch: 1313/2000... Training loss: 0.6057\n",
      "Epoch: 1313/2000... Training loss: 0.4432\n",
      "Epoch: 1313/2000... Training loss: 0.4872\n",
      "Epoch: 1313/2000... Training loss: 0.4601\n",
      "Epoch: 1313/2000... Training loss: 0.3943\n",
      "Epoch: 1313/2000... Training loss: 0.2527\n",
      "Epoch: 1313/2000... Training loss: 0.4766\n",
      "Epoch: 1313/2000... Training loss: 0.5023\n",
      "Epoch: 1313/2000... Training loss: 0.3590\n",
      "Epoch: 1313/2000... Training loss: 0.2151\n",
      "Epoch: 1313/2000... Training loss: 0.4351\n",
      "Epoch: 1313/2000... Training loss: 0.5042\n",
      "Epoch: 1313/2000... Training loss: 0.4018\n",
      "Epoch: 1313/2000... Training loss: 0.2988\n",
      "Epoch: 1313/2000... Training loss: 0.5086\n",
      "Epoch: 1313/2000... Training loss: 0.3973\n",
      "Epoch: 1313/2000... Training loss: 0.5760\n",
      "Epoch: 1313/2000... Training loss: 0.4502\n",
      "Epoch: 1313/2000... Training loss: 0.4035\n",
      "Epoch: 1314/2000... Training loss: 0.5053\n",
      "Epoch: 1314/2000... Training loss: 0.3542\n",
      "Epoch: 1314/2000... Training loss: 0.3807\n",
      "Epoch: 1314/2000... Training loss: 0.3590\n",
      "Epoch: 1314/2000... Training loss: 0.4705\n",
      "Epoch: 1314/2000... Training loss: 0.5289\n",
      "Epoch: 1314/2000... Training loss: 0.4568\n",
      "Epoch: 1314/2000... Training loss: 0.4967\n",
      "Epoch: 1314/2000... Training loss: 0.3327\n",
      "Epoch: 1314/2000... Training loss: 0.3714\n",
      "Epoch: 1314/2000... Training loss: 0.4786\n",
      "Epoch: 1314/2000... Training loss: 0.3753\n",
      "Epoch: 1314/2000... Training loss: 0.2961\n",
      "Epoch: 1314/2000... Training loss: 0.5146\n",
      "Epoch: 1314/2000... Training loss: 0.4197\n",
      "Epoch: 1314/2000... Training loss: 0.4856\n",
      "Epoch: 1314/2000... Training loss: 0.3991\n",
      "Epoch: 1314/2000... Training loss: 0.4283\n",
      "Epoch: 1314/2000... Training loss: 0.4730\n",
      "Epoch: 1314/2000... Training loss: 0.2551\n",
      "Epoch: 1314/2000... Training loss: 0.2824\n",
      "Epoch: 1314/2000... Training loss: 0.4516\n",
      "Epoch: 1314/2000... Training loss: 0.6544\n",
      "Epoch: 1314/2000... Training loss: 0.4556\n",
      "Epoch: 1314/2000... Training loss: 0.3245\n",
      "Epoch: 1314/2000... Training loss: 0.4651\n",
      "Epoch: 1314/2000... Training loss: 0.5450\n",
      "Epoch: 1314/2000... Training loss: 0.5091\n",
      "Epoch: 1314/2000... Training loss: 0.4769\n",
      "Epoch: 1314/2000... Training loss: 0.5800\n",
      "Epoch: 1314/2000... Training loss: 0.5787\n",
      "Epoch: 1315/2000... Training loss: 0.5346\n",
      "Epoch: 1315/2000... Training loss: 0.4534\n",
      "Epoch: 1315/2000... Training loss: 0.2453\n",
      "Epoch: 1315/2000... Training loss: 0.3915\n",
      "Epoch: 1315/2000... Training loss: 0.3983\n",
      "Epoch: 1315/2000... Training loss: 0.4894\n",
      "Epoch: 1315/2000... Training loss: 0.3661\n",
      "Epoch: 1315/2000... Training loss: 0.3768\n",
      "Epoch: 1315/2000... Training loss: 0.5114\n",
      "Epoch: 1315/2000... Training loss: 0.3684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1315/2000... Training loss: 0.3983\n",
      "Epoch: 1315/2000... Training loss: 0.5507\n",
      "Epoch: 1315/2000... Training loss: 0.5150\n",
      "Epoch: 1315/2000... Training loss: 0.4430\n",
      "Epoch: 1315/2000... Training loss: 0.3163\n",
      "Epoch: 1315/2000... Training loss: 0.4072\n",
      "Epoch: 1315/2000... Training loss: 0.3830\n",
      "Epoch: 1315/2000... Training loss: 0.2936\n",
      "Epoch: 1315/2000... Training loss: 0.3454\n",
      "Epoch: 1315/2000... Training loss: 0.5801\n",
      "Epoch: 1315/2000... Training loss: 0.3642\n",
      "Epoch: 1315/2000... Training loss: 0.5810\n",
      "Epoch: 1315/2000... Training loss: 0.3873\n",
      "Epoch: 1315/2000... Training loss: 0.6637\n",
      "Epoch: 1315/2000... Training loss: 0.4774\n",
      "Epoch: 1315/2000... Training loss: 0.4797\n",
      "Epoch: 1315/2000... Training loss: 0.3039\n",
      "Epoch: 1315/2000... Training loss: 0.5091\n",
      "Epoch: 1315/2000... Training loss: 0.2848\n",
      "Epoch: 1315/2000... Training loss: 0.3415\n",
      "Epoch: 1315/2000... Training loss: 0.4887\n",
      "Epoch: 1316/2000... Training loss: 0.3678\n",
      "Epoch: 1316/2000... Training loss: 0.3851\n",
      "Epoch: 1316/2000... Training loss: 0.4295\n",
      "Epoch: 1316/2000... Training loss: 0.3742\n",
      "Epoch: 1316/2000... Training loss: 0.4885\n",
      "Epoch: 1316/2000... Training loss: 0.2777\n",
      "Epoch: 1316/2000... Training loss: 0.5734\n",
      "Epoch: 1316/2000... Training loss: 0.3281\n",
      "Epoch: 1316/2000... Training loss: 0.4239\n",
      "Epoch: 1316/2000... Training loss: 0.4150\n",
      "Epoch: 1316/2000... Training loss: 0.6480\n",
      "Epoch: 1316/2000... Training loss: 0.3058\n",
      "Epoch: 1316/2000... Training loss: 0.3010\n",
      "Epoch: 1316/2000... Training loss: 0.4837\n",
      "Epoch: 1316/2000... Training loss: 0.4035\n",
      "Epoch: 1316/2000... Training loss: 0.4335\n",
      "Epoch: 1316/2000... Training loss: 0.5300\n",
      "Epoch: 1316/2000... Training loss: 0.3582\n",
      "Epoch: 1316/2000... Training loss: 0.4894\n",
      "Epoch: 1316/2000... Training loss: 0.3613\n",
      "Epoch: 1316/2000... Training loss: 0.3545\n",
      "Epoch: 1316/2000... Training loss: 0.4094\n",
      "Epoch: 1316/2000... Training loss: 0.3515\n",
      "Epoch: 1316/2000... Training loss: 0.4335\n",
      "Epoch: 1316/2000... Training loss: 0.3193\n",
      "Epoch: 1316/2000... Training loss: 0.4564\n",
      "Epoch: 1316/2000... Training loss: 0.4708\n",
      "Epoch: 1316/2000... Training loss: 0.3066\n",
      "Epoch: 1316/2000... Training loss: 0.2835\n",
      "Epoch: 1316/2000... Training loss: 0.4476\n",
      "Epoch: 1316/2000... Training loss: 0.5878\n",
      "Epoch: 1317/2000... Training loss: 0.3118\n",
      "Epoch: 1317/2000... Training loss: 0.4394\n",
      "Epoch: 1317/2000... Training loss: 0.4368\n",
      "Epoch: 1317/2000... Training loss: 0.3217\n",
      "Epoch: 1317/2000... Training loss: 0.3833\n",
      "Epoch: 1317/2000... Training loss: 0.5531\n",
      "Epoch: 1317/2000... Training loss: 0.4871\n",
      "Epoch: 1317/2000... Training loss: 0.2485\n",
      "Epoch: 1317/2000... Training loss: 0.3857\n",
      "Epoch: 1317/2000... Training loss: 0.5789\n",
      "Epoch: 1317/2000... Training loss: 0.4284\n",
      "Epoch: 1317/2000... Training loss: 0.4303\n",
      "Epoch: 1317/2000... Training loss: 0.5769\n",
      "Epoch: 1317/2000... Training loss: 0.4279\n",
      "Epoch: 1317/2000... Training loss: 0.4019\n",
      "Epoch: 1317/2000... Training loss: 0.4121\n",
      "Epoch: 1317/2000... Training loss: 0.6905\n",
      "Epoch: 1317/2000... Training loss: 0.4673\n",
      "Epoch: 1317/2000... Training loss: 0.3734\n",
      "Epoch: 1317/2000... Training loss: 0.4545\n",
      "Epoch: 1317/2000... Training loss: 0.3900\n",
      "Epoch: 1317/2000... Training loss: 0.5036\n",
      "Epoch: 1317/2000... Training loss: 0.4021\n",
      "Epoch: 1317/2000... Training loss: 0.3473\n",
      "Epoch: 1317/2000... Training loss: 0.4307\n",
      "Epoch: 1317/2000... Training loss: 0.5373\n",
      "Epoch: 1317/2000... Training loss: 0.4923\n",
      "Epoch: 1317/2000... Training loss: 0.3107\n",
      "Epoch: 1317/2000... Training loss: 0.3292\n",
      "Epoch: 1317/2000... Training loss: 0.3604\n",
      "Epoch: 1317/2000... Training loss: 0.3586\n",
      "Epoch: 1318/2000... Training loss: 0.3184\n",
      "Epoch: 1318/2000... Training loss: 0.5724\n",
      "Epoch: 1318/2000... Training loss: 0.3899\n",
      "Epoch: 1318/2000... Training loss: 0.5808\n",
      "Epoch: 1318/2000... Training loss: 0.4086\n",
      "Epoch: 1318/2000... Training loss: 0.5110\n",
      "Epoch: 1318/2000... Training loss: 0.5693\n",
      "Epoch: 1318/2000... Training loss: 0.4172\n",
      "Epoch: 1318/2000... Training loss: 0.3495\n",
      "Epoch: 1318/2000... Training loss: 0.5590\n",
      "Epoch: 1318/2000... Training loss: 0.2998\n",
      "Epoch: 1318/2000... Training loss: 0.4553\n",
      "Epoch: 1318/2000... Training loss: 0.3679\n",
      "Epoch: 1318/2000... Training loss: 0.3149\n",
      "Epoch: 1318/2000... Training loss: 0.4742\n",
      "Epoch: 1318/2000... Training loss: 0.3991\n",
      "Epoch: 1318/2000... Training loss: 0.4728\n",
      "Epoch: 1318/2000... Training loss: 0.4343\n",
      "Epoch: 1318/2000... Training loss: 0.5236\n",
      "Epoch: 1318/2000... Training loss: 0.3295\n",
      "Epoch: 1318/2000... Training loss: 0.6009\n",
      "Epoch: 1318/2000... Training loss: 0.3694\n",
      "Epoch: 1318/2000... Training loss: 0.6170\n",
      "Epoch: 1318/2000... Training loss: 0.4221\n",
      "Epoch: 1318/2000... Training loss: 0.3350\n",
      "Epoch: 1318/2000... Training loss: 0.4735\n",
      "Epoch: 1318/2000... Training loss: 0.3580\n",
      "Epoch: 1318/2000... Training loss: 0.4716\n",
      "Epoch: 1318/2000... Training loss: 0.5175\n",
      "Epoch: 1318/2000... Training loss: 0.6030\n",
      "Epoch: 1318/2000... Training loss: 0.3672\n",
      "Epoch: 1319/2000... Training loss: 0.4933\n",
      "Epoch: 1319/2000... Training loss: 0.3646\n",
      "Epoch: 1319/2000... Training loss: 0.3464\n",
      "Epoch: 1319/2000... Training loss: 0.5848\n",
      "Epoch: 1319/2000... Training loss: 0.2108\n",
      "Epoch: 1319/2000... Training loss: 0.3756\n",
      "Epoch: 1319/2000... Training loss: 0.2459\n",
      "Epoch: 1319/2000... Training loss: 0.3647\n",
      "Epoch: 1319/2000... Training loss: 0.3410\n",
      "Epoch: 1319/2000... Training loss: 0.5498\n",
      "Epoch: 1319/2000... Training loss: 0.5423\n",
      "Epoch: 1319/2000... Training loss: 0.5162\n",
      "Epoch: 1319/2000... Training loss: 0.3711\n",
      "Epoch: 1319/2000... Training loss: 0.5154\n",
      "Epoch: 1319/2000... Training loss: 0.4086\n",
      "Epoch: 1319/2000... Training loss: 0.5603\n",
      "Epoch: 1319/2000... Training loss: 0.4440\n",
      "Epoch: 1319/2000... Training loss: 0.4792\n",
      "Epoch: 1319/2000... Training loss: 0.3732\n",
      "Epoch: 1319/2000... Training loss: 0.4912\n",
      "Epoch: 1319/2000... Training loss: 0.3290\n",
      "Epoch: 1319/2000... Training loss: 0.5043\n",
      "Epoch: 1319/2000... Training loss: 0.4857\n",
      "Epoch: 1319/2000... Training loss: 0.3739\n",
      "Epoch: 1319/2000... Training loss: 0.5284\n",
      "Epoch: 1319/2000... Training loss: 0.4319\n",
      "Epoch: 1319/2000... Training loss: 0.4818\n",
      "Epoch: 1319/2000... Training loss: 0.3837\n",
      "Epoch: 1319/2000... Training loss: 0.3529\n",
      "Epoch: 1319/2000... Training loss: 0.5299\n",
      "Epoch: 1319/2000... Training loss: 0.5013\n",
      "Epoch: 1320/2000... Training loss: 0.4674\n",
      "Epoch: 1320/2000... Training loss: 0.5397\n",
      "Epoch: 1320/2000... Training loss: 0.5616\n",
      "Epoch: 1320/2000... Training loss: 0.3871\n",
      "Epoch: 1320/2000... Training loss: 0.5424\n",
      "Epoch: 1320/2000... Training loss: 0.5557\n",
      "Epoch: 1320/2000... Training loss: 0.3377\n",
      "Epoch: 1320/2000... Training loss: 0.4761\n",
      "Epoch: 1320/2000... Training loss: 0.4151\n",
      "Epoch: 1320/2000... Training loss: 0.4013\n",
      "Epoch: 1320/2000... Training loss: 0.3430\n",
      "Epoch: 1320/2000... Training loss: 0.5347\n",
      "Epoch: 1320/2000... Training loss: 0.4022\n",
      "Epoch: 1320/2000... Training loss: 0.3630\n",
      "Epoch: 1320/2000... Training loss: 0.4297\n",
      "Epoch: 1320/2000... Training loss: 0.4268\n",
      "Epoch: 1320/2000... Training loss: 0.5111\n",
      "Epoch: 1320/2000... Training loss: 0.5276\n",
      "Epoch: 1320/2000... Training loss: 0.3918\n",
      "Epoch: 1320/2000... Training loss: 0.6045\n",
      "Epoch: 1320/2000... Training loss: 0.3704\n",
      "Epoch: 1320/2000... Training loss: 0.4500\n",
      "Epoch: 1320/2000... Training loss: 0.3561\n",
      "Epoch: 1320/2000... Training loss: 0.5223\n",
      "Epoch: 1320/2000... Training loss: 0.5294\n",
      "Epoch: 1320/2000... Training loss: 0.3374\n",
      "Epoch: 1320/2000... Training loss: 0.4450\n",
      "Epoch: 1320/2000... Training loss: 0.3240\n",
      "Epoch: 1320/2000... Training loss: 0.5370\n",
      "Epoch: 1320/2000... Training loss: 0.3699\n",
      "Epoch: 1320/2000... Training loss: 0.4950\n",
      "Epoch: 1321/2000... Training loss: 0.3769\n",
      "Epoch: 1321/2000... Training loss: 0.4761\n",
      "Epoch: 1321/2000... Training loss: 0.5256\n",
      "Epoch: 1321/2000... Training loss: 0.4289\n",
      "Epoch: 1321/2000... Training loss: 0.4022\n",
      "Epoch: 1321/2000... Training loss: 0.3777\n",
      "Epoch: 1321/2000... Training loss: 0.3655\n",
      "Epoch: 1321/2000... Training loss: 0.3509\n",
      "Epoch: 1321/2000... Training loss: 0.3834\n",
      "Epoch: 1321/2000... Training loss: 0.5105\n",
      "Epoch: 1321/2000... Training loss: 0.2983\n",
      "Epoch: 1321/2000... Training loss: 0.5284\n",
      "Epoch: 1321/2000... Training loss: 0.3288\n",
      "Epoch: 1321/2000... Training loss: 0.4227\n",
      "Epoch: 1321/2000... Training loss: 0.4369\n",
      "Epoch: 1321/2000... Training loss: 0.3974\n",
      "Epoch: 1321/2000... Training loss: 0.4986\n",
      "Epoch: 1321/2000... Training loss: 0.4171\n",
      "Epoch: 1321/2000... Training loss: 0.4303\n",
      "Epoch: 1321/2000... Training loss: 0.5207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1321/2000... Training loss: 0.4292\n",
      "Epoch: 1321/2000... Training loss: 0.5084\n",
      "Epoch: 1321/2000... Training loss: 0.4151\n",
      "Epoch: 1321/2000... Training loss: 0.4442\n",
      "Epoch: 1321/2000... Training loss: 0.4539\n",
      "Epoch: 1321/2000... Training loss: 0.2705\n",
      "Epoch: 1321/2000... Training loss: 0.4353\n",
      "Epoch: 1321/2000... Training loss: 0.4323\n",
      "Epoch: 1321/2000... Training loss: 0.5914\n",
      "Epoch: 1321/2000... Training loss: 0.4982\n",
      "Epoch: 1321/2000... Training loss: 0.3495\n",
      "Epoch: 1322/2000... Training loss: 0.5214\n",
      "Epoch: 1322/2000... Training loss: 0.3815\n",
      "Epoch: 1322/2000... Training loss: 0.2385\n",
      "Epoch: 1322/2000... Training loss: 0.4988\n",
      "Epoch: 1322/2000... Training loss: 0.4913\n",
      "Epoch: 1322/2000... Training loss: 0.3731\n",
      "Epoch: 1322/2000... Training loss: 0.5582\n",
      "Epoch: 1322/2000... Training loss: 0.4583\n",
      "Epoch: 1322/2000... Training loss: 0.4951\n",
      "Epoch: 1322/2000... Training loss: 0.4536\n",
      "Epoch: 1322/2000... Training loss: 0.4715\n",
      "Epoch: 1322/2000... Training loss: 0.3951\n",
      "Epoch: 1322/2000... Training loss: 0.3791\n",
      "Epoch: 1322/2000... Training loss: 0.4065\n",
      "Epoch: 1322/2000... Training loss: 0.2564\n",
      "Epoch: 1322/2000... Training loss: 0.4315\n",
      "Epoch: 1322/2000... Training loss: 0.4825\n",
      "Epoch: 1322/2000... Training loss: 0.3242\n",
      "Epoch: 1322/2000... Training loss: 0.4834\n",
      "Epoch: 1322/2000... Training loss: 0.4967\n",
      "Epoch: 1322/2000... Training loss: 0.4049\n",
      "Epoch: 1322/2000... Training loss: 0.4698\n",
      "Epoch: 1322/2000... Training loss: 0.4655\n",
      "Epoch: 1322/2000... Training loss: 0.5200\n",
      "Epoch: 1322/2000... Training loss: 0.3679\n",
      "Epoch: 1322/2000... Training loss: 0.3669\n",
      "Epoch: 1322/2000... Training loss: 0.4501\n",
      "Epoch: 1322/2000... Training loss: 0.3877\n",
      "Epoch: 1322/2000... Training loss: 0.4190\n",
      "Epoch: 1322/2000... Training loss: 0.4786\n",
      "Epoch: 1322/2000... Training loss: 0.5620\n",
      "Epoch: 1323/2000... Training loss: 0.5410\n",
      "Epoch: 1323/2000... Training loss: 0.4028\n",
      "Epoch: 1323/2000... Training loss: 0.4314\n",
      "Epoch: 1323/2000... Training loss: 0.4949\n",
      "Epoch: 1323/2000... Training loss: 0.4110\n",
      "Epoch: 1323/2000... Training loss: 0.3754\n",
      "Epoch: 1323/2000... Training loss: 0.5008\n",
      "Epoch: 1323/2000... Training loss: 0.3946\n",
      "Epoch: 1323/2000... Training loss: 0.4970\n",
      "Epoch: 1323/2000... Training loss: 0.5738\n",
      "Epoch: 1323/2000... Training loss: 0.3456\n",
      "Epoch: 1323/2000... Training loss: 0.5310\n",
      "Epoch: 1323/2000... Training loss: 0.5728\n",
      "Epoch: 1323/2000... Training loss: 0.3959\n",
      "Epoch: 1323/2000... Training loss: 0.3109\n",
      "Epoch: 1323/2000... Training loss: 0.3048\n",
      "Epoch: 1323/2000... Training loss: 0.4758\n",
      "Epoch: 1323/2000... Training loss: 0.3061\n",
      "Epoch: 1323/2000... Training loss: 0.3933\n",
      "Epoch: 1323/2000... Training loss: 0.4698\n",
      "Epoch: 1323/2000... Training loss: 0.5020\n",
      "Epoch: 1323/2000... Training loss: 0.4421\n",
      "Epoch: 1323/2000... Training loss: 0.2428\n",
      "Epoch: 1323/2000... Training loss: 0.5277\n",
      "Epoch: 1323/2000... Training loss: 0.3637\n",
      "Epoch: 1323/2000... Training loss: 0.4558\n",
      "Epoch: 1323/2000... Training loss: 0.4898\n",
      "Epoch: 1323/2000... Training loss: 0.3528\n",
      "Epoch: 1323/2000... Training loss: 0.6794\n",
      "Epoch: 1323/2000... Training loss: 0.3071\n",
      "Epoch: 1323/2000... Training loss: 0.4290\n",
      "Epoch: 1324/2000... Training loss: 0.3484\n",
      "Epoch: 1324/2000... Training loss: 0.4044\n",
      "Epoch: 1324/2000... Training loss: 0.5023\n",
      "Epoch: 1324/2000... Training loss: 0.3809\n",
      "Epoch: 1324/2000... Training loss: 0.6710\n",
      "Epoch: 1324/2000... Training loss: 0.4586\n",
      "Epoch: 1324/2000... Training loss: 0.3905\n",
      "Epoch: 1324/2000... Training loss: 0.3128\n",
      "Epoch: 1324/2000... Training loss: 0.5236\n",
      "Epoch: 1324/2000... Training loss: 0.4426\n",
      "Epoch: 1324/2000... Training loss: 0.4810\n",
      "Epoch: 1324/2000... Training loss: 0.4162\n",
      "Epoch: 1324/2000... Training loss: 0.6285\n",
      "Epoch: 1324/2000... Training loss: 0.5314\n",
      "Epoch: 1324/2000... Training loss: 0.4556\n",
      "Epoch: 1324/2000... Training loss: 0.3018\n",
      "Epoch: 1324/2000... Training loss: 0.4936\n",
      "Epoch: 1324/2000... Training loss: 0.4415\n",
      "Epoch: 1324/2000... Training loss: 0.3881\n",
      "Epoch: 1324/2000... Training loss: 0.2413\n",
      "Epoch: 1324/2000... Training loss: 0.3573\n",
      "Epoch: 1324/2000... Training loss: 0.3627\n",
      "Epoch: 1324/2000... Training loss: 0.5279\n",
      "Epoch: 1324/2000... Training loss: 0.4575\n",
      "Epoch: 1324/2000... Training loss: 0.3754\n",
      "Epoch: 1324/2000... Training loss: 0.4612\n",
      "Epoch: 1324/2000... Training loss: 0.4295\n",
      "Epoch: 1324/2000... Training loss: 0.4962\n",
      "Epoch: 1324/2000... Training loss: 0.5478\n",
      "Epoch: 1324/2000... Training loss: 0.5913\n",
      "Epoch: 1324/2000... Training loss: 0.3523\n",
      "Epoch: 1325/2000... Training loss: 0.5463\n",
      "Epoch: 1325/2000... Training loss: 0.5297\n",
      "Epoch: 1325/2000... Training loss: 0.4391\n",
      "Epoch: 1325/2000... Training loss: 0.3359\n",
      "Epoch: 1325/2000... Training loss: 0.5799\n",
      "Epoch: 1325/2000... Training loss: 0.3738\n",
      "Epoch: 1325/2000... Training loss: 0.5527\n",
      "Epoch: 1325/2000... Training loss: 0.3267\n",
      "Epoch: 1325/2000... Training loss: 0.5254\n",
      "Epoch: 1325/2000... Training loss: 0.5309\n",
      "Epoch: 1325/2000... Training loss: 0.5514\n",
      "Epoch: 1325/2000... Training loss: 0.2784\n",
      "Epoch: 1325/2000... Training loss: 0.3540\n",
      "Epoch: 1325/2000... Training loss: 0.4242\n",
      "Epoch: 1325/2000... Training loss: 0.6172\n",
      "Epoch: 1325/2000... Training loss: 0.3558\n",
      "Epoch: 1325/2000... Training loss: 0.4083\n",
      "Epoch: 1325/2000... Training loss: 0.4393\n",
      "Epoch: 1325/2000... Training loss: 0.4585\n",
      "Epoch: 1325/2000... Training loss: 0.5544\n",
      "Epoch: 1325/2000... Training loss: 0.3332\n",
      "Epoch: 1325/2000... Training loss: 0.4713\n",
      "Epoch: 1325/2000... Training loss: 0.3539\n",
      "Epoch: 1325/2000... Training loss: 0.4623\n",
      "Epoch: 1325/2000... Training loss: 0.3081\n",
      "Epoch: 1325/2000... Training loss: 0.5751\n",
      "Epoch: 1325/2000... Training loss: 0.4376\n",
      "Epoch: 1325/2000... Training loss: 0.4487\n",
      "Epoch: 1325/2000... Training loss: 0.5233\n",
      "Epoch: 1325/2000... Training loss: 0.4439\n",
      "Epoch: 1325/2000... Training loss: 0.2859\n",
      "Epoch: 1326/2000... Training loss: 0.7081\n",
      "Epoch: 1326/2000... Training loss: 0.3508\n",
      "Epoch: 1326/2000... Training loss: 0.3883\n",
      "Epoch: 1326/2000... Training loss: 0.3500\n",
      "Epoch: 1326/2000... Training loss: 0.4982\n",
      "Epoch: 1326/2000... Training loss: 0.3644\n",
      "Epoch: 1326/2000... Training loss: 0.4559\n",
      "Epoch: 1326/2000... Training loss: 0.3305\n",
      "Epoch: 1326/2000... Training loss: 0.7739\n",
      "Epoch: 1326/2000... Training loss: 0.4079\n",
      "Epoch: 1326/2000... Training loss: 0.4333\n",
      "Epoch: 1326/2000... Training loss: 0.5539\n",
      "Epoch: 1326/2000... Training loss: 0.4278\n",
      "Epoch: 1326/2000... Training loss: 0.5795\n",
      "Epoch: 1326/2000... Training loss: 0.6332\n",
      "Epoch: 1326/2000... Training loss: 0.3542\n",
      "Epoch: 1326/2000... Training loss: 0.4646\n",
      "Epoch: 1326/2000... Training loss: 0.4738\n",
      "Epoch: 1326/2000... Training loss: 0.5448\n",
      "Epoch: 1326/2000... Training loss: 0.4672\n",
      "Epoch: 1326/2000... Training loss: 0.4760\n",
      "Epoch: 1326/2000... Training loss: 0.3946\n",
      "Epoch: 1326/2000... Training loss: 0.5534\n",
      "Epoch: 1326/2000... Training loss: 0.4903\n",
      "Epoch: 1326/2000... Training loss: 0.3614\n",
      "Epoch: 1326/2000... Training loss: 0.4739\n",
      "Epoch: 1326/2000... Training loss: 0.3861\n",
      "Epoch: 1326/2000... Training loss: 0.5050\n",
      "Epoch: 1326/2000... Training loss: 0.6503\n",
      "Epoch: 1326/2000... Training loss: 0.4746\n",
      "Epoch: 1326/2000... Training loss: 0.3371\n",
      "Epoch: 1327/2000... Training loss: 0.4593\n",
      "Epoch: 1327/2000... Training loss: 0.4259\n",
      "Epoch: 1327/2000... Training loss: 0.6095\n",
      "Epoch: 1327/2000... Training loss: 0.4272\n",
      "Epoch: 1327/2000... Training loss: 0.4329\n",
      "Epoch: 1327/2000... Training loss: 0.4812\n",
      "Epoch: 1327/2000... Training loss: 0.3929\n",
      "Epoch: 1327/2000... Training loss: 0.5758\n",
      "Epoch: 1327/2000... Training loss: 0.4677\n",
      "Epoch: 1327/2000... Training loss: 0.4365\n",
      "Epoch: 1327/2000... Training loss: 0.5639\n",
      "Epoch: 1327/2000... Training loss: 0.3584\n",
      "Epoch: 1327/2000... Training loss: 0.3381\n",
      "Epoch: 1327/2000... Training loss: 0.5351\n",
      "Epoch: 1327/2000... Training loss: 0.3852\n",
      "Epoch: 1327/2000... Training loss: 0.5815\n",
      "Epoch: 1327/2000... Training loss: 0.3847\n",
      "Epoch: 1327/2000... Training loss: 0.3808\n",
      "Epoch: 1327/2000... Training loss: 0.3581\n",
      "Epoch: 1327/2000... Training loss: 0.3091\n",
      "Epoch: 1327/2000... Training loss: 0.2766\n",
      "Epoch: 1327/2000... Training loss: 0.4142\n",
      "Epoch: 1327/2000... Training loss: 0.4830\n",
      "Epoch: 1327/2000... Training loss: 0.4204\n",
      "Epoch: 1327/2000... Training loss: 0.4031\n",
      "Epoch: 1327/2000... Training loss: 0.3324\n",
      "Epoch: 1327/2000... Training loss: 0.3796\n",
      "Epoch: 1327/2000... Training loss: 0.4061\n",
      "Epoch: 1327/2000... Training loss: 0.4839\n",
      "Epoch: 1327/2000... Training loss: 0.4644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1327/2000... Training loss: 0.3808\n",
      "Epoch: 1328/2000... Training loss: 0.4111\n",
      "Epoch: 1328/2000... Training loss: 0.4135\n",
      "Epoch: 1328/2000... Training loss: 0.6571\n",
      "Epoch: 1328/2000... Training loss: 0.5964\n",
      "Epoch: 1328/2000... Training loss: 0.3441\n",
      "Epoch: 1328/2000... Training loss: 0.5620\n",
      "Epoch: 1328/2000... Training loss: 0.5691\n",
      "Epoch: 1328/2000... Training loss: 0.4913\n",
      "Epoch: 1328/2000... Training loss: 0.2689\n",
      "Epoch: 1328/2000... Training loss: 0.4235\n",
      "Epoch: 1328/2000... Training loss: 0.3878\n",
      "Epoch: 1328/2000... Training loss: 0.7524\n",
      "Epoch: 1328/2000... Training loss: 0.4445\n",
      "Epoch: 1328/2000... Training loss: 0.4311\n",
      "Epoch: 1328/2000... Training loss: 0.4844\n",
      "Epoch: 1328/2000... Training loss: 0.2954\n",
      "Epoch: 1328/2000... Training loss: 0.5690\n",
      "Epoch: 1328/2000... Training loss: 0.5197\n",
      "Epoch: 1328/2000... Training loss: 0.3723\n",
      "Epoch: 1328/2000... Training loss: 0.5973\n",
      "Epoch: 1328/2000... Training loss: 0.3047\n",
      "Epoch: 1328/2000... Training loss: 0.3691\n",
      "Epoch: 1328/2000... Training loss: 0.4641\n",
      "Epoch: 1328/2000... Training loss: 0.5080\n",
      "Epoch: 1328/2000... Training loss: 0.5959\n",
      "Epoch: 1328/2000... Training loss: 0.5627\n",
      "Epoch: 1328/2000... Training loss: 0.3689\n",
      "Epoch: 1328/2000... Training loss: 0.3447\n",
      "Epoch: 1328/2000... Training loss: 0.7003\n",
      "Epoch: 1328/2000... Training loss: 0.4438\n",
      "Epoch: 1328/2000... Training loss: 0.5252\n",
      "Epoch: 1329/2000... Training loss: 0.5179\n",
      "Epoch: 1329/2000... Training loss: 0.3277\n",
      "Epoch: 1329/2000... Training loss: 0.2801\n",
      "Epoch: 1329/2000... Training loss: 0.5624\n",
      "Epoch: 1329/2000... Training loss: 0.4339\n",
      "Epoch: 1329/2000... Training loss: 0.3696\n",
      "Epoch: 1329/2000... Training loss: 0.5357\n",
      "Epoch: 1329/2000... Training loss: 0.3380\n",
      "Epoch: 1329/2000... Training loss: 0.3937\n",
      "Epoch: 1329/2000... Training loss: 0.4407\n",
      "Epoch: 1329/2000... Training loss: 0.4151\n",
      "Epoch: 1329/2000... Training loss: 0.6662\n",
      "Epoch: 1329/2000... Training loss: 0.3818\n",
      "Epoch: 1329/2000... Training loss: 0.3489\n",
      "Epoch: 1329/2000... Training loss: 0.3879\n",
      "Epoch: 1329/2000... Training loss: 0.4987\n",
      "Epoch: 1329/2000... Training loss: 0.5304\n",
      "Epoch: 1329/2000... Training loss: 0.3451\n",
      "Epoch: 1329/2000... Training loss: 0.2760\n",
      "Epoch: 1329/2000... Training loss: 0.3350\n",
      "Epoch: 1329/2000... Training loss: 0.3258\n",
      "Epoch: 1329/2000... Training loss: 0.2728\n",
      "Epoch: 1329/2000... Training loss: 0.4667\n",
      "Epoch: 1329/2000... Training loss: 0.4527\n",
      "Epoch: 1329/2000... Training loss: 0.3601\n",
      "Epoch: 1329/2000... Training loss: 0.5389\n",
      "Epoch: 1329/2000... Training loss: 0.4006\n",
      "Epoch: 1329/2000... Training loss: 0.4166\n",
      "Epoch: 1329/2000... Training loss: 0.4675\n",
      "Epoch: 1329/2000... Training loss: 0.2727\n",
      "Epoch: 1329/2000... Training loss: 0.3519\n",
      "Epoch: 1330/2000... Training loss: 0.5423\n",
      "Epoch: 1330/2000... Training loss: 0.4452\n",
      "Epoch: 1330/2000... Training loss: 0.4477\n",
      "Epoch: 1330/2000... Training loss: 0.3045\n",
      "Epoch: 1330/2000... Training loss: 0.5736\n",
      "Epoch: 1330/2000... Training loss: 0.5580\n",
      "Epoch: 1330/2000... Training loss: 0.4048\n",
      "Epoch: 1330/2000... Training loss: 0.5209\n",
      "Epoch: 1330/2000... Training loss: 0.3202\n",
      "Epoch: 1330/2000... Training loss: 0.3282\n",
      "Epoch: 1330/2000... Training loss: 0.5341\n",
      "Epoch: 1330/2000... Training loss: 0.4053\n",
      "Epoch: 1330/2000... Training loss: 0.4535\n",
      "Epoch: 1330/2000... Training loss: 0.5080\n",
      "Epoch: 1330/2000... Training loss: 0.4508\n",
      "Epoch: 1330/2000... Training loss: 0.5667\n",
      "Epoch: 1330/2000... Training loss: 0.4358\n",
      "Epoch: 1330/2000... Training loss: 0.6233\n",
      "Epoch: 1330/2000... Training loss: 0.5235\n",
      "Epoch: 1330/2000... Training loss: 0.5024\n",
      "Epoch: 1330/2000... Training loss: 0.4646\n",
      "Epoch: 1330/2000... Training loss: 0.5977\n",
      "Epoch: 1330/2000... Training loss: 0.4683\n",
      "Epoch: 1330/2000... Training loss: 0.2991\n",
      "Epoch: 1330/2000... Training loss: 0.5360\n",
      "Epoch: 1330/2000... Training loss: 0.4080\n",
      "Epoch: 1330/2000... Training loss: 0.6806\n",
      "Epoch: 1330/2000... Training loss: 0.5307\n",
      "Epoch: 1330/2000... Training loss: 0.3744\n",
      "Epoch: 1330/2000... Training loss: 0.5046\n",
      "Epoch: 1330/2000... Training loss: 0.3703\n",
      "Epoch: 1331/2000... Training loss: 0.4496\n",
      "Epoch: 1331/2000... Training loss: 0.2872\n",
      "Epoch: 1331/2000... Training loss: 0.4407\n",
      "Epoch: 1331/2000... Training loss: 0.4360\n",
      "Epoch: 1331/2000... Training loss: 0.5209\n",
      "Epoch: 1331/2000... Training loss: 0.2719\n",
      "Epoch: 1331/2000... Training loss: 0.4192\n",
      "Epoch: 1331/2000... Training loss: 0.4230\n",
      "Epoch: 1331/2000... Training loss: 0.4264\n",
      "Epoch: 1331/2000... Training loss: 0.4415\n",
      "Epoch: 1331/2000... Training loss: 0.4337\n",
      "Epoch: 1331/2000... Training loss: 0.4915\n",
      "Epoch: 1331/2000... Training loss: 0.4824\n",
      "Epoch: 1331/2000... Training loss: 0.3592\n",
      "Epoch: 1331/2000... Training loss: 0.4832\n",
      "Epoch: 1331/2000... Training loss: 0.3598\n",
      "Epoch: 1331/2000... Training loss: 0.4294\n",
      "Epoch: 1331/2000... Training loss: 0.4171\n",
      "Epoch: 1331/2000... Training loss: 0.3891\n",
      "Epoch: 1331/2000... Training loss: 0.5132\n",
      "Epoch: 1331/2000... Training loss: 0.4152\n",
      "Epoch: 1331/2000... Training loss: 0.4955\n",
      "Epoch: 1331/2000... Training loss: 0.5670\n",
      "Epoch: 1331/2000... Training loss: 0.4377\n",
      "Epoch: 1331/2000... Training loss: 0.3151\n",
      "Epoch: 1331/2000... Training loss: 0.5509\n",
      "Epoch: 1331/2000... Training loss: 0.5436\n",
      "Epoch: 1331/2000... Training loss: 0.3938\n",
      "Epoch: 1331/2000... Training loss: 0.6353\n",
      "Epoch: 1331/2000... Training loss: 0.5044\n",
      "Epoch: 1331/2000... Training loss: 0.3856\n",
      "Epoch: 1332/2000... Training loss: 0.5103\n",
      "Epoch: 1332/2000... Training loss: 0.4244\n",
      "Epoch: 1332/2000... Training loss: 0.3881\n",
      "Epoch: 1332/2000... Training loss: 0.3974\n",
      "Epoch: 1332/2000... Training loss: 0.4618\n",
      "Epoch: 1332/2000... Training loss: 0.6752\n",
      "Epoch: 1332/2000... Training loss: 0.5708\n",
      "Epoch: 1332/2000... Training loss: 0.3009\n",
      "Epoch: 1332/2000... Training loss: 0.2469\n",
      "Epoch: 1332/2000... Training loss: 0.4602\n",
      "Epoch: 1332/2000... Training loss: 0.3826\n",
      "Epoch: 1332/2000... Training loss: 0.2562\n",
      "Epoch: 1332/2000... Training loss: 0.4368\n",
      "Epoch: 1332/2000... Training loss: 0.4377\n",
      "Epoch: 1332/2000... Training loss: 0.4086\n",
      "Epoch: 1332/2000... Training loss: 0.2993\n",
      "Epoch: 1332/2000... Training loss: 0.4475\n",
      "Epoch: 1332/2000... Training loss: 0.5421\n",
      "Epoch: 1332/2000... Training loss: 0.5003\n",
      "Epoch: 1332/2000... Training loss: 0.3520\n",
      "Epoch: 1332/2000... Training loss: 0.2304\n",
      "Epoch: 1332/2000... Training loss: 0.3617\n",
      "Epoch: 1332/2000... Training loss: 0.3557\n",
      "Epoch: 1332/2000... Training loss: 0.4472\n",
      "Epoch: 1332/2000... Training loss: 0.4250\n",
      "Epoch: 1332/2000... Training loss: 0.3895\n",
      "Epoch: 1332/2000... Training loss: 0.2707\n",
      "Epoch: 1332/2000... Training loss: 0.4492\n",
      "Epoch: 1332/2000... Training loss: 0.5781\n",
      "Epoch: 1332/2000... Training loss: 0.4679\n",
      "Epoch: 1332/2000... Training loss: 0.5213\n",
      "Epoch: 1333/2000... Training loss: 0.5490\n",
      "Epoch: 1333/2000... Training loss: 0.5672\n",
      "Epoch: 1333/2000... Training loss: 0.3952\n",
      "Epoch: 1333/2000... Training loss: 0.3588\n",
      "Epoch: 1333/2000... Training loss: 0.4205\n",
      "Epoch: 1333/2000... Training loss: 0.5279\n",
      "Epoch: 1333/2000... Training loss: 0.5767\n",
      "Epoch: 1333/2000... Training loss: 0.4193\n",
      "Epoch: 1333/2000... Training loss: 0.5137\n",
      "Epoch: 1333/2000... Training loss: 0.4836\n",
      "Epoch: 1333/2000... Training loss: 0.3211\n",
      "Epoch: 1333/2000... Training loss: 0.3758\n",
      "Epoch: 1333/2000... Training loss: 0.3617\n",
      "Epoch: 1333/2000... Training loss: 0.4777\n",
      "Epoch: 1333/2000... Training loss: 0.4194\n",
      "Epoch: 1333/2000... Training loss: 0.3925\n",
      "Epoch: 1333/2000... Training loss: 0.3731\n",
      "Epoch: 1333/2000... Training loss: 0.5139\n",
      "Epoch: 1333/2000... Training loss: 0.4336\n",
      "Epoch: 1333/2000... Training loss: 0.3906\n",
      "Epoch: 1333/2000... Training loss: 0.3125\n",
      "Epoch: 1333/2000... Training loss: 0.4480\n",
      "Epoch: 1333/2000... Training loss: 0.4001\n",
      "Epoch: 1333/2000... Training loss: 0.5700\n",
      "Epoch: 1333/2000... Training loss: 0.3645\n",
      "Epoch: 1333/2000... Training loss: 0.4778\n",
      "Epoch: 1333/2000... Training loss: 0.4792\n",
      "Epoch: 1333/2000... Training loss: 0.3611\n",
      "Epoch: 1333/2000... Training loss: 0.4219\n",
      "Epoch: 1333/2000... Training loss: 0.4509\n",
      "Epoch: 1333/2000... Training loss: 0.4930\n",
      "Epoch: 1334/2000... Training loss: 0.5289\n",
      "Epoch: 1334/2000... Training loss: 0.4926\n",
      "Epoch: 1334/2000... Training loss: 0.4666\n",
      "Epoch: 1334/2000... Training loss: 0.3272\n",
      "Epoch: 1334/2000... Training loss: 0.3538\n",
      "Epoch: 1334/2000... Training loss: 0.3449\n",
      "Epoch: 1334/2000... Training loss: 0.4561\n",
      "Epoch: 1334/2000... Training loss: 0.5116\n",
      "Epoch: 1334/2000... Training loss: 0.6194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1334/2000... Training loss: 0.4791\n",
      "Epoch: 1334/2000... Training loss: 0.4427\n",
      "Epoch: 1334/2000... Training loss: 0.3553\n",
      "Epoch: 1334/2000... Training loss: 0.3560\n",
      "Epoch: 1334/2000... Training loss: 0.4503\n",
      "Epoch: 1334/2000... Training loss: 0.4835\n",
      "Epoch: 1334/2000... Training loss: 0.3221\n",
      "Epoch: 1334/2000... Training loss: 0.4833\n",
      "Epoch: 1334/2000... Training loss: 0.3819\n",
      "Epoch: 1334/2000... Training loss: 0.3754\n",
      "Epoch: 1334/2000... Training loss: 0.4059\n",
      "Epoch: 1334/2000... Training loss: 0.5229\n",
      "Epoch: 1334/2000... Training loss: 0.3577\n",
      "Epoch: 1334/2000... Training loss: 0.5231\n",
      "Epoch: 1334/2000... Training loss: 0.4382\n",
      "Epoch: 1334/2000... Training loss: 0.3991\n",
      "Epoch: 1334/2000... Training loss: 0.3275\n",
      "Epoch: 1334/2000... Training loss: 0.5011\n",
      "Epoch: 1334/2000... Training loss: 0.2993\n",
      "Epoch: 1334/2000... Training loss: 0.4384\n",
      "Epoch: 1334/2000... Training loss: 0.4999\n",
      "Epoch: 1334/2000... Training loss: 0.3535\n",
      "Epoch: 1335/2000... Training loss: 0.4455\n",
      "Epoch: 1335/2000... Training loss: 0.3612\n",
      "Epoch: 1335/2000... Training loss: 0.5668\n",
      "Epoch: 1335/2000... Training loss: 0.4741\n",
      "Epoch: 1335/2000... Training loss: 0.4166\n",
      "Epoch: 1335/2000... Training loss: 0.3401\n",
      "Epoch: 1335/2000... Training loss: 0.4371\n",
      "Epoch: 1335/2000... Training loss: 0.5335\n",
      "Epoch: 1335/2000... Training loss: 0.2445\n",
      "Epoch: 1335/2000... Training loss: 0.4410\n",
      "Epoch: 1335/2000... Training loss: 0.5773\n",
      "Epoch: 1335/2000... Training loss: 0.3027\n",
      "Epoch: 1335/2000... Training loss: 0.5161\n",
      "Epoch: 1335/2000... Training loss: 0.6754\n",
      "Epoch: 1335/2000... Training loss: 0.4398\n",
      "Epoch: 1335/2000... Training loss: 0.5886\n",
      "Epoch: 1335/2000... Training loss: 0.5275\n",
      "Epoch: 1335/2000... Training loss: 0.3982\n",
      "Epoch: 1335/2000... Training loss: 0.4416\n",
      "Epoch: 1335/2000... Training loss: 0.3752\n",
      "Epoch: 1335/2000... Training loss: 0.3787\n",
      "Epoch: 1335/2000... Training loss: 0.2578\n",
      "Epoch: 1335/2000... Training loss: 0.5271\n",
      "Epoch: 1335/2000... Training loss: 0.5951\n",
      "Epoch: 1335/2000... Training loss: 0.4412\n",
      "Epoch: 1335/2000... Training loss: 0.5358\n",
      "Epoch: 1335/2000... Training loss: 0.4366\n",
      "Epoch: 1335/2000... Training loss: 0.3372\n",
      "Epoch: 1335/2000... Training loss: 0.2818\n",
      "Epoch: 1335/2000... Training loss: 0.4001\n",
      "Epoch: 1335/2000... Training loss: 0.3142\n",
      "Epoch: 1336/2000... Training loss: 0.3670\n",
      "Epoch: 1336/2000... Training loss: 0.3345\n",
      "Epoch: 1336/2000... Training loss: 0.4312\n",
      "Epoch: 1336/2000... Training loss: 0.4162\n",
      "Epoch: 1336/2000... Training loss: 0.5193\n",
      "Epoch: 1336/2000... Training loss: 0.5656\n",
      "Epoch: 1336/2000... Training loss: 0.3376\n",
      "Epoch: 1336/2000... Training loss: 0.3686\n",
      "Epoch: 1336/2000... Training loss: 0.7198\n",
      "Epoch: 1336/2000... Training loss: 0.4462\n",
      "Epoch: 1336/2000... Training loss: 0.4279\n",
      "Epoch: 1336/2000... Training loss: 0.4059\n",
      "Epoch: 1336/2000... Training loss: 0.3664\n",
      "Epoch: 1336/2000... Training loss: 0.4404\n",
      "Epoch: 1336/2000... Training loss: 0.2717\n",
      "Epoch: 1336/2000... Training loss: 0.3546\n",
      "Epoch: 1336/2000... Training loss: 0.2458\n",
      "Epoch: 1336/2000... Training loss: 0.4011\n",
      "Epoch: 1336/2000... Training loss: 0.4122\n",
      "Epoch: 1336/2000... Training loss: 0.3710\n",
      "Epoch: 1336/2000... Training loss: 0.3217\n",
      "Epoch: 1336/2000... Training loss: 0.5130\n",
      "Epoch: 1336/2000... Training loss: 0.5101\n",
      "Epoch: 1336/2000... Training loss: 0.4556\n",
      "Epoch: 1336/2000... Training loss: 0.5059\n",
      "Epoch: 1336/2000... Training loss: 0.3955\n",
      "Epoch: 1336/2000... Training loss: 0.3313\n",
      "Epoch: 1336/2000... Training loss: 0.3158\n",
      "Epoch: 1336/2000... Training loss: 0.4488\n",
      "Epoch: 1336/2000... Training loss: 0.6032\n",
      "Epoch: 1336/2000... Training loss: 0.5081\n",
      "Epoch: 1337/2000... Training loss: 0.3972\n",
      "Epoch: 1337/2000... Training loss: 0.3845\n",
      "Epoch: 1337/2000... Training loss: 0.3394\n",
      "Epoch: 1337/2000... Training loss: 0.3555\n",
      "Epoch: 1337/2000... Training loss: 0.4509\n",
      "Epoch: 1337/2000... Training loss: 0.3693\n",
      "Epoch: 1337/2000... Training loss: 0.5441\n",
      "Epoch: 1337/2000... Training loss: 0.3510\n",
      "Epoch: 1337/2000... Training loss: 0.3223\n",
      "Epoch: 1337/2000... Training loss: 0.5574\n",
      "Epoch: 1337/2000... Training loss: 0.3133\n",
      "Epoch: 1337/2000... Training loss: 0.4055\n",
      "Epoch: 1337/2000... Training loss: 0.6518\n",
      "Epoch: 1337/2000... Training loss: 0.3869\n",
      "Epoch: 1337/2000... Training loss: 0.4544\n",
      "Epoch: 1337/2000... Training loss: 0.3183\n",
      "Epoch: 1337/2000... Training loss: 0.4535\n",
      "Epoch: 1337/2000... Training loss: 0.4241\n",
      "Epoch: 1337/2000... Training loss: 0.3065\n",
      "Epoch: 1337/2000... Training loss: 0.5600\n",
      "Epoch: 1337/2000... Training loss: 0.4429\n",
      "Epoch: 1337/2000... Training loss: 0.4331\n",
      "Epoch: 1337/2000... Training loss: 0.4502\n",
      "Epoch: 1337/2000... Training loss: 0.3098\n",
      "Epoch: 1337/2000... Training loss: 0.4990\n",
      "Epoch: 1337/2000... Training loss: 0.5645\n",
      "Epoch: 1337/2000... Training loss: 0.5026\n",
      "Epoch: 1337/2000... Training loss: 0.4715\n",
      "Epoch: 1337/2000... Training loss: 0.3541\n",
      "Epoch: 1337/2000... Training loss: 0.3560\n",
      "Epoch: 1337/2000... Training loss: 0.4073\n",
      "Epoch: 1338/2000... Training loss: 0.4596\n",
      "Epoch: 1338/2000... Training loss: 0.3072\n",
      "Epoch: 1338/2000... Training loss: 0.3341\n",
      "Epoch: 1338/2000... Training loss: 0.4174\n",
      "Epoch: 1338/2000... Training loss: 0.5917\n",
      "Epoch: 1338/2000... Training loss: 0.3822\n",
      "Epoch: 1338/2000... Training loss: 0.2085\n",
      "Epoch: 1338/2000... Training loss: 0.3116\n",
      "Epoch: 1338/2000... Training loss: 0.4441\n",
      "Epoch: 1338/2000... Training loss: 0.2937\n",
      "Epoch: 1338/2000... Training loss: 0.3842\n",
      "Epoch: 1338/2000... Training loss: 0.3700\n",
      "Epoch: 1338/2000... Training loss: 0.5416\n",
      "Epoch: 1338/2000... Training loss: 0.4719\n",
      "Epoch: 1338/2000... Training loss: 0.3442\n",
      "Epoch: 1338/2000... Training loss: 0.3720\n",
      "Epoch: 1338/2000... Training loss: 0.3085\n",
      "Epoch: 1338/2000... Training loss: 0.3466\n",
      "Epoch: 1338/2000... Training loss: 0.6420\n",
      "Epoch: 1338/2000... Training loss: 0.4094\n",
      "Epoch: 1338/2000... Training loss: 0.5914\n",
      "Epoch: 1338/2000... Training loss: 0.4106\n",
      "Epoch: 1338/2000... Training loss: 0.4484\n",
      "Epoch: 1338/2000... Training loss: 0.3589\n",
      "Epoch: 1338/2000... Training loss: 0.3489\n",
      "Epoch: 1338/2000... Training loss: 0.4265\n",
      "Epoch: 1338/2000... Training loss: 0.5379\n",
      "Epoch: 1338/2000... Training loss: 0.4871\n",
      "Epoch: 1338/2000... Training loss: 0.4365\n",
      "Epoch: 1338/2000... Training loss: 0.5038\n",
      "Epoch: 1338/2000... Training loss: 0.3638\n",
      "Epoch: 1339/2000... Training loss: 0.4076\n",
      "Epoch: 1339/2000... Training loss: 0.5464\n",
      "Epoch: 1339/2000... Training loss: 0.4832\n",
      "Epoch: 1339/2000... Training loss: 0.2822\n",
      "Epoch: 1339/2000... Training loss: 0.4143\n",
      "Epoch: 1339/2000... Training loss: 0.4333\n",
      "Epoch: 1339/2000... Training loss: 0.2954\n",
      "Epoch: 1339/2000... Training loss: 0.6397\n",
      "Epoch: 1339/2000... Training loss: 0.2710\n",
      "Epoch: 1339/2000... Training loss: 0.4632\n",
      "Epoch: 1339/2000... Training loss: 0.4298\n",
      "Epoch: 1339/2000... Training loss: 0.3801\n",
      "Epoch: 1339/2000... Training loss: 0.3174\n",
      "Epoch: 1339/2000... Training loss: 0.4472\n",
      "Epoch: 1339/2000... Training loss: 0.6424\n",
      "Epoch: 1339/2000... Training loss: 0.5351\n",
      "Epoch: 1339/2000... Training loss: 0.3782\n",
      "Epoch: 1339/2000... Training loss: 0.3349\n",
      "Epoch: 1339/2000... Training loss: 0.4838\n",
      "Epoch: 1339/2000... Training loss: 0.4314\n",
      "Epoch: 1339/2000... Training loss: 0.3911\n",
      "Epoch: 1339/2000... Training loss: 0.2955\n",
      "Epoch: 1339/2000... Training loss: 0.5051\n",
      "Epoch: 1339/2000... Training loss: 0.4489\n",
      "Epoch: 1339/2000... Training loss: 0.6168\n",
      "Epoch: 1339/2000... Training loss: 0.3222\n",
      "Epoch: 1339/2000... Training loss: 0.3173\n",
      "Epoch: 1339/2000... Training loss: 0.3712\n",
      "Epoch: 1339/2000... Training loss: 0.3357\n",
      "Epoch: 1339/2000... Training loss: 0.3457\n",
      "Epoch: 1339/2000... Training loss: 0.4288\n",
      "Epoch: 1340/2000... Training loss: 0.5331\n",
      "Epoch: 1340/2000... Training loss: 0.2849\n",
      "Epoch: 1340/2000... Training loss: 0.5721\n",
      "Epoch: 1340/2000... Training loss: 0.3796\n",
      "Epoch: 1340/2000... Training loss: 0.3596\n",
      "Epoch: 1340/2000... Training loss: 0.3190\n",
      "Epoch: 1340/2000... Training loss: 0.4103\n",
      "Epoch: 1340/2000... Training loss: 0.4046\n",
      "Epoch: 1340/2000... Training loss: 0.3361\n",
      "Epoch: 1340/2000... Training loss: 0.4898\n",
      "Epoch: 1340/2000... Training loss: 0.3934\n",
      "Epoch: 1340/2000... Training loss: 0.5983\n",
      "Epoch: 1340/2000... Training loss: 0.5420\n",
      "Epoch: 1340/2000... Training loss: 0.5314\n",
      "Epoch: 1340/2000... Training loss: 0.4049\n",
      "Epoch: 1340/2000... Training loss: 0.4386\n",
      "Epoch: 1340/2000... Training loss: 0.3654\n",
      "Epoch: 1340/2000... Training loss: 0.4522\n",
      "Epoch: 1340/2000... Training loss: 0.5576\n",
      "Epoch: 1340/2000... Training loss: 0.4842\n",
      "Epoch: 1340/2000... Training loss: 0.4261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1340/2000... Training loss: 0.3732\n",
      "Epoch: 1340/2000... Training loss: 0.5271\n",
      "Epoch: 1340/2000... Training loss: 0.6029\n",
      "Epoch: 1340/2000... Training loss: 0.5215\n",
      "Epoch: 1340/2000... Training loss: 0.4843\n",
      "Epoch: 1340/2000... Training loss: 0.4687\n",
      "Epoch: 1340/2000... Training loss: 0.5271\n",
      "Epoch: 1340/2000... Training loss: 0.3325\n",
      "Epoch: 1340/2000... Training loss: 0.6178\n",
      "Epoch: 1340/2000... Training loss: 0.4129\n",
      "Epoch: 1341/2000... Training loss: 0.6204\n",
      "Epoch: 1341/2000... Training loss: 0.3716\n",
      "Epoch: 1341/2000... Training loss: 0.5899\n",
      "Epoch: 1341/2000... Training loss: 0.4324\n",
      "Epoch: 1341/2000... Training loss: 0.3582\n",
      "Epoch: 1341/2000... Training loss: 0.5370\n",
      "Epoch: 1341/2000... Training loss: 0.2975\n",
      "Epoch: 1341/2000... Training loss: 0.3932\n",
      "Epoch: 1341/2000... Training loss: 0.2855\n",
      "Epoch: 1341/2000... Training loss: 0.5945\n",
      "Epoch: 1341/2000... Training loss: 0.4039\n",
      "Epoch: 1341/2000... Training loss: 0.3092\n",
      "Epoch: 1341/2000... Training loss: 0.2829\n",
      "Epoch: 1341/2000... Training loss: 0.3232\n",
      "Epoch: 1341/2000... Training loss: 0.3094\n",
      "Epoch: 1341/2000... Training loss: 0.4964\n",
      "Epoch: 1341/2000... Training loss: 0.4856\n",
      "Epoch: 1341/2000... Training loss: 0.3543\n",
      "Epoch: 1341/2000... Training loss: 0.1730\n",
      "Epoch: 1341/2000... Training loss: 0.5457\n",
      "Epoch: 1341/2000... Training loss: 0.3082\n",
      "Epoch: 1341/2000... Training loss: 0.3779\n",
      "Epoch: 1341/2000... Training loss: 0.4330\n",
      "Epoch: 1341/2000... Training loss: 0.4577\n",
      "Epoch: 1341/2000... Training loss: 0.3708\n",
      "Epoch: 1341/2000... Training loss: 0.4436\n",
      "Epoch: 1341/2000... Training loss: 0.5674\n",
      "Epoch: 1341/2000... Training loss: 0.5975\n",
      "Epoch: 1341/2000... Training loss: 0.4243\n",
      "Epoch: 1341/2000... Training loss: 0.2895\n",
      "Epoch: 1341/2000... Training loss: 0.4650\n",
      "Epoch: 1342/2000... Training loss: 0.3043\n",
      "Epoch: 1342/2000... Training loss: 0.3818\n",
      "Epoch: 1342/2000... Training loss: 0.4546\n",
      "Epoch: 1342/2000... Training loss: 0.2832\n",
      "Epoch: 1342/2000... Training loss: 0.4148\n",
      "Epoch: 1342/2000... Training loss: 0.5278\n",
      "Epoch: 1342/2000... Training loss: 0.3972\n",
      "Epoch: 1342/2000... Training loss: 0.5135\n",
      "Epoch: 1342/2000... Training loss: 0.3495\n",
      "Epoch: 1342/2000... Training loss: 0.3004\n",
      "Epoch: 1342/2000... Training loss: 0.4982\n",
      "Epoch: 1342/2000... Training loss: 0.3418\n",
      "Epoch: 1342/2000... Training loss: 0.2990\n",
      "Epoch: 1342/2000... Training loss: 0.4266\n",
      "Epoch: 1342/2000... Training loss: 0.4001\n",
      "Epoch: 1342/2000... Training loss: 0.3626\n",
      "Epoch: 1342/2000... Training loss: 0.5002\n",
      "Epoch: 1342/2000... Training loss: 0.3641\n",
      "Epoch: 1342/2000... Training loss: 0.2843\n",
      "Epoch: 1342/2000... Training loss: 0.4956\n",
      "Epoch: 1342/2000... Training loss: 0.7049\n",
      "Epoch: 1342/2000... Training loss: 0.4843\n",
      "Epoch: 1342/2000... Training loss: 0.4109\n",
      "Epoch: 1342/2000... Training loss: 0.3763\n",
      "Epoch: 1342/2000... Training loss: 0.3008\n",
      "Epoch: 1342/2000... Training loss: 0.5268\n",
      "Epoch: 1342/2000... Training loss: 0.4725\n",
      "Epoch: 1342/2000... Training loss: 0.5587\n",
      "Epoch: 1342/2000... Training loss: 0.6913\n",
      "Epoch: 1342/2000... Training loss: 0.4381\n",
      "Epoch: 1342/2000... Training loss: 0.4967\n",
      "Epoch: 1343/2000... Training loss: 0.5062\n",
      "Epoch: 1343/2000... Training loss: 0.3588\n",
      "Epoch: 1343/2000... Training loss: 0.3204\n",
      "Epoch: 1343/2000... Training loss: 0.4108\n",
      "Epoch: 1343/2000... Training loss: 0.2313\n",
      "Epoch: 1343/2000... Training loss: 0.2604\n",
      "Epoch: 1343/2000... Training loss: 0.3265\n",
      "Epoch: 1343/2000... Training loss: 0.5795\n",
      "Epoch: 1343/2000... Training loss: 0.3392\n",
      "Epoch: 1343/2000... Training loss: 0.5354\n",
      "Epoch: 1343/2000... Training loss: 0.4397\n",
      "Epoch: 1343/2000... Training loss: 0.5669\n",
      "Epoch: 1343/2000... Training loss: 0.4221\n",
      "Epoch: 1343/2000... Training loss: 0.3501\n",
      "Epoch: 1343/2000... Training loss: 0.3714\n",
      "Epoch: 1343/2000... Training loss: 0.3776\n",
      "Epoch: 1343/2000... Training loss: 0.4970\n",
      "Epoch: 1343/2000... Training loss: 0.6186\n",
      "Epoch: 1343/2000... Training loss: 0.4035\n",
      "Epoch: 1343/2000... Training loss: 0.4326\n",
      "Epoch: 1343/2000... Training loss: 0.3899\n",
      "Epoch: 1343/2000... Training loss: 0.3780\n",
      "Epoch: 1343/2000... Training loss: 0.5473\n",
      "Epoch: 1343/2000... Training loss: 0.5947\n",
      "Epoch: 1343/2000... Training loss: 0.3801\n",
      "Epoch: 1343/2000... Training loss: 0.3107\n",
      "Epoch: 1343/2000... Training loss: 0.3977\n",
      "Epoch: 1343/2000... Training loss: 0.4499\n",
      "Epoch: 1343/2000... Training loss: 0.4519\n",
      "Epoch: 1343/2000... Training loss: 0.4524\n",
      "Epoch: 1343/2000... Training loss: 0.3288\n",
      "Epoch: 1344/2000... Training loss: 0.5013\n",
      "Epoch: 1344/2000... Training loss: 0.5530\n",
      "Epoch: 1344/2000... Training loss: 0.4418\n",
      "Epoch: 1344/2000... Training loss: 0.4907\n",
      "Epoch: 1344/2000... Training loss: 0.6134\n",
      "Epoch: 1344/2000... Training loss: 0.3570\n",
      "Epoch: 1344/2000... Training loss: 0.4489\n",
      "Epoch: 1344/2000... Training loss: 0.5947\n",
      "Epoch: 1344/2000... Training loss: 0.4612\n",
      "Epoch: 1344/2000... Training loss: 0.4489\n",
      "Epoch: 1344/2000... Training loss: 0.3447\n",
      "Epoch: 1344/2000... Training loss: 0.3720\n",
      "Epoch: 1344/2000... Training loss: 0.5835\n",
      "Epoch: 1344/2000... Training loss: 0.5003\n",
      "Epoch: 1344/2000... Training loss: 0.3539\n",
      "Epoch: 1344/2000... Training loss: 0.3294\n",
      "Epoch: 1344/2000... Training loss: 0.5080\n",
      "Epoch: 1344/2000... Training loss: 0.4880\n",
      "Epoch: 1344/2000... Training loss: 0.3274\n",
      "Epoch: 1344/2000... Training loss: 0.4761\n",
      "Epoch: 1344/2000... Training loss: 0.2959\n",
      "Epoch: 1344/2000... Training loss: 0.5525\n",
      "Epoch: 1344/2000... Training loss: 0.4392\n",
      "Epoch: 1344/2000... Training loss: 0.3959\n",
      "Epoch: 1344/2000... Training loss: 0.3389\n",
      "Epoch: 1344/2000... Training loss: 0.4635\n",
      "Epoch: 1344/2000... Training loss: 0.4990\n",
      "Epoch: 1344/2000... Training loss: 0.4566\n",
      "Epoch: 1344/2000... Training loss: 0.4538\n",
      "Epoch: 1344/2000... Training loss: 0.5120\n",
      "Epoch: 1344/2000... Training loss: 0.3861\n",
      "Epoch: 1345/2000... Training loss: 0.4928\n",
      "Epoch: 1345/2000... Training loss: 0.5550\n",
      "Epoch: 1345/2000... Training loss: 0.3311\n",
      "Epoch: 1345/2000... Training loss: 0.3111\n",
      "Epoch: 1345/2000... Training loss: 0.2988\n",
      "Epoch: 1345/2000... Training loss: 0.5523\n",
      "Epoch: 1345/2000... Training loss: 0.3863\n",
      "Epoch: 1345/2000... Training loss: 0.3181\n",
      "Epoch: 1345/2000... Training loss: 0.3772\n",
      "Epoch: 1345/2000... Training loss: 0.2683\n",
      "Epoch: 1345/2000... Training loss: 0.3862\n",
      "Epoch: 1345/2000... Training loss: 0.3290\n",
      "Epoch: 1345/2000... Training loss: 0.3196\n",
      "Epoch: 1345/2000... Training loss: 0.3124\n",
      "Epoch: 1345/2000... Training loss: 0.4493\n",
      "Epoch: 1345/2000... Training loss: 0.4736\n",
      "Epoch: 1345/2000... Training loss: 0.5023\n",
      "Epoch: 1345/2000... Training loss: 0.4100\n",
      "Epoch: 1345/2000... Training loss: 0.4013\n",
      "Epoch: 1345/2000... Training loss: 0.3116\n",
      "Epoch: 1345/2000... Training loss: 0.4329\n",
      "Epoch: 1345/2000... Training loss: 0.4941\n",
      "Epoch: 1345/2000... Training loss: 0.3990\n",
      "Epoch: 1345/2000... Training loss: 0.3389\n",
      "Epoch: 1345/2000... Training loss: 0.3590\n",
      "Epoch: 1345/2000... Training loss: 0.4246\n",
      "Epoch: 1345/2000... Training loss: 0.3890\n",
      "Epoch: 1345/2000... Training loss: 0.5287\n",
      "Epoch: 1345/2000... Training loss: 0.3708\n",
      "Epoch: 1345/2000... Training loss: 0.3698\n",
      "Epoch: 1345/2000... Training loss: 0.3734\n",
      "Epoch: 1346/2000... Training loss: 0.4391\n",
      "Epoch: 1346/2000... Training loss: 0.4546\n",
      "Epoch: 1346/2000... Training loss: 0.5148\n",
      "Epoch: 1346/2000... Training loss: 0.6914\n",
      "Epoch: 1346/2000... Training loss: 0.5215\n",
      "Epoch: 1346/2000... Training loss: 0.3809\n",
      "Epoch: 1346/2000... Training loss: 0.2989\n",
      "Epoch: 1346/2000... Training loss: 0.3357\n",
      "Epoch: 1346/2000... Training loss: 0.3279\n",
      "Epoch: 1346/2000... Training loss: 0.4411\n",
      "Epoch: 1346/2000... Training loss: 0.4624\n",
      "Epoch: 1346/2000... Training loss: 0.3572\n",
      "Epoch: 1346/2000... Training loss: 0.4257\n",
      "Epoch: 1346/2000... Training loss: 0.5694\n",
      "Epoch: 1346/2000... Training loss: 0.3627\n",
      "Epoch: 1346/2000... Training loss: 0.4135\n",
      "Epoch: 1346/2000... Training loss: 0.5153\n",
      "Epoch: 1346/2000... Training loss: 0.5702\n",
      "Epoch: 1346/2000... Training loss: 0.3684\n",
      "Epoch: 1346/2000... Training loss: 0.2897\n",
      "Epoch: 1346/2000... Training loss: 0.4374\n",
      "Epoch: 1346/2000... Training loss: 0.4030\n",
      "Epoch: 1346/2000... Training loss: 0.4020\n",
      "Epoch: 1346/2000... Training loss: 0.8098\n",
      "Epoch: 1346/2000... Training loss: 0.4894\n",
      "Epoch: 1346/2000... Training loss: 0.5387\n",
      "Epoch: 1346/2000... Training loss: 0.4880\n",
      "Epoch: 1346/2000... Training loss: 0.4470\n",
      "Epoch: 1346/2000... Training loss: 0.4442\n",
      "Epoch: 1346/2000... Training loss: 0.3304\n",
      "Epoch: 1346/2000... Training loss: 0.5894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1347/2000... Training loss: 0.2916\n",
      "Epoch: 1347/2000... Training loss: 0.4936\n",
      "Epoch: 1347/2000... Training loss: 0.3839\n",
      "Epoch: 1347/2000... Training loss: 0.3437\n",
      "Epoch: 1347/2000... Training loss: 0.6184\n",
      "Epoch: 1347/2000... Training loss: 0.4267\n",
      "Epoch: 1347/2000... Training loss: 0.2854\n",
      "Epoch: 1347/2000... Training loss: 0.4121\n",
      "Epoch: 1347/2000... Training loss: 0.6315\n",
      "Epoch: 1347/2000... Training loss: 0.4373\n",
      "Epoch: 1347/2000... Training loss: 0.4543\n",
      "Epoch: 1347/2000... Training loss: 0.5561\n",
      "Epoch: 1347/2000... Training loss: 0.5650\n",
      "Epoch: 1347/2000... Training loss: 0.3752\n",
      "Epoch: 1347/2000... Training loss: 0.4408\n",
      "Epoch: 1347/2000... Training loss: 0.4186\n",
      "Epoch: 1347/2000... Training loss: 0.4313\n",
      "Epoch: 1347/2000... Training loss: 0.3525\n",
      "Epoch: 1347/2000... Training loss: 0.3647\n",
      "Epoch: 1347/2000... Training loss: 0.5483\n",
      "Epoch: 1347/2000... Training loss: 0.4054\n",
      "Epoch: 1347/2000... Training loss: 0.6023\n",
      "Epoch: 1347/2000... Training loss: 0.5375\n",
      "Epoch: 1347/2000... Training loss: 0.4014\n",
      "Epoch: 1347/2000... Training loss: 0.5449\n",
      "Epoch: 1347/2000... Training loss: 0.5089\n",
      "Epoch: 1347/2000... Training loss: 0.4959\n",
      "Epoch: 1347/2000... Training loss: 0.5249\n",
      "Epoch: 1347/2000... Training loss: 0.4935\n",
      "Epoch: 1347/2000... Training loss: 0.4833\n",
      "Epoch: 1347/2000... Training loss: 0.5350\n",
      "Epoch: 1348/2000... Training loss: 0.3629\n",
      "Epoch: 1348/2000... Training loss: 0.6579\n",
      "Epoch: 1348/2000... Training loss: 0.3024\n",
      "Epoch: 1348/2000... Training loss: 0.6169\n",
      "Epoch: 1348/2000... Training loss: 0.2957\n",
      "Epoch: 1348/2000... Training loss: 0.5610\n",
      "Epoch: 1348/2000... Training loss: 0.5808\n",
      "Epoch: 1348/2000... Training loss: 0.3419\n",
      "Epoch: 1348/2000... Training loss: 0.4379\n",
      "Epoch: 1348/2000... Training loss: 0.4987\n",
      "Epoch: 1348/2000... Training loss: 0.5437\n",
      "Epoch: 1348/2000... Training loss: 0.4705\n",
      "Epoch: 1348/2000... Training loss: 0.4134\n",
      "Epoch: 1348/2000... Training loss: 0.4854\n",
      "Epoch: 1348/2000... Training loss: 0.3263\n",
      "Epoch: 1348/2000... Training loss: 0.3858\n",
      "Epoch: 1348/2000... Training loss: 0.3213\n",
      "Epoch: 1348/2000... Training loss: 0.4532\n",
      "Epoch: 1348/2000... Training loss: 0.3917\n",
      "Epoch: 1348/2000... Training loss: 0.3296\n",
      "Epoch: 1348/2000... Training loss: 0.3956\n",
      "Epoch: 1348/2000... Training loss: 0.4332\n",
      "Epoch: 1348/2000... Training loss: 0.3748\n",
      "Epoch: 1348/2000... Training loss: 0.5456\n",
      "Epoch: 1348/2000... Training loss: 0.5178\n",
      "Epoch: 1348/2000... Training loss: 0.4831\n",
      "Epoch: 1348/2000... Training loss: 0.4032\n",
      "Epoch: 1348/2000... Training loss: 0.3627\n",
      "Epoch: 1348/2000... Training loss: 0.4208\n",
      "Epoch: 1348/2000... Training loss: 0.5275\n",
      "Epoch: 1348/2000... Training loss: 0.3375\n",
      "Epoch: 1349/2000... Training loss: 0.3327\n",
      "Epoch: 1349/2000... Training loss: 0.4291\n",
      "Epoch: 1349/2000... Training loss: 0.5820\n",
      "Epoch: 1349/2000... Training loss: 0.4784\n",
      "Epoch: 1349/2000... Training loss: 0.5801\n",
      "Epoch: 1349/2000... Training loss: 0.4101\n",
      "Epoch: 1349/2000... Training loss: 0.6123\n",
      "Epoch: 1349/2000... Training loss: 0.4224\n",
      "Epoch: 1349/2000... Training loss: 0.4606\n",
      "Epoch: 1349/2000... Training loss: 0.4942\n",
      "Epoch: 1349/2000... Training loss: 0.5410\n",
      "Epoch: 1349/2000... Training loss: 0.6359\n",
      "Epoch: 1349/2000... Training loss: 0.4569\n",
      "Epoch: 1349/2000... Training loss: 0.2950\n",
      "Epoch: 1349/2000... Training loss: 0.5144\n",
      "Epoch: 1349/2000... Training loss: 0.3305\n",
      "Epoch: 1349/2000... Training loss: 0.5840\n",
      "Epoch: 1349/2000... Training loss: 0.3411\n",
      "Epoch: 1349/2000... Training loss: 0.2688\n",
      "Epoch: 1349/2000... Training loss: 0.2429\n",
      "Epoch: 1349/2000... Training loss: 0.2948\n",
      "Epoch: 1349/2000... Training loss: 0.3716\n",
      "Epoch: 1349/2000... Training loss: 0.2816\n",
      "Epoch: 1349/2000... Training loss: 0.4806\n",
      "Epoch: 1349/2000... Training loss: 0.4342\n",
      "Epoch: 1349/2000... Training loss: 0.5533\n",
      "Epoch: 1349/2000... Training loss: 0.4584\n",
      "Epoch: 1349/2000... Training loss: 0.3277\n",
      "Epoch: 1349/2000... Training loss: 0.7146\n",
      "Epoch: 1349/2000... Training loss: 0.4535\n",
      "Epoch: 1349/2000... Training loss: 0.5172\n",
      "Epoch: 1350/2000... Training loss: 0.5149\n",
      "Epoch: 1350/2000... Training loss: 0.3335\n",
      "Epoch: 1350/2000... Training loss: 0.5295\n",
      "Epoch: 1350/2000... Training loss: 0.6068\n",
      "Epoch: 1350/2000... Training loss: 0.4495\n",
      "Epoch: 1350/2000... Training loss: 0.4861\n",
      "Epoch: 1350/2000... Training loss: 0.5239\n",
      "Epoch: 1350/2000... Training loss: 0.4009\n",
      "Epoch: 1350/2000... Training loss: 0.5266\n",
      "Epoch: 1350/2000... Training loss: 0.4620\n",
      "Epoch: 1350/2000... Training loss: 0.4238\n",
      "Epoch: 1350/2000... Training loss: 0.4046\n",
      "Epoch: 1350/2000... Training loss: 0.3408\n",
      "Epoch: 1350/2000... Training loss: 0.5657\n",
      "Epoch: 1350/2000... Training loss: 0.3332\n",
      "Epoch: 1350/2000... Training loss: 0.5269\n",
      "Epoch: 1350/2000... Training loss: 0.2642\n",
      "Epoch: 1350/2000... Training loss: 0.4478\n",
      "Epoch: 1350/2000... Training loss: 0.5208\n",
      "Epoch: 1350/2000... Training loss: 0.5843\n",
      "Epoch: 1350/2000... Training loss: 0.4132\n",
      "Epoch: 1350/2000... Training loss: 0.4606\n",
      "Epoch: 1350/2000... Training loss: 0.2096\n",
      "Epoch: 1350/2000... Training loss: 0.5210\n",
      "Epoch: 1350/2000... Training loss: 0.5830\n",
      "Epoch: 1350/2000... Training loss: 0.4629\n",
      "Epoch: 1350/2000... Training loss: 0.3663\n",
      "Epoch: 1350/2000... Training loss: 0.2736\n",
      "Epoch: 1350/2000... Training loss: 0.5039\n",
      "Epoch: 1350/2000... Training loss: 0.4693\n",
      "Epoch: 1350/2000... Training loss: 0.2525\n",
      "Epoch: 1351/2000... Training loss: 0.5972\n",
      "Epoch: 1351/2000... Training loss: 0.4012\n",
      "Epoch: 1351/2000... Training loss: 0.3559\n",
      "Epoch: 1351/2000... Training loss: 0.3708\n",
      "Epoch: 1351/2000... Training loss: 0.2985\n",
      "Epoch: 1351/2000... Training loss: 0.4720\n",
      "Epoch: 1351/2000... Training loss: 0.6113\n",
      "Epoch: 1351/2000... Training loss: 0.5009\n",
      "Epoch: 1351/2000... Training loss: 0.3966\n",
      "Epoch: 1351/2000... Training loss: 0.4306\n",
      "Epoch: 1351/2000... Training loss: 0.4351\n",
      "Epoch: 1351/2000... Training loss: 0.3226\n",
      "Epoch: 1351/2000... Training loss: 0.4345\n",
      "Epoch: 1351/2000... Training loss: 0.4270\n",
      "Epoch: 1351/2000... Training loss: 0.4686\n",
      "Epoch: 1351/2000... Training loss: 0.4545\n",
      "Epoch: 1351/2000... Training loss: 0.3612\n",
      "Epoch: 1351/2000... Training loss: 0.2961\n",
      "Epoch: 1351/2000... Training loss: 0.2432\n",
      "Epoch: 1351/2000... Training loss: 0.3952\n",
      "Epoch: 1351/2000... Training loss: 0.2801\n",
      "Epoch: 1351/2000... Training loss: 0.4173\n",
      "Epoch: 1351/2000... Training loss: 0.5997\n",
      "Epoch: 1351/2000... Training loss: 0.4562\n",
      "Epoch: 1351/2000... Training loss: 0.6490\n",
      "Epoch: 1351/2000... Training loss: 0.3874\n",
      "Epoch: 1351/2000... Training loss: 0.2689\n",
      "Epoch: 1351/2000... Training loss: 0.5330\n",
      "Epoch: 1351/2000... Training loss: 0.6191\n",
      "Epoch: 1351/2000... Training loss: 0.5031\n",
      "Epoch: 1351/2000... Training loss: 0.4332\n",
      "Epoch: 1352/2000... Training loss: 0.4453\n",
      "Epoch: 1352/2000... Training loss: 0.3407\n",
      "Epoch: 1352/2000... Training loss: 0.4551\n",
      "Epoch: 1352/2000... Training loss: 0.4069\n",
      "Epoch: 1352/2000... Training loss: 0.4054\n",
      "Epoch: 1352/2000... Training loss: 0.3612\n",
      "Epoch: 1352/2000... Training loss: 0.2405\n",
      "Epoch: 1352/2000... Training loss: 0.4088\n",
      "Epoch: 1352/2000... Training loss: 0.6073\n",
      "Epoch: 1352/2000... Training loss: 0.4464\n",
      "Epoch: 1352/2000... Training loss: 0.3808\n",
      "Epoch: 1352/2000... Training loss: 0.4326\n",
      "Epoch: 1352/2000... Training loss: 0.5098\n",
      "Epoch: 1352/2000... Training loss: 0.4699\n",
      "Epoch: 1352/2000... Training loss: 0.4267\n",
      "Epoch: 1352/2000... Training loss: 0.3269\n",
      "Epoch: 1352/2000... Training loss: 0.3784\n",
      "Epoch: 1352/2000... Training loss: 0.4595\n",
      "Epoch: 1352/2000... Training loss: 0.3812\n",
      "Epoch: 1352/2000... Training loss: 0.3721\n",
      "Epoch: 1352/2000... Training loss: 0.5010\n",
      "Epoch: 1352/2000... Training loss: 0.3173\n",
      "Epoch: 1352/2000... Training loss: 0.3937\n",
      "Epoch: 1352/2000... Training loss: 0.3985\n",
      "Epoch: 1352/2000... Training loss: 0.3540\n",
      "Epoch: 1352/2000... Training loss: 0.4319\n",
      "Epoch: 1352/2000... Training loss: 0.4003\n",
      "Epoch: 1352/2000... Training loss: 0.4147\n",
      "Epoch: 1352/2000... Training loss: 0.4067\n",
      "Epoch: 1352/2000... Training loss: 0.3546\n",
      "Epoch: 1352/2000... Training loss: 0.4223\n",
      "Epoch: 1353/2000... Training loss: 0.4820\n",
      "Epoch: 1353/2000... Training loss: 0.5451\n",
      "Epoch: 1353/2000... Training loss: 0.3920\n",
      "Epoch: 1353/2000... Training loss: 0.3500\n",
      "Epoch: 1353/2000... Training loss: 0.3523\n",
      "Epoch: 1353/2000... Training loss: 0.4369\n",
      "Epoch: 1353/2000... Training loss: 0.2726\n",
      "Epoch: 1353/2000... Training loss: 0.4696\n",
      "Epoch: 1353/2000... Training loss: 0.3719\n",
      "Epoch: 1353/2000... Training loss: 0.3710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1353/2000... Training loss: 0.5092\n",
      "Epoch: 1353/2000... Training loss: 0.4427\n",
      "Epoch: 1353/2000... Training loss: 0.2947\n",
      "Epoch: 1353/2000... Training loss: 0.6431\n",
      "Epoch: 1353/2000... Training loss: 0.3298\n",
      "Epoch: 1353/2000... Training loss: 0.3909\n",
      "Epoch: 1353/2000... Training loss: 0.4549\n",
      "Epoch: 1353/2000... Training loss: 0.5418\n",
      "Epoch: 1353/2000... Training loss: 0.3906\n",
      "Epoch: 1353/2000... Training loss: 0.4192\n",
      "Epoch: 1353/2000... Training loss: 0.3459\n",
      "Epoch: 1353/2000... Training loss: 0.6006\n",
      "Epoch: 1353/2000... Training loss: 0.5250\n",
      "Epoch: 1353/2000... Training loss: 0.4669\n",
      "Epoch: 1353/2000... Training loss: 0.5023\n",
      "Epoch: 1353/2000... Training loss: 0.3961\n",
      "Epoch: 1353/2000... Training loss: 0.6176\n",
      "Epoch: 1353/2000... Training loss: 0.2606\n",
      "Epoch: 1353/2000... Training loss: 0.4373\n",
      "Epoch: 1353/2000... Training loss: 0.3729\n",
      "Epoch: 1353/2000... Training loss: 0.3661\n",
      "Epoch: 1354/2000... Training loss: 0.4146\n",
      "Epoch: 1354/2000... Training loss: 0.3161\n",
      "Epoch: 1354/2000... Training loss: 0.5240\n",
      "Epoch: 1354/2000... Training loss: 0.3916\n",
      "Epoch: 1354/2000... Training loss: 0.5314\n",
      "Epoch: 1354/2000... Training loss: 0.4090\n",
      "Epoch: 1354/2000... Training loss: 0.3455\n",
      "Epoch: 1354/2000... Training loss: 0.3784\n",
      "Epoch: 1354/2000... Training loss: 0.5120\n",
      "Epoch: 1354/2000... Training loss: 0.5545\n",
      "Epoch: 1354/2000... Training loss: 0.3671\n",
      "Epoch: 1354/2000... Training loss: 0.4497\n",
      "Epoch: 1354/2000... Training loss: 0.4479\n",
      "Epoch: 1354/2000... Training loss: 0.3528\n",
      "Epoch: 1354/2000... Training loss: 0.4336\n",
      "Epoch: 1354/2000... Training loss: 0.2168\n",
      "Epoch: 1354/2000... Training loss: 0.2761\n",
      "Epoch: 1354/2000... Training loss: 0.3402\n",
      "Epoch: 1354/2000... Training loss: 0.4184\n",
      "Epoch: 1354/2000... Training loss: 0.3563\n",
      "Epoch: 1354/2000... Training loss: 0.4622\n",
      "Epoch: 1354/2000... Training loss: 0.3484\n",
      "Epoch: 1354/2000... Training loss: 0.2983\n",
      "Epoch: 1354/2000... Training loss: 0.3201\n",
      "Epoch: 1354/2000... Training loss: 0.4270\n",
      "Epoch: 1354/2000... Training loss: 0.4195\n",
      "Epoch: 1354/2000... Training loss: 0.5405\n",
      "Epoch: 1354/2000... Training loss: 0.4158\n",
      "Epoch: 1354/2000... Training loss: 0.4059\n",
      "Epoch: 1354/2000... Training loss: 0.3783\n",
      "Epoch: 1354/2000... Training loss: 0.5017\n",
      "Epoch: 1355/2000... Training loss: 0.3735\n",
      "Epoch: 1355/2000... Training loss: 0.3895\n",
      "Epoch: 1355/2000... Training loss: 0.4856\n",
      "Epoch: 1355/2000... Training loss: 0.3698\n",
      "Epoch: 1355/2000... Training loss: 0.5071\n",
      "Epoch: 1355/2000... Training loss: 0.2996\n",
      "Epoch: 1355/2000... Training loss: 0.3236\n",
      "Epoch: 1355/2000... Training loss: 0.5333\n",
      "Epoch: 1355/2000... Training loss: 0.3049\n",
      "Epoch: 1355/2000... Training loss: 0.4226\n",
      "Epoch: 1355/2000... Training loss: 0.2774\n",
      "Epoch: 1355/2000... Training loss: 0.5209\n",
      "Epoch: 1355/2000... Training loss: 0.4181\n",
      "Epoch: 1355/2000... Training loss: 0.2345\n",
      "Epoch: 1355/2000... Training loss: 0.3080\n",
      "Epoch: 1355/2000... Training loss: 0.5297\n",
      "Epoch: 1355/2000... Training loss: 0.4275\n",
      "Epoch: 1355/2000... Training loss: 0.3601\n",
      "Epoch: 1355/2000... Training loss: 0.4003\n",
      "Epoch: 1355/2000... Training loss: 0.4995\n",
      "Epoch: 1355/2000... Training loss: 0.5482\n",
      "Epoch: 1355/2000... Training loss: 0.2565\n",
      "Epoch: 1355/2000... Training loss: 0.4079\n",
      "Epoch: 1355/2000... Training loss: 0.3880\n",
      "Epoch: 1355/2000... Training loss: 0.5617\n",
      "Epoch: 1355/2000... Training loss: 0.3675\n",
      "Epoch: 1355/2000... Training loss: 0.4130\n",
      "Epoch: 1355/2000... Training loss: 0.4429\n",
      "Epoch: 1355/2000... Training loss: 0.4005\n",
      "Epoch: 1355/2000... Training loss: 0.4565\n",
      "Epoch: 1355/2000... Training loss: 0.2989\n",
      "Epoch: 1356/2000... Training loss: 0.4171\n",
      "Epoch: 1356/2000... Training loss: 0.3245\n",
      "Epoch: 1356/2000... Training loss: 0.3837\n",
      "Epoch: 1356/2000... Training loss: 0.4364\n",
      "Epoch: 1356/2000... Training loss: 0.3502\n",
      "Epoch: 1356/2000... Training loss: 0.5365\n",
      "Epoch: 1356/2000... Training loss: 0.4160\n",
      "Epoch: 1356/2000... Training loss: 0.3721\n",
      "Epoch: 1356/2000... Training loss: 0.3459\n",
      "Epoch: 1356/2000... Training loss: 0.3096\n",
      "Epoch: 1356/2000... Training loss: 0.6382\n",
      "Epoch: 1356/2000... Training loss: 0.4613\n",
      "Epoch: 1356/2000... Training loss: 0.3654\n",
      "Epoch: 1356/2000... Training loss: 0.4571\n",
      "Epoch: 1356/2000... Training loss: 0.5147\n",
      "Epoch: 1356/2000... Training loss: 0.5292\n",
      "Epoch: 1356/2000... Training loss: 0.4040\n",
      "Epoch: 1356/2000... Training loss: 0.4447\n",
      "Epoch: 1356/2000... Training loss: 0.2775\n",
      "Epoch: 1356/2000... Training loss: 0.4349\n",
      "Epoch: 1356/2000... Training loss: 0.3598\n",
      "Epoch: 1356/2000... Training loss: 0.4111\n",
      "Epoch: 1356/2000... Training loss: 0.3994\n",
      "Epoch: 1356/2000... Training loss: 0.3372\n",
      "Epoch: 1356/2000... Training loss: 0.3989\n",
      "Epoch: 1356/2000... Training loss: 0.4403\n",
      "Epoch: 1356/2000... Training loss: 0.4270\n",
      "Epoch: 1356/2000... Training loss: 0.4640\n",
      "Epoch: 1356/2000... Training loss: 0.3726\n",
      "Epoch: 1356/2000... Training loss: 0.3693\n",
      "Epoch: 1356/2000... Training loss: 0.4130\n",
      "Epoch: 1357/2000... Training loss: 0.4551\n",
      "Epoch: 1357/2000... Training loss: 0.3984\n",
      "Epoch: 1357/2000... Training loss: 0.5010\n",
      "Epoch: 1357/2000... Training loss: 0.4531\n",
      "Epoch: 1357/2000... Training loss: 0.5078\n",
      "Epoch: 1357/2000... Training loss: 0.4192\n",
      "Epoch: 1357/2000... Training loss: 0.4137\n",
      "Epoch: 1357/2000... Training loss: 0.5513\n",
      "Epoch: 1357/2000... Training loss: 0.4575\n",
      "Epoch: 1357/2000... Training loss: 0.4627\n",
      "Epoch: 1357/2000... Training loss: 0.3839\n",
      "Epoch: 1357/2000... Training loss: 0.2651\n",
      "Epoch: 1357/2000... Training loss: 0.4341\n",
      "Epoch: 1357/2000... Training loss: 0.5552\n",
      "Epoch: 1357/2000... Training loss: 0.4056\n",
      "Epoch: 1357/2000... Training loss: 0.4115\n",
      "Epoch: 1357/2000... Training loss: 0.4342\n",
      "Epoch: 1357/2000... Training loss: 0.5843\n",
      "Epoch: 1357/2000... Training loss: 0.4186\n",
      "Epoch: 1357/2000... Training loss: 0.4599\n",
      "Epoch: 1357/2000... Training loss: 0.2822\n",
      "Epoch: 1357/2000... Training loss: 0.5286\n",
      "Epoch: 1357/2000... Training loss: 0.5083\n",
      "Epoch: 1357/2000... Training loss: 0.3761\n",
      "Epoch: 1357/2000... Training loss: 0.4822\n",
      "Epoch: 1357/2000... Training loss: 0.4526\n",
      "Epoch: 1357/2000... Training loss: 0.2719\n",
      "Epoch: 1357/2000... Training loss: 0.4191\n",
      "Epoch: 1357/2000... Training loss: 0.4345\n",
      "Epoch: 1357/2000... Training loss: 0.3730\n",
      "Epoch: 1357/2000... Training loss: 0.3634\n",
      "Epoch: 1358/2000... Training loss: 0.4200\n",
      "Epoch: 1358/2000... Training loss: 0.3261\n",
      "Epoch: 1358/2000... Training loss: 0.3940\n",
      "Epoch: 1358/2000... Training loss: 0.2305\n",
      "Epoch: 1358/2000... Training loss: 0.3470\n",
      "Epoch: 1358/2000... Training loss: 0.4242\n",
      "Epoch: 1358/2000... Training loss: 0.4766\n",
      "Epoch: 1358/2000... Training loss: 0.3989\n",
      "Epoch: 1358/2000... Training loss: 0.5222\n",
      "Epoch: 1358/2000... Training loss: 0.6939\n",
      "Epoch: 1358/2000... Training loss: 0.4494\n",
      "Epoch: 1358/2000... Training loss: 0.4395\n",
      "Epoch: 1358/2000... Training loss: 0.4105\n",
      "Epoch: 1358/2000... Training loss: 0.5274\n",
      "Epoch: 1358/2000... Training loss: 0.3421\n",
      "Epoch: 1358/2000... Training loss: 0.3925\n",
      "Epoch: 1358/2000... Training loss: 0.4589\n",
      "Epoch: 1358/2000... Training loss: 0.4994\n",
      "Epoch: 1358/2000... Training loss: 0.3134\n",
      "Epoch: 1358/2000... Training loss: 0.4364\n",
      "Epoch: 1358/2000... Training loss: 0.3460\n",
      "Epoch: 1358/2000... Training loss: 0.4385\n",
      "Epoch: 1358/2000... Training loss: 0.4296\n",
      "Epoch: 1358/2000... Training loss: 0.4579\n",
      "Epoch: 1358/2000... Training loss: 0.4694\n",
      "Epoch: 1358/2000... Training loss: 0.4158\n",
      "Epoch: 1358/2000... Training loss: 0.2635\n",
      "Epoch: 1358/2000... Training loss: 0.3953\n",
      "Epoch: 1358/2000... Training loss: 0.2814\n",
      "Epoch: 1358/2000... Training loss: 0.4072\n",
      "Epoch: 1358/2000... Training loss: 0.4578\n",
      "Epoch: 1359/2000... Training loss: 0.4357\n",
      "Epoch: 1359/2000... Training loss: 0.4315\n",
      "Epoch: 1359/2000... Training loss: 0.4129\n",
      "Epoch: 1359/2000... Training loss: 0.4885\n",
      "Epoch: 1359/2000... Training loss: 0.4533\n",
      "Epoch: 1359/2000... Training loss: 0.5195\n",
      "Epoch: 1359/2000... Training loss: 0.2568\n",
      "Epoch: 1359/2000... Training loss: 0.2630\n",
      "Epoch: 1359/2000... Training loss: 0.6127\n",
      "Epoch: 1359/2000... Training loss: 0.4811\n",
      "Epoch: 1359/2000... Training loss: 0.4196\n",
      "Epoch: 1359/2000... Training loss: 0.4093\n",
      "Epoch: 1359/2000... Training loss: 0.5886\n",
      "Epoch: 1359/2000... Training loss: 0.4794\n",
      "Epoch: 1359/2000... Training loss: 0.4077\n",
      "Epoch: 1359/2000... Training loss: 0.3737\n",
      "Epoch: 1359/2000... Training loss: 0.4791\n",
      "Epoch: 1359/2000... Training loss: 0.2432\n",
      "Epoch: 1359/2000... Training loss: 0.4871\n",
      "Epoch: 1359/2000... Training loss: 0.5957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1359/2000... Training loss: 0.2871\n",
      "Epoch: 1359/2000... Training loss: 0.4509\n",
      "Epoch: 1359/2000... Training loss: 0.3931\n",
      "Epoch: 1359/2000... Training loss: 0.2942\n",
      "Epoch: 1359/2000... Training loss: 0.2899\n",
      "Epoch: 1359/2000... Training loss: 0.4702\n",
      "Epoch: 1359/2000... Training loss: 0.3808\n",
      "Epoch: 1359/2000... Training loss: 0.3047\n",
      "Epoch: 1359/2000... Training loss: 0.4730\n",
      "Epoch: 1359/2000... Training loss: 0.3246\n",
      "Epoch: 1359/2000... Training loss: 0.5172\n",
      "Epoch: 1360/2000... Training loss: 0.3393\n",
      "Epoch: 1360/2000... Training loss: 0.4667\n",
      "Epoch: 1360/2000... Training loss: 0.4616\n",
      "Epoch: 1360/2000... Training loss: 0.4550\n",
      "Epoch: 1360/2000... Training loss: 0.2704\n",
      "Epoch: 1360/2000... Training loss: 0.4584\n",
      "Epoch: 1360/2000... Training loss: 0.5796\n",
      "Epoch: 1360/2000... Training loss: 0.4866\n",
      "Epoch: 1360/2000... Training loss: 0.4722\n",
      "Epoch: 1360/2000... Training loss: 0.2770\n",
      "Epoch: 1360/2000... Training loss: 0.3595\n",
      "Epoch: 1360/2000... Training loss: 0.3976\n",
      "Epoch: 1360/2000... Training loss: 0.4958\n",
      "Epoch: 1360/2000... Training loss: 0.6185\n",
      "Epoch: 1360/2000... Training loss: 0.4378\n",
      "Epoch: 1360/2000... Training loss: 0.4399\n",
      "Epoch: 1360/2000... Training loss: 0.5151\n",
      "Epoch: 1360/2000... Training loss: 0.5045\n",
      "Epoch: 1360/2000... Training loss: 0.5805\n",
      "Epoch: 1360/2000... Training loss: 0.3425\n",
      "Epoch: 1360/2000... Training loss: 0.2954\n",
      "Epoch: 1360/2000... Training loss: 0.4767\n",
      "Epoch: 1360/2000... Training loss: 0.3008\n",
      "Epoch: 1360/2000... Training loss: 0.3246\n",
      "Epoch: 1360/2000... Training loss: 0.5691\n",
      "Epoch: 1360/2000... Training loss: 0.5003\n",
      "Epoch: 1360/2000... Training loss: 0.3895\n",
      "Epoch: 1360/2000... Training loss: 0.3013\n",
      "Epoch: 1360/2000... Training loss: 0.4135\n",
      "Epoch: 1360/2000... Training loss: 0.4859\n",
      "Epoch: 1360/2000... Training loss: 0.4882\n",
      "Epoch: 1361/2000... Training loss: 0.5384\n",
      "Epoch: 1361/2000... Training loss: 0.4276\n",
      "Epoch: 1361/2000... Training loss: 0.6056\n",
      "Epoch: 1361/2000... Training loss: 0.2359\n",
      "Epoch: 1361/2000... Training loss: 0.3534\n",
      "Epoch: 1361/2000... Training loss: 0.3555\n",
      "Epoch: 1361/2000... Training loss: 0.5750\n",
      "Epoch: 1361/2000... Training loss: 0.4988\n",
      "Epoch: 1361/2000... Training loss: 0.4913\n",
      "Epoch: 1361/2000... Training loss: 0.5490\n",
      "Epoch: 1361/2000... Training loss: 0.5688\n",
      "Epoch: 1361/2000... Training loss: 0.4515\n",
      "Epoch: 1361/2000... Training loss: 0.6376\n",
      "Epoch: 1361/2000... Training loss: 0.4781\n",
      "Epoch: 1361/2000... Training loss: 0.2832\n",
      "Epoch: 1361/2000... Training loss: 0.2396\n",
      "Epoch: 1361/2000... Training loss: 0.4758\n",
      "Epoch: 1361/2000... Training loss: 0.3776\n",
      "Epoch: 1361/2000... Training loss: 0.3451\n",
      "Epoch: 1361/2000... Training loss: 0.4450\n",
      "Epoch: 1361/2000... Training loss: 0.6025\n",
      "Epoch: 1361/2000... Training loss: 0.3818\n",
      "Epoch: 1361/2000... Training loss: 0.4799\n",
      "Epoch: 1361/2000... Training loss: 0.6712\n",
      "Epoch: 1361/2000... Training loss: 0.4277\n",
      "Epoch: 1361/2000... Training loss: 0.5498\n",
      "Epoch: 1361/2000... Training loss: 0.4830\n",
      "Epoch: 1361/2000... Training loss: 0.3166\n",
      "Epoch: 1361/2000... Training loss: 0.5932\n",
      "Epoch: 1361/2000... Training loss: 0.4496\n",
      "Epoch: 1361/2000... Training loss: 0.3647\n",
      "Epoch: 1362/2000... Training loss: 0.5059\n",
      "Epoch: 1362/2000... Training loss: 0.4434\n",
      "Epoch: 1362/2000... Training loss: 0.2883\n",
      "Epoch: 1362/2000... Training loss: 0.3924\n",
      "Epoch: 1362/2000... Training loss: 0.4194\n",
      "Epoch: 1362/2000... Training loss: 0.4412\n",
      "Epoch: 1362/2000... Training loss: 0.3580\n",
      "Epoch: 1362/2000... Training loss: 0.3228\n",
      "Epoch: 1362/2000... Training loss: 0.3299\n",
      "Epoch: 1362/2000... Training loss: 0.4279\n",
      "Epoch: 1362/2000... Training loss: 0.4358\n",
      "Epoch: 1362/2000... Training loss: 0.4721\n",
      "Epoch: 1362/2000... Training loss: 0.3369\n",
      "Epoch: 1362/2000... Training loss: 0.4358\n",
      "Epoch: 1362/2000... Training loss: 0.3738\n",
      "Epoch: 1362/2000... Training loss: 0.5554\n",
      "Epoch: 1362/2000... Training loss: 0.5006\n",
      "Epoch: 1362/2000... Training loss: 0.4596\n",
      "Epoch: 1362/2000... Training loss: 0.3589\n",
      "Epoch: 1362/2000... Training loss: 0.3902\n",
      "Epoch: 1362/2000... Training loss: 0.4310\n",
      "Epoch: 1362/2000... Training loss: 0.5094\n",
      "Epoch: 1362/2000... Training loss: 0.4327\n",
      "Epoch: 1362/2000... Training loss: 0.4135\n",
      "Epoch: 1362/2000... Training loss: 0.3817\n",
      "Epoch: 1362/2000... Training loss: 0.4957\n",
      "Epoch: 1362/2000... Training loss: 0.2806\n",
      "Epoch: 1362/2000... Training loss: 0.3238\n",
      "Epoch: 1362/2000... Training loss: 0.3572\n",
      "Epoch: 1362/2000... Training loss: 0.5738\n",
      "Epoch: 1362/2000... Training loss: 0.2474\n",
      "Epoch: 1363/2000... Training loss: 0.3518\n",
      "Epoch: 1363/2000... Training loss: 0.3364\n",
      "Epoch: 1363/2000... Training loss: 0.5068\n",
      "Epoch: 1363/2000... Training loss: 0.2886\n",
      "Epoch: 1363/2000... Training loss: 0.7188\n",
      "Epoch: 1363/2000... Training loss: 0.4688\n",
      "Epoch: 1363/2000... Training loss: 0.6027\n",
      "Epoch: 1363/2000... Training loss: 0.4228\n",
      "Epoch: 1363/2000... Training loss: 0.4757\n",
      "Epoch: 1363/2000... Training loss: 0.2966\n",
      "Epoch: 1363/2000... Training loss: 0.4785\n",
      "Epoch: 1363/2000... Training loss: 0.5082\n",
      "Epoch: 1363/2000... Training loss: 0.4478\n",
      "Epoch: 1363/2000... Training loss: 0.3432\n",
      "Epoch: 1363/2000... Training loss: 0.4615\n",
      "Epoch: 1363/2000... Training loss: 0.3843\n",
      "Epoch: 1363/2000... Training loss: 0.4329\n",
      "Epoch: 1363/2000... Training loss: 0.3966\n",
      "Epoch: 1363/2000... Training loss: 0.3592\n",
      "Epoch: 1363/2000... Training loss: 0.3029\n",
      "Epoch: 1363/2000... Training loss: 0.3236\n",
      "Epoch: 1363/2000... Training loss: 0.4124\n",
      "Epoch: 1363/2000... Training loss: 0.4391\n",
      "Epoch: 1363/2000... Training loss: 0.4666\n",
      "Epoch: 1363/2000... Training loss: 0.4606\n",
      "Epoch: 1363/2000... Training loss: 0.3676\n",
      "Epoch: 1363/2000... Training loss: 0.4290\n",
      "Epoch: 1363/2000... Training loss: 0.5526\n",
      "Epoch: 1363/2000... Training loss: 0.4043\n",
      "Epoch: 1363/2000... Training loss: 0.5961\n",
      "Epoch: 1363/2000... Training loss: 0.2939\n",
      "Epoch: 1364/2000... Training loss: 0.4166\n",
      "Epoch: 1364/2000... Training loss: 0.4245\n",
      "Epoch: 1364/2000... Training loss: 0.4586\n",
      "Epoch: 1364/2000... Training loss: 0.4568\n",
      "Epoch: 1364/2000... Training loss: 0.5721\n",
      "Epoch: 1364/2000... Training loss: 0.3789\n",
      "Epoch: 1364/2000... Training loss: 0.6130\n",
      "Epoch: 1364/2000... Training loss: 0.5580\n",
      "Epoch: 1364/2000... Training loss: 0.5247\n",
      "Epoch: 1364/2000... Training loss: 0.3987\n",
      "Epoch: 1364/2000... Training loss: 0.3505\n",
      "Epoch: 1364/2000... Training loss: 0.4560\n",
      "Epoch: 1364/2000... Training loss: 0.4571\n",
      "Epoch: 1364/2000... Training loss: 0.5561\n",
      "Epoch: 1364/2000... Training loss: 0.4816\n",
      "Epoch: 1364/2000... Training loss: 0.5290\n",
      "Epoch: 1364/2000... Training loss: 0.2669\n",
      "Epoch: 1364/2000... Training loss: 0.2551\n",
      "Epoch: 1364/2000... Training loss: 0.4023\n",
      "Epoch: 1364/2000... Training loss: 0.4660\n",
      "Epoch: 1364/2000... Training loss: 0.2884\n",
      "Epoch: 1364/2000... Training loss: 0.5535\n",
      "Epoch: 1364/2000... Training loss: 0.4327\n",
      "Epoch: 1364/2000... Training loss: 0.4770\n",
      "Epoch: 1364/2000... Training loss: 0.4878\n",
      "Epoch: 1364/2000... Training loss: 0.3609\n",
      "Epoch: 1364/2000... Training loss: 0.5022\n",
      "Epoch: 1364/2000... Training loss: 0.4558\n",
      "Epoch: 1364/2000... Training loss: 0.3296\n",
      "Epoch: 1364/2000... Training loss: 0.3551\n",
      "Epoch: 1364/2000... Training loss: 0.3060\n",
      "Epoch: 1365/2000... Training loss: 0.5681\n",
      "Epoch: 1365/2000... Training loss: 0.4927\n",
      "Epoch: 1365/2000... Training loss: 0.3440\n",
      "Epoch: 1365/2000... Training loss: 0.4474\n",
      "Epoch: 1365/2000... Training loss: 0.3450\n",
      "Epoch: 1365/2000... Training loss: 0.5775\n",
      "Epoch: 1365/2000... Training loss: 0.3747\n",
      "Epoch: 1365/2000... Training loss: 0.4185\n",
      "Epoch: 1365/2000... Training loss: 0.4268\n",
      "Epoch: 1365/2000... Training loss: 0.2838\n",
      "Epoch: 1365/2000... Training loss: 0.6231\n",
      "Epoch: 1365/2000... Training loss: 0.5101\n",
      "Epoch: 1365/2000... Training loss: 0.5818\n",
      "Epoch: 1365/2000... Training loss: 0.5356\n",
      "Epoch: 1365/2000... Training loss: 0.3592\n",
      "Epoch: 1365/2000... Training loss: 0.3015\n",
      "Epoch: 1365/2000... Training loss: 0.4948\n",
      "Epoch: 1365/2000... Training loss: 0.4262\n",
      "Epoch: 1365/2000... Training loss: 0.3273\n",
      "Epoch: 1365/2000... Training loss: 0.5026\n",
      "Epoch: 1365/2000... Training loss: 0.3843\n",
      "Epoch: 1365/2000... Training loss: 0.4737\n",
      "Epoch: 1365/2000... Training loss: 0.4562\n",
      "Epoch: 1365/2000... Training loss: 0.4446\n",
      "Epoch: 1365/2000... Training loss: 0.5254\n",
      "Epoch: 1365/2000... Training loss: 0.4151\n",
      "Epoch: 1365/2000... Training loss: 0.5026\n",
      "Epoch: 1365/2000... Training loss: 0.4208\n",
      "Epoch: 1365/2000... Training loss: 0.5718\n",
      "Epoch: 1365/2000... Training loss: 0.4389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1365/2000... Training loss: 0.5020\n",
      "Epoch: 1366/2000... Training loss: 0.4412\n",
      "Epoch: 1366/2000... Training loss: 0.6661\n",
      "Epoch: 1366/2000... Training loss: 0.3807\n",
      "Epoch: 1366/2000... Training loss: 0.5166\n",
      "Epoch: 1366/2000... Training loss: 0.4727\n",
      "Epoch: 1366/2000... Training loss: 0.3521\n",
      "Epoch: 1366/2000... Training loss: 0.3013\n",
      "Epoch: 1366/2000... Training loss: 0.4174\n",
      "Epoch: 1366/2000... Training loss: 0.3816\n",
      "Epoch: 1366/2000... Training loss: 0.5949\n",
      "Epoch: 1366/2000... Training loss: 0.3673\n",
      "Epoch: 1366/2000... Training loss: 0.1957\n",
      "Epoch: 1366/2000... Training loss: 0.4944\n",
      "Epoch: 1366/2000... Training loss: 0.3195\n",
      "Epoch: 1366/2000... Training loss: 0.4852\n",
      "Epoch: 1366/2000... Training loss: 0.7430\n",
      "Epoch: 1366/2000... Training loss: 0.3129\n",
      "Epoch: 1366/2000... Training loss: 0.3877\n",
      "Epoch: 1366/2000... Training loss: 0.4961\n",
      "Epoch: 1366/2000... Training loss: 0.4338\n",
      "Epoch: 1366/2000... Training loss: 0.4575\n",
      "Epoch: 1366/2000... Training loss: 0.3183\n",
      "Epoch: 1366/2000... Training loss: 0.6293\n",
      "Epoch: 1366/2000... Training loss: 0.5605\n",
      "Epoch: 1366/2000... Training loss: 0.6205\n",
      "Epoch: 1366/2000... Training loss: 0.5853\n",
      "Epoch: 1366/2000... Training loss: 0.3728\n",
      "Epoch: 1366/2000... Training loss: 0.6337\n",
      "Epoch: 1366/2000... Training loss: 0.4109\n",
      "Epoch: 1366/2000... Training loss: 0.3899\n",
      "Epoch: 1366/2000... Training loss: 0.5279\n",
      "Epoch: 1367/2000... Training loss: 0.5919\n",
      "Epoch: 1367/2000... Training loss: 0.4006\n",
      "Epoch: 1367/2000... Training loss: 0.6442\n",
      "Epoch: 1367/2000... Training loss: 0.3038\n",
      "Epoch: 1367/2000... Training loss: 0.4090\n",
      "Epoch: 1367/2000... Training loss: 0.4499\n",
      "Epoch: 1367/2000... Training loss: 0.4504\n",
      "Epoch: 1367/2000... Training loss: 0.4967\n",
      "Epoch: 1367/2000... Training loss: 0.5752\n",
      "Epoch: 1367/2000... Training loss: 0.4448\n",
      "Epoch: 1367/2000... Training loss: 0.3230\n",
      "Epoch: 1367/2000... Training loss: 0.3343\n",
      "Epoch: 1367/2000... Training loss: 0.5266\n",
      "Epoch: 1367/2000... Training loss: 0.4109\n",
      "Epoch: 1367/2000... Training loss: 0.3843\n",
      "Epoch: 1367/2000... Training loss: 0.2867\n",
      "Epoch: 1367/2000... Training loss: 0.5531\n",
      "Epoch: 1367/2000... Training loss: 0.4707\n",
      "Epoch: 1367/2000... Training loss: 0.4843\n",
      "Epoch: 1367/2000... Training loss: 0.4957\n",
      "Epoch: 1367/2000... Training loss: 0.4323\n",
      "Epoch: 1367/2000... Training loss: 0.4392\n",
      "Epoch: 1367/2000... Training loss: 0.4737\n",
      "Epoch: 1367/2000... Training loss: 0.7797\n",
      "Epoch: 1367/2000... Training loss: 0.3977\n",
      "Epoch: 1367/2000... Training loss: 0.5182\n",
      "Epoch: 1367/2000... Training loss: 0.4766\n",
      "Epoch: 1367/2000... Training loss: 0.3989\n",
      "Epoch: 1367/2000... Training loss: 0.4168\n",
      "Epoch: 1367/2000... Training loss: 0.4080\n",
      "Epoch: 1367/2000... Training loss: 0.2766\n",
      "Epoch: 1368/2000... Training loss: 0.4459\n",
      "Epoch: 1368/2000... Training loss: 0.3728\n",
      "Epoch: 1368/2000... Training loss: 0.4129\n",
      "Epoch: 1368/2000... Training loss: 0.5711\n",
      "Epoch: 1368/2000... Training loss: 0.4055\n",
      "Epoch: 1368/2000... Training loss: 0.3550\n",
      "Epoch: 1368/2000... Training loss: 0.3865\n",
      "Epoch: 1368/2000... Training loss: 0.4937\n",
      "Epoch: 1368/2000... Training loss: 0.4240\n",
      "Epoch: 1368/2000... Training loss: 0.6282\n",
      "Epoch: 1368/2000... Training loss: 0.3991\n",
      "Epoch: 1368/2000... Training loss: 0.4849\n",
      "Epoch: 1368/2000... Training loss: 0.4515\n",
      "Epoch: 1368/2000... Training loss: 0.3961\n",
      "Epoch: 1368/2000... Training loss: 0.4358\n",
      "Epoch: 1368/2000... Training loss: 0.2593\n",
      "Epoch: 1368/2000... Training loss: 0.4321\n",
      "Epoch: 1368/2000... Training loss: 0.5344\n",
      "Epoch: 1368/2000... Training loss: 0.4447\n",
      "Epoch: 1368/2000... Training loss: 0.3590\n",
      "Epoch: 1368/2000... Training loss: 0.3173\n",
      "Epoch: 1368/2000... Training loss: 0.4601\n",
      "Epoch: 1368/2000... Training loss: 0.4674\n",
      "Epoch: 1368/2000... Training loss: 0.3454\n",
      "Epoch: 1368/2000... Training loss: 0.4073\n",
      "Epoch: 1368/2000... Training loss: 0.6695\n",
      "Epoch: 1368/2000... Training loss: 0.4441\n",
      "Epoch: 1368/2000... Training loss: 0.4626\n",
      "Epoch: 1368/2000... Training loss: 0.4713\n",
      "Epoch: 1368/2000... Training loss: 0.3945\n",
      "Epoch: 1368/2000... Training loss: 0.3053\n",
      "Epoch: 1369/2000... Training loss: 0.4296\n",
      "Epoch: 1369/2000... Training loss: 0.4245\n",
      "Epoch: 1369/2000... Training loss: 0.4873\n",
      "Epoch: 1369/2000... Training loss: 0.3962\n",
      "Epoch: 1369/2000... Training loss: 0.5303\n",
      "Epoch: 1369/2000... Training loss: 0.5033\n",
      "Epoch: 1369/2000... Training loss: 0.3750\n",
      "Epoch: 1369/2000... Training loss: 0.3843\n",
      "Epoch: 1369/2000... Training loss: 0.5479\n",
      "Epoch: 1369/2000... Training loss: 0.4191\n",
      "Epoch: 1369/2000... Training loss: 0.3867\n",
      "Epoch: 1369/2000... Training loss: 0.2250\n",
      "Epoch: 1369/2000... Training loss: 0.3931\n",
      "Epoch: 1369/2000... Training loss: 0.4074\n",
      "Epoch: 1369/2000... Training loss: 0.4001\n",
      "Epoch: 1369/2000... Training loss: 0.2669\n",
      "Epoch: 1369/2000... Training loss: 0.3375\n",
      "Epoch: 1369/2000... Training loss: 0.4527\n",
      "Epoch: 1369/2000... Training loss: 0.4526\n",
      "Epoch: 1369/2000... Training loss: 0.3209\n",
      "Epoch: 1369/2000... Training loss: 0.4965\n",
      "Epoch: 1369/2000... Training loss: 0.4024\n",
      "Epoch: 1369/2000... Training loss: 0.3915\n",
      "Epoch: 1369/2000... Training loss: 0.4346\n",
      "Epoch: 1369/2000... Training loss: 0.6374\n",
      "Epoch: 1369/2000... Training loss: 0.3787\n",
      "Epoch: 1369/2000... Training loss: 0.5764\n",
      "Epoch: 1369/2000... Training loss: 0.4118\n",
      "Epoch: 1369/2000... Training loss: 0.4930\n",
      "Epoch: 1369/2000... Training loss: 0.5728\n",
      "Epoch: 1369/2000... Training loss: 0.3820\n",
      "Epoch: 1370/2000... Training loss: 0.3536\n",
      "Epoch: 1370/2000... Training loss: 0.6179\n",
      "Epoch: 1370/2000... Training loss: 0.4498\n",
      "Epoch: 1370/2000... Training loss: 0.4348\n",
      "Epoch: 1370/2000... Training loss: 0.3778\n",
      "Epoch: 1370/2000... Training loss: 0.4857\n",
      "Epoch: 1370/2000... Training loss: 0.5360\n",
      "Epoch: 1370/2000... Training loss: 0.5260\n",
      "Epoch: 1370/2000... Training loss: 0.3688\n",
      "Epoch: 1370/2000... Training loss: 0.3335\n",
      "Epoch: 1370/2000... Training loss: 0.4966\n",
      "Epoch: 1370/2000... Training loss: 0.5632\n",
      "Epoch: 1370/2000... Training loss: 0.3988\n",
      "Epoch: 1370/2000... Training loss: 0.4181\n",
      "Epoch: 1370/2000... Training loss: 0.3415\n",
      "Epoch: 1370/2000... Training loss: 0.4281\n",
      "Epoch: 1370/2000... Training loss: 0.3987\n",
      "Epoch: 1370/2000... Training loss: 0.4850\n",
      "Epoch: 1370/2000... Training loss: 0.3588\n",
      "Epoch: 1370/2000... Training loss: 0.3531\n",
      "Epoch: 1370/2000... Training loss: 0.5077\n",
      "Epoch: 1370/2000... Training loss: 0.4193\n",
      "Epoch: 1370/2000... Training loss: 0.3608\n",
      "Epoch: 1370/2000... Training loss: 0.3150\n",
      "Epoch: 1370/2000... Training loss: 0.3990\n",
      "Epoch: 1370/2000... Training loss: 0.5741\n",
      "Epoch: 1370/2000... Training loss: 0.3257\n",
      "Epoch: 1370/2000... Training loss: 0.5472\n",
      "Epoch: 1370/2000... Training loss: 0.4866\n",
      "Epoch: 1370/2000... Training loss: 0.4679\n",
      "Epoch: 1370/2000... Training loss: 0.5512\n",
      "Epoch: 1371/2000... Training loss: 0.4731\n",
      "Epoch: 1371/2000... Training loss: 0.5048\n",
      "Epoch: 1371/2000... Training loss: 0.4436\n",
      "Epoch: 1371/2000... Training loss: 0.3618\n",
      "Epoch: 1371/2000... Training loss: 0.3222\n",
      "Epoch: 1371/2000... Training loss: 0.4679\n",
      "Epoch: 1371/2000... Training loss: 0.4195\n",
      "Epoch: 1371/2000... Training loss: 0.3667\n",
      "Epoch: 1371/2000... Training loss: 0.3889\n",
      "Epoch: 1371/2000... Training loss: 0.4701\n",
      "Epoch: 1371/2000... Training loss: 0.4598\n",
      "Epoch: 1371/2000... Training loss: 0.5173\n",
      "Epoch: 1371/2000... Training loss: 0.5305\n",
      "Epoch: 1371/2000... Training loss: 0.3387\n",
      "Epoch: 1371/2000... Training loss: 0.5011\n",
      "Epoch: 1371/2000... Training loss: 0.5046\n",
      "Epoch: 1371/2000... Training loss: 0.4685\n",
      "Epoch: 1371/2000... Training loss: 0.3662\n",
      "Epoch: 1371/2000... Training loss: 0.3950\n",
      "Epoch: 1371/2000... Training loss: 0.4702\n",
      "Epoch: 1371/2000... Training loss: 0.4238\n",
      "Epoch: 1371/2000... Training loss: 0.5022\n",
      "Epoch: 1371/2000... Training loss: 0.3812\n",
      "Epoch: 1371/2000... Training loss: 0.4414\n",
      "Epoch: 1371/2000... Training loss: 0.3995\n",
      "Epoch: 1371/2000... Training loss: 0.4719\n",
      "Epoch: 1371/2000... Training loss: 0.4652\n",
      "Epoch: 1371/2000... Training loss: 0.5604\n",
      "Epoch: 1371/2000... Training loss: 0.3327\n",
      "Epoch: 1371/2000... Training loss: 0.3966\n",
      "Epoch: 1371/2000... Training loss: 0.3038\n",
      "Epoch: 1372/2000... Training loss: 0.4488\n",
      "Epoch: 1372/2000... Training loss: 0.4806\n",
      "Epoch: 1372/2000... Training loss: 0.4825\n",
      "Epoch: 1372/2000... Training loss: 0.4707\n",
      "Epoch: 1372/2000... Training loss: 0.4236\n",
      "Epoch: 1372/2000... Training loss: 0.5099\n",
      "Epoch: 1372/2000... Training loss: 0.2777\n",
      "Epoch: 1372/2000... Training loss: 0.2783\n",
      "Epoch: 1372/2000... Training loss: 0.3692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1372/2000... Training loss: 0.3810\n",
      "Epoch: 1372/2000... Training loss: 0.5052\n",
      "Epoch: 1372/2000... Training loss: 0.4435\n",
      "Epoch: 1372/2000... Training loss: 0.3665\n",
      "Epoch: 1372/2000... Training loss: 0.3138\n",
      "Epoch: 1372/2000... Training loss: 0.3697\n",
      "Epoch: 1372/2000... Training loss: 0.2816\n",
      "Epoch: 1372/2000... Training loss: 0.3062\n",
      "Epoch: 1372/2000... Training loss: 0.4345\n",
      "Epoch: 1372/2000... Training loss: 0.4684\n",
      "Epoch: 1372/2000... Training loss: 0.3823\n",
      "Epoch: 1372/2000... Training loss: 0.5124\n",
      "Epoch: 1372/2000... Training loss: 0.4738\n",
      "Epoch: 1372/2000... Training loss: 0.4293\n",
      "Epoch: 1372/2000... Training loss: 0.3517\n",
      "Epoch: 1372/2000... Training loss: 0.4736\n",
      "Epoch: 1372/2000... Training loss: 0.4883\n",
      "Epoch: 1372/2000... Training loss: 0.4551\n",
      "Epoch: 1372/2000... Training loss: 0.5507\n",
      "Epoch: 1372/2000... Training loss: 0.5762\n",
      "Epoch: 1372/2000... Training loss: 0.2948\n",
      "Epoch: 1372/2000... Training loss: 0.4425\n",
      "Epoch: 1373/2000... Training loss: 0.4850\n",
      "Epoch: 1373/2000... Training loss: 0.3646\n",
      "Epoch: 1373/2000... Training loss: 0.4121\n",
      "Epoch: 1373/2000... Training loss: 0.5066\n",
      "Epoch: 1373/2000... Training loss: 0.3873\n",
      "Epoch: 1373/2000... Training loss: 0.4578\n",
      "Epoch: 1373/2000... Training loss: 0.4086\n",
      "Epoch: 1373/2000... Training loss: 0.4018\n",
      "Epoch: 1373/2000... Training loss: 0.3621\n",
      "Epoch: 1373/2000... Training loss: 0.5541\n",
      "Epoch: 1373/2000... Training loss: 0.2792\n",
      "Epoch: 1373/2000... Training loss: 0.6024\n",
      "Epoch: 1373/2000... Training loss: 0.3464\n",
      "Epoch: 1373/2000... Training loss: 0.3934\n",
      "Epoch: 1373/2000... Training loss: 0.4140\n",
      "Epoch: 1373/2000... Training loss: 0.3960\n",
      "Epoch: 1373/2000... Training loss: 0.4809\n",
      "Epoch: 1373/2000... Training loss: 0.4471\n",
      "Epoch: 1373/2000... Training loss: 0.5290\n",
      "Epoch: 1373/2000... Training loss: 0.3476\n",
      "Epoch: 1373/2000... Training loss: 0.3944\n",
      "Epoch: 1373/2000... Training loss: 0.4973\n",
      "Epoch: 1373/2000... Training loss: 0.4604\n",
      "Epoch: 1373/2000... Training loss: 0.3593\n",
      "Epoch: 1373/2000... Training loss: 0.3637\n",
      "Epoch: 1373/2000... Training loss: 0.4240\n",
      "Epoch: 1373/2000... Training loss: 0.3594\n",
      "Epoch: 1373/2000... Training loss: 0.2682\n",
      "Epoch: 1373/2000... Training loss: 0.4929\n",
      "Epoch: 1373/2000... Training loss: 0.4419\n",
      "Epoch: 1373/2000... Training loss: 0.3558\n",
      "Epoch: 1374/2000... Training loss: 0.3393\n",
      "Epoch: 1374/2000... Training loss: 0.5020\n",
      "Epoch: 1374/2000... Training loss: 0.4533\n",
      "Epoch: 1374/2000... Training loss: 0.3410\n",
      "Epoch: 1374/2000... Training loss: 0.4517\n",
      "Epoch: 1374/2000... Training loss: 0.3295\n",
      "Epoch: 1374/2000... Training loss: 0.3186\n",
      "Epoch: 1374/2000... Training loss: 0.2814\n",
      "Epoch: 1374/2000... Training loss: 0.4792\n",
      "Epoch: 1374/2000... Training loss: 0.5863\n",
      "Epoch: 1374/2000... Training loss: 0.4089\n",
      "Epoch: 1374/2000... Training loss: 0.6128\n",
      "Epoch: 1374/2000... Training loss: 0.3938\n",
      "Epoch: 1374/2000... Training loss: 0.3729\n",
      "Epoch: 1374/2000... Training loss: 0.3491\n",
      "Epoch: 1374/2000... Training loss: 0.2850\n",
      "Epoch: 1374/2000... Training loss: 0.4444\n",
      "Epoch: 1374/2000... Training loss: 0.3742\n",
      "Epoch: 1374/2000... Training loss: 0.4308\n",
      "Epoch: 1374/2000... Training loss: 0.4535\n",
      "Epoch: 1374/2000... Training loss: 0.4657\n",
      "Epoch: 1374/2000... Training loss: 0.5830\n",
      "Epoch: 1374/2000... Training loss: 0.5895\n",
      "Epoch: 1374/2000... Training loss: 0.6396\n",
      "Epoch: 1374/2000... Training loss: 0.5812\n",
      "Epoch: 1374/2000... Training loss: 0.4742\n",
      "Epoch: 1374/2000... Training loss: 0.3917\n",
      "Epoch: 1374/2000... Training loss: 0.2894\n",
      "Epoch: 1374/2000... Training loss: 0.5948\n",
      "Epoch: 1374/2000... Training loss: 0.3996\n",
      "Epoch: 1374/2000... Training loss: 0.4482\n",
      "Epoch: 1375/2000... Training loss: 0.4206\n",
      "Epoch: 1375/2000... Training loss: 0.4144\n",
      "Epoch: 1375/2000... Training loss: 0.5253\n",
      "Epoch: 1375/2000... Training loss: 0.5558\n",
      "Epoch: 1375/2000... Training loss: 0.4437\n",
      "Epoch: 1375/2000... Training loss: 0.4835\n",
      "Epoch: 1375/2000... Training loss: 0.3541\n",
      "Epoch: 1375/2000... Training loss: 0.4635\n",
      "Epoch: 1375/2000... Training loss: 0.3308\n",
      "Epoch: 1375/2000... Training loss: 0.4122\n",
      "Epoch: 1375/2000... Training loss: 0.4490\n",
      "Epoch: 1375/2000... Training loss: 0.3643\n",
      "Epoch: 1375/2000... Training loss: 0.5930\n",
      "Epoch: 1375/2000... Training loss: 0.3622\n",
      "Epoch: 1375/2000... Training loss: 0.4698\n",
      "Epoch: 1375/2000... Training loss: 0.3639\n",
      "Epoch: 1375/2000... Training loss: 0.3074\n",
      "Epoch: 1375/2000... Training loss: 0.3742\n",
      "Epoch: 1375/2000... Training loss: 0.2679\n",
      "Epoch: 1375/2000... Training loss: 0.3987\n",
      "Epoch: 1375/2000... Training loss: 0.4434\n",
      "Epoch: 1375/2000... Training loss: 0.5959\n",
      "Epoch: 1375/2000... Training loss: 0.6537\n",
      "Epoch: 1375/2000... Training loss: 0.4452\n",
      "Epoch: 1375/2000... Training loss: 0.4580\n",
      "Epoch: 1375/2000... Training loss: 0.4936\n",
      "Epoch: 1375/2000... Training loss: 0.3556\n",
      "Epoch: 1375/2000... Training loss: 0.5633\n",
      "Epoch: 1375/2000... Training loss: 0.3197\n",
      "Epoch: 1375/2000... Training loss: 0.5072\n",
      "Epoch: 1375/2000... Training loss: 0.4147\n",
      "Epoch: 1376/2000... Training loss: 0.6078\n",
      "Epoch: 1376/2000... Training loss: 0.5035\n",
      "Epoch: 1376/2000... Training loss: 0.2641\n",
      "Epoch: 1376/2000... Training loss: 0.3851\n",
      "Epoch: 1376/2000... Training loss: 0.5043\n",
      "Epoch: 1376/2000... Training loss: 0.6983\n",
      "Epoch: 1376/2000... Training loss: 0.4843\n",
      "Epoch: 1376/2000... Training loss: 0.5719\n",
      "Epoch: 1376/2000... Training loss: 0.3863\n",
      "Epoch: 1376/2000... Training loss: 0.4828\n",
      "Epoch: 1376/2000... Training loss: 0.3599\n",
      "Epoch: 1376/2000... Training loss: 0.4741\n",
      "Epoch: 1376/2000... Training loss: 0.3719\n",
      "Epoch: 1376/2000... Training loss: 0.4116\n",
      "Epoch: 1376/2000... Training loss: 0.3780\n",
      "Epoch: 1376/2000... Training loss: 0.5290\n",
      "Epoch: 1376/2000... Training loss: 0.5299\n",
      "Epoch: 1376/2000... Training loss: 0.3908\n",
      "Epoch: 1376/2000... Training loss: 0.4222\n",
      "Epoch: 1376/2000... Training loss: 0.5065\n",
      "Epoch: 1376/2000... Training loss: 0.4033\n",
      "Epoch: 1376/2000... Training loss: 0.3837\n",
      "Epoch: 1376/2000... Training loss: 0.4410\n",
      "Epoch: 1376/2000... Training loss: 0.4965\n",
      "Epoch: 1376/2000... Training loss: 0.2320\n",
      "Epoch: 1376/2000... Training loss: 0.4559\n",
      "Epoch: 1376/2000... Training loss: 0.4305\n",
      "Epoch: 1376/2000... Training loss: 0.3807\n",
      "Epoch: 1376/2000... Training loss: 0.4843\n",
      "Epoch: 1376/2000... Training loss: 0.3996\n",
      "Epoch: 1376/2000... Training loss: 0.2612\n",
      "Epoch: 1377/2000... Training loss: 0.3881\n",
      "Epoch: 1377/2000... Training loss: 0.5293\n",
      "Epoch: 1377/2000... Training loss: 0.4410\n",
      "Epoch: 1377/2000... Training loss: 0.4102\n",
      "Epoch: 1377/2000... Training loss: 0.4839\n",
      "Epoch: 1377/2000... Training loss: 0.6331\n",
      "Epoch: 1377/2000... Training loss: 0.3017\n",
      "Epoch: 1377/2000... Training loss: 0.4326\n",
      "Epoch: 1377/2000... Training loss: 0.4571\n",
      "Epoch: 1377/2000... Training loss: 0.5798\n",
      "Epoch: 1377/2000... Training loss: 0.3270\n",
      "Epoch: 1377/2000... Training loss: 0.2624\n",
      "Epoch: 1377/2000... Training loss: 0.4068\n",
      "Epoch: 1377/2000... Training loss: 0.4177\n",
      "Epoch: 1377/2000... Training loss: 0.4548\n",
      "Epoch: 1377/2000... Training loss: 0.3559\n",
      "Epoch: 1377/2000... Training loss: 0.3245\n",
      "Epoch: 1377/2000... Training loss: 0.5534\n",
      "Epoch: 1377/2000... Training loss: 0.3602\n",
      "Epoch: 1377/2000... Training loss: 0.4041\n",
      "Epoch: 1377/2000... Training loss: 0.3448\n",
      "Epoch: 1377/2000... Training loss: 0.4089\n",
      "Epoch: 1377/2000... Training loss: 0.5744\n",
      "Epoch: 1377/2000... Training loss: 0.3541\n",
      "Epoch: 1377/2000... Training loss: 0.5200\n",
      "Epoch: 1377/2000... Training loss: 0.6184\n",
      "Epoch: 1377/2000... Training loss: 0.5410\n",
      "Epoch: 1377/2000... Training loss: 0.2970\n",
      "Epoch: 1377/2000... Training loss: 0.3634\n",
      "Epoch: 1377/2000... Training loss: 0.6134\n",
      "Epoch: 1377/2000... Training loss: 0.4311\n",
      "Epoch: 1378/2000... Training loss: 0.5596\n",
      "Epoch: 1378/2000... Training loss: 0.3787\n",
      "Epoch: 1378/2000... Training loss: 0.3955\n",
      "Epoch: 1378/2000... Training loss: 0.4091\n",
      "Epoch: 1378/2000... Training loss: 0.4258\n",
      "Epoch: 1378/2000... Training loss: 0.5193\n",
      "Epoch: 1378/2000... Training loss: 0.3126\n",
      "Epoch: 1378/2000... Training loss: 0.4087\n",
      "Epoch: 1378/2000... Training loss: 0.5072\n",
      "Epoch: 1378/2000... Training loss: 0.4264\n",
      "Epoch: 1378/2000... Training loss: 0.6168\n",
      "Epoch: 1378/2000... Training loss: 0.4276\n",
      "Epoch: 1378/2000... Training loss: 0.4082\n",
      "Epoch: 1378/2000... Training loss: 0.3560\n",
      "Epoch: 1378/2000... Training loss: 0.5461\n",
      "Epoch: 1378/2000... Training loss: 0.2372\n",
      "Epoch: 1378/2000... Training loss: 0.4327\n",
      "Epoch: 1378/2000... Training loss: 0.6271\n",
      "Epoch: 1378/2000... Training loss: 0.3252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1378/2000... Training loss: 0.3694\n",
      "Epoch: 1378/2000... Training loss: 0.3306\n",
      "Epoch: 1378/2000... Training loss: 0.5041\n",
      "Epoch: 1378/2000... Training loss: 0.3882\n",
      "Epoch: 1378/2000... Training loss: 0.4595\n",
      "Epoch: 1378/2000... Training loss: 0.5578\n",
      "Epoch: 1378/2000... Training loss: 0.3362\n",
      "Epoch: 1378/2000... Training loss: 0.3384\n",
      "Epoch: 1378/2000... Training loss: 0.4697\n",
      "Epoch: 1378/2000... Training loss: 0.4132\n",
      "Epoch: 1378/2000... Training loss: 0.5489\n",
      "Epoch: 1378/2000... Training loss: 0.2206\n",
      "Epoch: 1379/2000... Training loss: 0.4615\n",
      "Epoch: 1379/2000... Training loss: 0.2502\n",
      "Epoch: 1379/2000... Training loss: 0.5634\n",
      "Epoch: 1379/2000... Training loss: 0.4572\n",
      "Epoch: 1379/2000... Training loss: 0.5839\n",
      "Epoch: 1379/2000... Training loss: 0.3601\n",
      "Epoch: 1379/2000... Training loss: 0.5222\n",
      "Epoch: 1379/2000... Training loss: 0.4338\n",
      "Epoch: 1379/2000... Training loss: 0.5901\n",
      "Epoch: 1379/2000... Training loss: 0.3878\n",
      "Epoch: 1379/2000... Training loss: 0.3232\n",
      "Epoch: 1379/2000... Training loss: 0.2394\n",
      "Epoch: 1379/2000... Training loss: 0.5038\n",
      "Epoch: 1379/2000... Training loss: 0.4363\n",
      "Epoch: 1379/2000... Training loss: 0.3382\n",
      "Epoch: 1379/2000... Training loss: 0.2649\n",
      "Epoch: 1379/2000... Training loss: 0.5961\n",
      "Epoch: 1379/2000... Training loss: 0.4302\n",
      "Epoch: 1379/2000... Training loss: 0.4165\n",
      "Epoch: 1379/2000... Training loss: 0.4878\n",
      "Epoch: 1379/2000... Training loss: 0.3755\n",
      "Epoch: 1379/2000... Training loss: 0.2644\n",
      "Epoch: 1379/2000... Training loss: 0.3775\n",
      "Epoch: 1379/2000... Training loss: 0.3225\n",
      "Epoch: 1379/2000... Training loss: 0.4148\n",
      "Epoch: 1379/2000... Training loss: 0.4973\n",
      "Epoch: 1379/2000... Training loss: 0.3912\n",
      "Epoch: 1379/2000... Training loss: 0.4064\n",
      "Epoch: 1379/2000... Training loss: 0.4981\n",
      "Epoch: 1379/2000... Training loss: 0.3822\n",
      "Epoch: 1379/2000... Training loss: 0.1920\n",
      "Epoch: 1380/2000... Training loss: 0.5469\n",
      "Epoch: 1380/2000... Training loss: 0.6418\n",
      "Epoch: 1380/2000... Training loss: 0.4315\n",
      "Epoch: 1380/2000... Training loss: 0.5236\n",
      "Epoch: 1380/2000... Training loss: 0.4426\n",
      "Epoch: 1380/2000... Training loss: 0.5006\n",
      "Epoch: 1380/2000... Training loss: 0.5076\n",
      "Epoch: 1380/2000... Training loss: 0.2357\n",
      "Epoch: 1380/2000... Training loss: 0.3964\n",
      "Epoch: 1380/2000... Training loss: 0.3423\n",
      "Epoch: 1380/2000... Training loss: 0.3405\n",
      "Epoch: 1380/2000... Training loss: 0.5019\n",
      "Epoch: 1380/2000... Training loss: 0.3198\n",
      "Epoch: 1380/2000... Training loss: 0.4350\n",
      "Epoch: 1380/2000... Training loss: 0.4743\n",
      "Epoch: 1380/2000... Training loss: 0.5124\n",
      "Epoch: 1380/2000... Training loss: 0.4460\n",
      "Epoch: 1380/2000... Training loss: 0.3197\n",
      "Epoch: 1380/2000... Training loss: 0.3748\n",
      "Epoch: 1380/2000... Training loss: 0.4833\n",
      "Epoch: 1380/2000... Training loss: 0.3198\n",
      "Epoch: 1380/2000... Training loss: 0.3793\n",
      "Epoch: 1380/2000... Training loss: 0.3976\n",
      "Epoch: 1380/2000... Training loss: 0.4363\n",
      "Epoch: 1380/2000... Training loss: 0.2791\n",
      "Epoch: 1380/2000... Training loss: 0.3933\n",
      "Epoch: 1380/2000... Training loss: 0.3378\n",
      "Epoch: 1380/2000... Training loss: 0.4041\n",
      "Epoch: 1380/2000... Training loss: 0.2875\n",
      "Epoch: 1380/2000... Training loss: 0.3735\n",
      "Epoch: 1380/2000... Training loss: 0.2806\n",
      "Epoch: 1381/2000... Training loss: 0.3652\n",
      "Epoch: 1381/2000... Training loss: 0.3448\n",
      "Epoch: 1381/2000... Training loss: 0.5196\n",
      "Epoch: 1381/2000... Training loss: 0.5391\n",
      "Epoch: 1381/2000... Training loss: 0.3613\n",
      "Epoch: 1381/2000... Training loss: 0.3939\n",
      "Epoch: 1381/2000... Training loss: 0.2690\n",
      "Epoch: 1381/2000... Training loss: 0.4136\n",
      "Epoch: 1381/2000... Training loss: 0.3543\n",
      "Epoch: 1381/2000... Training loss: 0.3708\n",
      "Epoch: 1381/2000... Training loss: 0.2542\n",
      "Epoch: 1381/2000... Training loss: 0.3186\n",
      "Epoch: 1381/2000... Training loss: 0.2947\n",
      "Epoch: 1381/2000... Training loss: 0.3606\n",
      "Epoch: 1381/2000... Training loss: 0.3530\n",
      "Epoch: 1381/2000... Training loss: 0.4273\n",
      "Epoch: 1381/2000... Training loss: 0.2461\n",
      "Epoch: 1381/2000... Training loss: 0.3997\n",
      "Epoch: 1381/2000... Training loss: 0.4773\n",
      "Epoch: 1381/2000... Training loss: 0.3725\n",
      "Epoch: 1381/2000... Training loss: 0.3035\n",
      "Epoch: 1381/2000... Training loss: 0.5637\n",
      "Epoch: 1381/2000... Training loss: 0.5576\n",
      "Epoch: 1381/2000... Training loss: 0.5042\n",
      "Epoch: 1381/2000... Training loss: 0.3502\n",
      "Epoch: 1381/2000... Training loss: 0.3558\n",
      "Epoch: 1381/2000... Training loss: 0.4230\n",
      "Epoch: 1381/2000... Training loss: 0.5245\n",
      "Epoch: 1381/2000... Training loss: 0.3525\n",
      "Epoch: 1381/2000... Training loss: 0.3834\n",
      "Epoch: 1381/2000... Training loss: 0.4710\n",
      "Epoch: 1382/2000... Training loss: 0.6269\n",
      "Epoch: 1382/2000... Training loss: 0.5830\n",
      "Epoch: 1382/2000... Training loss: 0.4670\n",
      "Epoch: 1382/2000... Training loss: 0.4346\n",
      "Epoch: 1382/2000... Training loss: 0.3915\n",
      "Epoch: 1382/2000... Training loss: 0.4147\n",
      "Epoch: 1382/2000... Training loss: 0.5336\n",
      "Epoch: 1382/2000... Training loss: 0.2571\n",
      "Epoch: 1382/2000... Training loss: 0.5011\n",
      "Epoch: 1382/2000... Training loss: 0.4561\n",
      "Epoch: 1382/2000... Training loss: 0.4559\n",
      "Epoch: 1382/2000... Training loss: 0.5144\n",
      "Epoch: 1382/2000... Training loss: 0.4235\n",
      "Epoch: 1382/2000... Training loss: 0.4866\n",
      "Epoch: 1382/2000... Training loss: 0.3644\n",
      "Epoch: 1382/2000... Training loss: 0.3563\n",
      "Epoch: 1382/2000... Training loss: 0.2960\n",
      "Epoch: 1382/2000... Training loss: 0.3316\n",
      "Epoch: 1382/2000... Training loss: 0.5671\n",
      "Epoch: 1382/2000... Training loss: 0.4643\n",
      "Epoch: 1382/2000... Training loss: 0.3626\n",
      "Epoch: 1382/2000... Training loss: 0.6967\n",
      "Epoch: 1382/2000... Training loss: 0.4813\n",
      "Epoch: 1382/2000... Training loss: 0.6080\n",
      "Epoch: 1382/2000... Training loss: 0.4479\n",
      "Epoch: 1382/2000... Training loss: 0.5074\n",
      "Epoch: 1382/2000... Training loss: 0.3806\n",
      "Epoch: 1382/2000... Training loss: 0.4182\n",
      "Epoch: 1382/2000... Training loss: 0.4727\n",
      "Epoch: 1382/2000... Training loss: 0.5149\n",
      "Epoch: 1382/2000... Training loss: 0.4073\n",
      "Epoch: 1383/2000... Training loss: 0.3794\n",
      "Epoch: 1383/2000... Training loss: 0.5012\n",
      "Epoch: 1383/2000... Training loss: 0.4665\n",
      "Epoch: 1383/2000... Training loss: 0.4563\n",
      "Epoch: 1383/2000... Training loss: 0.5399\n",
      "Epoch: 1383/2000... Training loss: 0.4307\n",
      "Epoch: 1383/2000... Training loss: 0.5775\n",
      "Epoch: 1383/2000... Training loss: 0.5537\n",
      "Epoch: 1383/2000... Training loss: 0.2624\n",
      "Epoch: 1383/2000... Training loss: 0.2872\n",
      "Epoch: 1383/2000... Training loss: 0.6474\n",
      "Epoch: 1383/2000... Training loss: 0.3904\n",
      "Epoch: 1383/2000... Training loss: 0.4012\n",
      "Epoch: 1383/2000... Training loss: 0.3474\n",
      "Epoch: 1383/2000... Training loss: 0.3076\n",
      "Epoch: 1383/2000... Training loss: 0.5023\n",
      "Epoch: 1383/2000... Training loss: 0.5592\n",
      "Epoch: 1383/2000... Training loss: 0.4470\n",
      "Epoch: 1383/2000... Training loss: 0.4549\n",
      "Epoch: 1383/2000... Training loss: 0.4437\n",
      "Epoch: 1383/2000... Training loss: 0.4253\n",
      "Epoch: 1383/2000... Training loss: 0.3597\n",
      "Epoch: 1383/2000... Training loss: 0.4470\n",
      "Epoch: 1383/2000... Training loss: 0.4029\n",
      "Epoch: 1383/2000... Training loss: 0.4845\n",
      "Epoch: 1383/2000... Training loss: 0.4128\n",
      "Epoch: 1383/2000... Training loss: 0.2993\n",
      "Epoch: 1383/2000... Training loss: 0.4736\n",
      "Epoch: 1383/2000... Training loss: 0.6576\n",
      "Epoch: 1383/2000... Training loss: 0.6115\n",
      "Epoch: 1383/2000... Training loss: 0.3441\n",
      "Epoch: 1384/2000... Training loss: 0.6668\n",
      "Epoch: 1384/2000... Training loss: 0.6642\n",
      "Epoch: 1384/2000... Training loss: 0.3651\n",
      "Epoch: 1384/2000... Training loss: 0.5006\n",
      "Epoch: 1384/2000... Training loss: 0.5911\n",
      "Epoch: 1384/2000... Training loss: 0.6653\n",
      "Epoch: 1384/2000... Training loss: 0.4044\n",
      "Epoch: 1384/2000... Training loss: 0.3780\n",
      "Epoch: 1384/2000... Training loss: 0.7070\n",
      "Epoch: 1384/2000... Training loss: 0.2837\n",
      "Epoch: 1384/2000... Training loss: 0.3689\n",
      "Epoch: 1384/2000... Training loss: 0.4403\n",
      "Epoch: 1384/2000... Training loss: 0.5298\n",
      "Epoch: 1384/2000... Training loss: 0.2381\n",
      "Epoch: 1384/2000... Training loss: 0.3948\n",
      "Epoch: 1384/2000... Training loss: 0.4059\n",
      "Epoch: 1384/2000... Training loss: 0.4312\n",
      "Epoch: 1384/2000... Training loss: 0.6139\n",
      "Epoch: 1384/2000... Training loss: 0.3430\n",
      "Epoch: 1384/2000... Training loss: 0.4399\n",
      "Epoch: 1384/2000... Training loss: 0.5113\n",
      "Epoch: 1384/2000... Training loss: 0.5124\n",
      "Epoch: 1384/2000... Training loss: 0.3566\n",
      "Epoch: 1384/2000... Training loss: 0.5587\n",
      "Epoch: 1384/2000... Training loss: 0.4068\n",
      "Epoch: 1384/2000... Training loss: 0.3046\n",
      "Epoch: 1384/2000... Training loss: 0.2989\n",
      "Epoch: 1384/2000... Training loss: 0.3385\n",
      "Epoch: 1384/2000... Training loss: 0.4391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1384/2000... Training loss: 0.3157\n",
      "Epoch: 1384/2000... Training loss: 0.3937\n",
      "Epoch: 1385/2000... Training loss: 0.4618\n",
      "Epoch: 1385/2000... Training loss: 0.2687\n",
      "Epoch: 1385/2000... Training loss: 0.5222\n",
      "Epoch: 1385/2000... Training loss: 0.4438\n",
      "Epoch: 1385/2000... Training loss: 0.4523\n",
      "Epoch: 1385/2000... Training loss: 0.5015\n",
      "Epoch: 1385/2000... Training loss: 0.6572\n",
      "Epoch: 1385/2000... Training loss: 0.2659\n",
      "Epoch: 1385/2000... Training loss: 0.4125\n",
      "Epoch: 1385/2000... Training loss: 0.5796\n",
      "Epoch: 1385/2000... Training loss: 0.5094\n",
      "Epoch: 1385/2000... Training loss: 0.3134\n",
      "Epoch: 1385/2000... Training loss: 0.4058\n",
      "Epoch: 1385/2000... Training loss: 0.2600\n",
      "Epoch: 1385/2000... Training loss: 0.5162\n",
      "Epoch: 1385/2000... Training loss: 0.4240\n",
      "Epoch: 1385/2000... Training loss: 0.5481\n",
      "Epoch: 1385/2000... Training loss: 0.6413\n",
      "Epoch: 1385/2000... Training loss: 0.3929\n",
      "Epoch: 1385/2000... Training loss: 0.4047\n",
      "Epoch: 1385/2000... Training loss: 0.4915\n",
      "Epoch: 1385/2000... Training loss: 0.3504\n",
      "Epoch: 1385/2000... Training loss: 0.3668\n",
      "Epoch: 1385/2000... Training loss: 0.4281\n",
      "Epoch: 1385/2000... Training loss: 0.3798\n",
      "Epoch: 1385/2000... Training loss: 0.7085\n",
      "Epoch: 1385/2000... Training loss: 0.5437\n",
      "Epoch: 1385/2000... Training loss: 0.3953\n",
      "Epoch: 1385/2000... Training loss: 0.3378\n",
      "Epoch: 1385/2000... Training loss: 0.4855\n",
      "Epoch: 1385/2000... Training loss: 0.4537\n",
      "Epoch: 1386/2000... Training loss: 0.4582\n",
      "Epoch: 1386/2000... Training loss: 0.3709\n",
      "Epoch: 1386/2000... Training loss: 0.3598\n",
      "Epoch: 1386/2000... Training loss: 0.4375\n",
      "Epoch: 1386/2000... Training loss: 0.3237\n",
      "Epoch: 1386/2000... Training loss: 0.4191\n",
      "Epoch: 1386/2000... Training loss: 0.3018\n",
      "Epoch: 1386/2000... Training loss: 0.4008\n",
      "Epoch: 1386/2000... Training loss: 0.4857\n",
      "Epoch: 1386/2000... Training loss: 0.4764\n",
      "Epoch: 1386/2000... Training loss: 0.5086\n",
      "Epoch: 1386/2000... Training loss: 0.4001\n",
      "Epoch: 1386/2000... Training loss: 0.3872\n",
      "Epoch: 1386/2000... Training loss: 0.5688\n",
      "Epoch: 1386/2000... Training loss: 0.3706\n",
      "Epoch: 1386/2000... Training loss: 0.6061\n",
      "Epoch: 1386/2000... Training loss: 0.3883\n",
      "Epoch: 1386/2000... Training loss: 0.3421\n",
      "Epoch: 1386/2000... Training loss: 0.5345\n",
      "Epoch: 1386/2000... Training loss: 0.3182\n",
      "Epoch: 1386/2000... Training loss: 0.2955\n",
      "Epoch: 1386/2000... Training loss: 0.5107\n",
      "Epoch: 1386/2000... Training loss: 0.4825\n",
      "Epoch: 1386/2000... Training loss: 0.4320\n",
      "Epoch: 1386/2000... Training loss: 0.3146\n",
      "Epoch: 1386/2000... Training loss: 0.3404\n",
      "Epoch: 1386/2000... Training loss: 0.3435\n",
      "Epoch: 1386/2000... Training loss: 0.4150\n",
      "Epoch: 1386/2000... Training loss: 0.5391\n",
      "Epoch: 1386/2000... Training loss: 0.4346\n",
      "Epoch: 1386/2000... Training loss: 0.5388\n",
      "Epoch: 1387/2000... Training loss: 0.4688\n",
      "Epoch: 1387/2000... Training loss: 0.4312\n",
      "Epoch: 1387/2000... Training loss: 0.4872\n",
      "Epoch: 1387/2000... Training loss: 0.3645\n",
      "Epoch: 1387/2000... Training loss: 0.3185\n",
      "Epoch: 1387/2000... Training loss: 0.3598\n",
      "Epoch: 1387/2000... Training loss: 0.3687\n",
      "Epoch: 1387/2000... Training loss: 0.5017\n",
      "Epoch: 1387/2000... Training loss: 0.6776\n",
      "Epoch: 1387/2000... Training loss: 0.4710\n",
      "Epoch: 1387/2000... Training loss: 0.3417\n",
      "Epoch: 1387/2000... Training loss: 0.4267\n",
      "Epoch: 1387/2000... Training loss: 0.3919\n",
      "Epoch: 1387/2000... Training loss: 0.5105\n",
      "Epoch: 1387/2000... Training loss: 0.3011\n",
      "Epoch: 1387/2000... Training loss: 0.3495\n",
      "Epoch: 1387/2000... Training loss: 0.2761\n",
      "Epoch: 1387/2000... Training loss: 0.3487\n",
      "Epoch: 1387/2000... Training loss: 0.4530\n",
      "Epoch: 1387/2000... Training loss: 0.3923\n",
      "Epoch: 1387/2000... Training loss: 0.2885\n",
      "Epoch: 1387/2000... Training loss: 0.5297\n",
      "Epoch: 1387/2000... Training loss: 0.3718\n",
      "Epoch: 1387/2000... Training loss: 0.4160\n",
      "Epoch: 1387/2000... Training loss: 0.4139\n",
      "Epoch: 1387/2000... Training loss: 0.4691\n",
      "Epoch: 1387/2000... Training loss: 0.5403\n",
      "Epoch: 1387/2000... Training loss: 0.2780\n",
      "Epoch: 1387/2000... Training loss: 0.4888\n",
      "Epoch: 1387/2000... Training loss: 0.5130\n",
      "Epoch: 1387/2000... Training loss: 0.3021\n",
      "Epoch: 1388/2000... Training loss: 0.5011\n",
      "Epoch: 1388/2000... Training loss: 0.4057\n",
      "Epoch: 1388/2000... Training loss: 0.4149\n",
      "Epoch: 1388/2000... Training loss: 0.3837\n",
      "Epoch: 1388/2000... Training loss: 0.5456\n",
      "Epoch: 1388/2000... Training loss: 0.3736\n",
      "Epoch: 1388/2000... Training loss: 0.3010\n",
      "Epoch: 1388/2000... Training loss: 0.4121\n",
      "Epoch: 1388/2000... Training loss: 0.3531\n",
      "Epoch: 1388/2000... Training loss: 0.4729\n",
      "Epoch: 1388/2000... Training loss: 0.2819\n",
      "Epoch: 1388/2000... Training loss: 0.3255\n",
      "Epoch: 1388/2000... Training loss: 0.5228\n",
      "Epoch: 1388/2000... Training loss: 0.3281\n",
      "Epoch: 1388/2000... Training loss: 0.4422\n",
      "Epoch: 1388/2000... Training loss: 0.4290\n",
      "Epoch: 1388/2000... Training loss: 0.4198\n",
      "Epoch: 1388/2000... Training loss: 0.4243\n",
      "Epoch: 1388/2000... Training loss: 0.3774\n",
      "Epoch: 1388/2000... Training loss: 0.4110\n",
      "Epoch: 1388/2000... Training loss: 0.4132\n",
      "Epoch: 1388/2000... Training loss: 0.4509\n",
      "Epoch: 1388/2000... Training loss: 0.3317\n",
      "Epoch: 1388/2000... Training loss: 0.4106\n",
      "Epoch: 1388/2000... Training loss: 0.5656\n",
      "Epoch: 1388/2000... Training loss: 0.3818\n",
      "Epoch: 1388/2000... Training loss: 0.4592\n",
      "Epoch: 1388/2000... Training loss: 0.5351\n",
      "Epoch: 1388/2000... Training loss: 0.3650\n",
      "Epoch: 1388/2000... Training loss: 0.4022\n",
      "Epoch: 1388/2000... Training loss: 0.4749\n",
      "Epoch: 1389/2000... Training loss: 0.5584\n",
      "Epoch: 1389/2000... Training loss: 0.5375\n",
      "Epoch: 1389/2000... Training loss: 0.4715\n",
      "Epoch: 1389/2000... Training loss: 0.5790\n",
      "Epoch: 1389/2000... Training loss: 0.3050\n",
      "Epoch: 1389/2000... Training loss: 0.4799\n",
      "Epoch: 1389/2000... Training loss: 0.3559\n",
      "Epoch: 1389/2000... Training loss: 0.3537\n",
      "Epoch: 1389/2000... Training loss: 0.4790\n",
      "Epoch: 1389/2000... Training loss: 0.4119\n",
      "Epoch: 1389/2000... Training loss: 0.4299\n",
      "Epoch: 1389/2000... Training loss: 0.3887\n",
      "Epoch: 1389/2000... Training loss: 0.4034\n",
      "Epoch: 1389/2000... Training loss: 0.3292\n",
      "Epoch: 1389/2000... Training loss: 0.4818\n",
      "Epoch: 1389/2000... Training loss: 0.4322\n",
      "Epoch: 1389/2000... Training loss: 0.4599\n",
      "Epoch: 1389/2000... Training loss: 0.3209\n",
      "Epoch: 1389/2000... Training loss: 0.6173\n",
      "Epoch: 1389/2000... Training loss: 0.4458\n",
      "Epoch: 1389/2000... Training loss: 0.4653\n",
      "Epoch: 1389/2000... Training loss: 0.3700\n",
      "Epoch: 1389/2000... Training loss: 0.4076\n",
      "Epoch: 1389/2000... Training loss: 0.4548\n",
      "Epoch: 1389/2000... Training loss: 0.7053\n",
      "Epoch: 1389/2000... Training loss: 0.4002\n",
      "Epoch: 1389/2000... Training loss: 0.3764\n",
      "Epoch: 1389/2000... Training loss: 0.3353\n",
      "Epoch: 1389/2000... Training loss: 0.4663\n",
      "Epoch: 1389/2000... Training loss: 0.4590\n",
      "Epoch: 1389/2000... Training loss: 0.4822\n",
      "Epoch: 1390/2000... Training loss: 0.6177\n",
      "Epoch: 1390/2000... Training loss: 0.4154\n",
      "Epoch: 1390/2000... Training loss: 0.3986\n",
      "Epoch: 1390/2000... Training loss: 0.4032\n",
      "Epoch: 1390/2000... Training loss: 0.3054\n",
      "Epoch: 1390/2000... Training loss: 0.5096\n",
      "Epoch: 1390/2000... Training loss: 0.3788\n",
      "Epoch: 1390/2000... Training loss: 0.3079\n",
      "Epoch: 1390/2000... Training loss: 0.4007\n",
      "Epoch: 1390/2000... Training loss: 0.5369\n",
      "Epoch: 1390/2000... Training loss: 0.4294\n",
      "Epoch: 1390/2000... Training loss: 0.3476\n",
      "Epoch: 1390/2000... Training loss: 0.5355\n",
      "Epoch: 1390/2000... Training loss: 0.3588\n",
      "Epoch: 1390/2000... Training loss: 0.3875\n",
      "Epoch: 1390/2000... Training loss: 0.4446\n",
      "Epoch: 1390/2000... Training loss: 0.3234\n",
      "Epoch: 1390/2000... Training loss: 0.3548\n",
      "Epoch: 1390/2000... Training loss: 0.4713\n",
      "Epoch: 1390/2000... Training loss: 0.5346\n",
      "Epoch: 1390/2000... Training loss: 0.4098\n",
      "Epoch: 1390/2000... Training loss: 0.3756\n",
      "Epoch: 1390/2000... Training loss: 0.5546\n",
      "Epoch: 1390/2000... Training loss: 0.5054\n",
      "Epoch: 1390/2000... Training loss: 0.3661\n",
      "Epoch: 1390/2000... Training loss: 0.4495\n",
      "Epoch: 1390/2000... Training loss: 0.4245\n",
      "Epoch: 1390/2000... Training loss: 0.3732\n",
      "Epoch: 1390/2000... Training loss: 0.5694\n",
      "Epoch: 1390/2000... Training loss: 0.4701\n",
      "Epoch: 1390/2000... Training loss: 0.6365\n",
      "Epoch: 1391/2000... Training loss: 0.2134\n",
      "Epoch: 1391/2000... Training loss: 0.3540\n",
      "Epoch: 1391/2000... Training loss: 0.5161\n",
      "Epoch: 1391/2000... Training loss: 0.3499\n",
      "Epoch: 1391/2000... Training loss: 0.3688\n",
      "Epoch: 1391/2000... Training loss: 0.4769\n",
      "Epoch: 1391/2000... Training loss: 0.3962\n",
      "Epoch: 1391/2000... Training loss: 0.4442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1391/2000... Training loss: 0.5163\n",
      "Epoch: 1391/2000... Training loss: 0.4799\n",
      "Epoch: 1391/2000... Training loss: 0.3823\n",
      "Epoch: 1391/2000... Training loss: 0.4426\n",
      "Epoch: 1391/2000... Training loss: 0.4019\n",
      "Epoch: 1391/2000... Training loss: 0.4776\n",
      "Epoch: 1391/2000... Training loss: 0.3785\n",
      "Epoch: 1391/2000... Training loss: 0.4004\n",
      "Epoch: 1391/2000... Training loss: 0.4976\n",
      "Epoch: 1391/2000... Training loss: 0.4970\n",
      "Epoch: 1391/2000... Training loss: 0.3885\n",
      "Epoch: 1391/2000... Training loss: 0.4470\n",
      "Epoch: 1391/2000... Training loss: 0.5584\n",
      "Epoch: 1391/2000... Training loss: 0.3102\n",
      "Epoch: 1391/2000... Training loss: 0.5745\n",
      "Epoch: 1391/2000... Training loss: 0.3951\n",
      "Epoch: 1391/2000... Training loss: 0.3948\n",
      "Epoch: 1391/2000... Training loss: 0.4462\n",
      "Epoch: 1391/2000... Training loss: 0.3759\n",
      "Epoch: 1391/2000... Training loss: 0.3447\n",
      "Epoch: 1391/2000... Training loss: 0.4930\n",
      "Epoch: 1391/2000... Training loss: 0.3865\n",
      "Epoch: 1391/2000... Training loss: 0.4631\n",
      "Epoch: 1392/2000... Training loss: 0.6870\n",
      "Epoch: 1392/2000... Training loss: 0.4668\n",
      "Epoch: 1392/2000... Training loss: 0.4127\n",
      "Epoch: 1392/2000... Training loss: 0.3571\n",
      "Epoch: 1392/2000... Training loss: 0.4957\n",
      "Epoch: 1392/2000... Training loss: 0.3856\n",
      "Epoch: 1392/2000... Training loss: 0.4694\n",
      "Epoch: 1392/2000... Training loss: 0.4148\n",
      "Epoch: 1392/2000... Training loss: 0.4212\n",
      "Epoch: 1392/2000... Training loss: 0.5532\n",
      "Epoch: 1392/2000... Training loss: 0.6149\n",
      "Epoch: 1392/2000... Training loss: 0.5494\n",
      "Epoch: 1392/2000... Training loss: 0.3081\n",
      "Epoch: 1392/2000... Training loss: 0.3162\n",
      "Epoch: 1392/2000... Training loss: 0.5529\n",
      "Epoch: 1392/2000... Training loss: 0.4970\n",
      "Epoch: 1392/2000... Training loss: 0.2976\n",
      "Epoch: 1392/2000... Training loss: 0.4070\n",
      "Epoch: 1392/2000... Training loss: 0.3485\n",
      "Epoch: 1392/2000... Training loss: 0.5751\n",
      "Epoch: 1392/2000... Training loss: 0.3967\n",
      "Epoch: 1392/2000... Training loss: 0.3173\n",
      "Epoch: 1392/2000... Training loss: 0.5271\n",
      "Epoch: 1392/2000... Training loss: 0.4233\n",
      "Epoch: 1392/2000... Training loss: 0.5833\n",
      "Epoch: 1392/2000... Training loss: 0.3434\n",
      "Epoch: 1392/2000... Training loss: 0.4185\n",
      "Epoch: 1392/2000... Training loss: 0.2923\n",
      "Epoch: 1392/2000... Training loss: 0.4133\n",
      "Epoch: 1392/2000... Training loss: 0.6442\n",
      "Epoch: 1392/2000... Training loss: 0.3400\n",
      "Epoch: 1393/2000... Training loss: 0.2636\n",
      "Epoch: 1393/2000... Training loss: 0.4339\n",
      "Epoch: 1393/2000... Training loss: 0.5215\n",
      "Epoch: 1393/2000... Training loss: 0.3666\n",
      "Epoch: 1393/2000... Training loss: 0.3304\n",
      "Epoch: 1393/2000... Training loss: 0.4302\n",
      "Epoch: 1393/2000... Training loss: 0.3988\n",
      "Epoch: 1393/2000... Training loss: 0.3839\n",
      "Epoch: 1393/2000... Training loss: 0.5053\n",
      "Epoch: 1393/2000... Training loss: 0.4856\n",
      "Epoch: 1393/2000... Training loss: 0.3034\n",
      "Epoch: 1393/2000... Training loss: 0.2966\n",
      "Epoch: 1393/2000... Training loss: 0.2343\n",
      "Epoch: 1393/2000... Training loss: 0.3596\n",
      "Epoch: 1393/2000... Training loss: 0.4764\n",
      "Epoch: 1393/2000... Training loss: 0.5198\n",
      "Epoch: 1393/2000... Training loss: 0.3479\n",
      "Epoch: 1393/2000... Training loss: 0.5657\n",
      "Epoch: 1393/2000... Training loss: 0.3194\n",
      "Epoch: 1393/2000... Training loss: 0.3821\n",
      "Epoch: 1393/2000... Training loss: 0.4084\n",
      "Epoch: 1393/2000... Training loss: 0.4308\n",
      "Epoch: 1393/2000... Training loss: 0.4287\n",
      "Epoch: 1393/2000... Training loss: 0.4173\n",
      "Epoch: 1393/2000... Training loss: 0.3755\n",
      "Epoch: 1393/2000... Training loss: 0.3524\n",
      "Epoch: 1393/2000... Training loss: 0.3917\n",
      "Epoch: 1393/2000... Training loss: 0.2940\n",
      "Epoch: 1393/2000... Training loss: 0.4586\n",
      "Epoch: 1393/2000... Training loss: 0.4633\n",
      "Epoch: 1393/2000... Training loss: 0.5413\n",
      "Epoch: 1394/2000... Training loss: 0.6795\n",
      "Epoch: 1394/2000... Training loss: 0.2567\n",
      "Epoch: 1394/2000... Training loss: 0.6018\n",
      "Epoch: 1394/2000... Training loss: 0.5101\n",
      "Epoch: 1394/2000... Training loss: 0.7285\n",
      "Epoch: 1394/2000... Training loss: 0.5350\n",
      "Epoch: 1394/2000... Training loss: 0.3833\n",
      "Epoch: 1394/2000... Training loss: 0.5452\n",
      "Epoch: 1394/2000... Training loss: 0.3763\n",
      "Epoch: 1394/2000... Training loss: 0.4055\n",
      "Epoch: 1394/2000... Training loss: 0.3514\n",
      "Epoch: 1394/2000... Training loss: 0.4720\n",
      "Epoch: 1394/2000... Training loss: 0.4147\n",
      "Epoch: 1394/2000... Training loss: 0.3692\n",
      "Epoch: 1394/2000... Training loss: 0.3631\n",
      "Epoch: 1394/2000... Training loss: 0.4892\n",
      "Epoch: 1394/2000... Training loss: 0.5026\n",
      "Epoch: 1394/2000... Training loss: 0.4627\n",
      "Epoch: 1394/2000... Training loss: 0.6357\n",
      "Epoch: 1394/2000... Training loss: 0.2992\n",
      "Epoch: 1394/2000... Training loss: 0.4448\n",
      "Epoch: 1394/2000... Training loss: 0.4916\n",
      "Epoch: 1394/2000... Training loss: 0.5240\n",
      "Epoch: 1394/2000... Training loss: 0.4778\n",
      "Epoch: 1394/2000... Training loss: 0.3696\n",
      "Epoch: 1394/2000... Training loss: 0.5926\n",
      "Epoch: 1394/2000... Training loss: 0.4549\n",
      "Epoch: 1394/2000... Training loss: 0.3307\n",
      "Epoch: 1394/2000... Training loss: 0.5257\n",
      "Epoch: 1394/2000... Training loss: 0.5683\n",
      "Epoch: 1394/2000... Training loss: 0.4167\n",
      "Epoch: 1395/2000... Training loss: 0.4307\n",
      "Epoch: 1395/2000... Training loss: 0.4009\n",
      "Epoch: 1395/2000... Training loss: 0.4765\n",
      "Epoch: 1395/2000... Training loss: 0.4625\n",
      "Epoch: 1395/2000... Training loss: 0.5552\n",
      "Epoch: 1395/2000... Training loss: 0.5096\n",
      "Epoch: 1395/2000... Training loss: 0.3564\n",
      "Epoch: 1395/2000... Training loss: 0.6245\n",
      "Epoch: 1395/2000... Training loss: 0.4467\n",
      "Epoch: 1395/2000... Training loss: 0.5185\n",
      "Epoch: 1395/2000... Training loss: 0.3918\n",
      "Epoch: 1395/2000... Training loss: 0.3367\n",
      "Epoch: 1395/2000... Training loss: 0.5671\n",
      "Epoch: 1395/2000... Training loss: 0.5018\n",
      "Epoch: 1395/2000... Training loss: 0.3299\n",
      "Epoch: 1395/2000... Training loss: 0.4105\n",
      "Epoch: 1395/2000... Training loss: 0.3944\n",
      "Epoch: 1395/2000... Training loss: 0.3104\n",
      "Epoch: 1395/2000... Training loss: 0.3844\n",
      "Epoch: 1395/2000... Training loss: 0.5412\n",
      "Epoch: 1395/2000... Training loss: 0.4677\n",
      "Epoch: 1395/2000... Training loss: 0.3933\n",
      "Epoch: 1395/2000... Training loss: 0.3888\n",
      "Epoch: 1395/2000... Training loss: 0.5912\n",
      "Epoch: 1395/2000... Training loss: 0.2787\n",
      "Epoch: 1395/2000... Training loss: 0.3158\n",
      "Epoch: 1395/2000... Training loss: 0.4989\n",
      "Epoch: 1395/2000... Training loss: 0.4637\n",
      "Epoch: 1395/2000... Training loss: 0.3820\n",
      "Epoch: 1395/2000... Training loss: 0.3491\n",
      "Epoch: 1395/2000... Training loss: 0.3099\n",
      "Epoch: 1396/2000... Training loss: 0.3096\n",
      "Epoch: 1396/2000... Training loss: 0.2273\n",
      "Epoch: 1396/2000... Training loss: 0.4717\n",
      "Epoch: 1396/2000... Training loss: 0.3889\n",
      "Epoch: 1396/2000... Training loss: 0.3773\n",
      "Epoch: 1396/2000... Training loss: 0.4268\n",
      "Epoch: 1396/2000... Training loss: 0.3605\n",
      "Epoch: 1396/2000... Training loss: 0.4075\n",
      "Epoch: 1396/2000... Training loss: 0.4605\n",
      "Epoch: 1396/2000... Training loss: 0.4290\n",
      "Epoch: 1396/2000... Training loss: 0.4445\n",
      "Epoch: 1396/2000... Training loss: 0.5225\n",
      "Epoch: 1396/2000... Training loss: 0.4440\n",
      "Epoch: 1396/2000... Training loss: 0.3455\n",
      "Epoch: 1396/2000... Training loss: 0.4477\n",
      "Epoch: 1396/2000... Training loss: 0.4227\n",
      "Epoch: 1396/2000... Training loss: 0.3376\n",
      "Epoch: 1396/2000... Training loss: 0.2979\n",
      "Epoch: 1396/2000... Training loss: 0.4297\n",
      "Epoch: 1396/2000... Training loss: 0.2919\n",
      "Epoch: 1396/2000... Training loss: 0.4657\n",
      "Epoch: 1396/2000... Training loss: 0.3600\n",
      "Epoch: 1396/2000... Training loss: 0.4009\n",
      "Epoch: 1396/2000... Training loss: 0.6439\n",
      "Epoch: 1396/2000... Training loss: 0.5793\n",
      "Epoch: 1396/2000... Training loss: 0.2710\n",
      "Epoch: 1396/2000... Training loss: 0.3299\n",
      "Epoch: 1396/2000... Training loss: 0.3212\n",
      "Epoch: 1396/2000... Training loss: 0.4259\n",
      "Epoch: 1396/2000... Training loss: 0.4051\n",
      "Epoch: 1396/2000... Training loss: 0.4904\n",
      "Epoch: 1397/2000... Training loss: 0.3848\n",
      "Epoch: 1397/2000... Training loss: 0.3900\n",
      "Epoch: 1397/2000... Training loss: 0.4057\n",
      "Epoch: 1397/2000... Training loss: 0.3481\n",
      "Epoch: 1397/2000... Training loss: 0.3240\n",
      "Epoch: 1397/2000... Training loss: 0.2972\n",
      "Epoch: 1397/2000... Training loss: 0.4413\n",
      "Epoch: 1397/2000... Training loss: 0.3647\n",
      "Epoch: 1397/2000... Training loss: 0.3301\n",
      "Epoch: 1397/2000... Training loss: 0.3427\n",
      "Epoch: 1397/2000... Training loss: 0.4550\n",
      "Epoch: 1397/2000... Training loss: 0.3264\n",
      "Epoch: 1397/2000... Training loss: 0.5241\n",
      "Epoch: 1397/2000... Training loss: 0.4525\n",
      "Epoch: 1397/2000... Training loss: 0.3868\n",
      "Epoch: 1397/2000... Training loss: 0.4730\n",
      "Epoch: 1397/2000... Training loss: 0.3313\n",
      "Epoch: 1397/2000... Training loss: 0.3523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1397/2000... Training loss: 0.2153\n",
      "Epoch: 1397/2000... Training loss: 0.2972\n",
      "Epoch: 1397/2000... Training loss: 0.4511\n",
      "Epoch: 1397/2000... Training loss: 0.4702\n",
      "Epoch: 1397/2000... Training loss: 0.4811\n",
      "Epoch: 1397/2000... Training loss: 0.4527\n",
      "Epoch: 1397/2000... Training loss: 0.4336\n",
      "Epoch: 1397/2000... Training loss: 0.4453\n",
      "Epoch: 1397/2000... Training loss: 0.3910\n",
      "Epoch: 1397/2000... Training loss: 0.3490\n",
      "Epoch: 1397/2000... Training loss: 0.4326\n",
      "Epoch: 1397/2000... Training loss: 0.5055\n",
      "Epoch: 1397/2000... Training loss: 0.5194\n",
      "Epoch: 1398/2000... Training loss: 0.3924\n",
      "Epoch: 1398/2000... Training loss: 0.4816\n",
      "Epoch: 1398/2000... Training loss: 0.2623\n",
      "Epoch: 1398/2000... Training loss: 0.3448\n",
      "Epoch: 1398/2000... Training loss: 0.3994\n",
      "Epoch: 1398/2000... Training loss: 0.4699\n",
      "Epoch: 1398/2000... Training loss: 0.3319\n",
      "Epoch: 1398/2000... Training loss: 0.5342\n",
      "Epoch: 1398/2000... Training loss: 0.4165\n",
      "Epoch: 1398/2000... Training loss: 0.4303\n",
      "Epoch: 1398/2000... Training loss: 0.5531\n",
      "Epoch: 1398/2000... Training loss: 0.5126\n",
      "Epoch: 1398/2000... Training loss: 0.3341\n",
      "Epoch: 1398/2000... Training loss: 0.3736\n",
      "Epoch: 1398/2000... Training loss: 0.3905\n",
      "Epoch: 1398/2000... Training loss: 0.5072\n",
      "Epoch: 1398/2000... Training loss: 0.4067\n",
      "Epoch: 1398/2000... Training loss: 0.3383\n",
      "Epoch: 1398/2000... Training loss: 0.3261\n",
      "Epoch: 1398/2000... Training loss: 0.3605\n",
      "Epoch: 1398/2000... Training loss: 0.4205\n",
      "Epoch: 1398/2000... Training loss: 0.3595\n",
      "Epoch: 1398/2000... Training loss: 0.5521\n",
      "Epoch: 1398/2000... Training loss: 0.4349\n",
      "Epoch: 1398/2000... Training loss: 0.3166\n",
      "Epoch: 1398/2000... Training loss: 0.4201\n",
      "Epoch: 1398/2000... Training loss: 0.3997\n",
      "Epoch: 1398/2000... Training loss: 0.5825\n",
      "Epoch: 1398/2000... Training loss: 0.4025\n",
      "Epoch: 1398/2000... Training loss: 0.4194\n",
      "Epoch: 1398/2000... Training loss: 0.4169\n",
      "Epoch: 1399/2000... Training loss: 0.5442\n",
      "Epoch: 1399/2000... Training loss: 0.2838\n",
      "Epoch: 1399/2000... Training loss: 0.4237\n",
      "Epoch: 1399/2000... Training loss: 0.3755\n",
      "Epoch: 1399/2000... Training loss: 0.3837\n",
      "Epoch: 1399/2000... Training loss: 0.3998\n",
      "Epoch: 1399/2000... Training loss: 0.5154\n",
      "Epoch: 1399/2000... Training loss: 0.3778\n",
      "Epoch: 1399/2000... Training loss: 0.4234\n",
      "Epoch: 1399/2000... Training loss: 0.5223\n",
      "Epoch: 1399/2000... Training loss: 0.4536\n",
      "Epoch: 1399/2000... Training loss: 0.3092\n",
      "Epoch: 1399/2000... Training loss: 0.4249\n",
      "Epoch: 1399/2000... Training loss: 0.4028\n",
      "Epoch: 1399/2000... Training loss: 0.3556\n",
      "Epoch: 1399/2000... Training loss: 0.3554\n",
      "Epoch: 1399/2000... Training loss: 0.3222\n",
      "Epoch: 1399/2000... Training loss: 0.4275\n",
      "Epoch: 1399/2000... Training loss: 0.3118\n",
      "Epoch: 1399/2000... Training loss: 0.3469\n",
      "Epoch: 1399/2000... Training loss: 0.2699\n",
      "Epoch: 1399/2000... Training loss: 0.4764\n",
      "Epoch: 1399/2000... Training loss: 0.5195\n",
      "Epoch: 1399/2000... Training loss: 0.4969\n",
      "Epoch: 1399/2000... Training loss: 0.5973\n",
      "Epoch: 1399/2000... Training loss: 0.4125\n",
      "Epoch: 1399/2000... Training loss: 0.4608\n",
      "Epoch: 1399/2000... Training loss: 0.4958\n",
      "Epoch: 1399/2000... Training loss: 0.4158\n",
      "Epoch: 1399/2000... Training loss: 0.4375\n",
      "Epoch: 1399/2000... Training loss: 0.4245\n",
      "Epoch: 1400/2000... Training loss: 0.3856\n",
      "Epoch: 1400/2000... Training loss: 0.3297\n",
      "Epoch: 1400/2000... Training loss: 0.5348\n",
      "Epoch: 1400/2000... Training loss: 0.3771\n",
      "Epoch: 1400/2000... Training loss: 0.3368\n",
      "Epoch: 1400/2000... Training loss: 0.3642\n",
      "Epoch: 1400/2000... Training loss: 0.4781\n",
      "Epoch: 1400/2000... Training loss: 0.3225\n",
      "Epoch: 1400/2000... Training loss: 0.5239\n",
      "Epoch: 1400/2000... Training loss: 0.4364\n",
      "Epoch: 1400/2000... Training loss: 0.4746\n",
      "Epoch: 1400/2000... Training loss: 0.4206\n",
      "Epoch: 1400/2000... Training loss: 0.4068\n",
      "Epoch: 1400/2000... Training loss: 0.4649\n",
      "Epoch: 1400/2000... Training loss: 0.3323\n",
      "Epoch: 1400/2000... Training loss: 0.4021\n",
      "Epoch: 1400/2000... Training loss: 0.3857\n",
      "Epoch: 1400/2000... Training loss: 0.3343\n",
      "Epoch: 1400/2000... Training loss: 0.6190\n",
      "Epoch: 1400/2000... Training loss: 0.4048\n",
      "Epoch: 1400/2000... Training loss: 0.4391\n",
      "Epoch: 1400/2000... Training loss: 0.3934\n",
      "Epoch: 1400/2000... Training loss: 0.2887\n",
      "Epoch: 1400/2000... Training loss: 0.3401\n",
      "Epoch: 1400/2000... Training loss: 0.3337\n",
      "Epoch: 1400/2000... Training loss: 0.4605\n",
      "Epoch: 1400/2000... Training loss: 0.3470\n",
      "Epoch: 1400/2000... Training loss: 0.4503\n",
      "Epoch: 1400/2000... Training loss: 0.6022\n",
      "Epoch: 1400/2000... Training loss: 0.4112\n",
      "Epoch: 1400/2000... Training loss: 0.6240\n",
      "Epoch: 1401/2000... Training loss: 0.4740\n",
      "Epoch: 1401/2000... Training loss: 0.4359\n",
      "Epoch: 1401/2000... Training loss: 0.4525\n",
      "Epoch: 1401/2000... Training loss: 0.3527\n",
      "Epoch: 1401/2000... Training loss: 0.4391\n",
      "Epoch: 1401/2000... Training loss: 0.5609\n",
      "Epoch: 1401/2000... Training loss: 0.3156\n",
      "Epoch: 1401/2000... Training loss: 0.4143\n",
      "Epoch: 1401/2000... Training loss: 0.5307\n",
      "Epoch: 1401/2000... Training loss: 0.4502\n",
      "Epoch: 1401/2000... Training loss: 0.5719\n",
      "Epoch: 1401/2000... Training loss: 0.2851\n",
      "Epoch: 1401/2000... Training loss: 0.4713\n",
      "Epoch: 1401/2000... Training loss: 0.3928\n",
      "Epoch: 1401/2000... Training loss: 0.3257\n",
      "Epoch: 1401/2000... Training loss: 0.3409\n",
      "Epoch: 1401/2000... Training loss: 0.3644\n",
      "Epoch: 1401/2000... Training loss: 0.3702\n",
      "Epoch: 1401/2000... Training loss: 0.4464\n",
      "Epoch: 1401/2000... Training loss: 0.4879\n",
      "Epoch: 1401/2000... Training loss: 0.2732\n",
      "Epoch: 1401/2000... Training loss: 0.3888\n",
      "Epoch: 1401/2000... Training loss: 0.4386\n",
      "Epoch: 1401/2000... Training loss: 0.6483\n",
      "Epoch: 1401/2000... Training loss: 0.3899\n",
      "Epoch: 1401/2000... Training loss: 0.3153\n",
      "Epoch: 1401/2000... Training loss: 0.3250\n",
      "Epoch: 1401/2000... Training loss: 0.5367\n",
      "Epoch: 1401/2000... Training loss: 0.4233\n",
      "Epoch: 1401/2000... Training loss: 0.3815\n",
      "Epoch: 1401/2000... Training loss: 0.4947\n",
      "Epoch: 1402/2000... Training loss: 0.3659\n",
      "Epoch: 1402/2000... Training loss: 0.3765\n",
      "Epoch: 1402/2000... Training loss: 0.4876\n",
      "Epoch: 1402/2000... Training loss: 0.4515\n",
      "Epoch: 1402/2000... Training loss: 0.3085\n",
      "Epoch: 1402/2000... Training loss: 0.4340\n",
      "Epoch: 1402/2000... Training loss: 0.3558\n",
      "Epoch: 1402/2000... Training loss: 0.4886\n",
      "Epoch: 1402/2000... Training loss: 0.3828\n",
      "Epoch: 1402/2000... Training loss: 0.4696\n",
      "Epoch: 1402/2000... Training loss: 0.3676\n",
      "Epoch: 1402/2000... Training loss: 0.3271\n",
      "Epoch: 1402/2000... Training loss: 0.3228\n",
      "Epoch: 1402/2000... Training loss: 0.4383\n",
      "Epoch: 1402/2000... Training loss: 0.5450\n",
      "Epoch: 1402/2000... Training loss: 0.2451\n",
      "Epoch: 1402/2000... Training loss: 0.5077\n",
      "Epoch: 1402/2000... Training loss: 0.4040\n",
      "Epoch: 1402/2000... Training loss: 0.3337\n",
      "Epoch: 1402/2000... Training loss: 0.3896\n",
      "Epoch: 1402/2000... Training loss: 0.4426\n",
      "Epoch: 1402/2000... Training loss: 0.4082\n",
      "Epoch: 1402/2000... Training loss: 0.3341\n",
      "Epoch: 1402/2000... Training loss: 0.4086\n",
      "Epoch: 1402/2000... Training loss: 0.5951\n",
      "Epoch: 1402/2000... Training loss: 0.4588\n",
      "Epoch: 1402/2000... Training loss: 0.3885\n",
      "Epoch: 1402/2000... Training loss: 0.3822\n",
      "Epoch: 1402/2000... Training loss: 0.4265\n",
      "Epoch: 1402/2000... Training loss: 0.5152\n",
      "Epoch: 1402/2000... Training loss: 0.3557\n",
      "Epoch: 1403/2000... Training loss: 0.5295\n",
      "Epoch: 1403/2000... Training loss: 0.3132\n",
      "Epoch: 1403/2000... Training loss: 0.4790\n",
      "Epoch: 1403/2000... Training loss: 0.3597\n",
      "Epoch: 1403/2000... Training loss: 0.4337\n",
      "Epoch: 1403/2000... Training loss: 0.4655\n",
      "Epoch: 1403/2000... Training loss: 0.4284\n",
      "Epoch: 1403/2000... Training loss: 0.4908\n",
      "Epoch: 1403/2000... Training loss: 0.4025\n",
      "Epoch: 1403/2000... Training loss: 0.5009\n",
      "Epoch: 1403/2000... Training loss: 0.3405\n",
      "Epoch: 1403/2000... Training loss: 0.6184\n",
      "Epoch: 1403/2000... Training loss: 0.4041\n",
      "Epoch: 1403/2000... Training loss: 0.2493\n",
      "Epoch: 1403/2000... Training loss: 0.4175\n",
      "Epoch: 1403/2000... Training loss: 0.3784\n",
      "Epoch: 1403/2000... Training loss: 0.5355\n",
      "Epoch: 1403/2000... Training loss: 0.4167\n",
      "Epoch: 1403/2000... Training loss: 0.3318\n",
      "Epoch: 1403/2000... Training loss: 0.4197\n",
      "Epoch: 1403/2000... Training loss: 0.3928\n",
      "Epoch: 1403/2000... Training loss: 0.3734\n",
      "Epoch: 1403/2000... Training loss: 0.4688\n",
      "Epoch: 1403/2000... Training loss: 0.4438\n",
      "Epoch: 1403/2000... Training loss: 0.4098\n",
      "Epoch: 1403/2000... Training loss: 0.4462\n",
      "Epoch: 1403/2000... Training loss: 0.4154\n",
      "Epoch: 1403/2000... Training loss: 0.5386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1403/2000... Training loss: 0.2854\n",
      "Epoch: 1403/2000... Training loss: 0.4824\n",
      "Epoch: 1403/2000... Training loss: 0.4202\n",
      "Epoch: 1404/2000... Training loss: 0.3857\n",
      "Epoch: 1404/2000... Training loss: 0.5082\n",
      "Epoch: 1404/2000... Training loss: 0.4511\n",
      "Epoch: 1404/2000... Training loss: 0.3856\n",
      "Epoch: 1404/2000... Training loss: 0.4706\n",
      "Epoch: 1404/2000... Training loss: 0.2811\n",
      "Epoch: 1404/2000... Training loss: 0.5513\n",
      "Epoch: 1404/2000... Training loss: 0.5838\n",
      "Epoch: 1404/2000... Training loss: 0.2350\n",
      "Epoch: 1404/2000... Training loss: 0.4687\n",
      "Epoch: 1404/2000... Training loss: 0.3355\n",
      "Epoch: 1404/2000... Training loss: 0.2513\n",
      "Epoch: 1404/2000... Training loss: 0.3630\n",
      "Epoch: 1404/2000... Training loss: 0.3042\n",
      "Epoch: 1404/2000... Training loss: 0.4344\n",
      "Epoch: 1404/2000... Training loss: 0.5360\n",
      "Epoch: 1404/2000... Training loss: 0.2373\n",
      "Epoch: 1404/2000... Training loss: 0.3935\n",
      "Epoch: 1404/2000... Training loss: 0.3753\n",
      "Epoch: 1404/2000... Training loss: 0.5359\n",
      "Epoch: 1404/2000... Training loss: 0.4748\n",
      "Epoch: 1404/2000... Training loss: 0.2602\n",
      "Epoch: 1404/2000... Training loss: 0.4178\n",
      "Epoch: 1404/2000... Training loss: 0.3406\n",
      "Epoch: 1404/2000... Training loss: 0.3698\n",
      "Epoch: 1404/2000... Training loss: 0.4452\n",
      "Epoch: 1404/2000... Training loss: 0.5398\n",
      "Epoch: 1404/2000... Training loss: 0.4804\n",
      "Epoch: 1404/2000... Training loss: 0.3911\n",
      "Epoch: 1404/2000... Training loss: 0.4849\n",
      "Epoch: 1404/2000... Training loss: 0.3399\n",
      "Epoch: 1405/2000... Training loss: 0.4245\n",
      "Epoch: 1405/2000... Training loss: 0.5601\n",
      "Epoch: 1405/2000... Training loss: 0.5120\n",
      "Epoch: 1405/2000... Training loss: 0.5398\n",
      "Epoch: 1405/2000... Training loss: 0.6022\n",
      "Epoch: 1405/2000... Training loss: 0.6738\n",
      "Epoch: 1405/2000... Training loss: 0.1849\n",
      "Epoch: 1405/2000... Training loss: 0.2687\n",
      "Epoch: 1405/2000... Training loss: 0.3730\n",
      "Epoch: 1405/2000... Training loss: 0.5637\n",
      "Epoch: 1405/2000... Training loss: 0.3936\n",
      "Epoch: 1405/2000... Training loss: 0.4485\n",
      "Epoch: 1405/2000... Training loss: 0.3300\n",
      "Epoch: 1405/2000... Training loss: 0.4262\n",
      "Epoch: 1405/2000... Training loss: 0.3047\n",
      "Epoch: 1405/2000... Training loss: 0.5831\n",
      "Epoch: 1405/2000... Training loss: 0.4068\n",
      "Epoch: 1405/2000... Training loss: 0.2868\n",
      "Epoch: 1405/2000... Training loss: 0.2083\n",
      "Epoch: 1405/2000... Training loss: 0.2995\n",
      "Epoch: 1405/2000... Training loss: 0.3861\n",
      "Epoch: 1405/2000... Training loss: 0.3768\n",
      "Epoch: 1405/2000... Training loss: 0.4327\n",
      "Epoch: 1405/2000... Training loss: 0.3907\n",
      "Epoch: 1405/2000... Training loss: 0.2907\n",
      "Epoch: 1405/2000... Training loss: 0.3764\n",
      "Epoch: 1405/2000... Training loss: 0.4494\n",
      "Epoch: 1405/2000... Training loss: 0.4146\n",
      "Epoch: 1405/2000... Training loss: 0.6131\n",
      "Epoch: 1405/2000... Training loss: 0.4005\n",
      "Epoch: 1405/2000... Training loss: 0.2309\n",
      "Epoch: 1406/2000... Training loss: 0.3911\n",
      "Epoch: 1406/2000... Training loss: 0.4165\n",
      "Epoch: 1406/2000... Training loss: 0.4033\n",
      "Epoch: 1406/2000... Training loss: 0.6130\n",
      "Epoch: 1406/2000... Training loss: 0.4121\n",
      "Epoch: 1406/2000... Training loss: 0.3799\n",
      "Epoch: 1406/2000... Training loss: 0.5227\n",
      "Epoch: 1406/2000... Training loss: 0.4622\n",
      "Epoch: 1406/2000... Training loss: 0.4265\n",
      "Epoch: 1406/2000... Training loss: 0.4505\n",
      "Epoch: 1406/2000... Training loss: 0.4559\n",
      "Epoch: 1406/2000... Training loss: 0.4604\n",
      "Epoch: 1406/2000... Training loss: 0.4732\n",
      "Epoch: 1406/2000... Training loss: 0.3646\n",
      "Epoch: 1406/2000... Training loss: 0.4439\n",
      "Epoch: 1406/2000... Training loss: 0.2906\n",
      "Epoch: 1406/2000... Training loss: 0.3009\n",
      "Epoch: 1406/2000... Training loss: 0.5055\n",
      "Epoch: 1406/2000... Training loss: 0.4187\n",
      "Epoch: 1406/2000... Training loss: 0.3295\n",
      "Epoch: 1406/2000... Training loss: 0.3538\n",
      "Epoch: 1406/2000... Training loss: 0.5710\n",
      "Epoch: 1406/2000... Training loss: 0.5357\n",
      "Epoch: 1406/2000... Training loss: 0.4866\n",
      "Epoch: 1406/2000... Training loss: 0.4164\n",
      "Epoch: 1406/2000... Training loss: 0.3615\n",
      "Epoch: 1406/2000... Training loss: 0.4738\n",
      "Epoch: 1406/2000... Training loss: 0.2751\n",
      "Epoch: 1406/2000... Training loss: 0.4424\n",
      "Epoch: 1406/2000... Training loss: 0.4534\n",
      "Epoch: 1406/2000... Training loss: 0.3752\n",
      "Epoch: 1407/2000... Training loss: 0.5594\n",
      "Epoch: 1407/2000... Training loss: 0.4252\n",
      "Epoch: 1407/2000... Training loss: 0.3979\n",
      "Epoch: 1407/2000... Training loss: 0.5872\n",
      "Epoch: 1407/2000... Training loss: 0.5221\n",
      "Epoch: 1407/2000... Training loss: 0.5347\n",
      "Epoch: 1407/2000... Training loss: 0.3367\n",
      "Epoch: 1407/2000... Training loss: 0.4473\n",
      "Epoch: 1407/2000... Training loss: 0.5966\n",
      "Epoch: 1407/2000... Training loss: 0.2619\n",
      "Epoch: 1407/2000... Training loss: 0.5398\n",
      "Epoch: 1407/2000... Training loss: 0.4429\n",
      "Epoch: 1407/2000... Training loss: 0.4988\n",
      "Epoch: 1407/2000... Training loss: 0.5140\n",
      "Epoch: 1407/2000... Training loss: 0.4634\n",
      "Epoch: 1407/2000... Training loss: 0.3136\n",
      "Epoch: 1407/2000... Training loss: 0.4674\n",
      "Epoch: 1407/2000... Training loss: 0.3917\n",
      "Epoch: 1407/2000... Training loss: 0.3848\n",
      "Epoch: 1407/2000... Training loss: 0.3667\n",
      "Epoch: 1407/2000... Training loss: 0.4814\n",
      "Epoch: 1407/2000... Training loss: 0.4106\n",
      "Epoch: 1407/2000... Training loss: 0.5551\n",
      "Epoch: 1407/2000... Training loss: 0.5130\n",
      "Epoch: 1407/2000... Training loss: 0.3477\n",
      "Epoch: 1407/2000... Training loss: 0.4629\n",
      "Epoch: 1407/2000... Training loss: 0.4956\n",
      "Epoch: 1407/2000... Training loss: 0.4044\n",
      "Epoch: 1407/2000... Training loss: 0.3781\n",
      "Epoch: 1407/2000... Training loss: 0.6821\n",
      "Epoch: 1407/2000... Training loss: 0.3016\n",
      "Epoch: 1408/2000... Training loss: 0.2938\n",
      "Epoch: 1408/2000... Training loss: 0.3125\n",
      "Epoch: 1408/2000... Training loss: 0.4286\n",
      "Epoch: 1408/2000... Training loss: 0.4915\n",
      "Epoch: 1408/2000... Training loss: 0.4047\n",
      "Epoch: 1408/2000... Training loss: 0.5625\n",
      "Epoch: 1408/2000... Training loss: 0.5918\n",
      "Epoch: 1408/2000... Training loss: 0.3947\n",
      "Epoch: 1408/2000... Training loss: 0.4294\n",
      "Epoch: 1408/2000... Training loss: 0.3884\n",
      "Epoch: 1408/2000... Training loss: 0.4294\n",
      "Epoch: 1408/2000... Training loss: 0.3991\n",
      "Epoch: 1408/2000... Training loss: 0.5176\n",
      "Epoch: 1408/2000... Training loss: 0.5034\n",
      "Epoch: 1408/2000... Training loss: 0.3888\n",
      "Epoch: 1408/2000... Training loss: 0.6336\n",
      "Epoch: 1408/2000... Training loss: 0.3613\n",
      "Epoch: 1408/2000... Training loss: 0.4447\n",
      "Epoch: 1408/2000... Training loss: 0.5603\n",
      "Epoch: 1408/2000... Training loss: 0.2602\n",
      "Epoch: 1408/2000... Training loss: 0.3191\n",
      "Epoch: 1408/2000... Training loss: 0.4769\n",
      "Epoch: 1408/2000... Training loss: 0.4219\n",
      "Epoch: 1408/2000... Training loss: 0.3212\n",
      "Epoch: 1408/2000... Training loss: 0.5822\n",
      "Epoch: 1408/2000... Training loss: 0.3373\n",
      "Epoch: 1408/2000... Training loss: 0.5690\n",
      "Epoch: 1408/2000... Training loss: 0.2531\n",
      "Epoch: 1408/2000... Training loss: 0.3662\n",
      "Epoch: 1408/2000... Training loss: 0.5248\n",
      "Epoch: 1408/2000... Training loss: 0.4339\n",
      "Epoch: 1409/2000... Training loss: 0.3608\n",
      "Epoch: 1409/2000... Training loss: 0.6282\n",
      "Epoch: 1409/2000... Training loss: 0.4603\n",
      "Epoch: 1409/2000... Training loss: 0.3842\n",
      "Epoch: 1409/2000... Training loss: 0.4440\n",
      "Epoch: 1409/2000... Training loss: 0.3529\n",
      "Epoch: 1409/2000... Training loss: 0.4580\n",
      "Epoch: 1409/2000... Training loss: 0.3369\n",
      "Epoch: 1409/2000... Training loss: 0.4281\n",
      "Epoch: 1409/2000... Training loss: 0.3880\n",
      "Epoch: 1409/2000... Training loss: 0.5415\n",
      "Epoch: 1409/2000... Training loss: 0.4798\n",
      "Epoch: 1409/2000... Training loss: 0.4727\n",
      "Epoch: 1409/2000... Training loss: 0.4138\n",
      "Epoch: 1409/2000... Training loss: 0.5089\n",
      "Epoch: 1409/2000... Training loss: 0.4125\n",
      "Epoch: 1409/2000... Training loss: 0.4473\n",
      "Epoch: 1409/2000... Training loss: 0.5566\n",
      "Epoch: 1409/2000... Training loss: 0.3310\n",
      "Epoch: 1409/2000... Training loss: 0.4829\n",
      "Epoch: 1409/2000... Training loss: 0.4770\n",
      "Epoch: 1409/2000... Training loss: 0.4558\n",
      "Epoch: 1409/2000... Training loss: 0.3452\n",
      "Epoch: 1409/2000... Training loss: 0.3370\n",
      "Epoch: 1409/2000... Training loss: 0.4528\n",
      "Epoch: 1409/2000... Training loss: 0.3574\n",
      "Epoch: 1409/2000... Training loss: 0.4454\n",
      "Epoch: 1409/2000... Training loss: 0.3195\n",
      "Epoch: 1409/2000... Training loss: 0.4449\n",
      "Epoch: 1409/2000... Training loss: 0.4112\n",
      "Epoch: 1409/2000... Training loss: 0.4219\n",
      "Epoch: 1410/2000... Training loss: 0.3516\n",
      "Epoch: 1410/2000... Training loss: 0.4180\n",
      "Epoch: 1410/2000... Training loss: 0.3500\n",
      "Epoch: 1410/2000... Training loss: 0.3176\n",
      "Epoch: 1410/2000... Training loss: 0.5390\n",
      "Epoch: 1410/2000... Training loss: 0.4299\n",
      "Epoch: 1410/2000... Training loss: 0.2505\n",
      "Epoch: 1410/2000... Training loss: 0.5058\n",
      "Epoch: 1410/2000... Training loss: 0.3915\n",
      "Epoch: 1410/2000... Training loss: 0.4114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1410/2000... Training loss: 0.5608\n",
      "Epoch: 1410/2000... Training loss: 0.2355\n",
      "Epoch: 1410/2000... Training loss: 0.3808\n",
      "Epoch: 1410/2000... Training loss: 0.3738\n",
      "Epoch: 1410/2000... Training loss: 0.4534\n",
      "Epoch: 1410/2000... Training loss: 0.5158\n",
      "Epoch: 1410/2000... Training loss: 0.4429\n",
      "Epoch: 1410/2000... Training loss: 0.4555\n",
      "Epoch: 1410/2000... Training loss: 0.3361\n",
      "Epoch: 1410/2000... Training loss: 0.3056\n",
      "Epoch: 1410/2000... Training loss: 0.4482\n",
      "Epoch: 1410/2000... Training loss: 0.4252\n",
      "Epoch: 1410/2000... Training loss: 0.6455\n",
      "Epoch: 1410/2000... Training loss: 0.3659\n",
      "Epoch: 1410/2000... Training loss: 0.4704\n",
      "Epoch: 1410/2000... Training loss: 0.5357\n",
      "Epoch: 1410/2000... Training loss: 0.2267\n",
      "Epoch: 1410/2000... Training loss: 0.5315\n",
      "Epoch: 1410/2000... Training loss: 0.4286\n",
      "Epoch: 1410/2000... Training loss: 0.3962\n",
      "Epoch: 1410/2000... Training loss: 0.3519\n",
      "Epoch: 1411/2000... Training loss: 0.4090\n",
      "Epoch: 1411/2000... Training loss: 0.4011\n",
      "Epoch: 1411/2000... Training loss: 0.4535\n",
      "Epoch: 1411/2000... Training loss: 0.5574\n",
      "Epoch: 1411/2000... Training loss: 0.2731\n",
      "Epoch: 1411/2000... Training loss: 0.3701\n",
      "Epoch: 1411/2000... Training loss: 0.4813\n",
      "Epoch: 1411/2000... Training loss: 0.3865\n",
      "Epoch: 1411/2000... Training loss: 0.6192\n",
      "Epoch: 1411/2000... Training loss: 0.3756\n",
      "Epoch: 1411/2000... Training loss: 0.4823\n",
      "Epoch: 1411/2000... Training loss: 0.3880\n",
      "Epoch: 1411/2000... Training loss: 0.3179\n",
      "Epoch: 1411/2000... Training loss: 0.4552\n",
      "Epoch: 1411/2000... Training loss: 0.3563\n",
      "Epoch: 1411/2000... Training loss: 0.2939\n",
      "Epoch: 1411/2000... Training loss: 0.3399\n",
      "Epoch: 1411/2000... Training loss: 0.6014\n",
      "Epoch: 1411/2000... Training loss: 0.5075\n",
      "Epoch: 1411/2000... Training loss: 0.4672\n",
      "Epoch: 1411/2000... Training loss: 0.2817\n",
      "Epoch: 1411/2000... Training loss: 0.4096\n",
      "Epoch: 1411/2000... Training loss: 0.5545\n",
      "Epoch: 1411/2000... Training loss: 0.5388\n",
      "Epoch: 1411/2000... Training loss: 0.6643\n",
      "Epoch: 1411/2000... Training loss: 0.4159\n",
      "Epoch: 1411/2000... Training loss: 0.3484\n",
      "Epoch: 1411/2000... Training loss: 0.3401\n",
      "Epoch: 1411/2000... Training loss: 0.4704\n",
      "Epoch: 1411/2000... Training loss: 0.4539\n",
      "Epoch: 1411/2000... Training loss: 0.4014\n",
      "Epoch: 1412/2000... Training loss: 0.6704\n",
      "Epoch: 1412/2000... Training loss: 0.4550\n",
      "Epoch: 1412/2000... Training loss: 0.4583\n",
      "Epoch: 1412/2000... Training loss: 0.4351\n",
      "Epoch: 1412/2000... Training loss: 0.5647\n",
      "Epoch: 1412/2000... Training loss: 0.5067\n",
      "Epoch: 1412/2000... Training loss: 0.3143\n",
      "Epoch: 1412/2000... Training loss: 0.4282\n",
      "Epoch: 1412/2000... Training loss: 0.5217\n",
      "Epoch: 1412/2000... Training loss: 0.5067\n",
      "Epoch: 1412/2000... Training loss: 0.4766\n",
      "Epoch: 1412/2000... Training loss: 0.4955\n",
      "Epoch: 1412/2000... Training loss: 0.3498\n",
      "Epoch: 1412/2000... Training loss: 0.3548\n",
      "Epoch: 1412/2000... Training loss: 0.4229\n",
      "Epoch: 1412/2000... Training loss: 0.3993\n",
      "Epoch: 1412/2000... Training loss: 0.2875\n",
      "Epoch: 1412/2000... Training loss: 0.3890\n",
      "Epoch: 1412/2000... Training loss: 0.5026\n",
      "Epoch: 1412/2000... Training loss: 0.3676\n",
      "Epoch: 1412/2000... Training loss: 0.3965\n",
      "Epoch: 1412/2000... Training loss: 0.2953\n",
      "Epoch: 1412/2000... Training loss: 0.3172\n",
      "Epoch: 1412/2000... Training loss: 0.7003\n",
      "Epoch: 1412/2000... Training loss: 0.3566\n",
      "Epoch: 1412/2000... Training loss: 0.4487\n",
      "Epoch: 1412/2000... Training loss: 0.4311\n",
      "Epoch: 1412/2000... Training loss: 0.3993\n",
      "Epoch: 1412/2000... Training loss: 0.4340\n",
      "Epoch: 1412/2000... Training loss: 0.3780\n",
      "Epoch: 1412/2000... Training loss: 0.4995\n",
      "Epoch: 1413/2000... Training loss: 0.4284\n",
      "Epoch: 1413/2000... Training loss: 0.5151\n",
      "Epoch: 1413/2000... Training loss: 0.3298\n",
      "Epoch: 1413/2000... Training loss: 0.4043\n",
      "Epoch: 1413/2000... Training loss: 0.3812\n",
      "Epoch: 1413/2000... Training loss: 0.3914\n",
      "Epoch: 1413/2000... Training loss: 0.5750\n",
      "Epoch: 1413/2000... Training loss: 0.4499\n",
      "Epoch: 1413/2000... Training loss: 0.3844\n",
      "Epoch: 1413/2000... Training loss: 0.3919\n",
      "Epoch: 1413/2000... Training loss: 0.2992\n",
      "Epoch: 1413/2000... Training loss: 0.5556\n",
      "Epoch: 1413/2000... Training loss: 0.3780\n",
      "Epoch: 1413/2000... Training loss: 0.4606\n",
      "Epoch: 1413/2000... Training loss: 0.2324\n",
      "Epoch: 1413/2000... Training loss: 0.4656\n",
      "Epoch: 1413/2000... Training loss: 0.5856\n",
      "Epoch: 1413/2000... Training loss: 0.2784\n",
      "Epoch: 1413/2000... Training loss: 0.3965\n",
      "Epoch: 1413/2000... Training loss: 0.4478\n",
      "Epoch: 1413/2000... Training loss: 0.3914\n",
      "Epoch: 1413/2000... Training loss: 0.4668\n",
      "Epoch: 1413/2000... Training loss: 0.4409\n",
      "Epoch: 1413/2000... Training loss: 0.3118\n",
      "Epoch: 1413/2000... Training loss: 0.3578\n",
      "Epoch: 1413/2000... Training loss: 0.4717\n",
      "Epoch: 1413/2000... Training loss: 0.4095\n",
      "Epoch: 1413/2000... Training loss: 0.3358\n",
      "Epoch: 1413/2000... Training loss: 0.3193\n",
      "Epoch: 1413/2000... Training loss: 0.3552\n",
      "Epoch: 1413/2000... Training loss: 0.5864\n",
      "Epoch: 1414/2000... Training loss: 0.3721\n",
      "Epoch: 1414/2000... Training loss: 0.4702\n",
      "Epoch: 1414/2000... Training loss: 0.4828\n",
      "Epoch: 1414/2000... Training loss: 0.4045\n",
      "Epoch: 1414/2000... Training loss: 0.3749\n",
      "Epoch: 1414/2000... Training loss: 0.4063\n",
      "Epoch: 1414/2000... Training loss: 0.5797\n",
      "Epoch: 1414/2000... Training loss: 0.3810\n",
      "Epoch: 1414/2000... Training loss: 0.3755\n",
      "Epoch: 1414/2000... Training loss: 0.4492\n",
      "Epoch: 1414/2000... Training loss: 0.3896\n",
      "Epoch: 1414/2000... Training loss: 0.3927\n",
      "Epoch: 1414/2000... Training loss: 0.3893\n",
      "Epoch: 1414/2000... Training loss: 0.4128\n",
      "Epoch: 1414/2000... Training loss: 0.4511\n",
      "Epoch: 1414/2000... Training loss: 0.3081\n",
      "Epoch: 1414/2000... Training loss: 0.4721\n",
      "Epoch: 1414/2000... Training loss: 0.4890\n",
      "Epoch: 1414/2000... Training loss: 0.3133\n",
      "Epoch: 1414/2000... Training loss: 0.5267\n",
      "Epoch: 1414/2000... Training loss: 0.2892\n",
      "Epoch: 1414/2000... Training loss: 0.5010\n",
      "Epoch: 1414/2000... Training loss: 0.4645\n",
      "Epoch: 1414/2000... Training loss: 0.3671\n",
      "Epoch: 1414/2000... Training loss: 0.3820\n",
      "Epoch: 1414/2000... Training loss: 0.4835\n",
      "Epoch: 1414/2000... Training loss: 0.4957\n",
      "Epoch: 1414/2000... Training loss: 0.4971\n",
      "Epoch: 1414/2000... Training loss: 0.4871\n",
      "Epoch: 1414/2000... Training loss: 0.4625\n",
      "Epoch: 1414/2000... Training loss: 0.2333\n",
      "Epoch: 1415/2000... Training loss: 0.3258\n",
      "Epoch: 1415/2000... Training loss: 0.4446\n",
      "Epoch: 1415/2000... Training loss: 0.5996\n",
      "Epoch: 1415/2000... Training loss: 0.3334\n",
      "Epoch: 1415/2000... Training loss: 0.3381\n",
      "Epoch: 1415/2000... Training loss: 0.4852\n",
      "Epoch: 1415/2000... Training loss: 0.4513\n",
      "Epoch: 1415/2000... Training loss: 0.4312\n",
      "Epoch: 1415/2000... Training loss: 0.4079\n",
      "Epoch: 1415/2000... Training loss: 0.4343\n",
      "Epoch: 1415/2000... Training loss: 0.3131\n",
      "Epoch: 1415/2000... Training loss: 0.2955\n",
      "Epoch: 1415/2000... Training loss: 0.2876\n",
      "Epoch: 1415/2000... Training loss: 0.5826\n",
      "Epoch: 1415/2000... Training loss: 0.5173\n",
      "Epoch: 1415/2000... Training loss: 0.3241\n",
      "Epoch: 1415/2000... Training loss: 0.4202\n",
      "Epoch: 1415/2000... Training loss: 0.5440\n",
      "Epoch: 1415/2000... Training loss: 0.3483\n",
      "Epoch: 1415/2000... Training loss: 0.3019\n",
      "Epoch: 1415/2000... Training loss: 0.3234\n",
      "Epoch: 1415/2000... Training loss: 0.3603\n",
      "Epoch: 1415/2000... Training loss: 0.4404\n",
      "Epoch: 1415/2000... Training loss: 0.3844\n",
      "Epoch: 1415/2000... Training loss: 0.3845\n",
      "Epoch: 1415/2000... Training loss: 0.2228\n",
      "Epoch: 1415/2000... Training loss: 0.4451\n",
      "Epoch: 1415/2000... Training loss: 0.3931\n",
      "Epoch: 1415/2000... Training loss: 0.3817\n",
      "Epoch: 1415/2000... Training loss: 0.4182\n",
      "Epoch: 1415/2000... Training loss: 0.5468\n",
      "Epoch: 1416/2000... Training loss: 0.5394\n",
      "Epoch: 1416/2000... Training loss: 0.5179\n",
      "Epoch: 1416/2000... Training loss: 0.5239\n",
      "Epoch: 1416/2000... Training loss: 0.4687\n",
      "Epoch: 1416/2000... Training loss: 0.3730\n",
      "Epoch: 1416/2000... Training loss: 0.5271\n",
      "Epoch: 1416/2000... Training loss: 0.3680\n",
      "Epoch: 1416/2000... Training loss: 0.4636\n",
      "Epoch: 1416/2000... Training loss: 0.4229\n",
      "Epoch: 1416/2000... Training loss: 0.3032\n",
      "Epoch: 1416/2000... Training loss: 0.3716\n",
      "Epoch: 1416/2000... Training loss: 0.3433\n",
      "Epoch: 1416/2000... Training loss: 0.5685\n",
      "Epoch: 1416/2000... Training loss: 0.4160\n",
      "Epoch: 1416/2000... Training loss: 0.2602\n",
      "Epoch: 1416/2000... Training loss: 0.3479\n",
      "Epoch: 1416/2000... Training loss: 0.4806\n",
      "Epoch: 1416/2000... Training loss: 0.2941\n",
      "Epoch: 1416/2000... Training loss: 0.2989\n",
      "Epoch: 1416/2000... Training loss: 0.5150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1416/2000... Training loss: 0.5285\n",
      "Epoch: 1416/2000... Training loss: 0.3773\n",
      "Epoch: 1416/2000... Training loss: 0.3581\n",
      "Epoch: 1416/2000... Training loss: 0.3312\n",
      "Epoch: 1416/2000... Training loss: 0.5242\n",
      "Epoch: 1416/2000... Training loss: 0.4770\n",
      "Epoch: 1416/2000... Training loss: 0.4776\n",
      "Epoch: 1416/2000... Training loss: 0.6464\n",
      "Epoch: 1416/2000... Training loss: 0.3768\n",
      "Epoch: 1416/2000... Training loss: 0.5236\n",
      "Epoch: 1416/2000... Training loss: 0.5277\n",
      "Epoch: 1417/2000... Training loss: 0.3718\n",
      "Epoch: 1417/2000... Training loss: 0.3791\n",
      "Epoch: 1417/2000... Training loss: 0.5853\n",
      "Epoch: 1417/2000... Training loss: 0.3054\n",
      "Epoch: 1417/2000... Training loss: 0.3025\n",
      "Epoch: 1417/2000... Training loss: 0.3725\n",
      "Epoch: 1417/2000... Training loss: 0.3236\n",
      "Epoch: 1417/2000... Training loss: 0.3895\n",
      "Epoch: 1417/2000... Training loss: 0.4144\n",
      "Epoch: 1417/2000... Training loss: 0.3547\n",
      "Epoch: 1417/2000... Training loss: 0.4371\n",
      "Epoch: 1417/2000... Training loss: 0.3894\n",
      "Epoch: 1417/2000... Training loss: 0.4961\n",
      "Epoch: 1417/2000... Training loss: 0.3334\n",
      "Epoch: 1417/2000... Training loss: 0.3757\n",
      "Epoch: 1417/2000... Training loss: 0.4442\n",
      "Epoch: 1417/2000... Training loss: 0.3786\n",
      "Epoch: 1417/2000... Training loss: 0.4789\n",
      "Epoch: 1417/2000... Training loss: 0.5028\n",
      "Epoch: 1417/2000... Training loss: 0.4430\n",
      "Epoch: 1417/2000... Training loss: 0.3265\n",
      "Epoch: 1417/2000... Training loss: 0.4792\n",
      "Epoch: 1417/2000... Training loss: 0.4684\n",
      "Epoch: 1417/2000... Training loss: 0.5188\n",
      "Epoch: 1417/2000... Training loss: 0.4352\n",
      "Epoch: 1417/2000... Training loss: 0.4921\n",
      "Epoch: 1417/2000... Training loss: 0.5101\n",
      "Epoch: 1417/2000... Training loss: 0.4356\n",
      "Epoch: 1417/2000... Training loss: 0.5337\n",
      "Epoch: 1417/2000... Training loss: 0.5213\n",
      "Epoch: 1417/2000... Training loss: 0.4432\n",
      "Epoch: 1418/2000... Training loss: 0.4078\n",
      "Epoch: 1418/2000... Training loss: 0.2639\n",
      "Epoch: 1418/2000... Training loss: 0.3775\n",
      "Epoch: 1418/2000... Training loss: 0.3645\n",
      "Epoch: 1418/2000... Training loss: 0.4071\n",
      "Epoch: 1418/2000... Training loss: 0.4881\n",
      "Epoch: 1418/2000... Training loss: 0.2420\n",
      "Epoch: 1418/2000... Training loss: 0.4823\n",
      "Epoch: 1418/2000... Training loss: 0.5218\n",
      "Epoch: 1418/2000... Training loss: 0.3561\n",
      "Epoch: 1418/2000... Training loss: 0.2536\n",
      "Epoch: 1418/2000... Training loss: 0.2865\n",
      "Epoch: 1418/2000... Training loss: 0.3359\n",
      "Epoch: 1418/2000... Training loss: 0.5103\n",
      "Epoch: 1418/2000... Training loss: 0.4115\n",
      "Epoch: 1418/2000... Training loss: 0.5477\n",
      "Epoch: 1418/2000... Training loss: 0.4957\n",
      "Epoch: 1418/2000... Training loss: 0.3626\n",
      "Epoch: 1418/2000... Training loss: 0.4523\n",
      "Epoch: 1418/2000... Training loss: 0.3705\n",
      "Epoch: 1418/2000... Training loss: 0.3698\n",
      "Epoch: 1418/2000... Training loss: 0.6329\n",
      "Epoch: 1418/2000... Training loss: 0.5901\n",
      "Epoch: 1418/2000... Training loss: 0.4113\n",
      "Epoch: 1418/2000... Training loss: 0.3655\n",
      "Epoch: 1418/2000... Training loss: 0.4250\n",
      "Epoch: 1418/2000... Training loss: 0.3766\n",
      "Epoch: 1418/2000... Training loss: 0.3665\n",
      "Epoch: 1418/2000... Training loss: 0.2862\n",
      "Epoch: 1418/2000... Training loss: 0.3073\n",
      "Epoch: 1418/2000... Training loss: 0.4015\n",
      "Epoch: 1419/2000... Training loss: 0.5020\n",
      "Epoch: 1419/2000... Training loss: 0.5737\n",
      "Epoch: 1419/2000... Training loss: 0.4775\n",
      "Epoch: 1419/2000... Training loss: 0.3675\n",
      "Epoch: 1419/2000... Training loss: 0.5099\n",
      "Epoch: 1419/2000... Training loss: 0.4130\n",
      "Epoch: 1419/2000... Training loss: 0.4209\n",
      "Epoch: 1419/2000... Training loss: 0.4323\n",
      "Epoch: 1419/2000... Training loss: 0.3341\n",
      "Epoch: 1419/2000... Training loss: 0.3548\n",
      "Epoch: 1419/2000... Training loss: 0.4948\n",
      "Epoch: 1419/2000... Training loss: 0.4739\n",
      "Epoch: 1419/2000... Training loss: 0.3759\n",
      "Epoch: 1419/2000... Training loss: 0.3793\n",
      "Epoch: 1419/2000... Training loss: 0.2949\n",
      "Epoch: 1419/2000... Training loss: 0.4386\n",
      "Epoch: 1419/2000... Training loss: 0.5641\n",
      "Epoch: 1419/2000... Training loss: 0.4490\n",
      "Epoch: 1419/2000... Training loss: 0.2742\n",
      "Epoch: 1419/2000... Training loss: 0.3245\n",
      "Epoch: 1419/2000... Training loss: 0.4194\n",
      "Epoch: 1419/2000... Training loss: 0.2664\n",
      "Epoch: 1419/2000... Training loss: 0.6655\n",
      "Epoch: 1419/2000... Training loss: 0.4697\n",
      "Epoch: 1419/2000... Training loss: 0.4402\n",
      "Epoch: 1419/2000... Training loss: 0.4537\n",
      "Epoch: 1419/2000... Training loss: 0.3862\n",
      "Epoch: 1419/2000... Training loss: 0.4682\n",
      "Epoch: 1419/2000... Training loss: 0.5579\n",
      "Epoch: 1419/2000... Training loss: 0.4049\n",
      "Epoch: 1419/2000... Training loss: 0.4783\n",
      "Epoch: 1420/2000... Training loss: 0.4050\n",
      "Epoch: 1420/2000... Training loss: 0.3856\n",
      "Epoch: 1420/2000... Training loss: 0.4431\n",
      "Epoch: 1420/2000... Training loss: 0.2612\n",
      "Epoch: 1420/2000... Training loss: 0.4850\n",
      "Epoch: 1420/2000... Training loss: 0.5277\n",
      "Epoch: 1420/2000... Training loss: 0.3843\n",
      "Epoch: 1420/2000... Training loss: 0.4386\n",
      "Epoch: 1420/2000... Training loss: 0.3706\n",
      "Epoch: 1420/2000... Training loss: 0.4021\n",
      "Epoch: 1420/2000... Training loss: 0.5538\n",
      "Epoch: 1420/2000... Training loss: 0.3809\n",
      "Epoch: 1420/2000... Training loss: 0.3619\n",
      "Epoch: 1420/2000... Training loss: 0.4494\n",
      "Epoch: 1420/2000... Training loss: 0.3412\n",
      "Epoch: 1420/2000... Training loss: 0.4380\n",
      "Epoch: 1420/2000... Training loss: 0.4622\n",
      "Epoch: 1420/2000... Training loss: 0.4470\n",
      "Epoch: 1420/2000... Training loss: 0.3411\n",
      "Epoch: 1420/2000... Training loss: 0.4710\n",
      "Epoch: 1420/2000... Training loss: 0.5016\n",
      "Epoch: 1420/2000... Training loss: 0.4252\n",
      "Epoch: 1420/2000... Training loss: 0.3492\n",
      "Epoch: 1420/2000... Training loss: 0.4481\n",
      "Epoch: 1420/2000... Training loss: 0.4138\n",
      "Epoch: 1420/2000... Training loss: 0.5427\n",
      "Epoch: 1420/2000... Training loss: 0.5386\n",
      "Epoch: 1420/2000... Training loss: 0.3146\n",
      "Epoch: 1420/2000... Training loss: 0.4356\n",
      "Epoch: 1420/2000... Training loss: 0.5816\n",
      "Epoch: 1420/2000... Training loss: 0.3945\n",
      "Epoch: 1421/2000... Training loss: 0.6138\n",
      "Epoch: 1421/2000... Training loss: 0.3821\n",
      "Epoch: 1421/2000... Training loss: 0.4051\n",
      "Epoch: 1421/2000... Training loss: 0.5860\n",
      "Epoch: 1421/2000... Training loss: 0.2516\n",
      "Epoch: 1421/2000... Training loss: 0.5311\n",
      "Epoch: 1421/2000... Training loss: 0.4035\n",
      "Epoch: 1421/2000... Training loss: 0.3420\n",
      "Epoch: 1421/2000... Training loss: 0.4193\n",
      "Epoch: 1421/2000... Training loss: 0.5482\n",
      "Epoch: 1421/2000... Training loss: 0.3696\n",
      "Epoch: 1421/2000... Training loss: 0.2697\n",
      "Epoch: 1421/2000... Training loss: 0.4992\n",
      "Epoch: 1421/2000... Training loss: 0.4585\n",
      "Epoch: 1421/2000... Training loss: 0.3650\n",
      "Epoch: 1421/2000... Training loss: 0.3039\n",
      "Epoch: 1421/2000... Training loss: 0.5185\n",
      "Epoch: 1421/2000... Training loss: 0.4273\n",
      "Epoch: 1421/2000... Training loss: 0.3921\n",
      "Epoch: 1421/2000... Training loss: 0.4392\n",
      "Epoch: 1421/2000... Training loss: 0.5302\n",
      "Epoch: 1421/2000... Training loss: 0.6679\n",
      "Epoch: 1421/2000... Training loss: 0.4576\n",
      "Epoch: 1421/2000... Training loss: 0.5944\n",
      "Epoch: 1421/2000... Training loss: 0.4720\n",
      "Epoch: 1421/2000... Training loss: 0.4127\n",
      "Epoch: 1421/2000... Training loss: 0.4757\n",
      "Epoch: 1421/2000... Training loss: 0.3203\n",
      "Epoch: 1421/2000... Training loss: 0.4206\n",
      "Epoch: 1421/2000... Training loss: 0.2976\n",
      "Epoch: 1421/2000... Training loss: 0.6551\n",
      "Epoch: 1422/2000... Training loss: 0.3704\n",
      "Epoch: 1422/2000... Training loss: 0.3933\n",
      "Epoch: 1422/2000... Training loss: 0.6999\n",
      "Epoch: 1422/2000... Training loss: 0.5392\n",
      "Epoch: 1422/2000... Training loss: 0.3522\n",
      "Epoch: 1422/2000... Training loss: 0.4894\n",
      "Epoch: 1422/2000... Training loss: 0.2834\n",
      "Epoch: 1422/2000... Training loss: 0.3748\n",
      "Epoch: 1422/2000... Training loss: 0.3499\n",
      "Epoch: 1422/2000... Training loss: 0.4030\n",
      "Epoch: 1422/2000... Training loss: 0.5490\n",
      "Epoch: 1422/2000... Training loss: 0.3341\n",
      "Epoch: 1422/2000... Training loss: 0.4454\n",
      "Epoch: 1422/2000... Training loss: 0.2626\n",
      "Epoch: 1422/2000... Training loss: 0.4827\n",
      "Epoch: 1422/2000... Training loss: 0.3342\n",
      "Epoch: 1422/2000... Training loss: 0.3841\n",
      "Epoch: 1422/2000... Training loss: 0.4342\n",
      "Epoch: 1422/2000... Training loss: 0.3880\n",
      "Epoch: 1422/2000... Training loss: 0.5899\n",
      "Epoch: 1422/2000... Training loss: 0.5094\n",
      "Epoch: 1422/2000... Training loss: 0.3949\n",
      "Epoch: 1422/2000... Training loss: 0.3450\n",
      "Epoch: 1422/2000... Training loss: 0.3706\n",
      "Epoch: 1422/2000... Training loss: 0.4382\n",
      "Epoch: 1422/2000... Training loss: 0.3651\n",
      "Epoch: 1422/2000... Training loss: 0.4436\n",
      "Epoch: 1422/2000... Training loss: 0.5548\n",
      "Epoch: 1422/2000... Training loss: 0.3810\n",
      "Epoch: 1422/2000... Training loss: 0.4385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1422/2000... Training loss: 0.5107\n",
      "Epoch: 1423/2000... Training loss: 0.4311\n",
      "Epoch: 1423/2000... Training loss: 0.5002\n",
      "Epoch: 1423/2000... Training loss: 0.5082\n",
      "Epoch: 1423/2000... Training loss: 0.3366\n",
      "Epoch: 1423/2000... Training loss: 0.4802\n",
      "Epoch: 1423/2000... Training loss: 0.2687\n",
      "Epoch: 1423/2000... Training loss: 0.5891\n",
      "Epoch: 1423/2000... Training loss: 0.4467\n",
      "Epoch: 1423/2000... Training loss: 0.5092\n",
      "Epoch: 1423/2000... Training loss: 0.5415\n",
      "Epoch: 1423/2000... Training loss: 0.3950\n",
      "Epoch: 1423/2000... Training loss: 0.4992\n",
      "Epoch: 1423/2000... Training loss: 0.3634\n",
      "Epoch: 1423/2000... Training loss: 0.4266\n",
      "Epoch: 1423/2000... Training loss: 0.4285\n",
      "Epoch: 1423/2000... Training loss: 0.6626\n",
      "Epoch: 1423/2000... Training loss: 0.3573\n",
      "Epoch: 1423/2000... Training loss: 0.3129\n",
      "Epoch: 1423/2000... Training loss: 0.4519\n",
      "Epoch: 1423/2000... Training loss: 0.4943\n",
      "Epoch: 1423/2000... Training loss: 0.3918\n",
      "Epoch: 1423/2000... Training loss: 0.2910\n",
      "Epoch: 1423/2000... Training loss: 0.5250\n",
      "Epoch: 1423/2000... Training loss: 0.4531\n",
      "Epoch: 1423/2000... Training loss: 0.4530\n",
      "Epoch: 1423/2000... Training loss: 0.4919\n",
      "Epoch: 1423/2000... Training loss: 0.4599\n",
      "Epoch: 1423/2000... Training loss: 0.4325\n",
      "Epoch: 1423/2000... Training loss: 0.5417\n",
      "Epoch: 1423/2000... Training loss: 0.5683\n",
      "Epoch: 1423/2000... Training loss: 0.4977\n",
      "Epoch: 1424/2000... Training loss: 0.2952\n",
      "Epoch: 1424/2000... Training loss: 0.3837\n",
      "Epoch: 1424/2000... Training loss: 0.5096\n",
      "Epoch: 1424/2000... Training loss: 0.3938\n",
      "Epoch: 1424/2000... Training loss: 0.2999\n",
      "Epoch: 1424/2000... Training loss: 0.3297\n",
      "Epoch: 1424/2000... Training loss: 0.4069\n",
      "Epoch: 1424/2000... Training loss: 0.4882\n",
      "Epoch: 1424/2000... Training loss: 0.2537\n",
      "Epoch: 1424/2000... Training loss: 0.5128\n",
      "Epoch: 1424/2000... Training loss: 0.4573\n",
      "Epoch: 1424/2000... Training loss: 0.3251\n",
      "Epoch: 1424/2000... Training loss: 0.5311\n",
      "Epoch: 1424/2000... Training loss: 0.5544\n",
      "Epoch: 1424/2000... Training loss: 0.4000\n",
      "Epoch: 1424/2000... Training loss: 0.2680\n",
      "Epoch: 1424/2000... Training loss: 0.3790\n",
      "Epoch: 1424/2000... Training loss: 0.6454\n",
      "Epoch: 1424/2000... Training loss: 0.4579\n",
      "Epoch: 1424/2000... Training loss: 0.4238\n",
      "Epoch: 1424/2000... Training loss: 0.4461\n",
      "Epoch: 1424/2000... Training loss: 0.4882\n",
      "Epoch: 1424/2000... Training loss: 0.3741\n",
      "Epoch: 1424/2000... Training loss: 0.4702\n",
      "Epoch: 1424/2000... Training loss: 0.4150\n",
      "Epoch: 1424/2000... Training loss: 0.4634\n",
      "Epoch: 1424/2000... Training loss: 0.5207\n",
      "Epoch: 1424/2000... Training loss: 0.4701\n",
      "Epoch: 1424/2000... Training loss: 0.3009\n",
      "Epoch: 1424/2000... Training loss: 0.4127\n",
      "Epoch: 1424/2000... Training loss: 0.3861\n",
      "Epoch: 1425/2000... Training loss: 0.3471\n",
      "Epoch: 1425/2000... Training loss: 0.2143\n",
      "Epoch: 1425/2000... Training loss: 0.3726\n",
      "Epoch: 1425/2000... Training loss: 0.4707\n",
      "Epoch: 1425/2000... Training loss: 0.6545\n",
      "Epoch: 1425/2000... Training loss: 0.4791\n",
      "Epoch: 1425/2000... Training loss: 0.3588\n",
      "Epoch: 1425/2000... Training loss: 0.4235\n",
      "Epoch: 1425/2000... Training loss: 0.5711\n",
      "Epoch: 1425/2000... Training loss: 0.4656\n",
      "Epoch: 1425/2000... Training loss: 0.5752\n",
      "Epoch: 1425/2000... Training loss: 0.4418\n",
      "Epoch: 1425/2000... Training loss: 0.4256\n",
      "Epoch: 1425/2000... Training loss: 0.5298\n",
      "Epoch: 1425/2000... Training loss: 0.3904\n",
      "Epoch: 1425/2000... Training loss: 0.3019\n",
      "Epoch: 1425/2000... Training loss: 0.4308\n",
      "Epoch: 1425/2000... Training loss: 0.3352\n",
      "Epoch: 1425/2000... Training loss: 0.4884\n",
      "Epoch: 1425/2000... Training loss: 0.3870\n",
      "Epoch: 1425/2000... Training loss: 0.3944\n",
      "Epoch: 1425/2000... Training loss: 0.4128\n",
      "Epoch: 1425/2000... Training loss: 0.4784\n",
      "Epoch: 1425/2000... Training loss: 0.5084\n",
      "Epoch: 1425/2000... Training loss: 0.5596\n",
      "Epoch: 1425/2000... Training loss: 0.3093\n",
      "Epoch: 1425/2000... Training loss: 0.3889\n",
      "Epoch: 1425/2000... Training loss: 0.3616\n",
      "Epoch: 1425/2000... Training loss: 0.4283\n",
      "Epoch: 1425/2000... Training loss: 0.5176\n",
      "Epoch: 1425/2000... Training loss: 0.3004\n",
      "Epoch: 1426/2000... Training loss: 0.4911\n",
      "Epoch: 1426/2000... Training loss: 0.3034\n",
      "Epoch: 1426/2000... Training loss: 0.5547\n",
      "Epoch: 1426/2000... Training loss: 0.3884\n",
      "Epoch: 1426/2000... Training loss: 0.4001\n",
      "Epoch: 1426/2000... Training loss: 0.5528\n",
      "Epoch: 1426/2000... Training loss: 0.2448\n",
      "Epoch: 1426/2000... Training loss: 0.3904\n",
      "Epoch: 1426/2000... Training loss: 0.3913\n",
      "Epoch: 1426/2000... Training loss: 0.3405\n",
      "Epoch: 1426/2000... Training loss: 0.3217\n",
      "Epoch: 1426/2000... Training loss: 0.4884\n",
      "Epoch: 1426/2000... Training loss: 0.3942\n",
      "Epoch: 1426/2000... Training loss: 0.3011\n",
      "Epoch: 1426/2000... Training loss: 0.4642\n",
      "Epoch: 1426/2000... Training loss: 0.5321\n",
      "Epoch: 1426/2000... Training loss: 0.3245\n",
      "Epoch: 1426/2000... Training loss: 0.4079\n",
      "Epoch: 1426/2000... Training loss: 0.3971\n",
      "Epoch: 1426/2000... Training loss: 0.4595\n",
      "Epoch: 1426/2000... Training loss: 0.3777\n",
      "Epoch: 1426/2000... Training loss: 0.4264\n",
      "Epoch: 1426/2000... Training loss: 0.3998\n",
      "Epoch: 1426/2000... Training loss: 0.5622\n",
      "Epoch: 1426/2000... Training loss: 0.6169\n",
      "Epoch: 1426/2000... Training loss: 0.3683\n",
      "Epoch: 1426/2000... Training loss: 0.4764\n",
      "Epoch: 1426/2000... Training loss: 0.6779\n",
      "Epoch: 1426/2000... Training loss: 0.3972\n",
      "Epoch: 1426/2000... Training loss: 0.5465\n",
      "Epoch: 1426/2000... Training loss: 0.4605\n",
      "Epoch: 1427/2000... Training loss: 0.3960\n",
      "Epoch: 1427/2000... Training loss: 0.3539\n",
      "Epoch: 1427/2000... Training loss: 0.4872\n",
      "Epoch: 1427/2000... Training loss: 0.3693\n",
      "Epoch: 1427/2000... Training loss: 0.2888\n",
      "Epoch: 1427/2000... Training loss: 0.4493\n",
      "Epoch: 1427/2000... Training loss: 0.3690\n",
      "Epoch: 1427/2000... Training loss: 0.3348\n",
      "Epoch: 1427/2000... Training loss: 0.4237\n",
      "Epoch: 1427/2000... Training loss: 0.3522\n",
      "Epoch: 1427/2000... Training loss: 0.3717\n",
      "Epoch: 1427/2000... Training loss: 0.4654\n",
      "Epoch: 1427/2000... Training loss: 0.4819\n",
      "Epoch: 1427/2000... Training loss: 0.3154\n",
      "Epoch: 1427/2000... Training loss: 0.5007\n",
      "Epoch: 1427/2000... Training loss: 0.4450\n",
      "Epoch: 1427/2000... Training loss: 0.3690\n",
      "Epoch: 1427/2000... Training loss: 0.4480\n",
      "Epoch: 1427/2000... Training loss: 0.4207\n",
      "Epoch: 1427/2000... Training loss: 0.3715\n",
      "Epoch: 1427/2000... Training loss: 0.3571\n",
      "Epoch: 1427/2000... Training loss: 0.4071\n",
      "Epoch: 1427/2000... Training loss: 0.3568\n",
      "Epoch: 1427/2000... Training loss: 0.4173\n",
      "Epoch: 1427/2000... Training loss: 0.3940\n",
      "Epoch: 1427/2000... Training loss: 0.5498\n",
      "Epoch: 1427/2000... Training loss: 0.4941\n",
      "Epoch: 1427/2000... Training loss: 0.3389\n",
      "Epoch: 1427/2000... Training loss: 0.4931\n",
      "Epoch: 1427/2000... Training loss: 0.3721\n",
      "Epoch: 1427/2000... Training loss: 0.4944\n",
      "Epoch: 1428/2000... Training loss: 0.4116\n",
      "Epoch: 1428/2000... Training loss: 0.5016\n",
      "Epoch: 1428/2000... Training loss: 0.5015\n",
      "Epoch: 1428/2000... Training loss: 0.3349\n",
      "Epoch: 1428/2000... Training loss: 0.2730\n",
      "Epoch: 1428/2000... Training loss: 0.3841\n",
      "Epoch: 1428/2000... Training loss: 0.5222\n",
      "Epoch: 1428/2000... Training loss: 0.3513\n",
      "Epoch: 1428/2000... Training loss: 0.4289\n",
      "Epoch: 1428/2000... Training loss: 0.4656\n",
      "Epoch: 1428/2000... Training loss: 0.4957\n",
      "Epoch: 1428/2000... Training loss: 0.4835\n",
      "Epoch: 1428/2000... Training loss: 0.4023\n",
      "Epoch: 1428/2000... Training loss: 0.5177\n",
      "Epoch: 1428/2000... Training loss: 0.4005\n",
      "Epoch: 1428/2000... Training loss: 0.3722\n",
      "Epoch: 1428/2000... Training loss: 0.2270\n",
      "Epoch: 1428/2000... Training loss: 0.4057\n",
      "Epoch: 1428/2000... Training loss: 0.5566\n",
      "Epoch: 1428/2000... Training loss: 0.7181\n",
      "Epoch: 1428/2000... Training loss: 0.3464\n",
      "Epoch: 1428/2000... Training loss: 0.5132\n",
      "Epoch: 1428/2000... Training loss: 0.4598\n",
      "Epoch: 1428/2000... Training loss: 0.2807\n",
      "Epoch: 1428/2000... Training loss: 0.3351\n",
      "Epoch: 1428/2000... Training loss: 0.3773\n",
      "Epoch: 1428/2000... Training loss: 0.3870\n",
      "Epoch: 1428/2000... Training loss: 0.3928\n",
      "Epoch: 1428/2000... Training loss: 0.4964\n",
      "Epoch: 1428/2000... Training loss: 0.3452\n",
      "Epoch: 1428/2000... Training loss: 0.3795\n",
      "Epoch: 1429/2000... Training loss: 0.4893\n",
      "Epoch: 1429/2000... Training loss: 0.5640\n",
      "Epoch: 1429/2000... Training loss: 0.6190\n",
      "Epoch: 1429/2000... Training loss: 0.3917\n",
      "Epoch: 1429/2000... Training loss: 0.3348\n",
      "Epoch: 1429/2000... Training loss: 0.3522\n",
      "Epoch: 1429/2000... Training loss: 0.5190\n",
      "Epoch: 1429/2000... Training loss: 0.6324\n",
      "Epoch: 1429/2000... Training loss: 0.2744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1429/2000... Training loss: 0.3803\n",
      "Epoch: 1429/2000... Training loss: 0.6840\n",
      "Epoch: 1429/2000... Training loss: 0.3269\n",
      "Epoch: 1429/2000... Training loss: 0.3740\n",
      "Epoch: 1429/2000... Training loss: 0.4635\n",
      "Epoch: 1429/2000... Training loss: 0.4663\n",
      "Epoch: 1429/2000... Training loss: 0.3501\n",
      "Epoch: 1429/2000... Training loss: 0.5240\n",
      "Epoch: 1429/2000... Training loss: 0.4067\n",
      "Epoch: 1429/2000... Training loss: 0.3490\n",
      "Epoch: 1429/2000... Training loss: 0.5660\n",
      "Epoch: 1429/2000... Training loss: 0.5550\n",
      "Epoch: 1429/2000... Training loss: 0.3714\n",
      "Epoch: 1429/2000... Training loss: 0.3795\n",
      "Epoch: 1429/2000... Training loss: 0.4073\n",
      "Epoch: 1429/2000... Training loss: 0.3809\n",
      "Epoch: 1429/2000... Training loss: 0.5468\n",
      "Epoch: 1429/2000... Training loss: 0.3563\n",
      "Epoch: 1429/2000... Training loss: 0.3494\n",
      "Epoch: 1429/2000... Training loss: 0.4645\n",
      "Epoch: 1429/2000... Training loss: 0.4381\n",
      "Epoch: 1429/2000... Training loss: 0.4891\n",
      "Epoch: 1430/2000... Training loss: 0.4374\n",
      "Epoch: 1430/2000... Training loss: 0.3326\n",
      "Epoch: 1430/2000... Training loss: 0.6024\n",
      "Epoch: 1430/2000... Training loss: 0.4852\n",
      "Epoch: 1430/2000... Training loss: 0.3500\n",
      "Epoch: 1430/2000... Training loss: 0.5712\n",
      "Epoch: 1430/2000... Training loss: 0.4629\n",
      "Epoch: 1430/2000... Training loss: 0.3464\n",
      "Epoch: 1430/2000... Training loss: 0.4676\n",
      "Epoch: 1430/2000... Training loss: 0.2640\n",
      "Epoch: 1430/2000... Training loss: 0.3856\n",
      "Epoch: 1430/2000... Training loss: 0.4645\n",
      "Epoch: 1430/2000... Training loss: 0.3482\n",
      "Epoch: 1430/2000... Training loss: 0.4264\n",
      "Epoch: 1430/2000... Training loss: 0.5449\n",
      "Epoch: 1430/2000... Training loss: 0.3428\n",
      "Epoch: 1430/2000... Training loss: 0.4660\n",
      "Epoch: 1430/2000... Training loss: 0.3434\n",
      "Epoch: 1430/2000... Training loss: 0.3397\n",
      "Epoch: 1430/2000... Training loss: 0.5285\n",
      "Epoch: 1430/2000... Training loss: 0.2783\n",
      "Epoch: 1430/2000... Training loss: 0.4298\n",
      "Epoch: 1430/2000... Training loss: 0.3754\n",
      "Epoch: 1430/2000... Training loss: 0.6062\n",
      "Epoch: 1430/2000... Training loss: 0.3150\n",
      "Epoch: 1430/2000... Training loss: 0.3869\n",
      "Epoch: 1430/2000... Training loss: 0.4782\n",
      "Epoch: 1430/2000... Training loss: 0.5356\n",
      "Epoch: 1430/2000... Training loss: 0.5333\n",
      "Epoch: 1430/2000... Training loss: 0.3778\n",
      "Epoch: 1430/2000... Training loss: 0.4971\n",
      "Epoch: 1431/2000... Training loss: 0.3249\n",
      "Epoch: 1431/2000... Training loss: 0.4800\n",
      "Epoch: 1431/2000... Training loss: 0.4072\n",
      "Epoch: 1431/2000... Training loss: 0.4576\n",
      "Epoch: 1431/2000... Training loss: 0.6226\n",
      "Epoch: 1431/2000... Training loss: 0.4131\n",
      "Epoch: 1431/2000... Training loss: 0.3162\n",
      "Epoch: 1431/2000... Training loss: 0.4111\n",
      "Epoch: 1431/2000... Training loss: 0.4589\n",
      "Epoch: 1431/2000... Training loss: 0.4629\n",
      "Epoch: 1431/2000... Training loss: 0.4601\n",
      "Epoch: 1431/2000... Training loss: 0.5273\n",
      "Epoch: 1431/2000... Training loss: 0.3666\n",
      "Epoch: 1431/2000... Training loss: 0.5762\n",
      "Epoch: 1431/2000... Training loss: 0.4825\n",
      "Epoch: 1431/2000... Training loss: 0.4545\n",
      "Epoch: 1431/2000... Training loss: 0.3430\n",
      "Epoch: 1431/2000... Training loss: 0.3528\n",
      "Epoch: 1431/2000... Training loss: 0.4203\n",
      "Epoch: 1431/2000... Training loss: 0.4435\n",
      "Epoch: 1431/2000... Training loss: 0.4433\n",
      "Epoch: 1431/2000... Training loss: 0.4333\n",
      "Epoch: 1431/2000... Training loss: 0.5684\n",
      "Epoch: 1431/2000... Training loss: 0.4649\n",
      "Epoch: 1431/2000... Training loss: 0.3121\n",
      "Epoch: 1431/2000... Training loss: 0.5122\n",
      "Epoch: 1431/2000... Training loss: 0.5869\n",
      "Epoch: 1431/2000... Training loss: 0.3832\n",
      "Epoch: 1431/2000... Training loss: 0.4352\n",
      "Epoch: 1431/2000... Training loss: 0.5646\n",
      "Epoch: 1431/2000... Training loss: 0.5603\n",
      "Epoch: 1432/2000... Training loss: 0.4508\n",
      "Epoch: 1432/2000... Training loss: 0.4209\n",
      "Epoch: 1432/2000... Training loss: 0.5323\n",
      "Epoch: 1432/2000... Training loss: 0.3322\n",
      "Epoch: 1432/2000... Training loss: 0.4082\n",
      "Epoch: 1432/2000... Training loss: 0.6262\n",
      "Epoch: 1432/2000... Training loss: 0.3259\n",
      "Epoch: 1432/2000... Training loss: 0.4737\n",
      "Epoch: 1432/2000... Training loss: 0.5765\n",
      "Epoch: 1432/2000... Training loss: 0.6099\n",
      "Epoch: 1432/2000... Training loss: 0.5913\n",
      "Epoch: 1432/2000... Training loss: 0.3470\n",
      "Epoch: 1432/2000... Training loss: 0.5361\n",
      "Epoch: 1432/2000... Training loss: 0.3113\n",
      "Epoch: 1432/2000... Training loss: 0.4835\n",
      "Epoch: 1432/2000... Training loss: 0.2973\n",
      "Epoch: 1432/2000... Training loss: 0.3094\n",
      "Epoch: 1432/2000... Training loss: 0.5635\n",
      "Epoch: 1432/2000... Training loss: 0.5324\n",
      "Epoch: 1432/2000... Training loss: 0.3704\n",
      "Epoch: 1432/2000... Training loss: 0.5712\n",
      "Epoch: 1432/2000... Training loss: 0.3148\n",
      "Epoch: 1432/2000... Training loss: 0.3865\n",
      "Epoch: 1432/2000... Training loss: 0.3294\n",
      "Epoch: 1432/2000... Training loss: 0.3613\n",
      "Epoch: 1432/2000... Training loss: 0.3631\n",
      "Epoch: 1432/2000... Training loss: 0.4877\n",
      "Epoch: 1432/2000... Training loss: 0.5379\n",
      "Epoch: 1432/2000... Training loss: 0.4368\n",
      "Epoch: 1432/2000... Training loss: 0.5444\n",
      "Epoch: 1432/2000... Training loss: 0.4055\n",
      "Epoch: 1433/2000... Training loss: 0.7144\n",
      "Epoch: 1433/2000... Training loss: 0.3590\n",
      "Epoch: 1433/2000... Training loss: 0.4777\n",
      "Epoch: 1433/2000... Training loss: 0.4400\n",
      "Epoch: 1433/2000... Training loss: 0.3204\n",
      "Epoch: 1433/2000... Training loss: 0.4074\n",
      "Epoch: 1433/2000... Training loss: 0.3859\n",
      "Epoch: 1433/2000... Training loss: 0.5441\n",
      "Epoch: 1433/2000... Training loss: 0.6501\n",
      "Epoch: 1433/2000... Training loss: 0.6560\n",
      "Epoch: 1433/2000... Training loss: 0.3537\n",
      "Epoch: 1433/2000... Training loss: 0.3387\n",
      "Epoch: 1433/2000... Training loss: 0.5134\n",
      "Epoch: 1433/2000... Training loss: 0.5174\n",
      "Epoch: 1433/2000... Training loss: 0.2879\n",
      "Epoch: 1433/2000... Training loss: 0.3848\n",
      "Epoch: 1433/2000... Training loss: 0.4545\n",
      "Epoch: 1433/2000... Training loss: 0.3754\n",
      "Epoch: 1433/2000... Training loss: 0.4930\n",
      "Epoch: 1433/2000... Training loss: 0.4040\n",
      "Epoch: 1433/2000... Training loss: 0.4645\n",
      "Epoch: 1433/2000... Training loss: 0.6376\n",
      "Epoch: 1433/2000... Training loss: 0.3749\n",
      "Epoch: 1433/2000... Training loss: 0.3010\n",
      "Epoch: 1433/2000... Training loss: 0.3965\n",
      "Epoch: 1433/2000... Training loss: 0.3545\n",
      "Epoch: 1433/2000... Training loss: 0.4344\n",
      "Epoch: 1433/2000... Training loss: 0.3654\n",
      "Epoch: 1433/2000... Training loss: 0.3489\n",
      "Epoch: 1433/2000... Training loss: 0.5149\n",
      "Epoch: 1433/2000... Training loss: 0.2960\n",
      "Epoch: 1434/2000... Training loss: 0.3263\n",
      "Epoch: 1434/2000... Training loss: 0.3367\n",
      "Epoch: 1434/2000... Training loss: 0.4742\n",
      "Epoch: 1434/2000... Training loss: 0.4822\n",
      "Epoch: 1434/2000... Training loss: 0.3661\n",
      "Epoch: 1434/2000... Training loss: 0.5169\n",
      "Epoch: 1434/2000... Training loss: 0.5122\n",
      "Epoch: 1434/2000... Training loss: 0.3575\n",
      "Epoch: 1434/2000... Training loss: 0.2533\n",
      "Epoch: 1434/2000... Training loss: 0.4193\n",
      "Epoch: 1434/2000... Training loss: 0.5242\n",
      "Epoch: 1434/2000... Training loss: 0.3206\n",
      "Epoch: 1434/2000... Training loss: 0.3122\n",
      "Epoch: 1434/2000... Training loss: 0.5164\n",
      "Epoch: 1434/2000... Training loss: 0.4772\n",
      "Epoch: 1434/2000... Training loss: 0.4707\n",
      "Epoch: 1434/2000... Training loss: 0.4063\n",
      "Epoch: 1434/2000... Training loss: 0.4158\n",
      "Epoch: 1434/2000... Training loss: 0.4268\n",
      "Epoch: 1434/2000... Training loss: 0.3770\n",
      "Epoch: 1434/2000... Training loss: 0.4052\n",
      "Epoch: 1434/2000... Training loss: 0.3241\n",
      "Epoch: 1434/2000... Training loss: 0.4024\n",
      "Epoch: 1434/2000... Training loss: 0.4906\n",
      "Epoch: 1434/2000... Training loss: 0.4697\n",
      "Epoch: 1434/2000... Training loss: 0.5633\n",
      "Epoch: 1434/2000... Training loss: 0.2305\n",
      "Epoch: 1434/2000... Training loss: 0.3750\n",
      "Epoch: 1434/2000... Training loss: 0.3700\n",
      "Epoch: 1434/2000... Training loss: 0.3884\n",
      "Epoch: 1434/2000... Training loss: 0.4503\n",
      "Epoch: 1435/2000... Training loss: 0.3279\n",
      "Epoch: 1435/2000... Training loss: 0.4465\n",
      "Epoch: 1435/2000... Training loss: 0.4446\n",
      "Epoch: 1435/2000... Training loss: 0.3180\n",
      "Epoch: 1435/2000... Training loss: 0.4041\n",
      "Epoch: 1435/2000... Training loss: 0.4321\n",
      "Epoch: 1435/2000... Training loss: 0.3757\n",
      "Epoch: 1435/2000... Training loss: 0.3242\n",
      "Epoch: 1435/2000... Training loss: 0.5442\n",
      "Epoch: 1435/2000... Training loss: 0.5906\n",
      "Epoch: 1435/2000... Training loss: 0.4303\n",
      "Epoch: 1435/2000... Training loss: 0.3538\n",
      "Epoch: 1435/2000... Training loss: 0.3482\n",
      "Epoch: 1435/2000... Training loss: 0.4683\n",
      "Epoch: 1435/2000... Training loss: 0.2362\n",
      "Epoch: 1435/2000... Training loss: 0.2903\n",
      "Epoch: 1435/2000... Training loss: 0.3753\n",
      "Epoch: 1435/2000... Training loss: 0.5435\n",
      "Epoch: 1435/2000... Training loss: 0.5586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1435/2000... Training loss: 0.2961\n",
      "Epoch: 1435/2000... Training loss: 0.4998\n",
      "Epoch: 1435/2000... Training loss: 0.2783\n",
      "Epoch: 1435/2000... Training loss: 0.4220\n",
      "Epoch: 1435/2000... Training loss: 0.4046\n",
      "Epoch: 1435/2000... Training loss: 0.3545\n",
      "Epoch: 1435/2000... Training loss: 0.3630\n",
      "Epoch: 1435/2000... Training loss: 0.5528\n",
      "Epoch: 1435/2000... Training loss: 0.5633\n",
      "Epoch: 1435/2000... Training loss: 0.4506\n",
      "Epoch: 1435/2000... Training loss: 0.3793\n",
      "Epoch: 1435/2000... Training loss: 0.3603\n",
      "Epoch: 1436/2000... Training loss: 0.4098\n",
      "Epoch: 1436/2000... Training loss: 0.4134\n",
      "Epoch: 1436/2000... Training loss: 0.3338\n",
      "Epoch: 1436/2000... Training loss: 0.5218\n",
      "Epoch: 1436/2000... Training loss: 0.3864\n",
      "Epoch: 1436/2000... Training loss: 0.3744\n",
      "Epoch: 1436/2000... Training loss: 0.5026\n",
      "Epoch: 1436/2000... Training loss: 0.3848\n",
      "Epoch: 1436/2000... Training loss: 0.4236\n",
      "Epoch: 1436/2000... Training loss: 0.4094\n",
      "Epoch: 1436/2000... Training loss: 0.4030\n",
      "Epoch: 1436/2000... Training loss: 0.3614\n",
      "Epoch: 1436/2000... Training loss: 0.4119\n",
      "Epoch: 1436/2000... Training loss: 0.4284\n",
      "Epoch: 1436/2000... Training loss: 0.2826\n",
      "Epoch: 1436/2000... Training loss: 0.3902\n",
      "Epoch: 1436/2000... Training loss: 0.4200\n",
      "Epoch: 1436/2000... Training loss: 0.4723\n",
      "Epoch: 1436/2000... Training loss: 0.2658\n",
      "Epoch: 1436/2000... Training loss: 0.4464\n",
      "Epoch: 1436/2000... Training loss: 0.4235\n",
      "Epoch: 1436/2000... Training loss: 0.4170\n",
      "Epoch: 1436/2000... Training loss: 0.5297\n",
      "Epoch: 1436/2000... Training loss: 0.5015\n",
      "Epoch: 1436/2000... Training loss: 0.4275\n",
      "Epoch: 1436/2000... Training loss: 0.3670\n",
      "Epoch: 1436/2000... Training loss: 0.3919\n",
      "Epoch: 1436/2000... Training loss: 0.2983\n",
      "Epoch: 1436/2000... Training loss: 0.3962\n",
      "Epoch: 1436/2000... Training loss: 0.7070\n",
      "Epoch: 1436/2000... Training loss: 0.4282\n",
      "Epoch: 1437/2000... Training loss: 0.3787\n",
      "Epoch: 1437/2000... Training loss: 0.4352\n",
      "Epoch: 1437/2000... Training loss: 0.5400\n",
      "Epoch: 1437/2000... Training loss: 0.6643\n",
      "Epoch: 1437/2000... Training loss: 0.3141\n",
      "Epoch: 1437/2000... Training loss: 0.3910\n",
      "Epoch: 1437/2000... Training loss: 0.4043\n",
      "Epoch: 1437/2000... Training loss: 0.4620\n",
      "Epoch: 1437/2000... Training loss: 0.3764\n",
      "Epoch: 1437/2000... Training loss: 0.3834\n",
      "Epoch: 1437/2000... Training loss: 0.3446\n",
      "Epoch: 1437/2000... Training loss: 0.2885\n",
      "Epoch: 1437/2000... Training loss: 0.2595\n",
      "Epoch: 1437/2000... Training loss: 0.4246\n",
      "Epoch: 1437/2000... Training loss: 0.5613\n",
      "Epoch: 1437/2000... Training loss: 0.3781\n",
      "Epoch: 1437/2000... Training loss: 0.3319\n",
      "Epoch: 1437/2000... Training loss: 0.3204\n",
      "Epoch: 1437/2000... Training loss: 0.4103\n",
      "Epoch: 1437/2000... Training loss: 0.2961\n",
      "Epoch: 1437/2000... Training loss: 0.3207\n",
      "Epoch: 1437/2000... Training loss: 0.3468\n",
      "Epoch: 1437/2000... Training loss: 0.3410\n",
      "Epoch: 1437/2000... Training loss: 0.4632\n",
      "Epoch: 1437/2000... Training loss: 0.3812\n",
      "Epoch: 1437/2000... Training loss: 0.3027\n",
      "Epoch: 1437/2000... Training loss: 0.3787\n",
      "Epoch: 1437/2000... Training loss: 0.5398\n",
      "Epoch: 1437/2000... Training loss: 0.4889\n",
      "Epoch: 1437/2000... Training loss: 0.3685\n",
      "Epoch: 1437/2000... Training loss: 0.3021\n",
      "Epoch: 1438/2000... Training loss: 0.5127\n",
      "Epoch: 1438/2000... Training loss: 0.3130\n",
      "Epoch: 1438/2000... Training loss: 0.3380\n",
      "Epoch: 1438/2000... Training loss: 0.5536\n",
      "Epoch: 1438/2000... Training loss: 0.2772\n",
      "Epoch: 1438/2000... Training loss: 0.3647\n",
      "Epoch: 1438/2000... Training loss: 0.4120\n",
      "Epoch: 1438/2000... Training loss: 0.5159\n",
      "Epoch: 1438/2000... Training loss: 0.4251\n",
      "Epoch: 1438/2000... Training loss: 0.2682\n",
      "Epoch: 1438/2000... Training loss: 0.5302\n",
      "Epoch: 1438/2000... Training loss: 0.3876\n",
      "Epoch: 1438/2000... Training loss: 0.4475\n",
      "Epoch: 1438/2000... Training loss: 0.4587\n",
      "Epoch: 1438/2000... Training loss: 0.3801\n",
      "Epoch: 1438/2000... Training loss: 0.4638\n",
      "Epoch: 1438/2000... Training loss: 0.3716\n",
      "Epoch: 1438/2000... Training loss: 0.4133\n",
      "Epoch: 1438/2000... Training loss: 0.5875\n",
      "Epoch: 1438/2000... Training loss: 0.5038\n",
      "Epoch: 1438/2000... Training loss: 0.3651\n",
      "Epoch: 1438/2000... Training loss: 0.5780\n",
      "Epoch: 1438/2000... Training loss: 0.3885\n",
      "Epoch: 1438/2000... Training loss: 0.3569\n",
      "Epoch: 1438/2000... Training loss: 0.4057\n",
      "Epoch: 1438/2000... Training loss: 0.4470\n",
      "Epoch: 1438/2000... Training loss: 0.4787\n",
      "Epoch: 1438/2000... Training loss: 0.4230\n",
      "Epoch: 1438/2000... Training loss: 0.5364\n",
      "Epoch: 1438/2000... Training loss: 0.5299\n",
      "Epoch: 1438/2000... Training loss: 0.5153\n",
      "Epoch: 1439/2000... Training loss: 0.4327\n",
      "Epoch: 1439/2000... Training loss: 0.5056\n",
      "Epoch: 1439/2000... Training loss: 0.5242\n",
      "Epoch: 1439/2000... Training loss: 0.4670\n",
      "Epoch: 1439/2000... Training loss: 0.4236\n",
      "Epoch: 1439/2000... Training loss: 0.4842\n",
      "Epoch: 1439/2000... Training loss: 0.3282\n",
      "Epoch: 1439/2000... Training loss: 0.4143\n",
      "Epoch: 1439/2000... Training loss: 0.3252\n",
      "Epoch: 1439/2000... Training loss: 0.5447\n",
      "Epoch: 1439/2000... Training loss: 0.3486\n",
      "Epoch: 1439/2000... Training loss: 0.3586\n",
      "Epoch: 1439/2000... Training loss: 0.5797\n",
      "Epoch: 1439/2000... Training loss: 0.5153\n",
      "Epoch: 1439/2000... Training loss: 0.3253\n",
      "Epoch: 1439/2000... Training loss: 0.3413\n",
      "Epoch: 1439/2000... Training loss: 0.5296\n",
      "Epoch: 1439/2000... Training loss: 0.5118\n",
      "Epoch: 1439/2000... Training loss: 0.4124\n",
      "Epoch: 1439/2000... Training loss: 0.4386\n",
      "Epoch: 1439/2000... Training loss: 0.5441\n",
      "Epoch: 1439/2000... Training loss: 0.6039\n",
      "Epoch: 1439/2000... Training loss: 0.4066\n",
      "Epoch: 1439/2000... Training loss: 0.3983\n",
      "Epoch: 1439/2000... Training loss: 0.2803\n",
      "Epoch: 1439/2000... Training loss: 0.4188\n",
      "Epoch: 1439/2000... Training loss: 0.3594\n",
      "Epoch: 1439/2000... Training loss: 0.2711\n",
      "Epoch: 1439/2000... Training loss: 0.3867\n",
      "Epoch: 1439/2000... Training loss: 0.5297\n",
      "Epoch: 1439/2000... Training loss: 0.3551\n",
      "Epoch: 1440/2000... Training loss: 0.5163\n",
      "Epoch: 1440/2000... Training loss: 0.3868\n",
      "Epoch: 1440/2000... Training loss: 0.4061\n",
      "Epoch: 1440/2000... Training loss: 0.5078\n",
      "Epoch: 1440/2000... Training loss: 0.4590\n",
      "Epoch: 1440/2000... Training loss: 0.4558\n",
      "Epoch: 1440/2000... Training loss: 0.3027\n",
      "Epoch: 1440/2000... Training loss: 0.4315\n",
      "Epoch: 1440/2000... Training loss: 0.3441\n",
      "Epoch: 1440/2000... Training loss: 0.4808\n",
      "Epoch: 1440/2000... Training loss: 0.3840\n",
      "Epoch: 1440/2000... Training loss: 0.4019\n",
      "Epoch: 1440/2000... Training loss: 0.3085\n",
      "Epoch: 1440/2000... Training loss: 0.2942\n",
      "Epoch: 1440/2000... Training loss: 0.5560\n",
      "Epoch: 1440/2000... Training loss: 0.3482\n",
      "Epoch: 1440/2000... Training loss: 0.4929\n",
      "Epoch: 1440/2000... Training loss: 0.3821\n",
      "Epoch: 1440/2000... Training loss: 0.3742\n",
      "Epoch: 1440/2000... Training loss: 0.2639\n",
      "Epoch: 1440/2000... Training loss: 0.4088\n",
      "Epoch: 1440/2000... Training loss: 0.2792\n",
      "Epoch: 1440/2000... Training loss: 0.4960\n",
      "Epoch: 1440/2000... Training loss: 0.6288\n",
      "Epoch: 1440/2000... Training loss: 0.5075\n",
      "Epoch: 1440/2000... Training loss: 0.2189\n",
      "Epoch: 1440/2000... Training loss: 0.5171\n",
      "Epoch: 1440/2000... Training loss: 0.4989\n",
      "Epoch: 1440/2000... Training loss: 0.4491\n",
      "Epoch: 1440/2000... Training loss: 0.4924\n",
      "Epoch: 1440/2000... Training loss: 0.3609\n",
      "Epoch: 1441/2000... Training loss: 0.6023\n",
      "Epoch: 1441/2000... Training loss: 0.4251\n",
      "Epoch: 1441/2000... Training loss: 0.3918\n",
      "Epoch: 1441/2000... Training loss: 0.3929\n",
      "Epoch: 1441/2000... Training loss: 0.2723\n",
      "Epoch: 1441/2000... Training loss: 0.2918\n",
      "Epoch: 1441/2000... Training loss: 0.4121\n",
      "Epoch: 1441/2000... Training loss: 0.5607\n",
      "Epoch: 1441/2000... Training loss: 0.3294\n",
      "Epoch: 1441/2000... Training loss: 0.5134\n",
      "Epoch: 1441/2000... Training loss: 0.3572\n",
      "Epoch: 1441/2000... Training loss: 0.4600\n",
      "Epoch: 1441/2000... Training loss: 0.3959\n",
      "Epoch: 1441/2000... Training loss: 0.3244\n",
      "Epoch: 1441/2000... Training loss: 0.4669\n",
      "Epoch: 1441/2000... Training loss: 0.2609\n",
      "Epoch: 1441/2000... Training loss: 0.5189\n",
      "Epoch: 1441/2000... Training loss: 0.4120\n",
      "Epoch: 1441/2000... Training loss: 0.5383\n",
      "Epoch: 1441/2000... Training loss: 0.3282\n",
      "Epoch: 1441/2000... Training loss: 0.3918\n",
      "Epoch: 1441/2000... Training loss: 0.6237\n",
      "Epoch: 1441/2000... Training loss: 0.5724\n",
      "Epoch: 1441/2000... Training loss: 0.5882\n",
      "Epoch: 1441/2000... Training loss: 0.3568\n",
      "Epoch: 1441/2000... Training loss: 0.4346\n",
      "Epoch: 1441/2000... Training loss: 0.4321\n",
      "Epoch: 1441/2000... Training loss: 0.2544\n",
      "Epoch: 1441/2000... Training loss: 0.4019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1441/2000... Training loss: 0.4448\n",
      "Epoch: 1441/2000... Training loss: 0.4757\n",
      "Epoch: 1442/2000... Training loss: 0.2685\n",
      "Epoch: 1442/2000... Training loss: 0.3377\n",
      "Epoch: 1442/2000... Training loss: 0.3363\n",
      "Epoch: 1442/2000... Training loss: 0.3811\n",
      "Epoch: 1442/2000... Training loss: 0.4750\n",
      "Epoch: 1442/2000... Training loss: 0.6431\n",
      "Epoch: 1442/2000... Training loss: 0.4177\n",
      "Epoch: 1442/2000... Training loss: 0.4659\n",
      "Epoch: 1442/2000... Training loss: 0.4451\n",
      "Epoch: 1442/2000... Training loss: 0.3548\n",
      "Epoch: 1442/2000... Training loss: 0.3259\n",
      "Epoch: 1442/2000... Training loss: 0.3094\n",
      "Epoch: 1442/2000... Training loss: 0.3850\n",
      "Epoch: 1442/2000... Training loss: 0.3611\n",
      "Epoch: 1442/2000... Training loss: 0.2676\n",
      "Epoch: 1442/2000... Training loss: 0.4418\n",
      "Epoch: 1442/2000... Training loss: 0.4474\n",
      "Epoch: 1442/2000... Training loss: 0.4031\n",
      "Epoch: 1442/2000... Training loss: 0.3917\n",
      "Epoch: 1442/2000... Training loss: 0.3993\n",
      "Epoch: 1442/2000... Training loss: 0.2489\n",
      "Epoch: 1442/2000... Training loss: 0.4017\n",
      "Epoch: 1442/2000... Training loss: 0.3845\n",
      "Epoch: 1442/2000... Training loss: 0.5579\n",
      "Epoch: 1442/2000... Training loss: 0.4213\n",
      "Epoch: 1442/2000... Training loss: 0.6090\n",
      "Epoch: 1442/2000... Training loss: 0.5341\n",
      "Epoch: 1442/2000... Training loss: 0.4056\n",
      "Epoch: 1442/2000... Training loss: 0.2948\n",
      "Epoch: 1442/2000... Training loss: 0.4356\n",
      "Epoch: 1442/2000... Training loss: 0.3161\n",
      "Epoch: 1443/2000... Training loss: 0.3310\n",
      "Epoch: 1443/2000... Training loss: 0.4802\n",
      "Epoch: 1443/2000... Training loss: 0.4635\n",
      "Epoch: 1443/2000... Training loss: 0.5921\n",
      "Epoch: 1443/2000... Training loss: 0.5379\n",
      "Epoch: 1443/2000... Training loss: 0.2275\n",
      "Epoch: 1443/2000... Training loss: 0.4210\n",
      "Epoch: 1443/2000... Training loss: 0.4052\n",
      "Epoch: 1443/2000... Training loss: 0.3913\n",
      "Epoch: 1443/2000... Training loss: 0.2705\n",
      "Epoch: 1443/2000... Training loss: 0.4783\n",
      "Epoch: 1443/2000... Training loss: 0.5676\n",
      "Epoch: 1443/2000... Training loss: 0.3781\n",
      "Epoch: 1443/2000... Training loss: 0.3669\n",
      "Epoch: 1443/2000... Training loss: 0.4273\n",
      "Epoch: 1443/2000... Training loss: 0.4880\n",
      "Epoch: 1443/2000... Training loss: 0.3443\n",
      "Epoch: 1443/2000... Training loss: 0.4919\n",
      "Epoch: 1443/2000... Training loss: 0.3780\n",
      "Epoch: 1443/2000... Training loss: 0.5183\n",
      "Epoch: 1443/2000... Training loss: 0.3884\n",
      "Epoch: 1443/2000... Training loss: 0.4511\n",
      "Epoch: 1443/2000... Training loss: 0.4872\n",
      "Epoch: 1443/2000... Training loss: 0.3843\n",
      "Epoch: 1443/2000... Training loss: 0.3697\n",
      "Epoch: 1443/2000... Training loss: 0.5484\n",
      "Epoch: 1443/2000... Training loss: 0.5472\n",
      "Epoch: 1443/2000... Training loss: 0.4312\n",
      "Epoch: 1443/2000... Training loss: 0.6216\n",
      "Epoch: 1443/2000... Training loss: 0.4632\n",
      "Epoch: 1443/2000... Training loss: 0.4253\n",
      "Epoch: 1444/2000... Training loss: 0.4451\n",
      "Epoch: 1444/2000... Training loss: 0.3615\n",
      "Epoch: 1444/2000... Training loss: 0.4956\n",
      "Epoch: 1444/2000... Training loss: 0.2832\n",
      "Epoch: 1444/2000... Training loss: 0.4043\n",
      "Epoch: 1444/2000... Training loss: 0.5847\n",
      "Epoch: 1444/2000... Training loss: 0.4069\n",
      "Epoch: 1444/2000... Training loss: 0.2805\n",
      "Epoch: 1444/2000... Training loss: 0.4543\n",
      "Epoch: 1444/2000... Training loss: 0.3358\n",
      "Epoch: 1444/2000... Training loss: 0.4694\n",
      "Epoch: 1444/2000... Training loss: 0.4482\n",
      "Epoch: 1444/2000... Training loss: 0.4325\n",
      "Epoch: 1444/2000... Training loss: 0.3901\n",
      "Epoch: 1444/2000... Training loss: 0.4359\n",
      "Epoch: 1444/2000... Training loss: 0.2910\n",
      "Epoch: 1444/2000... Training loss: 0.4553\n",
      "Epoch: 1444/2000... Training loss: 0.5186\n",
      "Epoch: 1444/2000... Training loss: 0.3087\n",
      "Epoch: 1444/2000... Training loss: 0.3504\n",
      "Epoch: 1444/2000... Training loss: 0.3789\n",
      "Epoch: 1444/2000... Training loss: 0.2369\n",
      "Epoch: 1444/2000... Training loss: 0.4653\n",
      "Epoch: 1444/2000... Training loss: 0.4032\n",
      "Epoch: 1444/2000... Training loss: 0.5215\n",
      "Epoch: 1444/2000... Training loss: 0.6353\n",
      "Epoch: 1444/2000... Training loss: 0.3586\n",
      "Epoch: 1444/2000... Training loss: 0.3500\n",
      "Epoch: 1444/2000... Training loss: 0.5983\n",
      "Epoch: 1444/2000... Training loss: 0.5216\n",
      "Epoch: 1444/2000... Training loss: 0.5736\n",
      "Epoch: 1445/2000... Training loss: 0.4440\n",
      "Epoch: 1445/2000... Training loss: 0.3905\n",
      "Epoch: 1445/2000... Training loss: 0.3588\n",
      "Epoch: 1445/2000... Training loss: 0.3311\n",
      "Epoch: 1445/2000... Training loss: 0.4157\n",
      "Epoch: 1445/2000... Training loss: 0.5039\n",
      "Epoch: 1445/2000... Training loss: 0.3019\n",
      "Epoch: 1445/2000... Training loss: 0.2431\n",
      "Epoch: 1445/2000... Training loss: 0.5260\n",
      "Epoch: 1445/2000... Training loss: 0.4818\n",
      "Epoch: 1445/2000... Training loss: 0.3105\n",
      "Epoch: 1445/2000... Training loss: 0.4784\n",
      "Epoch: 1445/2000... Training loss: 0.3654\n",
      "Epoch: 1445/2000... Training loss: 0.3942\n",
      "Epoch: 1445/2000... Training loss: 0.5818\n",
      "Epoch: 1445/2000... Training loss: 0.3885\n",
      "Epoch: 1445/2000... Training loss: 0.5038\n",
      "Epoch: 1445/2000... Training loss: 0.6453\n",
      "Epoch: 1445/2000... Training loss: 0.3831\n",
      "Epoch: 1445/2000... Training loss: 0.4231\n",
      "Epoch: 1445/2000... Training loss: 0.3569\n",
      "Epoch: 1445/2000... Training loss: 0.4228\n",
      "Epoch: 1445/2000... Training loss: 0.5875\n",
      "Epoch: 1445/2000... Training loss: 0.4045\n",
      "Epoch: 1445/2000... Training loss: 0.6020\n",
      "Epoch: 1445/2000... Training loss: 0.6195\n",
      "Epoch: 1445/2000... Training loss: 0.4377\n",
      "Epoch: 1445/2000... Training loss: 0.4435\n",
      "Epoch: 1445/2000... Training loss: 0.2566\n",
      "Epoch: 1445/2000... Training loss: 0.5562\n",
      "Epoch: 1445/2000... Training loss: 0.3884\n",
      "Epoch: 1446/2000... Training loss: 0.4451\n",
      "Epoch: 1446/2000... Training loss: 0.4854\n",
      "Epoch: 1446/2000... Training loss: 0.7399\n",
      "Epoch: 1446/2000... Training loss: 0.3521\n",
      "Epoch: 1446/2000... Training loss: 0.4741\n",
      "Epoch: 1446/2000... Training loss: 0.3731\n",
      "Epoch: 1446/2000... Training loss: 0.4362\n",
      "Epoch: 1446/2000... Training loss: 0.3870\n",
      "Epoch: 1446/2000... Training loss: 0.4327\n",
      "Epoch: 1446/2000... Training loss: 0.4617\n",
      "Epoch: 1446/2000... Training loss: 0.4311\n",
      "Epoch: 1446/2000... Training loss: 0.2678\n",
      "Epoch: 1446/2000... Training loss: 0.3118\n",
      "Epoch: 1446/2000... Training loss: 0.3359\n",
      "Epoch: 1446/2000... Training loss: 0.3764\n",
      "Epoch: 1446/2000... Training loss: 0.4083\n",
      "Epoch: 1446/2000... Training loss: 0.5909\n",
      "Epoch: 1446/2000... Training loss: 0.3448\n",
      "Epoch: 1446/2000... Training loss: 0.3421\n",
      "Epoch: 1446/2000... Training loss: 0.4622\n",
      "Epoch: 1446/2000... Training loss: 0.4138\n",
      "Epoch: 1446/2000... Training loss: 0.3760\n",
      "Epoch: 1446/2000... Training loss: 0.4372\n",
      "Epoch: 1446/2000... Training loss: 0.3130\n",
      "Epoch: 1446/2000... Training loss: 0.2588\n",
      "Epoch: 1446/2000... Training loss: 0.6510\n",
      "Epoch: 1446/2000... Training loss: 0.5834\n",
      "Epoch: 1446/2000... Training loss: 0.4616\n",
      "Epoch: 1446/2000... Training loss: 0.3706\n",
      "Epoch: 1446/2000... Training loss: 0.5344\n",
      "Epoch: 1446/2000... Training loss: 0.5454\n",
      "Epoch: 1447/2000... Training loss: 0.5276\n",
      "Epoch: 1447/2000... Training loss: 0.4728\n",
      "Epoch: 1447/2000... Training loss: 0.4898\n",
      "Epoch: 1447/2000... Training loss: 0.3400\n",
      "Epoch: 1447/2000... Training loss: 0.3727\n",
      "Epoch: 1447/2000... Training loss: 0.5023\n",
      "Epoch: 1447/2000... Training loss: 0.3468\n",
      "Epoch: 1447/2000... Training loss: 0.5374\n",
      "Epoch: 1447/2000... Training loss: 0.3941\n",
      "Epoch: 1447/2000... Training loss: 0.3488\n",
      "Epoch: 1447/2000... Training loss: 0.2705\n",
      "Epoch: 1447/2000... Training loss: 0.5280\n",
      "Epoch: 1447/2000... Training loss: 0.4486\n",
      "Epoch: 1447/2000... Training loss: 0.4742\n",
      "Epoch: 1447/2000... Training loss: 0.2015\n",
      "Epoch: 1447/2000... Training loss: 0.4280\n",
      "Epoch: 1447/2000... Training loss: 0.4013\n",
      "Epoch: 1447/2000... Training loss: 0.2977\n",
      "Epoch: 1447/2000... Training loss: 0.4089\n",
      "Epoch: 1447/2000... Training loss: 0.3352\n",
      "Epoch: 1447/2000... Training loss: 0.3615\n",
      "Epoch: 1447/2000... Training loss: 0.5661\n",
      "Epoch: 1447/2000... Training loss: 0.4179\n",
      "Epoch: 1447/2000... Training loss: 0.3433\n",
      "Epoch: 1447/2000... Training loss: 0.3265\n",
      "Epoch: 1447/2000... Training loss: 0.4427\n",
      "Epoch: 1447/2000... Training loss: 0.4109\n",
      "Epoch: 1447/2000... Training loss: 0.3722\n",
      "Epoch: 1447/2000... Training loss: 0.4375\n",
      "Epoch: 1447/2000... Training loss: 0.6133\n",
      "Epoch: 1447/2000... Training loss: 0.4470\n",
      "Epoch: 1448/2000... Training loss: 0.4598\n",
      "Epoch: 1448/2000... Training loss: 0.3691\n",
      "Epoch: 1448/2000... Training loss: 0.2701\n",
      "Epoch: 1448/2000... Training loss: 0.4708\n",
      "Epoch: 1448/2000... Training loss: 0.2634\n",
      "Epoch: 1448/2000... Training loss: 0.7098\n",
      "Epoch: 1448/2000... Training loss: 0.4473\n",
      "Epoch: 1448/2000... Training loss: 0.3788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1448/2000... Training loss: 0.3951\n",
      "Epoch: 1448/2000... Training loss: 0.4335\n",
      "Epoch: 1448/2000... Training loss: 0.2731\n",
      "Epoch: 1448/2000... Training loss: 0.5773\n",
      "Epoch: 1448/2000... Training loss: 0.4942\n",
      "Epoch: 1448/2000... Training loss: 0.4436\n",
      "Epoch: 1448/2000... Training loss: 0.4364\n",
      "Epoch: 1448/2000... Training loss: 0.3876\n",
      "Epoch: 1448/2000... Training loss: 0.3670\n",
      "Epoch: 1448/2000... Training loss: 0.2949\n",
      "Epoch: 1448/2000... Training loss: 0.4265\n",
      "Epoch: 1448/2000... Training loss: 0.4729\n",
      "Epoch: 1448/2000... Training loss: 0.3778\n",
      "Epoch: 1448/2000... Training loss: 0.3769\n",
      "Epoch: 1448/2000... Training loss: 0.3757\n",
      "Epoch: 1448/2000... Training loss: 0.5874\n",
      "Epoch: 1448/2000... Training loss: 0.4518\n",
      "Epoch: 1448/2000... Training loss: 0.4686\n",
      "Epoch: 1448/2000... Training loss: 0.5633\n",
      "Epoch: 1448/2000... Training loss: 0.6217\n",
      "Epoch: 1448/2000... Training loss: 0.3079\n",
      "Epoch: 1448/2000... Training loss: 0.6082\n",
      "Epoch: 1448/2000... Training loss: 0.3426\n",
      "Epoch: 1449/2000... Training loss: 0.3766\n",
      "Epoch: 1449/2000... Training loss: 0.4382\n",
      "Epoch: 1449/2000... Training loss: 0.3909\n",
      "Epoch: 1449/2000... Training loss: 0.4690\n",
      "Epoch: 1449/2000... Training loss: 0.3334\n",
      "Epoch: 1449/2000... Training loss: 0.4068\n",
      "Epoch: 1449/2000... Training loss: 0.4141\n",
      "Epoch: 1449/2000... Training loss: 0.3436\n",
      "Epoch: 1449/2000... Training loss: 0.4007\n",
      "Epoch: 1449/2000... Training loss: 0.5153\n",
      "Epoch: 1449/2000... Training loss: 0.3183\n",
      "Epoch: 1449/2000... Training loss: 0.3774\n",
      "Epoch: 1449/2000... Training loss: 0.4598\n",
      "Epoch: 1449/2000... Training loss: 0.3053\n",
      "Epoch: 1449/2000... Training loss: 0.5394\n",
      "Epoch: 1449/2000... Training loss: 0.3092\n",
      "Epoch: 1449/2000... Training loss: 0.5670\n",
      "Epoch: 1449/2000... Training loss: 0.3991\n",
      "Epoch: 1449/2000... Training loss: 0.3195\n",
      "Epoch: 1449/2000... Training loss: 0.4555\n",
      "Epoch: 1449/2000... Training loss: 0.3531\n",
      "Epoch: 1449/2000... Training loss: 0.2997\n",
      "Epoch: 1449/2000... Training loss: 0.4873\n",
      "Epoch: 1449/2000... Training loss: 0.4710\n",
      "Epoch: 1449/2000... Training loss: 0.4344\n",
      "Epoch: 1449/2000... Training loss: 0.6539\n",
      "Epoch: 1449/2000... Training loss: 0.3553\n",
      "Epoch: 1449/2000... Training loss: 0.3891\n",
      "Epoch: 1449/2000... Training loss: 0.4451\n",
      "Epoch: 1449/2000... Training loss: 0.6338\n",
      "Epoch: 1449/2000... Training loss: 0.3532\n",
      "Epoch: 1450/2000... Training loss: 0.3839\n",
      "Epoch: 1450/2000... Training loss: 0.4027\n",
      "Epoch: 1450/2000... Training loss: 0.4543\n",
      "Epoch: 1450/2000... Training loss: 0.3904\n",
      "Epoch: 1450/2000... Training loss: 0.4245\n",
      "Epoch: 1450/2000... Training loss: 0.3895\n",
      "Epoch: 1450/2000... Training loss: 0.3096\n",
      "Epoch: 1450/2000... Training loss: 0.3884\n",
      "Epoch: 1450/2000... Training loss: 0.4790\n",
      "Epoch: 1450/2000... Training loss: 0.3822\n",
      "Epoch: 1450/2000... Training loss: 0.3880\n",
      "Epoch: 1450/2000... Training loss: 0.3522\n",
      "Epoch: 1450/2000... Training loss: 0.2807\n",
      "Epoch: 1450/2000... Training loss: 0.3518\n",
      "Epoch: 1450/2000... Training loss: 0.4917\n",
      "Epoch: 1450/2000... Training loss: 0.3266\n",
      "Epoch: 1450/2000... Training loss: 0.3430\n",
      "Epoch: 1450/2000... Training loss: 0.4150\n",
      "Epoch: 1450/2000... Training loss: 0.3929\n",
      "Epoch: 1450/2000... Training loss: 0.4778\n",
      "Epoch: 1450/2000... Training loss: 0.3582\n",
      "Epoch: 1450/2000... Training loss: 0.5819\n",
      "Epoch: 1450/2000... Training loss: 0.3192\n",
      "Epoch: 1450/2000... Training loss: 0.4561\n",
      "Epoch: 1450/2000... Training loss: 0.6253\n",
      "Epoch: 1450/2000... Training loss: 0.3760\n",
      "Epoch: 1450/2000... Training loss: 0.3832\n",
      "Epoch: 1450/2000... Training loss: 0.4949\n",
      "Epoch: 1450/2000... Training loss: 0.3987\n",
      "Epoch: 1450/2000... Training loss: 0.3161\n",
      "Epoch: 1450/2000... Training loss: 0.4199\n",
      "Epoch: 1451/2000... Training loss: 0.2059\n",
      "Epoch: 1451/2000... Training loss: 0.4389\n",
      "Epoch: 1451/2000... Training loss: 0.4136\n",
      "Epoch: 1451/2000... Training loss: 0.3846\n",
      "Epoch: 1451/2000... Training loss: 0.2962\n",
      "Epoch: 1451/2000... Training loss: 0.4210\n",
      "Epoch: 1451/2000... Training loss: 0.3734\n",
      "Epoch: 1451/2000... Training loss: 0.3716\n",
      "Epoch: 1451/2000... Training loss: 0.3941\n",
      "Epoch: 1451/2000... Training loss: 0.3006\n",
      "Epoch: 1451/2000... Training loss: 0.4633\n",
      "Epoch: 1451/2000... Training loss: 0.3408\n",
      "Epoch: 1451/2000... Training loss: 0.4900\n",
      "Epoch: 1451/2000... Training loss: 0.4073\n",
      "Epoch: 1451/2000... Training loss: 0.5141\n",
      "Epoch: 1451/2000... Training loss: 0.4935\n",
      "Epoch: 1451/2000... Training loss: 0.3729\n",
      "Epoch: 1451/2000... Training loss: 0.4519\n",
      "Epoch: 1451/2000... Training loss: 0.2907\n",
      "Epoch: 1451/2000... Training loss: 0.2806\n",
      "Epoch: 1451/2000... Training loss: 0.4427\n",
      "Epoch: 1451/2000... Training loss: 0.3588\n",
      "Epoch: 1451/2000... Training loss: 0.4670\n",
      "Epoch: 1451/2000... Training loss: 0.3652\n",
      "Epoch: 1451/2000... Training loss: 0.2984\n",
      "Epoch: 1451/2000... Training loss: 0.4359\n",
      "Epoch: 1451/2000... Training loss: 0.3075\n",
      "Epoch: 1451/2000... Training loss: 0.4830\n",
      "Epoch: 1451/2000... Training loss: 0.3952\n",
      "Epoch: 1451/2000... Training loss: 0.3782\n",
      "Epoch: 1451/2000... Training loss: 0.2851\n",
      "Epoch: 1452/2000... Training loss: 0.3679\n",
      "Epoch: 1452/2000... Training loss: 0.3724\n",
      "Epoch: 1452/2000... Training loss: 0.4731\n",
      "Epoch: 1452/2000... Training loss: 0.6328\n",
      "Epoch: 1452/2000... Training loss: 0.3507\n",
      "Epoch: 1452/2000... Training loss: 0.3545\n",
      "Epoch: 1452/2000... Training loss: 0.3617\n",
      "Epoch: 1452/2000... Training loss: 0.4939\n",
      "Epoch: 1452/2000... Training loss: 0.2882\n",
      "Epoch: 1452/2000... Training loss: 0.4061\n",
      "Epoch: 1452/2000... Training loss: 0.4772\n",
      "Epoch: 1452/2000... Training loss: 0.3483\n",
      "Epoch: 1452/2000... Training loss: 0.3258\n",
      "Epoch: 1452/2000... Training loss: 0.4319\n",
      "Epoch: 1452/2000... Training loss: 0.4250\n",
      "Epoch: 1452/2000... Training loss: 0.3282\n",
      "Epoch: 1452/2000... Training loss: 0.4513\n",
      "Epoch: 1452/2000... Training loss: 0.4782\n",
      "Epoch: 1452/2000... Training loss: 0.5047\n",
      "Epoch: 1452/2000... Training loss: 0.3012\n",
      "Epoch: 1452/2000... Training loss: 0.5076\n",
      "Epoch: 1452/2000... Training loss: 0.4323\n",
      "Epoch: 1452/2000... Training loss: 0.4440\n",
      "Epoch: 1452/2000... Training loss: 0.4468\n",
      "Epoch: 1452/2000... Training loss: 0.5932\n",
      "Epoch: 1452/2000... Training loss: 0.4239\n",
      "Epoch: 1452/2000... Training loss: 0.4309\n",
      "Epoch: 1452/2000... Training loss: 0.3573\n",
      "Epoch: 1452/2000... Training loss: 0.2983\n",
      "Epoch: 1452/2000... Training loss: 0.7606\n",
      "Epoch: 1452/2000... Training loss: 0.4127\n",
      "Epoch: 1453/2000... Training loss: 0.4518\n",
      "Epoch: 1453/2000... Training loss: 0.4863\n",
      "Epoch: 1453/2000... Training loss: 0.5502\n",
      "Epoch: 1453/2000... Training loss: 0.3558\n",
      "Epoch: 1453/2000... Training loss: 0.3273\n",
      "Epoch: 1453/2000... Training loss: 0.4590\n",
      "Epoch: 1453/2000... Training loss: 0.3654\n",
      "Epoch: 1453/2000... Training loss: 0.3320\n",
      "Epoch: 1453/2000... Training loss: 0.3704\n",
      "Epoch: 1453/2000... Training loss: 0.3659\n",
      "Epoch: 1453/2000... Training loss: 0.4136\n",
      "Epoch: 1453/2000... Training loss: 0.5603\n",
      "Epoch: 1453/2000... Training loss: 0.3890\n",
      "Epoch: 1453/2000... Training loss: 0.5822\n",
      "Epoch: 1453/2000... Training loss: 0.1723\n",
      "Epoch: 1453/2000... Training loss: 0.4400\n",
      "Epoch: 1453/2000... Training loss: 0.4896\n",
      "Epoch: 1453/2000... Training loss: 0.6210\n",
      "Epoch: 1453/2000... Training loss: 0.2896\n",
      "Epoch: 1453/2000... Training loss: 0.3293\n",
      "Epoch: 1453/2000... Training loss: 0.4561\n",
      "Epoch: 1453/2000... Training loss: 0.3255\n",
      "Epoch: 1453/2000... Training loss: 0.2724\n",
      "Epoch: 1453/2000... Training loss: 0.4676\n",
      "Epoch: 1453/2000... Training loss: 0.4200\n",
      "Epoch: 1453/2000... Training loss: 0.4561\n",
      "Epoch: 1453/2000... Training loss: 0.2813\n",
      "Epoch: 1453/2000... Training loss: 0.4006\n",
      "Epoch: 1453/2000... Training loss: 0.6280\n",
      "Epoch: 1453/2000... Training loss: 0.3153\n",
      "Epoch: 1453/2000... Training loss: 0.5065\n",
      "Epoch: 1454/2000... Training loss: 0.3097\n",
      "Epoch: 1454/2000... Training loss: 0.5349\n",
      "Epoch: 1454/2000... Training loss: 0.4965\n",
      "Epoch: 1454/2000... Training loss: 0.3556\n",
      "Epoch: 1454/2000... Training loss: 0.3943\n",
      "Epoch: 1454/2000... Training loss: 0.3790\n",
      "Epoch: 1454/2000... Training loss: 0.3369\n",
      "Epoch: 1454/2000... Training loss: 0.3172\n",
      "Epoch: 1454/2000... Training loss: 0.4605\n",
      "Epoch: 1454/2000... Training loss: 0.4721\n",
      "Epoch: 1454/2000... Training loss: 0.5115\n",
      "Epoch: 1454/2000... Training loss: 0.4320\n",
      "Epoch: 1454/2000... Training loss: 0.4631\n",
      "Epoch: 1454/2000... Training loss: 0.3831\n",
      "Epoch: 1454/2000... Training loss: 0.5368\n",
      "Epoch: 1454/2000... Training loss: 0.3071\n",
      "Epoch: 1454/2000... Training loss: 0.4624\n",
      "Epoch: 1454/2000... Training loss: 0.5474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1454/2000... Training loss: 0.3410\n",
      "Epoch: 1454/2000... Training loss: 0.6253\n",
      "Epoch: 1454/2000... Training loss: 0.3041\n",
      "Epoch: 1454/2000... Training loss: 0.4661\n",
      "Epoch: 1454/2000... Training loss: 0.3802\n",
      "Epoch: 1454/2000... Training loss: 0.3926\n",
      "Epoch: 1454/2000... Training loss: 0.2518\n",
      "Epoch: 1454/2000... Training loss: 0.4280\n",
      "Epoch: 1454/2000... Training loss: 0.2549\n",
      "Epoch: 1454/2000... Training loss: 0.3939\n",
      "Epoch: 1454/2000... Training loss: 0.3814\n",
      "Epoch: 1454/2000... Training loss: 0.4212\n",
      "Epoch: 1454/2000... Training loss: 0.3835\n",
      "Epoch: 1455/2000... Training loss: 0.4080\n",
      "Epoch: 1455/2000... Training loss: 0.5258\n",
      "Epoch: 1455/2000... Training loss: 0.5054\n",
      "Epoch: 1455/2000... Training loss: 0.2636\n",
      "Epoch: 1455/2000... Training loss: 0.4775\n",
      "Epoch: 1455/2000... Training loss: 0.4413\n",
      "Epoch: 1455/2000... Training loss: 0.3535\n",
      "Epoch: 1455/2000... Training loss: 0.3570\n",
      "Epoch: 1455/2000... Training loss: 0.5099\n",
      "Epoch: 1455/2000... Training loss: 0.3342\n",
      "Epoch: 1455/2000... Training loss: 0.5700\n",
      "Epoch: 1455/2000... Training loss: 0.2260\n",
      "Epoch: 1455/2000... Training loss: 0.3339\n",
      "Epoch: 1455/2000... Training loss: 0.4018\n",
      "Epoch: 1455/2000... Training loss: 0.4688\n",
      "Epoch: 1455/2000... Training loss: 0.3439\n",
      "Epoch: 1455/2000... Training loss: 0.3049\n",
      "Epoch: 1455/2000... Training loss: 0.4004\n",
      "Epoch: 1455/2000... Training loss: 0.4165\n",
      "Epoch: 1455/2000... Training loss: 0.3039\n",
      "Epoch: 1455/2000... Training loss: 0.2752\n",
      "Epoch: 1455/2000... Training loss: 0.2702\n",
      "Epoch: 1455/2000... Training loss: 0.5398\n",
      "Epoch: 1455/2000... Training loss: 0.4840\n",
      "Epoch: 1455/2000... Training loss: 0.4363\n",
      "Epoch: 1455/2000... Training loss: 0.2806\n",
      "Epoch: 1455/2000... Training loss: 0.4879\n",
      "Epoch: 1455/2000... Training loss: 0.2693\n",
      "Epoch: 1455/2000... Training loss: 0.4773\n",
      "Epoch: 1455/2000... Training loss: 0.4295\n",
      "Epoch: 1455/2000... Training loss: 0.5031\n",
      "Epoch: 1456/2000... Training loss: 0.6928\n",
      "Epoch: 1456/2000... Training loss: 0.4178\n",
      "Epoch: 1456/2000... Training loss: 0.3539\n",
      "Epoch: 1456/2000... Training loss: 0.4776\n",
      "Epoch: 1456/2000... Training loss: 0.5073\n",
      "Epoch: 1456/2000... Training loss: 0.3650\n",
      "Epoch: 1456/2000... Training loss: 0.4197\n",
      "Epoch: 1456/2000... Training loss: 0.3527\n",
      "Epoch: 1456/2000... Training loss: 0.3852\n",
      "Epoch: 1456/2000... Training loss: 0.3313\n",
      "Epoch: 1456/2000... Training loss: 0.3579\n",
      "Epoch: 1456/2000... Training loss: 0.4206\n",
      "Epoch: 1456/2000... Training loss: 0.2804\n",
      "Epoch: 1456/2000... Training loss: 0.4794\n",
      "Epoch: 1456/2000... Training loss: 0.4431\n",
      "Epoch: 1456/2000... Training loss: 0.4361\n",
      "Epoch: 1456/2000... Training loss: 0.3656\n",
      "Epoch: 1456/2000... Training loss: 0.3194\n",
      "Epoch: 1456/2000... Training loss: 0.4702\n",
      "Epoch: 1456/2000... Training loss: 0.5074\n",
      "Epoch: 1456/2000... Training loss: 0.4380\n",
      "Epoch: 1456/2000... Training loss: 0.3597\n",
      "Epoch: 1456/2000... Training loss: 0.2970\n",
      "Epoch: 1456/2000... Training loss: 0.5591\n",
      "Epoch: 1456/2000... Training loss: 0.4750\n",
      "Epoch: 1456/2000... Training loss: 0.4793\n",
      "Epoch: 1456/2000... Training loss: 0.4662\n",
      "Epoch: 1456/2000... Training loss: 0.3239\n",
      "Epoch: 1456/2000... Training loss: 0.4592\n",
      "Epoch: 1456/2000... Training loss: 0.5288\n",
      "Epoch: 1456/2000... Training loss: 0.6241\n",
      "Epoch: 1457/2000... Training loss: 0.2493\n",
      "Epoch: 1457/2000... Training loss: 0.3672\n",
      "Epoch: 1457/2000... Training loss: 0.4447\n",
      "Epoch: 1457/2000... Training loss: 0.3223\n",
      "Epoch: 1457/2000... Training loss: 0.3928\n",
      "Epoch: 1457/2000... Training loss: 0.1335\n",
      "Epoch: 1457/2000... Training loss: 0.5002\n",
      "Epoch: 1457/2000... Training loss: 0.5293\n",
      "Epoch: 1457/2000... Training loss: 0.2492\n",
      "Epoch: 1457/2000... Training loss: 0.3289\n",
      "Epoch: 1457/2000... Training loss: 0.5038\n",
      "Epoch: 1457/2000... Training loss: 0.3710\n",
      "Epoch: 1457/2000... Training loss: 0.4625\n",
      "Epoch: 1457/2000... Training loss: 0.3709\n",
      "Epoch: 1457/2000... Training loss: 0.3261\n",
      "Epoch: 1457/2000... Training loss: 0.4658\n",
      "Epoch: 1457/2000... Training loss: 0.3007\n",
      "Epoch: 1457/2000... Training loss: 0.4337\n",
      "Epoch: 1457/2000... Training loss: 0.2744\n",
      "Epoch: 1457/2000... Training loss: 0.2748\n",
      "Epoch: 1457/2000... Training loss: 0.4192\n",
      "Epoch: 1457/2000... Training loss: 0.3359\n",
      "Epoch: 1457/2000... Training loss: 0.5109\n",
      "Epoch: 1457/2000... Training loss: 0.4867\n",
      "Epoch: 1457/2000... Training loss: 0.3950\n",
      "Epoch: 1457/2000... Training loss: 0.4913\n",
      "Epoch: 1457/2000... Training loss: 0.4030\n",
      "Epoch: 1457/2000... Training loss: 0.4736\n",
      "Epoch: 1457/2000... Training loss: 0.3502\n",
      "Epoch: 1457/2000... Training loss: 0.2358\n",
      "Epoch: 1457/2000... Training loss: 0.4157\n",
      "Epoch: 1458/2000... Training loss: 0.6013\n",
      "Epoch: 1458/2000... Training loss: 0.4816\n",
      "Epoch: 1458/2000... Training loss: 0.5207\n",
      "Epoch: 1458/2000... Training loss: 0.4216\n",
      "Epoch: 1458/2000... Training loss: 0.4037\n",
      "Epoch: 1458/2000... Training loss: 0.3449\n",
      "Epoch: 1458/2000... Training loss: 0.3864\n",
      "Epoch: 1458/2000... Training loss: 0.3437\n",
      "Epoch: 1458/2000... Training loss: 0.3023\n",
      "Epoch: 1458/2000... Training loss: 0.3443\n",
      "Epoch: 1458/2000... Training loss: 0.3953\n",
      "Epoch: 1458/2000... Training loss: 0.3554\n",
      "Epoch: 1458/2000... Training loss: 0.4733\n",
      "Epoch: 1458/2000... Training loss: 0.5552\n",
      "Epoch: 1458/2000... Training loss: 0.5260\n",
      "Epoch: 1458/2000... Training loss: 0.6269\n",
      "Epoch: 1458/2000... Training loss: 0.5381\n",
      "Epoch: 1458/2000... Training loss: 0.3064\n",
      "Epoch: 1458/2000... Training loss: 0.3412\n",
      "Epoch: 1458/2000... Training loss: 0.2919\n",
      "Epoch: 1458/2000... Training loss: 0.5498\n",
      "Epoch: 1458/2000... Training loss: 0.4052\n",
      "Epoch: 1458/2000... Training loss: 0.5977\n",
      "Epoch: 1458/2000... Training loss: 0.4724\n",
      "Epoch: 1458/2000... Training loss: 0.4261\n",
      "Epoch: 1458/2000... Training loss: 0.5188\n",
      "Epoch: 1458/2000... Training loss: 0.3976\n",
      "Epoch: 1458/2000... Training loss: 0.3350\n",
      "Epoch: 1458/2000... Training loss: 0.3758\n",
      "Epoch: 1458/2000... Training loss: 0.3822\n",
      "Epoch: 1458/2000... Training loss: 0.4897\n",
      "Epoch: 1459/2000... Training loss: 0.4164\n",
      "Epoch: 1459/2000... Training loss: 0.3335\n",
      "Epoch: 1459/2000... Training loss: 0.4539\n",
      "Epoch: 1459/2000... Training loss: 0.6232\n",
      "Epoch: 1459/2000... Training loss: 0.5563\n",
      "Epoch: 1459/2000... Training loss: 0.3438\n",
      "Epoch: 1459/2000... Training loss: 0.4098\n",
      "Epoch: 1459/2000... Training loss: 0.5017\n",
      "Epoch: 1459/2000... Training loss: 0.6573\n",
      "Epoch: 1459/2000... Training loss: 0.4628\n",
      "Epoch: 1459/2000... Training loss: 0.5828\n",
      "Epoch: 1459/2000... Training loss: 0.3717\n",
      "Epoch: 1459/2000... Training loss: 0.4315\n",
      "Epoch: 1459/2000... Training loss: 0.5523\n",
      "Epoch: 1459/2000... Training loss: 0.3561\n",
      "Epoch: 1459/2000... Training loss: 0.3539\n",
      "Epoch: 1459/2000... Training loss: 0.4052\n",
      "Epoch: 1459/2000... Training loss: 0.4324\n",
      "Epoch: 1459/2000... Training loss: 0.4579\n",
      "Epoch: 1459/2000... Training loss: 0.2350\n",
      "Epoch: 1459/2000... Training loss: 0.4083\n",
      "Epoch: 1459/2000... Training loss: 0.4310\n",
      "Epoch: 1459/2000... Training loss: 0.3905\n",
      "Epoch: 1459/2000... Training loss: 0.2691\n",
      "Epoch: 1459/2000... Training loss: 0.4888\n",
      "Epoch: 1459/2000... Training loss: 0.4223\n",
      "Epoch: 1459/2000... Training loss: 0.3771\n",
      "Epoch: 1459/2000... Training loss: 0.3893\n",
      "Epoch: 1459/2000... Training loss: 0.4010\n",
      "Epoch: 1459/2000... Training loss: 0.4232\n",
      "Epoch: 1459/2000... Training loss: 0.5463\n",
      "Epoch: 1460/2000... Training loss: 0.4706\n",
      "Epoch: 1460/2000... Training loss: 0.3292\n",
      "Epoch: 1460/2000... Training loss: 0.4423\n",
      "Epoch: 1460/2000... Training loss: 0.4735\n",
      "Epoch: 1460/2000... Training loss: 0.4679\n",
      "Epoch: 1460/2000... Training loss: 0.4675\n",
      "Epoch: 1460/2000... Training loss: 0.3564\n",
      "Epoch: 1460/2000... Training loss: 0.2703\n",
      "Epoch: 1460/2000... Training loss: 0.4481\n",
      "Epoch: 1460/2000... Training loss: 0.6121\n",
      "Epoch: 1460/2000... Training loss: 0.5044\n",
      "Epoch: 1460/2000... Training loss: 0.3312\n",
      "Epoch: 1460/2000... Training loss: 0.3504\n",
      "Epoch: 1460/2000... Training loss: 0.3843\n",
      "Epoch: 1460/2000... Training loss: 0.4901\n",
      "Epoch: 1460/2000... Training loss: 0.4078\n",
      "Epoch: 1460/2000... Training loss: 0.2356\n",
      "Epoch: 1460/2000... Training loss: 0.4008\n",
      "Epoch: 1460/2000... Training loss: 0.3994\n",
      "Epoch: 1460/2000... Training loss: 0.6789\n",
      "Epoch: 1460/2000... Training loss: 0.4106\n",
      "Epoch: 1460/2000... Training loss: 0.2708\n",
      "Epoch: 1460/2000... Training loss: 0.6001\n",
      "Epoch: 1460/2000... Training loss: 0.4492\n",
      "Epoch: 1460/2000... Training loss: 0.4039\n",
      "Epoch: 1460/2000... Training loss: 0.5266\n",
      "Epoch: 1460/2000... Training loss: 0.4774\n",
      "Epoch: 1460/2000... Training loss: 0.4373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1460/2000... Training loss: 0.4713\n",
      "Epoch: 1460/2000... Training loss: 0.5438\n",
      "Epoch: 1460/2000... Training loss: 0.4104\n",
      "Epoch: 1461/2000... Training loss: 0.6012\n",
      "Epoch: 1461/2000... Training loss: 0.4007\n",
      "Epoch: 1461/2000... Training loss: 0.3828\n",
      "Epoch: 1461/2000... Training loss: 0.4605\n",
      "Epoch: 1461/2000... Training loss: 0.2660\n",
      "Epoch: 1461/2000... Training loss: 0.3979\n",
      "Epoch: 1461/2000... Training loss: 0.3074\n",
      "Epoch: 1461/2000... Training loss: 0.3945\n",
      "Epoch: 1461/2000... Training loss: 0.4649\n",
      "Epoch: 1461/2000... Training loss: 0.4312\n",
      "Epoch: 1461/2000... Training loss: 0.2546\n",
      "Epoch: 1461/2000... Training loss: 0.3930\n",
      "Epoch: 1461/2000... Training loss: 0.3216\n",
      "Epoch: 1461/2000... Training loss: 0.3912\n",
      "Epoch: 1461/2000... Training loss: 0.4253\n",
      "Epoch: 1461/2000... Training loss: 0.2797\n",
      "Epoch: 1461/2000... Training loss: 0.5239\n",
      "Epoch: 1461/2000... Training loss: 0.5615\n",
      "Epoch: 1461/2000... Training loss: 0.2980\n",
      "Epoch: 1461/2000... Training loss: 0.2997\n",
      "Epoch: 1461/2000... Training loss: 0.5685\n",
      "Epoch: 1461/2000... Training loss: 0.6426\n",
      "Epoch: 1461/2000... Training loss: 0.3866\n",
      "Epoch: 1461/2000... Training loss: 0.4412\n",
      "Epoch: 1461/2000... Training loss: 0.4177\n",
      "Epoch: 1461/2000... Training loss: 0.2208\n",
      "Epoch: 1461/2000... Training loss: 0.4753\n",
      "Epoch: 1461/2000... Training loss: 0.4080\n",
      "Epoch: 1461/2000... Training loss: 0.3877\n",
      "Epoch: 1461/2000... Training loss: 0.4806\n",
      "Epoch: 1461/2000... Training loss: 0.4719\n",
      "Epoch: 1462/2000... Training loss: 0.4129\n",
      "Epoch: 1462/2000... Training loss: 0.3746\n",
      "Epoch: 1462/2000... Training loss: 0.4367\n",
      "Epoch: 1462/2000... Training loss: 0.3149\n",
      "Epoch: 1462/2000... Training loss: 0.4761\n",
      "Epoch: 1462/2000... Training loss: 0.3064\n",
      "Epoch: 1462/2000... Training loss: 0.3514\n",
      "Epoch: 1462/2000... Training loss: 0.5245\n",
      "Epoch: 1462/2000... Training loss: 0.3012\n",
      "Epoch: 1462/2000... Training loss: 0.6043\n",
      "Epoch: 1462/2000... Training loss: 0.4654\n",
      "Epoch: 1462/2000... Training loss: 0.3665\n",
      "Epoch: 1462/2000... Training loss: 0.3308\n",
      "Epoch: 1462/2000... Training loss: 0.6446\n",
      "Epoch: 1462/2000... Training loss: 0.3763\n",
      "Epoch: 1462/2000... Training loss: 0.3243\n",
      "Epoch: 1462/2000... Training loss: 0.4192\n",
      "Epoch: 1462/2000... Training loss: 0.5225\n",
      "Epoch: 1462/2000... Training loss: 0.4552\n",
      "Epoch: 1462/2000... Training loss: 0.4184\n",
      "Epoch: 1462/2000... Training loss: 0.4855\n",
      "Epoch: 1462/2000... Training loss: 0.3746\n",
      "Epoch: 1462/2000... Training loss: 0.4443\n",
      "Epoch: 1462/2000... Training loss: 0.2630\n",
      "Epoch: 1462/2000... Training loss: 0.7211\n",
      "Epoch: 1462/2000... Training loss: 0.2377\n",
      "Epoch: 1462/2000... Training loss: 0.6568\n",
      "Epoch: 1462/2000... Training loss: 0.3705\n",
      "Epoch: 1462/2000... Training loss: 0.3632\n",
      "Epoch: 1462/2000... Training loss: 0.4395\n",
      "Epoch: 1462/2000... Training loss: 0.2362\n",
      "Epoch: 1463/2000... Training loss: 0.3608\n",
      "Epoch: 1463/2000... Training loss: 0.2994\n",
      "Epoch: 1463/2000... Training loss: 0.4813\n",
      "Epoch: 1463/2000... Training loss: 0.4449\n",
      "Epoch: 1463/2000... Training loss: 0.4508\n",
      "Epoch: 1463/2000... Training loss: 0.3356\n",
      "Epoch: 1463/2000... Training loss: 0.4522\n",
      "Epoch: 1463/2000... Training loss: 0.3450\n",
      "Epoch: 1463/2000... Training loss: 0.4314\n",
      "Epoch: 1463/2000... Training loss: 0.4495\n",
      "Epoch: 1463/2000... Training loss: 0.4484\n",
      "Epoch: 1463/2000... Training loss: 0.4452\n",
      "Epoch: 1463/2000... Training loss: 0.3700\n",
      "Epoch: 1463/2000... Training loss: 0.4350\n",
      "Epoch: 1463/2000... Training loss: 0.4601\n",
      "Epoch: 1463/2000... Training loss: 0.3329\n",
      "Epoch: 1463/2000... Training loss: 0.3392\n",
      "Epoch: 1463/2000... Training loss: 0.3608\n",
      "Epoch: 1463/2000... Training loss: 0.4072\n",
      "Epoch: 1463/2000... Training loss: 0.3863\n",
      "Epoch: 1463/2000... Training loss: 0.3924\n",
      "Epoch: 1463/2000... Training loss: 0.5589\n",
      "Epoch: 1463/2000... Training loss: 0.3941\n",
      "Epoch: 1463/2000... Training loss: 0.4314\n",
      "Epoch: 1463/2000... Training loss: 0.5024\n",
      "Epoch: 1463/2000... Training loss: 0.4269\n",
      "Epoch: 1463/2000... Training loss: 0.4126\n",
      "Epoch: 1463/2000... Training loss: 0.3722\n",
      "Epoch: 1463/2000... Training loss: 0.4640\n",
      "Epoch: 1463/2000... Training loss: 0.4447\n",
      "Epoch: 1463/2000... Training loss: 0.3293\n",
      "Epoch: 1464/2000... Training loss: 0.4539\n",
      "Epoch: 1464/2000... Training loss: 0.3025\n",
      "Epoch: 1464/2000... Training loss: 0.5140\n",
      "Epoch: 1464/2000... Training loss: 0.4308\n",
      "Epoch: 1464/2000... Training loss: 0.4620\n",
      "Epoch: 1464/2000... Training loss: 0.5466\n",
      "Epoch: 1464/2000... Training loss: 0.3863\n",
      "Epoch: 1464/2000... Training loss: 0.3751\n",
      "Epoch: 1464/2000... Training loss: 0.3516\n",
      "Epoch: 1464/2000... Training loss: 0.4468\n",
      "Epoch: 1464/2000... Training loss: 0.3507\n",
      "Epoch: 1464/2000... Training loss: 0.4630\n",
      "Epoch: 1464/2000... Training loss: 0.3177\n",
      "Epoch: 1464/2000... Training loss: 0.4400\n",
      "Epoch: 1464/2000... Training loss: 0.3446\n",
      "Epoch: 1464/2000... Training loss: 0.5336\n",
      "Epoch: 1464/2000... Training loss: 0.2752\n",
      "Epoch: 1464/2000... Training loss: 0.4404\n",
      "Epoch: 1464/2000... Training loss: 0.4785\n",
      "Epoch: 1464/2000... Training loss: 0.4108\n",
      "Epoch: 1464/2000... Training loss: 0.5025\n",
      "Epoch: 1464/2000... Training loss: 0.4958\n",
      "Epoch: 1464/2000... Training loss: 0.4038\n",
      "Epoch: 1464/2000... Training loss: 0.4926\n",
      "Epoch: 1464/2000... Training loss: 0.3779\n",
      "Epoch: 1464/2000... Training loss: 0.4265\n",
      "Epoch: 1464/2000... Training loss: 0.3579\n",
      "Epoch: 1464/2000... Training loss: 0.5020\n",
      "Epoch: 1464/2000... Training loss: 0.3685\n",
      "Epoch: 1464/2000... Training loss: 0.4091\n",
      "Epoch: 1464/2000... Training loss: 0.4120\n",
      "Epoch: 1465/2000... Training loss: 0.4770\n",
      "Epoch: 1465/2000... Training loss: 0.4570\n",
      "Epoch: 1465/2000... Training loss: 0.4057\n",
      "Epoch: 1465/2000... Training loss: 0.3929\n",
      "Epoch: 1465/2000... Training loss: 0.4269\n",
      "Epoch: 1465/2000... Training loss: 0.5313\n",
      "Epoch: 1465/2000... Training loss: 0.3888\n",
      "Epoch: 1465/2000... Training loss: 0.4612\n",
      "Epoch: 1465/2000... Training loss: 0.3503\n",
      "Epoch: 1465/2000... Training loss: 0.3518\n",
      "Epoch: 1465/2000... Training loss: 0.2027\n",
      "Epoch: 1465/2000... Training loss: 0.2604\n",
      "Epoch: 1465/2000... Training loss: 0.3776\n",
      "Epoch: 1465/2000... Training loss: 0.5885\n",
      "Epoch: 1465/2000... Training loss: 0.5249\n",
      "Epoch: 1465/2000... Training loss: 0.3801\n",
      "Epoch: 1465/2000... Training loss: 0.4765\n",
      "Epoch: 1465/2000... Training loss: 0.5304\n",
      "Epoch: 1465/2000... Training loss: 0.4355\n",
      "Epoch: 1465/2000... Training loss: 0.3282\n",
      "Epoch: 1465/2000... Training loss: 0.5230\n",
      "Epoch: 1465/2000... Training loss: 0.4097\n",
      "Epoch: 1465/2000... Training loss: 0.4181\n",
      "Epoch: 1465/2000... Training loss: 0.3209\n",
      "Epoch: 1465/2000... Training loss: 0.3436\n",
      "Epoch: 1465/2000... Training loss: 0.5039\n",
      "Epoch: 1465/2000... Training loss: 0.3199\n",
      "Epoch: 1465/2000... Training loss: 0.2612\n",
      "Epoch: 1465/2000... Training loss: 0.3987\n",
      "Epoch: 1465/2000... Training loss: 0.3306\n",
      "Epoch: 1465/2000... Training loss: 0.4107\n",
      "Epoch: 1466/2000... Training loss: 0.3711\n",
      "Epoch: 1466/2000... Training loss: 0.3691\n",
      "Epoch: 1466/2000... Training loss: 0.3961\n",
      "Epoch: 1466/2000... Training loss: 0.3484\n",
      "Epoch: 1466/2000... Training loss: 0.3337\n",
      "Epoch: 1466/2000... Training loss: 0.4450\n",
      "Epoch: 1466/2000... Training loss: 0.4288\n",
      "Epoch: 1466/2000... Training loss: 0.4470\n",
      "Epoch: 1466/2000... Training loss: 0.3707\n",
      "Epoch: 1466/2000... Training loss: 0.5133\n",
      "Epoch: 1466/2000... Training loss: 0.5616\n",
      "Epoch: 1466/2000... Training loss: 0.4865\n",
      "Epoch: 1466/2000... Training loss: 0.4017\n",
      "Epoch: 1466/2000... Training loss: 0.6494\n",
      "Epoch: 1466/2000... Training loss: 0.3394\n",
      "Epoch: 1466/2000... Training loss: 0.4308\n",
      "Epoch: 1466/2000... Training loss: 0.2950\n",
      "Epoch: 1466/2000... Training loss: 0.4563\n",
      "Epoch: 1466/2000... Training loss: 0.3806\n",
      "Epoch: 1466/2000... Training loss: 0.3300\n",
      "Epoch: 1466/2000... Training loss: 0.4733\n",
      "Epoch: 1466/2000... Training loss: 0.4643\n",
      "Epoch: 1466/2000... Training loss: 0.5799\n",
      "Epoch: 1466/2000... Training loss: 0.4239\n",
      "Epoch: 1466/2000... Training loss: 0.4694\n",
      "Epoch: 1466/2000... Training loss: 0.2745\n",
      "Epoch: 1466/2000... Training loss: 0.3988\n",
      "Epoch: 1466/2000... Training loss: 0.1654\n",
      "Epoch: 1466/2000... Training loss: 0.4208\n",
      "Epoch: 1466/2000... Training loss: 0.5069\n",
      "Epoch: 1466/2000... Training loss: 0.3527\n",
      "Epoch: 1467/2000... Training loss: 0.3747\n",
      "Epoch: 1467/2000... Training loss: 0.3910\n",
      "Epoch: 1467/2000... Training loss: 0.4860\n",
      "Epoch: 1467/2000... Training loss: 0.4628\n",
      "Epoch: 1467/2000... Training loss: 0.5364\n",
      "Epoch: 1467/2000... Training loss: 0.3966\n",
      "Epoch: 1467/2000... Training loss: 0.4427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1467/2000... Training loss: 0.5002\n",
      "Epoch: 1467/2000... Training loss: 0.3582\n",
      "Epoch: 1467/2000... Training loss: 0.3689\n",
      "Epoch: 1467/2000... Training loss: 0.3938\n",
      "Epoch: 1467/2000... Training loss: 0.3076\n",
      "Epoch: 1467/2000... Training loss: 0.3978\n",
      "Epoch: 1467/2000... Training loss: 0.4424\n",
      "Epoch: 1467/2000... Training loss: 0.3432\n",
      "Epoch: 1467/2000... Training loss: 0.5195\n",
      "Epoch: 1467/2000... Training loss: 0.4837\n",
      "Epoch: 1467/2000... Training loss: 0.4912\n",
      "Epoch: 1467/2000... Training loss: 0.5459\n",
      "Epoch: 1467/2000... Training loss: 0.2847\n",
      "Epoch: 1467/2000... Training loss: 0.4070\n",
      "Epoch: 1467/2000... Training loss: 0.3510\n",
      "Epoch: 1467/2000... Training loss: 0.6121\n",
      "Epoch: 1467/2000... Training loss: 0.5170\n",
      "Epoch: 1467/2000... Training loss: 0.2900\n",
      "Epoch: 1467/2000... Training loss: 0.3708\n",
      "Epoch: 1467/2000... Training loss: 0.6000\n",
      "Epoch: 1467/2000... Training loss: 0.4462\n",
      "Epoch: 1467/2000... Training loss: 0.3857\n",
      "Epoch: 1467/2000... Training loss: 0.4673\n",
      "Epoch: 1467/2000... Training loss: 0.3278\n",
      "Epoch: 1468/2000... Training loss: 0.4822\n",
      "Epoch: 1468/2000... Training loss: 0.4365\n",
      "Epoch: 1468/2000... Training loss: 0.4121\n",
      "Epoch: 1468/2000... Training loss: 0.4521\n",
      "Epoch: 1468/2000... Training loss: 0.5285\n",
      "Epoch: 1468/2000... Training loss: 0.3209\n",
      "Epoch: 1468/2000... Training loss: 0.4888\n",
      "Epoch: 1468/2000... Training loss: 0.5643\n",
      "Epoch: 1468/2000... Training loss: 0.4784\n",
      "Epoch: 1468/2000... Training loss: 0.5443\n",
      "Epoch: 1468/2000... Training loss: 0.2921\n",
      "Epoch: 1468/2000... Training loss: 0.3328\n",
      "Epoch: 1468/2000... Training loss: 0.4406\n",
      "Epoch: 1468/2000... Training loss: 0.4473\n",
      "Epoch: 1468/2000... Training loss: 0.4458\n",
      "Epoch: 1468/2000... Training loss: 0.3445\n",
      "Epoch: 1468/2000... Training loss: 0.3911\n",
      "Epoch: 1468/2000... Training loss: 0.3764\n",
      "Epoch: 1468/2000... Training loss: 0.4563\n",
      "Epoch: 1468/2000... Training loss: 0.5692\n",
      "Epoch: 1468/2000... Training loss: 0.3484\n",
      "Epoch: 1468/2000... Training loss: 0.3892\n",
      "Epoch: 1468/2000... Training loss: 0.3666\n",
      "Epoch: 1468/2000... Training loss: 0.3318\n",
      "Epoch: 1468/2000... Training loss: 0.6203\n",
      "Epoch: 1468/2000... Training loss: 0.3911\n",
      "Epoch: 1468/2000... Training loss: 0.4126\n",
      "Epoch: 1468/2000... Training loss: 0.4252\n",
      "Epoch: 1468/2000... Training loss: 0.4616\n",
      "Epoch: 1468/2000... Training loss: 0.5765\n",
      "Epoch: 1468/2000... Training loss: 0.3185\n",
      "Epoch: 1469/2000... Training loss: 0.4613\n",
      "Epoch: 1469/2000... Training loss: 0.5531\n",
      "Epoch: 1469/2000... Training loss: 0.5066\n",
      "Epoch: 1469/2000... Training loss: 0.5335\n",
      "Epoch: 1469/2000... Training loss: 0.3868\n",
      "Epoch: 1469/2000... Training loss: 0.4889\n",
      "Epoch: 1469/2000... Training loss: 0.4315\n",
      "Epoch: 1469/2000... Training loss: 0.3728\n",
      "Epoch: 1469/2000... Training loss: 0.3016\n",
      "Epoch: 1469/2000... Training loss: 0.4805\n",
      "Epoch: 1469/2000... Training loss: 0.3789\n",
      "Epoch: 1469/2000... Training loss: 0.4370\n",
      "Epoch: 1469/2000... Training loss: 0.3399\n",
      "Epoch: 1469/2000... Training loss: 0.3192\n",
      "Epoch: 1469/2000... Training loss: 0.3436\n",
      "Epoch: 1469/2000... Training loss: 0.4346\n",
      "Epoch: 1469/2000... Training loss: 0.3784\n",
      "Epoch: 1469/2000... Training loss: 0.4747\n",
      "Epoch: 1469/2000... Training loss: 0.3827\n",
      "Epoch: 1469/2000... Training loss: 0.5391\n",
      "Epoch: 1469/2000... Training loss: 0.4943\n",
      "Epoch: 1469/2000... Training loss: 0.3120\n",
      "Epoch: 1469/2000... Training loss: 0.3787\n",
      "Epoch: 1469/2000... Training loss: 0.5415\n",
      "Epoch: 1469/2000... Training loss: 0.3749\n",
      "Epoch: 1469/2000... Training loss: 0.3288\n",
      "Epoch: 1469/2000... Training loss: 0.5952\n",
      "Epoch: 1469/2000... Training loss: 0.4435\n",
      "Epoch: 1469/2000... Training loss: 0.4986\n",
      "Epoch: 1469/2000... Training loss: 0.2735\n",
      "Epoch: 1469/2000... Training loss: 0.3516\n",
      "Epoch: 1470/2000... Training loss: 0.4621\n",
      "Epoch: 1470/2000... Training loss: 0.4007\n",
      "Epoch: 1470/2000... Training loss: 0.5355\n",
      "Epoch: 1470/2000... Training loss: 0.4353\n",
      "Epoch: 1470/2000... Training loss: 0.2652\n",
      "Epoch: 1470/2000... Training loss: 0.4164\n",
      "Epoch: 1470/2000... Training loss: 0.4515\n",
      "Epoch: 1470/2000... Training loss: 0.3811\n",
      "Epoch: 1470/2000... Training loss: 0.4030\n",
      "Epoch: 1470/2000... Training loss: 0.5097\n",
      "Epoch: 1470/2000... Training loss: 0.4567\n",
      "Epoch: 1470/2000... Training loss: 0.3890\n",
      "Epoch: 1470/2000... Training loss: 0.4765\n",
      "Epoch: 1470/2000... Training loss: 0.4912\n",
      "Epoch: 1470/2000... Training loss: 0.3550\n",
      "Epoch: 1470/2000... Training loss: 0.3875\n",
      "Epoch: 1470/2000... Training loss: 0.3660\n",
      "Epoch: 1470/2000... Training loss: 0.3198\n",
      "Epoch: 1470/2000... Training loss: 0.4841\n",
      "Epoch: 1470/2000... Training loss: 0.3852\n",
      "Epoch: 1470/2000... Training loss: 0.4777\n",
      "Epoch: 1470/2000... Training loss: 0.2264\n",
      "Epoch: 1470/2000... Training loss: 0.4089\n",
      "Epoch: 1470/2000... Training loss: 0.3659\n",
      "Epoch: 1470/2000... Training loss: 0.4438\n",
      "Epoch: 1470/2000... Training loss: 0.5210\n",
      "Epoch: 1470/2000... Training loss: 0.3750\n",
      "Epoch: 1470/2000... Training loss: 0.4910\n",
      "Epoch: 1470/2000... Training loss: 0.4121\n",
      "Epoch: 1470/2000... Training loss: 0.2876\n",
      "Epoch: 1470/2000... Training loss: 0.4203\n",
      "Epoch: 1471/2000... Training loss: 0.2790\n",
      "Epoch: 1471/2000... Training loss: 0.3608\n",
      "Epoch: 1471/2000... Training loss: 0.3544\n",
      "Epoch: 1471/2000... Training loss: 0.5903\n",
      "Epoch: 1471/2000... Training loss: 0.5042\n",
      "Epoch: 1471/2000... Training loss: 0.4351\n",
      "Epoch: 1471/2000... Training loss: 0.4056\n",
      "Epoch: 1471/2000... Training loss: 0.3947\n",
      "Epoch: 1471/2000... Training loss: 0.2858\n",
      "Epoch: 1471/2000... Training loss: 0.4588\n",
      "Epoch: 1471/2000... Training loss: 0.3815\n",
      "Epoch: 1471/2000... Training loss: 0.3769\n",
      "Epoch: 1471/2000... Training loss: 0.3225\n",
      "Epoch: 1471/2000... Training loss: 0.2722\n",
      "Epoch: 1471/2000... Training loss: 0.3858\n",
      "Epoch: 1471/2000... Training loss: 0.4529\n",
      "Epoch: 1471/2000... Training loss: 0.3786\n",
      "Epoch: 1471/2000... Training loss: 0.4266\n",
      "Epoch: 1471/2000... Training loss: 0.3109\n",
      "Epoch: 1471/2000... Training loss: 0.3793\n",
      "Epoch: 1471/2000... Training loss: 0.6139\n",
      "Epoch: 1471/2000... Training loss: 0.3070\n",
      "Epoch: 1471/2000... Training loss: 0.3622\n",
      "Epoch: 1471/2000... Training loss: 0.3763\n",
      "Epoch: 1471/2000... Training loss: 0.4050\n",
      "Epoch: 1471/2000... Training loss: 0.3529\n",
      "Epoch: 1471/2000... Training loss: 0.4595\n",
      "Epoch: 1471/2000... Training loss: 0.4745\n",
      "Epoch: 1471/2000... Training loss: 0.3906\n",
      "Epoch: 1471/2000... Training loss: 0.3555\n",
      "Epoch: 1471/2000... Training loss: 0.3964\n",
      "Epoch: 1472/2000... Training loss: 0.4546\n",
      "Epoch: 1472/2000... Training loss: 0.4339\n",
      "Epoch: 1472/2000... Training loss: 0.4887\n",
      "Epoch: 1472/2000... Training loss: 0.3736\n",
      "Epoch: 1472/2000... Training loss: 0.4012\n",
      "Epoch: 1472/2000... Training loss: 0.3547\n",
      "Epoch: 1472/2000... Training loss: 0.2379\n",
      "Epoch: 1472/2000... Training loss: 0.5435\n",
      "Epoch: 1472/2000... Training loss: 0.4588\n",
      "Epoch: 1472/2000... Training loss: 0.4224\n",
      "Epoch: 1472/2000... Training loss: 0.3578\n",
      "Epoch: 1472/2000... Training loss: 0.5268\n",
      "Epoch: 1472/2000... Training loss: 0.3614\n",
      "Epoch: 1472/2000... Training loss: 0.5002\n",
      "Epoch: 1472/2000... Training loss: 0.4269\n",
      "Epoch: 1472/2000... Training loss: 0.3616\n",
      "Epoch: 1472/2000... Training loss: 0.4552\n",
      "Epoch: 1472/2000... Training loss: 0.3009\n",
      "Epoch: 1472/2000... Training loss: 0.2900\n",
      "Epoch: 1472/2000... Training loss: 0.4303\n",
      "Epoch: 1472/2000... Training loss: 0.4431\n",
      "Epoch: 1472/2000... Training loss: 0.4135\n",
      "Epoch: 1472/2000... Training loss: 0.4660\n",
      "Epoch: 1472/2000... Training loss: 0.3861\n",
      "Epoch: 1472/2000... Training loss: 0.2671\n",
      "Epoch: 1472/2000... Training loss: 0.4244\n",
      "Epoch: 1472/2000... Training loss: 0.3320\n",
      "Epoch: 1472/2000... Training loss: 0.3995\n",
      "Epoch: 1472/2000... Training loss: 0.4728\n",
      "Epoch: 1472/2000... Training loss: 0.4583\n",
      "Epoch: 1472/2000... Training loss: 0.4087\n",
      "Epoch: 1473/2000... Training loss: 0.3509\n",
      "Epoch: 1473/2000... Training loss: 0.5167\n",
      "Epoch: 1473/2000... Training loss: 0.4222\n",
      "Epoch: 1473/2000... Training loss: 0.5261\n",
      "Epoch: 1473/2000... Training loss: 0.3036\n",
      "Epoch: 1473/2000... Training loss: 0.2843\n",
      "Epoch: 1473/2000... Training loss: 0.1882\n",
      "Epoch: 1473/2000... Training loss: 0.3417\n",
      "Epoch: 1473/2000... Training loss: 0.4870\n",
      "Epoch: 1473/2000... Training loss: 0.3623\n",
      "Epoch: 1473/2000... Training loss: 0.4242\n",
      "Epoch: 1473/2000... Training loss: 0.2582\n",
      "Epoch: 1473/2000... Training loss: 0.4932\n",
      "Epoch: 1473/2000... Training loss: 0.6724\n",
      "Epoch: 1473/2000... Training loss: 0.5308\n",
      "Epoch: 1473/2000... Training loss: 0.4818\n",
      "Epoch: 1473/2000... Training loss: 0.2865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1473/2000... Training loss: 0.5413\n",
      "Epoch: 1473/2000... Training loss: 0.2580\n",
      "Epoch: 1473/2000... Training loss: 0.3403\n",
      "Epoch: 1473/2000... Training loss: 0.4054\n",
      "Epoch: 1473/2000... Training loss: 0.3800\n",
      "Epoch: 1473/2000... Training loss: 0.6058\n",
      "Epoch: 1473/2000... Training loss: 0.3551\n",
      "Epoch: 1473/2000... Training loss: 0.5624\n",
      "Epoch: 1473/2000... Training loss: 0.4392\n",
      "Epoch: 1473/2000... Training loss: 0.4721\n",
      "Epoch: 1473/2000... Training loss: 0.4460\n",
      "Epoch: 1473/2000... Training loss: 0.3776\n",
      "Epoch: 1473/2000... Training loss: 0.4200\n",
      "Epoch: 1473/2000... Training loss: 0.3904\n",
      "Epoch: 1474/2000... Training loss: 0.6182\n",
      "Epoch: 1474/2000... Training loss: 0.3936\n",
      "Epoch: 1474/2000... Training loss: 0.3538\n",
      "Epoch: 1474/2000... Training loss: 0.3496\n",
      "Epoch: 1474/2000... Training loss: 0.4795\n",
      "Epoch: 1474/2000... Training loss: 0.4664\n",
      "Epoch: 1474/2000... Training loss: 0.3622\n",
      "Epoch: 1474/2000... Training loss: 0.4951\n",
      "Epoch: 1474/2000... Training loss: 0.4874\n",
      "Epoch: 1474/2000... Training loss: 0.3951\n",
      "Epoch: 1474/2000... Training loss: 0.3765\n",
      "Epoch: 1474/2000... Training loss: 0.3775\n",
      "Epoch: 1474/2000... Training loss: 0.4258\n",
      "Epoch: 1474/2000... Training loss: 0.5304\n",
      "Epoch: 1474/2000... Training loss: 0.3324\n",
      "Epoch: 1474/2000... Training loss: 0.3655\n",
      "Epoch: 1474/2000... Training loss: 0.2777\n",
      "Epoch: 1474/2000... Training loss: 0.3019\n",
      "Epoch: 1474/2000... Training loss: 0.3930\n",
      "Epoch: 1474/2000... Training loss: 0.4194\n",
      "Epoch: 1474/2000... Training loss: 0.3602\n",
      "Epoch: 1474/2000... Training loss: 0.4057\n",
      "Epoch: 1474/2000... Training loss: 0.2765\n",
      "Epoch: 1474/2000... Training loss: 0.4738\n",
      "Epoch: 1474/2000... Training loss: 0.3659\n",
      "Epoch: 1474/2000... Training loss: 0.5059\n",
      "Epoch: 1474/2000... Training loss: 0.3908\n",
      "Epoch: 1474/2000... Training loss: 0.4389\n",
      "Epoch: 1474/2000... Training loss: 0.4817\n",
      "Epoch: 1474/2000... Training loss: 0.3709\n",
      "Epoch: 1474/2000... Training loss: 0.4701\n",
      "Epoch: 1475/2000... Training loss: 0.3588\n",
      "Epoch: 1475/2000... Training loss: 0.3182\n",
      "Epoch: 1475/2000... Training loss: 0.4301\n",
      "Epoch: 1475/2000... Training loss: 0.5385\n",
      "Epoch: 1475/2000... Training loss: 0.3257\n",
      "Epoch: 1475/2000... Training loss: 0.3230\n",
      "Epoch: 1475/2000... Training loss: 0.5480\n",
      "Epoch: 1475/2000... Training loss: 0.5729\n",
      "Epoch: 1475/2000... Training loss: 0.6686\n",
      "Epoch: 1475/2000... Training loss: 0.3811\n",
      "Epoch: 1475/2000... Training loss: 0.4754\n",
      "Epoch: 1475/2000... Training loss: 0.3363\n",
      "Epoch: 1475/2000... Training loss: 0.4698\n",
      "Epoch: 1475/2000... Training loss: 0.4959\n",
      "Epoch: 1475/2000... Training loss: 0.2786\n",
      "Epoch: 1475/2000... Training loss: 0.4350\n",
      "Epoch: 1475/2000... Training loss: 0.4235\n",
      "Epoch: 1475/2000... Training loss: 0.4233\n",
      "Epoch: 1475/2000... Training loss: 0.4156\n",
      "Epoch: 1475/2000... Training loss: 0.3945\n",
      "Epoch: 1475/2000... Training loss: 0.2932\n",
      "Epoch: 1475/2000... Training loss: 0.4110\n",
      "Epoch: 1475/2000... Training loss: 0.5313\n",
      "Epoch: 1475/2000... Training loss: 0.4584\n",
      "Epoch: 1475/2000... Training loss: 0.3045\n",
      "Epoch: 1475/2000... Training loss: 0.5256\n",
      "Epoch: 1475/2000... Training loss: 0.3587\n",
      "Epoch: 1475/2000... Training loss: 0.4353\n",
      "Epoch: 1475/2000... Training loss: 0.5347\n",
      "Epoch: 1475/2000... Training loss: 0.3557\n",
      "Epoch: 1475/2000... Training loss: 0.3727\n",
      "Epoch: 1476/2000... Training loss: 0.3966\n",
      "Epoch: 1476/2000... Training loss: 0.4662\n",
      "Epoch: 1476/2000... Training loss: 0.3691\n",
      "Epoch: 1476/2000... Training loss: 0.3677\n",
      "Epoch: 1476/2000... Training loss: 0.4224\n",
      "Epoch: 1476/2000... Training loss: 0.4980\n",
      "Epoch: 1476/2000... Training loss: 0.4728\n",
      "Epoch: 1476/2000... Training loss: 0.4905\n",
      "Epoch: 1476/2000... Training loss: 0.4035\n",
      "Epoch: 1476/2000... Training loss: 0.2742\n",
      "Epoch: 1476/2000... Training loss: 0.3759\n",
      "Epoch: 1476/2000... Training loss: 0.5665\n",
      "Epoch: 1476/2000... Training loss: 0.3693\n",
      "Epoch: 1476/2000... Training loss: 0.4387\n",
      "Epoch: 1476/2000... Training loss: 0.2714\n",
      "Epoch: 1476/2000... Training loss: 0.3210\n",
      "Epoch: 1476/2000... Training loss: 0.3995\n",
      "Epoch: 1476/2000... Training loss: 0.4769\n",
      "Epoch: 1476/2000... Training loss: 0.4950\n",
      "Epoch: 1476/2000... Training loss: 0.3123\n",
      "Epoch: 1476/2000... Training loss: 0.2744\n",
      "Epoch: 1476/2000... Training loss: 0.4624\n",
      "Epoch: 1476/2000... Training loss: 0.6146\n",
      "Epoch: 1476/2000... Training loss: 0.3781\n",
      "Epoch: 1476/2000... Training loss: 0.3447\n",
      "Epoch: 1476/2000... Training loss: 0.5098\n",
      "Epoch: 1476/2000... Training loss: 0.5923\n",
      "Epoch: 1476/2000... Training loss: 0.4352\n",
      "Epoch: 1476/2000... Training loss: 0.3304\n",
      "Epoch: 1476/2000... Training loss: 0.2535\n",
      "Epoch: 1476/2000... Training loss: 0.3481\n",
      "Epoch: 1477/2000... Training loss: 0.5837\n",
      "Epoch: 1477/2000... Training loss: 0.4722\n",
      "Epoch: 1477/2000... Training loss: 0.3194\n",
      "Epoch: 1477/2000... Training loss: 0.2988\n",
      "Epoch: 1477/2000... Training loss: 0.2923\n",
      "Epoch: 1477/2000... Training loss: 0.4764\n",
      "Epoch: 1477/2000... Training loss: 0.5484\n",
      "Epoch: 1477/2000... Training loss: 0.2907\n",
      "Epoch: 1477/2000... Training loss: 0.5103\n",
      "Epoch: 1477/2000... Training loss: 0.4633\n",
      "Epoch: 1477/2000... Training loss: 0.5170\n",
      "Epoch: 1477/2000... Training loss: 0.3729\n",
      "Epoch: 1477/2000... Training loss: 0.3880\n",
      "Epoch: 1477/2000... Training loss: 0.4952\n",
      "Epoch: 1477/2000... Training loss: 0.5436\n",
      "Epoch: 1477/2000... Training loss: 0.4402\n",
      "Epoch: 1477/2000... Training loss: 0.3735\n",
      "Epoch: 1477/2000... Training loss: 0.4271\n",
      "Epoch: 1477/2000... Training loss: 0.5417\n",
      "Epoch: 1477/2000... Training loss: 0.4774\n",
      "Epoch: 1477/2000... Training loss: 0.5882\n",
      "Epoch: 1477/2000... Training loss: 0.2925\n",
      "Epoch: 1477/2000... Training loss: 0.5140\n",
      "Epoch: 1477/2000... Training loss: 0.4671\n",
      "Epoch: 1477/2000... Training loss: 0.6339\n",
      "Epoch: 1477/2000... Training loss: 0.5274\n",
      "Epoch: 1477/2000... Training loss: 0.3964\n",
      "Epoch: 1477/2000... Training loss: 0.2722\n",
      "Epoch: 1477/2000... Training loss: 0.3654\n",
      "Epoch: 1477/2000... Training loss: 0.3939\n",
      "Epoch: 1477/2000... Training loss: 0.4132\n",
      "Epoch: 1478/2000... Training loss: 0.6602\n",
      "Epoch: 1478/2000... Training loss: 0.3961\n",
      "Epoch: 1478/2000... Training loss: 0.4480\n",
      "Epoch: 1478/2000... Training loss: 0.3678\n",
      "Epoch: 1478/2000... Training loss: 0.3898\n",
      "Epoch: 1478/2000... Training loss: 0.5402\n",
      "Epoch: 1478/2000... Training loss: 0.4533\n",
      "Epoch: 1478/2000... Training loss: 0.4635\n",
      "Epoch: 1478/2000... Training loss: 0.4673\n",
      "Epoch: 1478/2000... Training loss: 0.5895\n",
      "Epoch: 1478/2000... Training loss: 0.4703\n",
      "Epoch: 1478/2000... Training loss: 0.4874\n",
      "Epoch: 1478/2000... Training loss: 0.6332\n",
      "Epoch: 1478/2000... Training loss: 0.2753\n",
      "Epoch: 1478/2000... Training loss: 0.6525\n",
      "Epoch: 1478/2000... Training loss: 0.4109\n",
      "Epoch: 1478/2000... Training loss: 0.4045\n",
      "Epoch: 1478/2000... Training loss: 0.3711\n",
      "Epoch: 1478/2000... Training loss: 0.3951\n",
      "Epoch: 1478/2000... Training loss: 0.4874\n",
      "Epoch: 1478/2000... Training loss: 0.3030\n",
      "Epoch: 1478/2000... Training loss: 0.3485\n",
      "Epoch: 1478/2000... Training loss: 0.2823\n",
      "Epoch: 1478/2000... Training loss: 0.5406\n",
      "Epoch: 1478/2000... Training loss: 0.3356\n",
      "Epoch: 1478/2000... Training loss: 0.4195\n",
      "Epoch: 1478/2000... Training loss: 0.4445\n",
      "Epoch: 1478/2000... Training loss: 0.3683\n",
      "Epoch: 1478/2000... Training loss: 0.3985\n",
      "Epoch: 1478/2000... Training loss: 0.4763\n",
      "Epoch: 1478/2000... Training loss: 0.4762\n",
      "Epoch: 1479/2000... Training loss: 0.4016\n",
      "Epoch: 1479/2000... Training loss: 0.3415\n",
      "Epoch: 1479/2000... Training loss: 0.3787\n",
      "Epoch: 1479/2000... Training loss: 0.5947\n",
      "Epoch: 1479/2000... Training loss: 0.3923\n",
      "Epoch: 1479/2000... Training loss: 0.3739\n",
      "Epoch: 1479/2000... Training loss: 0.4262\n",
      "Epoch: 1479/2000... Training loss: 0.3556\n",
      "Epoch: 1479/2000... Training loss: 0.2503\n",
      "Epoch: 1479/2000... Training loss: 0.5352\n",
      "Epoch: 1479/2000... Training loss: 0.4086\n",
      "Epoch: 1479/2000... Training loss: 0.3784\n",
      "Epoch: 1479/2000... Training loss: 0.3314\n",
      "Epoch: 1479/2000... Training loss: 0.5311\n",
      "Epoch: 1479/2000... Training loss: 0.3752\n",
      "Epoch: 1479/2000... Training loss: 0.3143\n",
      "Epoch: 1479/2000... Training loss: 0.4726\n",
      "Epoch: 1479/2000... Training loss: 0.4625\n",
      "Epoch: 1479/2000... Training loss: 0.3400\n",
      "Epoch: 1479/2000... Training loss: 0.3912\n",
      "Epoch: 1479/2000... Training loss: 0.3230\n",
      "Epoch: 1479/2000... Training loss: 0.4784\n",
      "Epoch: 1479/2000... Training loss: 0.4251\n",
      "Epoch: 1479/2000... Training loss: 0.3248\n",
      "Epoch: 1479/2000... Training loss: 0.5639\n",
      "Epoch: 1479/2000... Training loss: 0.4043\n",
      "Epoch: 1479/2000... Training loss: 0.8224\n",
      "Epoch: 1479/2000... Training loss: 0.4239\n",
      "Epoch: 1479/2000... Training loss: 0.3347\n",
      "Epoch: 1479/2000... Training loss: 0.3830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1479/2000... Training loss: 0.2952\n",
      "Epoch: 1480/2000... Training loss: 0.4601\n",
      "Epoch: 1480/2000... Training loss: 0.4641\n",
      "Epoch: 1480/2000... Training loss: 0.3704\n",
      "Epoch: 1480/2000... Training loss: 0.3686\n",
      "Epoch: 1480/2000... Training loss: 0.4029\n",
      "Epoch: 1480/2000... Training loss: 0.3639\n",
      "Epoch: 1480/2000... Training loss: 0.4313\n",
      "Epoch: 1480/2000... Training loss: 0.4086\n",
      "Epoch: 1480/2000... Training loss: 0.5047\n",
      "Epoch: 1480/2000... Training loss: 0.4030\n",
      "Epoch: 1480/2000... Training loss: 0.3564\n",
      "Epoch: 1480/2000... Training loss: 0.4937\n",
      "Epoch: 1480/2000... Training loss: 0.6564\n",
      "Epoch: 1480/2000... Training loss: 0.3492\n",
      "Epoch: 1480/2000... Training loss: 0.3828\n",
      "Epoch: 1480/2000... Training loss: 0.6514\n",
      "Epoch: 1480/2000... Training loss: 0.3665\n",
      "Epoch: 1480/2000... Training loss: 0.3495\n",
      "Epoch: 1480/2000... Training loss: 0.3940\n",
      "Epoch: 1480/2000... Training loss: 0.3566\n",
      "Epoch: 1480/2000... Training loss: 0.5202\n",
      "Epoch: 1480/2000... Training loss: 0.3183\n",
      "Epoch: 1480/2000... Training loss: 0.5683\n",
      "Epoch: 1480/2000... Training loss: 0.5326\n",
      "Epoch: 1480/2000... Training loss: 0.5064\n",
      "Epoch: 1480/2000... Training loss: 0.5628\n",
      "Epoch: 1480/2000... Training loss: 0.5245\n",
      "Epoch: 1480/2000... Training loss: 0.3287\n",
      "Epoch: 1480/2000... Training loss: 0.2791\n",
      "Epoch: 1480/2000... Training loss: 0.4274\n",
      "Epoch: 1480/2000... Training loss: 0.3252\n",
      "Epoch: 1481/2000... Training loss: 0.4107\n",
      "Epoch: 1481/2000... Training loss: 0.3972\n",
      "Epoch: 1481/2000... Training loss: 0.5354\n",
      "Epoch: 1481/2000... Training loss: 0.3869\n",
      "Epoch: 1481/2000... Training loss: 0.4597\n",
      "Epoch: 1481/2000... Training loss: 0.2920\n",
      "Epoch: 1481/2000... Training loss: 0.3505\n",
      "Epoch: 1481/2000... Training loss: 0.5108\n",
      "Epoch: 1481/2000... Training loss: 0.4141\n",
      "Epoch: 1481/2000... Training loss: 0.4199\n",
      "Epoch: 1481/2000... Training loss: 0.4032\n",
      "Epoch: 1481/2000... Training loss: 0.3660\n",
      "Epoch: 1481/2000... Training loss: 0.5181\n",
      "Epoch: 1481/2000... Training loss: 0.2848\n",
      "Epoch: 1481/2000... Training loss: 0.3216\n",
      "Epoch: 1481/2000... Training loss: 0.4691\n",
      "Epoch: 1481/2000... Training loss: 0.2249\n",
      "Epoch: 1481/2000... Training loss: 0.4469\n",
      "Epoch: 1481/2000... Training loss: 0.4755\n",
      "Epoch: 1481/2000... Training loss: 0.3463\n",
      "Epoch: 1481/2000... Training loss: 0.2486\n",
      "Epoch: 1481/2000... Training loss: 0.4838\n",
      "Epoch: 1481/2000... Training loss: 0.3811\n",
      "Epoch: 1481/2000... Training loss: 0.4234\n",
      "Epoch: 1481/2000... Training loss: 0.4735\n",
      "Epoch: 1481/2000... Training loss: 0.4269\n",
      "Epoch: 1481/2000... Training loss: 0.2903\n",
      "Epoch: 1481/2000... Training loss: 0.3458\n",
      "Epoch: 1481/2000... Training loss: 0.4188\n",
      "Epoch: 1481/2000... Training loss: 0.4173\n",
      "Epoch: 1481/2000... Training loss: 0.3395\n",
      "Epoch: 1482/2000... Training loss: 0.3768\n",
      "Epoch: 1482/2000... Training loss: 0.6729\n",
      "Epoch: 1482/2000... Training loss: 0.4084\n",
      "Epoch: 1482/2000... Training loss: 0.5705\n",
      "Epoch: 1482/2000... Training loss: 0.3935\n",
      "Epoch: 1482/2000... Training loss: 0.5312\n",
      "Epoch: 1482/2000... Training loss: 0.5674\n",
      "Epoch: 1482/2000... Training loss: 0.3190\n",
      "Epoch: 1482/2000... Training loss: 0.3327\n",
      "Epoch: 1482/2000... Training loss: 0.3497\n",
      "Epoch: 1482/2000... Training loss: 0.4223\n",
      "Epoch: 1482/2000... Training loss: 0.3119\n",
      "Epoch: 1482/2000... Training loss: 0.4577\n",
      "Epoch: 1482/2000... Training loss: 0.4137\n",
      "Epoch: 1482/2000... Training loss: 0.2816\n",
      "Epoch: 1482/2000... Training loss: 0.2954\n",
      "Epoch: 1482/2000... Training loss: 0.3469\n",
      "Epoch: 1482/2000... Training loss: 0.3173\n",
      "Epoch: 1482/2000... Training loss: 0.3255\n",
      "Epoch: 1482/2000... Training loss: 0.3769\n",
      "Epoch: 1482/2000... Training loss: 0.4070\n",
      "Epoch: 1482/2000... Training loss: 0.2549\n",
      "Epoch: 1482/2000... Training loss: 0.4526\n",
      "Epoch: 1482/2000... Training loss: 0.4092\n",
      "Epoch: 1482/2000... Training loss: 0.4640\n",
      "Epoch: 1482/2000... Training loss: 0.3791\n",
      "Epoch: 1482/2000... Training loss: 0.3193\n",
      "Epoch: 1482/2000... Training loss: 0.3601\n",
      "Epoch: 1482/2000... Training loss: 0.3518\n",
      "Epoch: 1482/2000... Training loss: 0.4255\n",
      "Epoch: 1482/2000... Training loss: 0.5029\n",
      "Epoch: 1483/2000... Training loss: 0.5782\n",
      "Epoch: 1483/2000... Training loss: 0.5460\n",
      "Epoch: 1483/2000... Training loss: 0.3688\n",
      "Epoch: 1483/2000... Training loss: 0.4426\n",
      "Epoch: 1483/2000... Training loss: 0.3205\n",
      "Epoch: 1483/2000... Training loss: 0.4577\n",
      "Epoch: 1483/2000... Training loss: 0.4617\n",
      "Epoch: 1483/2000... Training loss: 0.3808\n",
      "Epoch: 1483/2000... Training loss: 0.3269\n",
      "Epoch: 1483/2000... Training loss: 0.4962\n",
      "Epoch: 1483/2000... Training loss: 0.5443\n",
      "Epoch: 1483/2000... Training loss: 0.4573\n",
      "Epoch: 1483/2000... Training loss: 0.2826\n",
      "Epoch: 1483/2000... Training loss: 0.4657\n",
      "Epoch: 1483/2000... Training loss: 0.2643\n",
      "Epoch: 1483/2000... Training loss: 0.3985\n",
      "Epoch: 1483/2000... Training loss: 0.4716\n",
      "Epoch: 1483/2000... Training loss: 0.4105\n",
      "Epoch: 1483/2000... Training loss: 0.5039\n",
      "Epoch: 1483/2000... Training loss: 0.3684\n",
      "Epoch: 1483/2000... Training loss: 0.3759\n",
      "Epoch: 1483/2000... Training loss: 0.6463\n",
      "Epoch: 1483/2000... Training loss: 0.3417\n",
      "Epoch: 1483/2000... Training loss: 0.4666\n",
      "Epoch: 1483/2000... Training loss: 0.3876\n",
      "Epoch: 1483/2000... Training loss: 0.3891\n",
      "Epoch: 1483/2000... Training loss: 0.3993\n",
      "Epoch: 1483/2000... Training loss: 0.4374\n",
      "Epoch: 1483/2000... Training loss: 0.3487\n",
      "Epoch: 1483/2000... Training loss: 0.3638\n",
      "Epoch: 1483/2000... Training loss: 0.4004\n",
      "Epoch: 1484/2000... Training loss: 0.3704\n",
      "Epoch: 1484/2000... Training loss: 0.3923\n",
      "Epoch: 1484/2000... Training loss: 0.5454\n",
      "Epoch: 1484/2000... Training loss: 0.2618\n",
      "Epoch: 1484/2000... Training loss: 0.4120\n",
      "Epoch: 1484/2000... Training loss: 0.3682\n",
      "Epoch: 1484/2000... Training loss: 0.2579\n",
      "Epoch: 1484/2000... Training loss: 0.2803\n",
      "Epoch: 1484/2000... Training loss: 0.4330\n",
      "Epoch: 1484/2000... Training loss: 0.3701\n",
      "Epoch: 1484/2000... Training loss: 0.4290\n",
      "Epoch: 1484/2000... Training loss: 0.3592\n",
      "Epoch: 1484/2000... Training loss: 0.3861\n",
      "Epoch: 1484/2000... Training loss: 0.3161\n",
      "Epoch: 1484/2000... Training loss: 0.3422\n",
      "Epoch: 1484/2000... Training loss: 0.3392\n",
      "Epoch: 1484/2000... Training loss: 0.3614\n",
      "Epoch: 1484/2000... Training loss: 0.3420\n",
      "Epoch: 1484/2000... Training loss: 0.4495\n",
      "Epoch: 1484/2000... Training loss: 0.2784\n",
      "Epoch: 1484/2000... Training loss: 0.3103\n",
      "Epoch: 1484/2000... Training loss: 0.2676\n",
      "Epoch: 1484/2000... Training loss: 0.4915\n",
      "Epoch: 1484/2000... Training loss: 0.2709\n",
      "Epoch: 1484/2000... Training loss: 0.5095\n",
      "Epoch: 1484/2000... Training loss: 0.2297\n",
      "Epoch: 1484/2000... Training loss: 0.4050\n",
      "Epoch: 1484/2000... Training loss: 0.2614\n",
      "Epoch: 1484/2000... Training loss: 0.2899\n",
      "Epoch: 1484/2000... Training loss: 0.6609\n",
      "Epoch: 1484/2000... Training loss: 0.5267\n",
      "Epoch: 1485/2000... Training loss: 0.5452\n",
      "Epoch: 1485/2000... Training loss: 0.3778\n",
      "Epoch: 1485/2000... Training loss: 0.6041\n",
      "Epoch: 1485/2000... Training loss: 0.3307\n",
      "Epoch: 1485/2000... Training loss: 0.4824\n",
      "Epoch: 1485/2000... Training loss: 0.5456\n",
      "Epoch: 1485/2000... Training loss: 0.3308\n",
      "Epoch: 1485/2000... Training loss: 0.2388\n",
      "Epoch: 1485/2000... Training loss: 0.4368\n",
      "Epoch: 1485/2000... Training loss: 0.4022\n",
      "Epoch: 1485/2000... Training loss: 0.4475\n",
      "Epoch: 1485/2000... Training loss: 0.3930\n",
      "Epoch: 1485/2000... Training loss: 0.2851\n",
      "Epoch: 1485/2000... Training loss: 0.3073\n",
      "Epoch: 1485/2000... Training loss: 0.3680\n",
      "Epoch: 1485/2000... Training loss: 0.4906\n",
      "Epoch: 1485/2000... Training loss: 0.3192\n",
      "Epoch: 1485/2000... Training loss: 0.3525\n",
      "Epoch: 1485/2000... Training loss: 0.3798\n",
      "Epoch: 1485/2000... Training loss: 0.4069\n",
      "Epoch: 1485/2000... Training loss: 0.4080\n",
      "Epoch: 1485/2000... Training loss: 0.4835\n",
      "Epoch: 1485/2000... Training loss: 0.7007\n",
      "Epoch: 1485/2000... Training loss: 0.3291\n",
      "Epoch: 1485/2000... Training loss: 0.3575\n",
      "Epoch: 1485/2000... Training loss: 0.5763\n",
      "Epoch: 1485/2000... Training loss: 0.4500\n",
      "Epoch: 1485/2000... Training loss: 0.4355\n",
      "Epoch: 1485/2000... Training loss: 0.6212\n",
      "Epoch: 1485/2000... Training loss: 0.4288\n",
      "Epoch: 1485/2000... Training loss: 0.5401\n",
      "Epoch: 1486/2000... Training loss: 0.3122\n",
      "Epoch: 1486/2000... Training loss: 0.3879\n",
      "Epoch: 1486/2000... Training loss: 0.4893\n",
      "Epoch: 1486/2000... Training loss: 0.2568\n",
      "Epoch: 1486/2000... Training loss: 0.3758\n",
      "Epoch: 1486/2000... Training loss: 0.5188\n",
      "Epoch: 1486/2000... Training loss: 0.3316\n",
      "Epoch: 1486/2000... Training loss: 0.5562\n",
      "Epoch: 1486/2000... Training loss: 0.4529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1486/2000... Training loss: 0.3790\n",
      "Epoch: 1486/2000... Training loss: 0.2916\n",
      "Epoch: 1486/2000... Training loss: 0.4098\n",
      "Epoch: 1486/2000... Training loss: 0.3426\n",
      "Epoch: 1486/2000... Training loss: 0.4726\n",
      "Epoch: 1486/2000... Training loss: 0.3426\n",
      "Epoch: 1486/2000... Training loss: 0.3605\n",
      "Epoch: 1486/2000... Training loss: 0.3103\n",
      "Epoch: 1486/2000... Training loss: 0.4220\n",
      "Epoch: 1486/2000... Training loss: 0.4353\n",
      "Epoch: 1486/2000... Training loss: 0.4763\n",
      "Epoch: 1486/2000... Training loss: 0.2892\n",
      "Epoch: 1486/2000... Training loss: 0.4225\n",
      "Epoch: 1486/2000... Training loss: 0.5404\n",
      "Epoch: 1486/2000... Training loss: 0.3715\n",
      "Epoch: 1486/2000... Training loss: 0.3708\n",
      "Epoch: 1486/2000... Training loss: 0.4043\n",
      "Epoch: 1486/2000... Training loss: 0.3591\n",
      "Epoch: 1486/2000... Training loss: 0.2921\n",
      "Epoch: 1486/2000... Training loss: 0.3508\n",
      "Epoch: 1486/2000... Training loss: 0.3791\n",
      "Epoch: 1486/2000... Training loss: 0.3861\n",
      "Epoch: 1487/2000... Training loss: 0.4023\n",
      "Epoch: 1487/2000... Training loss: 0.4003\n",
      "Epoch: 1487/2000... Training loss: 0.5173\n",
      "Epoch: 1487/2000... Training loss: 0.3429\n",
      "Epoch: 1487/2000... Training loss: 0.4340\n",
      "Epoch: 1487/2000... Training loss: 0.4134\n",
      "Epoch: 1487/2000... Training loss: 0.3761\n",
      "Epoch: 1487/2000... Training loss: 0.2290\n",
      "Epoch: 1487/2000... Training loss: 0.3575\n",
      "Epoch: 1487/2000... Training loss: 0.2546\n",
      "Epoch: 1487/2000... Training loss: 0.4036\n",
      "Epoch: 1487/2000... Training loss: 0.5083\n",
      "Epoch: 1487/2000... Training loss: 0.3493\n",
      "Epoch: 1487/2000... Training loss: 0.4057\n",
      "Epoch: 1487/2000... Training loss: 0.2548\n",
      "Epoch: 1487/2000... Training loss: 0.5247\n",
      "Epoch: 1487/2000... Training loss: 0.3736\n",
      "Epoch: 1487/2000... Training loss: 0.4558\n",
      "Epoch: 1487/2000... Training loss: 0.2982\n",
      "Epoch: 1487/2000... Training loss: 0.3519\n",
      "Epoch: 1487/2000... Training loss: 0.4288\n",
      "Epoch: 1487/2000... Training loss: 0.4453\n",
      "Epoch: 1487/2000... Training loss: 0.3184\n",
      "Epoch: 1487/2000... Training loss: 0.4380\n",
      "Epoch: 1487/2000... Training loss: 0.2520\n",
      "Epoch: 1487/2000... Training loss: 0.3309\n",
      "Epoch: 1487/2000... Training loss: 0.3850\n",
      "Epoch: 1487/2000... Training loss: 0.4352\n",
      "Epoch: 1487/2000... Training loss: 0.4772\n",
      "Epoch: 1487/2000... Training loss: 0.3394\n",
      "Epoch: 1487/2000... Training loss: 0.3516\n",
      "Epoch: 1488/2000... Training loss: 0.3873\n",
      "Epoch: 1488/2000... Training loss: 0.4936\n",
      "Epoch: 1488/2000... Training loss: 0.4327\n",
      "Epoch: 1488/2000... Training loss: 0.4247\n",
      "Epoch: 1488/2000... Training loss: 0.5200\n",
      "Epoch: 1488/2000... Training loss: 0.3243\n",
      "Epoch: 1488/2000... Training loss: 0.2965\n",
      "Epoch: 1488/2000... Training loss: 0.4629\n",
      "Epoch: 1488/2000... Training loss: 0.2794\n",
      "Epoch: 1488/2000... Training loss: 0.3642\n",
      "Epoch: 1488/2000... Training loss: 0.4577\n",
      "Epoch: 1488/2000... Training loss: 0.3705\n",
      "Epoch: 1488/2000... Training loss: 0.3119\n",
      "Epoch: 1488/2000... Training loss: 0.3914\n",
      "Epoch: 1488/2000... Training loss: 0.5797\n",
      "Epoch: 1488/2000... Training loss: 0.3591\n",
      "Epoch: 1488/2000... Training loss: 0.3795\n",
      "Epoch: 1488/2000... Training loss: 0.5074\n",
      "Epoch: 1488/2000... Training loss: 0.4676\n",
      "Epoch: 1488/2000... Training loss: 0.4031\n",
      "Epoch: 1488/2000... Training loss: 0.5300\n",
      "Epoch: 1488/2000... Training loss: 0.4850\n",
      "Epoch: 1488/2000... Training loss: 0.4204\n",
      "Epoch: 1488/2000... Training loss: 0.4110\n",
      "Epoch: 1488/2000... Training loss: 0.3524\n",
      "Epoch: 1488/2000... Training loss: 0.4369\n",
      "Epoch: 1488/2000... Training loss: 0.4254\n",
      "Epoch: 1488/2000... Training loss: 0.3668\n",
      "Epoch: 1488/2000... Training loss: 0.3900\n",
      "Epoch: 1488/2000... Training loss: 0.5941\n",
      "Epoch: 1488/2000... Training loss: 0.4099\n",
      "Epoch: 1489/2000... Training loss: 0.4438\n",
      "Epoch: 1489/2000... Training loss: 0.5055\n",
      "Epoch: 1489/2000... Training loss: 0.5743\n",
      "Epoch: 1489/2000... Training loss: 0.3003\n",
      "Epoch: 1489/2000... Training loss: 0.4887\n",
      "Epoch: 1489/2000... Training loss: 0.4769\n",
      "Epoch: 1489/2000... Training loss: 0.3273\n",
      "Epoch: 1489/2000... Training loss: 0.4743\n",
      "Epoch: 1489/2000... Training loss: 0.3202\n",
      "Epoch: 1489/2000... Training loss: 0.5392\n",
      "Epoch: 1489/2000... Training loss: 0.4706\n",
      "Epoch: 1489/2000... Training loss: 0.4432\n",
      "Epoch: 1489/2000... Training loss: 0.4919\n",
      "Epoch: 1489/2000... Training loss: 0.5493\n",
      "Epoch: 1489/2000... Training loss: 0.4459\n",
      "Epoch: 1489/2000... Training loss: 0.2723\n",
      "Epoch: 1489/2000... Training loss: 0.5428\n",
      "Epoch: 1489/2000... Training loss: 0.3565\n",
      "Epoch: 1489/2000... Training loss: 0.4327\n",
      "Epoch: 1489/2000... Training loss: 0.5280\n",
      "Epoch: 1489/2000... Training loss: 0.3798\n",
      "Epoch: 1489/2000... Training loss: 0.3942\n",
      "Epoch: 1489/2000... Training loss: 0.3712\n",
      "Epoch: 1489/2000... Training loss: 0.3401\n",
      "Epoch: 1489/2000... Training loss: 0.4537\n",
      "Epoch: 1489/2000... Training loss: 0.4211\n",
      "Epoch: 1489/2000... Training loss: 0.5108\n",
      "Epoch: 1489/2000... Training loss: 0.6896\n",
      "Epoch: 1489/2000... Training loss: 0.4057\n",
      "Epoch: 1489/2000... Training loss: 0.4283\n",
      "Epoch: 1489/2000... Training loss: 0.3759\n",
      "Epoch: 1490/2000... Training loss: 0.5212\n",
      "Epoch: 1490/2000... Training loss: 0.4429\n",
      "Epoch: 1490/2000... Training loss: 0.3980\n",
      "Epoch: 1490/2000... Training loss: 0.3426\n",
      "Epoch: 1490/2000... Training loss: 0.6987\n",
      "Epoch: 1490/2000... Training loss: 0.2140\n",
      "Epoch: 1490/2000... Training loss: 0.2905\n",
      "Epoch: 1490/2000... Training loss: 0.4258\n",
      "Epoch: 1490/2000... Training loss: 0.4963\n",
      "Epoch: 1490/2000... Training loss: 0.5127\n",
      "Epoch: 1490/2000... Training loss: 0.5069\n",
      "Epoch: 1490/2000... Training loss: 0.5027\n",
      "Epoch: 1490/2000... Training loss: 0.5564\n",
      "Epoch: 1490/2000... Training loss: 0.4611\n",
      "Epoch: 1490/2000... Training loss: 0.2899\n",
      "Epoch: 1490/2000... Training loss: 0.6237\n",
      "Epoch: 1490/2000... Training loss: 0.3943\n",
      "Epoch: 1490/2000... Training loss: 0.3935\n",
      "Epoch: 1490/2000... Training loss: 0.4695\n",
      "Epoch: 1490/2000... Training loss: 0.4417\n",
      "Epoch: 1490/2000... Training loss: 0.6563\n",
      "Epoch: 1490/2000... Training loss: 0.2685\n",
      "Epoch: 1490/2000... Training loss: 0.3199\n",
      "Epoch: 1490/2000... Training loss: 0.7224\n",
      "Epoch: 1490/2000... Training loss: 0.5309\n",
      "Epoch: 1490/2000... Training loss: 0.4653\n",
      "Epoch: 1490/2000... Training loss: 0.4788\n",
      "Epoch: 1490/2000... Training loss: 0.4095\n",
      "Epoch: 1490/2000... Training loss: 0.5666\n",
      "Epoch: 1490/2000... Training loss: 0.5716\n",
      "Epoch: 1490/2000... Training loss: 0.2642\n",
      "Epoch: 1491/2000... Training loss: 0.2868\n",
      "Epoch: 1491/2000... Training loss: 0.4093\n",
      "Epoch: 1491/2000... Training loss: 0.5511\n",
      "Epoch: 1491/2000... Training loss: 0.4326\n",
      "Epoch: 1491/2000... Training loss: 0.5546\n",
      "Epoch: 1491/2000... Training loss: 0.4399\n",
      "Epoch: 1491/2000... Training loss: 0.2802\n",
      "Epoch: 1491/2000... Training loss: 0.3632\n",
      "Epoch: 1491/2000... Training loss: 0.3925\n",
      "Epoch: 1491/2000... Training loss: 0.5395\n",
      "Epoch: 1491/2000... Training loss: 0.2558\n",
      "Epoch: 1491/2000... Training loss: 0.5032\n",
      "Epoch: 1491/2000... Training loss: 0.5105\n",
      "Epoch: 1491/2000... Training loss: 0.5653\n",
      "Epoch: 1491/2000... Training loss: 0.6204\n",
      "Epoch: 1491/2000... Training loss: 0.4070\n",
      "Epoch: 1491/2000... Training loss: 0.4308\n",
      "Epoch: 1491/2000... Training loss: 0.4898\n",
      "Epoch: 1491/2000... Training loss: 0.4100\n",
      "Epoch: 1491/2000... Training loss: 0.4204\n",
      "Epoch: 1491/2000... Training loss: 0.5142\n",
      "Epoch: 1491/2000... Training loss: 0.3734\n",
      "Epoch: 1491/2000... Training loss: 0.4028\n",
      "Epoch: 1491/2000... Training loss: 0.3748\n",
      "Epoch: 1491/2000... Training loss: 0.3852\n",
      "Epoch: 1491/2000... Training loss: 0.3191\n",
      "Epoch: 1491/2000... Training loss: 0.5882\n",
      "Epoch: 1491/2000... Training loss: 0.2021\n",
      "Epoch: 1491/2000... Training loss: 0.4545\n",
      "Epoch: 1491/2000... Training loss: 0.4616\n",
      "Epoch: 1491/2000... Training loss: 0.3090\n",
      "Epoch: 1492/2000... Training loss: 0.5493\n",
      "Epoch: 1492/2000... Training loss: 0.5174\n",
      "Epoch: 1492/2000... Training loss: 0.2778\n",
      "Epoch: 1492/2000... Training loss: 0.3445\n",
      "Epoch: 1492/2000... Training loss: 0.3238\n",
      "Epoch: 1492/2000... Training loss: 0.4475\n",
      "Epoch: 1492/2000... Training loss: 0.4107\n",
      "Epoch: 1492/2000... Training loss: 0.4142\n",
      "Epoch: 1492/2000... Training loss: 0.3619\n",
      "Epoch: 1492/2000... Training loss: 0.3716\n",
      "Epoch: 1492/2000... Training loss: 0.3055\n",
      "Epoch: 1492/2000... Training loss: 0.3694\n",
      "Epoch: 1492/2000... Training loss: 0.3627\n",
      "Epoch: 1492/2000... Training loss: 0.3740\n",
      "Epoch: 1492/2000... Training loss: 0.5393\n",
      "Epoch: 1492/2000... Training loss: 0.3971\n",
      "Epoch: 1492/2000... Training loss: 0.4030\n",
      "Epoch: 1492/2000... Training loss: 0.4202\n",
      "Epoch: 1492/2000... Training loss: 0.4656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1492/2000... Training loss: 0.3287\n",
      "Epoch: 1492/2000... Training loss: 0.4306\n",
      "Epoch: 1492/2000... Training loss: 0.3118\n",
      "Epoch: 1492/2000... Training loss: 0.4382\n",
      "Epoch: 1492/2000... Training loss: 0.2678\n",
      "Epoch: 1492/2000... Training loss: 0.3799\n",
      "Epoch: 1492/2000... Training loss: 0.4125\n",
      "Epoch: 1492/2000... Training loss: 0.3672\n",
      "Epoch: 1492/2000... Training loss: 0.3720\n",
      "Epoch: 1492/2000... Training loss: 0.3536\n",
      "Epoch: 1492/2000... Training loss: 0.3563\n",
      "Epoch: 1492/2000... Training loss: 0.1820\n",
      "Epoch: 1493/2000... Training loss: 0.3810\n",
      "Epoch: 1493/2000... Training loss: 0.5009\n",
      "Epoch: 1493/2000... Training loss: 0.3304\n",
      "Epoch: 1493/2000... Training loss: 0.3219\n",
      "Epoch: 1493/2000... Training loss: 0.3908\n",
      "Epoch: 1493/2000... Training loss: 0.3133\n",
      "Epoch: 1493/2000... Training loss: 0.3938\n",
      "Epoch: 1493/2000... Training loss: 0.3197\n",
      "Epoch: 1493/2000... Training loss: 0.4401\n",
      "Epoch: 1493/2000... Training loss: 0.2981\n",
      "Epoch: 1493/2000... Training loss: 0.4423\n",
      "Epoch: 1493/2000... Training loss: 0.5232\n",
      "Epoch: 1493/2000... Training loss: 0.3542\n",
      "Epoch: 1493/2000... Training loss: 0.2972\n",
      "Epoch: 1493/2000... Training loss: 0.3487\n",
      "Epoch: 1493/2000... Training loss: 0.3512\n",
      "Epoch: 1493/2000... Training loss: 0.4269\n",
      "Epoch: 1493/2000... Training loss: 0.4157\n",
      "Epoch: 1493/2000... Training loss: 0.4438\n",
      "Epoch: 1493/2000... Training loss: 0.2152\n",
      "Epoch: 1493/2000... Training loss: 0.5044\n",
      "Epoch: 1493/2000... Training loss: 0.3986\n",
      "Epoch: 1493/2000... Training loss: 0.3659\n",
      "Epoch: 1493/2000... Training loss: 0.4315\n",
      "Epoch: 1493/2000... Training loss: 0.4257\n",
      "Epoch: 1493/2000... Training loss: 0.3822\n",
      "Epoch: 1493/2000... Training loss: 0.3370\n",
      "Epoch: 1493/2000... Training loss: 0.4751\n",
      "Epoch: 1493/2000... Training loss: 0.3198\n",
      "Epoch: 1493/2000... Training loss: 0.4041\n",
      "Epoch: 1493/2000... Training loss: 0.5404\n",
      "Epoch: 1494/2000... Training loss: 0.4058\n",
      "Epoch: 1494/2000... Training loss: 0.3883\n",
      "Epoch: 1494/2000... Training loss: 0.4818\n",
      "Epoch: 1494/2000... Training loss: 0.4195\n",
      "Epoch: 1494/2000... Training loss: 0.5815\n",
      "Epoch: 1494/2000... Training loss: 0.4195\n",
      "Epoch: 1494/2000... Training loss: 0.4993\n",
      "Epoch: 1494/2000... Training loss: 0.2678\n",
      "Epoch: 1494/2000... Training loss: 0.4004\n",
      "Epoch: 1494/2000... Training loss: 0.4445\n",
      "Epoch: 1494/2000... Training loss: 0.3410\n",
      "Epoch: 1494/2000... Training loss: 0.3901\n",
      "Epoch: 1494/2000... Training loss: 0.3686\n",
      "Epoch: 1494/2000... Training loss: 0.3570\n",
      "Epoch: 1494/2000... Training loss: 0.2280\n",
      "Epoch: 1494/2000... Training loss: 0.3460\n",
      "Epoch: 1494/2000... Training loss: 0.3766\n",
      "Epoch: 1494/2000... Training loss: 0.5998\n",
      "Epoch: 1494/2000... Training loss: 0.5414\n",
      "Epoch: 1494/2000... Training loss: 0.4012\n",
      "Epoch: 1494/2000... Training loss: 0.3589\n",
      "Epoch: 1494/2000... Training loss: 0.2594\n",
      "Epoch: 1494/2000... Training loss: 0.4982\n",
      "Epoch: 1494/2000... Training loss: 0.3789\n",
      "Epoch: 1494/2000... Training loss: 0.4460\n",
      "Epoch: 1494/2000... Training loss: 0.3926\n",
      "Epoch: 1494/2000... Training loss: 0.3664\n",
      "Epoch: 1494/2000... Training loss: 0.4599\n",
      "Epoch: 1494/2000... Training loss: 0.4166\n",
      "Epoch: 1494/2000... Training loss: 0.2056\n",
      "Epoch: 1494/2000... Training loss: 0.5009\n",
      "Epoch: 1495/2000... Training loss: 0.2977\n",
      "Epoch: 1495/2000... Training loss: 0.4977\n",
      "Epoch: 1495/2000... Training loss: 0.4470\n",
      "Epoch: 1495/2000... Training loss: 0.3888\n",
      "Epoch: 1495/2000... Training loss: 0.3250\n",
      "Epoch: 1495/2000... Training loss: 0.3101\n",
      "Epoch: 1495/2000... Training loss: 0.3069\n",
      "Epoch: 1495/2000... Training loss: 0.3893\n",
      "Epoch: 1495/2000... Training loss: 0.4000\n",
      "Epoch: 1495/2000... Training loss: 0.3996\n",
      "Epoch: 1495/2000... Training loss: 0.3220\n",
      "Epoch: 1495/2000... Training loss: 0.4450\n",
      "Epoch: 1495/2000... Training loss: 0.2649\n",
      "Epoch: 1495/2000... Training loss: 0.5117\n",
      "Epoch: 1495/2000... Training loss: 0.4408\n",
      "Epoch: 1495/2000... Training loss: 0.4412\n",
      "Epoch: 1495/2000... Training loss: 0.5095\n",
      "Epoch: 1495/2000... Training loss: 0.3729\n",
      "Epoch: 1495/2000... Training loss: 0.3451\n",
      "Epoch: 1495/2000... Training loss: 0.3996\n",
      "Epoch: 1495/2000... Training loss: 0.3779\n",
      "Epoch: 1495/2000... Training loss: 0.4476\n",
      "Epoch: 1495/2000... Training loss: 0.3536\n",
      "Epoch: 1495/2000... Training loss: 0.3988\n",
      "Epoch: 1495/2000... Training loss: 0.5383\n",
      "Epoch: 1495/2000... Training loss: 0.5182\n",
      "Epoch: 1495/2000... Training loss: 0.5009\n",
      "Epoch: 1495/2000... Training loss: 0.4031\n",
      "Epoch: 1495/2000... Training loss: 0.5396\n",
      "Epoch: 1495/2000... Training loss: 0.3359\n",
      "Epoch: 1495/2000... Training loss: 0.4900\n",
      "Epoch: 1496/2000... Training loss: 0.4197\n",
      "Epoch: 1496/2000... Training loss: 0.4742\n",
      "Epoch: 1496/2000... Training loss: 0.5678\n",
      "Epoch: 1496/2000... Training loss: 0.4573\n",
      "Epoch: 1496/2000... Training loss: 0.2898\n",
      "Epoch: 1496/2000... Training loss: 0.5306\n",
      "Epoch: 1496/2000... Training loss: 0.2725\n",
      "Epoch: 1496/2000... Training loss: 0.4670\n",
      "Epoch: 1496/2000... Training loss: 0.3886\n",
      "Epoch: 1496/2000... Training loss: 0.3622\n",
      "Epoch: 1496/2000... Training loss: 0.3937\n",
      "Epoch: 1496/2000... Training loss: 0.5934\n",
      "Epoch: 1496/2000... Training loss: 0.4738\n",
      "Epoch: 1496/2000... Training loss: 0.3220\n",
      "Epoch: 1496/2000... Training loss: 0.2596\n",
      "Epoch: 1496/2000... Training loss: 0.5227\n",
      "Epoch: 1496/2000... Training loss: 0.3927\n",
      "Epoch: 1496/2000... Training loss: 0.4410\n",
      "Epoch: 1496/2000... Training loss: 0.3872\n",
      "Epoch: 1496/2000... Training loss: 0.4265\n",
      "Epoch: 1496/2000... Training loss: 0.4856\n",
      "Epoch: 1496/2000... Training loss: 0.4647\n",
      "Epoch: 1496/2000... Training loss: 0.3293\n",
      "Epoch: 1496/2000... Training loss: 0.4041\n",
      "Epoch: 1496/2000... Training loss: 0.4836\n",
      "Epoch: 1496/2000... Training loss: 0.3314\n",
      "Epoch: 1496/2000... Training loss: 0.4764\n",
      "Epoch: 1496/2000... Training loss: 0.3612\n",
      "Epoch: 1496/2000... Training loss: 0.4858\n",
      "Epoch: 1496/2000... Training loss: 0.3199\n",
      "Epoch: 1496/2000... Training loss: 0.2719\n",
      "Epoch: 1497/2000... Training loss: 0.4848\n",
      "Epoch: 1497/2000... Training loss: 0.5029\n",
      "Epoch: 1497/2000... Training loss: 0.5278\n",
      "Epoch: 1497/2000... Training loss: 0.3444\n",
      "Epoch: 1497/2000... Training loss: 0.4302\n",
      "Epoch: 1497/2000... Training loss: 0.4638\n",
      "Epoch: 1497/2000... Training loss: 0.4774\n",
      "Epoch: 1497/2000... Training loss: 0.5584\n",
      "Epoch: 1497/2000... Training loss: 0.5284\n",
      "Epoch: 1497/2000... Training loss: 0.6955\n",
      "Epoch: 1497/2000... Training loss: 0.6355\n",
      "Epoch: 1497/2000... Training loss: 0.4761\n",
      "Epoch: 1497/2000... Training loss: 0.3130\n",
      "Epoch: 1497/2000... Training loss: 0.4676\n",
      "Epoch: 1497/2000... Training loss: 0.2353\n",
      "Epoch: 1497/2000... Training loss: 0.3108\n",
      "Epoch: 1497/2000... Training loss: 0.2993\n",
      "Epoch: 1497/2000... Training loss: 0.5646\n",
      "Epoch: 1497/2000... Training loss: 0.5057\n",
      "Epoch: 1497/2000... Training loss: 0.3642\n",
      "Epoch: 1497/2000... Training loss: 0.4015\n",
      "Epoch: 1497/2000... Training loss: 0.4532\n",
      "Epoch: 1497/2000... Training loss: 0.5319\n",
      "Epoch: 1497/2000... Training loss: 0.4624\n",
      "Epoch: 1497/2000... Training loss: 0.4288\n",
      "Epoch: 1497/2000... Training loss: 0.5117\n",
      "Epoch: 1497/2000... Training loss: 0.5642\n",
      "Epoch: 1497/2000... Training loss: 0.4013\n",
      "Epoch: 1497/2000... Training loss: 0.3879\n",
      "Epoch: 1497/2000... Training loss: 0.3652\n",
      "Epoch: 1497/2000... Training loss: 0.3850\n",
      "Epoch: 1498/2000... Training loss: 0.5060\n",
      "Epoch: 1498/2000... Training loss: 0.3810\n",
      "Epoch: 1498/2000... Training loss: 0.3850\n",
      "Epoch: 1498/2000... Training loss: 0.3562\n",
      "Epoch: 1498/2000... Training loss: 0.3826\n",
      "Epoch: 1498/2000... Training loss: 0.3580\n",
      "Epoch: 1498/2000... Training loss: 0.4820\n",
      "Epoch: 1498/2000... Training loss: 0.3464\n",
      "Epoch: 1498/2000... Training loss: 0.3771\n",
      "Epoch: 1498/2000... Training loss: 0.5401\n",
      "Epoch: 1498/2000... Training loss: 0.2946\n",
      "Epoch: 1498/2000... Training loss: 0.4576\n",
      "Epoch: 1498/2000... Training loss: 0.3586\n",
      "Epoch: 1498/2000... Training loss: 0.5164\n",
      "Epoch: 1498/2000... Training loss: 0.4551\n",
      "Epoch: 1498/2000... Training loss: 0.3928\n",
      "Epoch: 1498/2000... Training loss: 0.3534\n",
      "Epoch: 1498/2000... Training loss: 0.3230\n",
      "Epoch: 1498/2000... Training loss: 0.4754\n",
      "Epoch: 1498/2000... Training loss: 0.3374\n",
      "Epoch: 1498/2000... Training loss: 0.4150\n",
      "Epoch: 1498/2000... Training loss: 0.3919\n",
      "Epoch: 1498/2000... Training loss: 0.2731\n",
      "Epoch: 1498/2000... Training loss: 0.2393\n",
      "Epoch: 1498/2000... Training loss: 0.4091\n",
      "Epoch: 1498/2000... Training loss: 0.4285\n",
      "Epoch: 1498/2000... Training loss: 0.3445\n",
      "Epoch: 1498/2000... Training loss: 0.5232\n",
      "Epoch: 1498/2000... Training loss: 0.4144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1498/2000... Training loss: 0.4920\n",
      "Epoch: 1498/2000... Training loss: 0.4689\n",
      "Epoch: 1499/2000... Training loss: 0.5192\n",
      "Epoch: 1499/2000... Training loss: 0.3170\n",
      "Epoch: 1499/2000... Training loss: 0.3842\n",
      "Epoch: 1499/2000... Training loss: 0.4713\n",
      "Epoch: 1499/2000... Training loss: 0.2446\n",
      "Epoch: 1499/2000... Training loss: 0.3972\n",
      "Epoch: 1499/2000... Training loss: 0.1912\n",
      "Epoch: 1499/2000... Training loss: 0.3406\n",
      "Epoch: 1499/2000... Training loss: 0.3586\n",
      "Epoch: 1499/2000... Training loss: 0.4099\n",
      "Epoch: 1499/2000... Training loss: 0.5025\n",
      "Epoch: 1499/2000... Training loss: 0.4480\n",
      "Epoch: 1499/2000... Training loss: 0.3704\n",
      "Epoch: 1499/2000... Training loss: 0.5355\n",
      "Epoch: 1499/2000... Training loss: 0.5004\n",
      "Epoch: 1499/2000... Training loss: 0.2822\n",
      "Epoch: 1499/2000... Training loss: 0.2776\n",
      "Epoch: 1499/2000... Training loss: 0.1735\n",
      "Epoch: 1499/2000... Training loss: 0.3316\n",
      "Epoch: 1499/2000... Training loss: 0.5194\n",
      "Epoch: 1499/2000... Training loss: 0.2745\n",
      "Epoch: 1499/2000... Training loss: 0.2541\n",
      "Epoch: 1499/2000... Training loss: 0.5202\n",
      "Epoch: 1499/2000... Training loss: 0.3346\n",
      "Epoch: 1499/2000... Training loss: 0.3777\n",
      "Epoch: 1499/2000... Training loss: 0.3227\n",
      "Epoch: 1499/2000... Training loss: 0.4098\n",
      "Epoch: 1499/2000... Training loss: 0.4038\n",
      "Epoch: 1499/2000... Training loss: 0.3912\n",
      "Epoch: 1499/2000... Training loss: 0.3649\n",
      "Epoch: 1499/2000... Training loss: 0.4482\n",
      "Epoch: 1500/2000... Training loss: 0.3691\n",
      "Epoch: 1500/2000... Training loss: 0.4303\n",
      "Epoch: 1500/2000... Training loss: 0.4696\n",
      "Epoch: 1500/2000... Training loss: 0.4880\n",
      "Epoch: 1500/2000... Training loss: 0.4103\n",
      "Epoch: 1500/2000... Training loss: 0.4796\n",
      "Epoch: 1500/2000... Training loss: 0.3162\n",
      "Epoch: 1500/2000... Training loss: 0.4650\n",
      "Epoch: 1500/2000... Training loss: 0.4371\n",
      "Epoch: 1500/2000... Training loss: 0.2650\n",
      "Epoch: 1500/2000... Training loss: 0.2832\n",
      "Epoch: 1500/2000... Training loss: 0.3052\n",
      "Epoch: 1500/2000... Training loss: 0.4268\n",
      "Epoch: 1500/2000... Training loss: 0.3742\n",
      "Epoch: 1500/2000... Training loss: 0.4085\n",
      "Epoch: 1500/2000... Training loss: 0.5189\n",
      "Epoch: 1500/2000... Training loss: 0.4065\n",
      "Epoch: 1500/2000... Training loss: 0.4904\n",
      "Epoch: 1500/2000... Training loss: 0.5823\n",
      "Epoch: 1500/2000... Training loss: 0.3629\n",
      "Epoch: 1500/2000... Training loss: 0.5445\n",
      "Epoch: 1500/2000... Training loss: 0.2719\n",
      "Epoch: 1500/2000... Training loss: 0.4294\n",
      "Epoch: 1500/2000... Training loss: 0.4879\n",
      "Epoch: 1500/2000... Training loss: 0.4444\n",
      "Epoch: 1500/2000... Training loss: 0.2698\n",
      "Epoch: 1500/2000... Training loss: 0.3183\n",
      "Epoch: 1500/2000... Training loss: 0.4247\n",
      "Epoch: 1500/2000... Training loss: 0.4482\n",
      "Epoch: 1500/2000... Training loss: 0.4337\n",
      "Epoch: 1500/2000... Training loss: 0.4808\n",
      "Epoch: 1501/2000... Training loss: 0.4448\n",
      "Epoch: 1501/2000... Training loss: 0.2475\n",
      "Epoch: 1501/2000... Training loss: 0.4632\n",
      "Epoch: 1501/2000... Training loss: 0.3832\n",
      "Epoch: 1501/2000... Training loss: 0.2438\n",
      "Epoch: 1501/2000... Training loss: 0.5588\n",
      "Epoch: 1501/2000... Training loss: 0.3767\n",
      "Epoch: 1501/2000... Training loss: 0.4420\n",
      "Epoch: 1501/2000... Training loss: 0.5036\n",
      "Epoch: 1501/2000... Training loss: 0.4157\n",
      "Epoch: 1501/2000... Training loss: 0.3807\n",
      "Epoch: 1501/2000... Training loss: 0.3773\n",
      "Epoch: 1501/2000... Training loss: 0.4455\n",
      "Epoch: 1501/2000... Training loss: 0.4045\n",
      "Epoch: 1501/2000... Training loss: 0.4459\n",
      "Epoch: 1501/2000... Training loss: 0.3574\n",
      "Epoch: 1501/2000... Training loss: 0.3255\n",
      "Epoch: 1501/2000... Training loss: 0.2154\n",
      "Epoch: 1501/2000... Training loss: 0.4832\n",
      "Epoch: 1501/2000... Training loss: 0.5827\n",
      "Epoch: 1501/2000... Training loss: 0.4852\n",
      "Epoch: 1501/2000... Training loss: 0.3989\n",
      "Epoch: 1501/2000... Training loss: 0.4027\n",
      "Epoch: 1501/2000... Training loss: 0.6006\n",
      "Epoch: 1501/2000... Training loss: 0.3738\n",
      "Epoch: 1501/2000... Training loss: 0.4537\n",
      "Epoch: 1501/2000... Training loss: 0.4702\n",
      "Epoch: 1501/2000... Training loss: 0.4680\n",
      "Epoch: 1501/2000... Training loss: 0.4638\n",
      "Epoch: 1501/2000... Training loss: 0.2852\n",
      "Epoch: 1501/2000... Training loss: 0.5457\n",
      "Epoch: 1502/2000... Training loss: 0.5071\n",
      "Epoch: 1502/2000... Training loss: 0.2206\n",
      "Epoch: 1502/2000... Training loss: 0.5049\n",
      "Epoch: 1502/2000... Training loss: 0.3364\n",
      "Epoch: 1502/2000... Training loss: 0.5994\n",
      "Epoch: 1502/2000... Training loss: 0.3452\n",
      "Epoch: 1502/2000... Training loss: 0.3255\n",
      "Epoch: 1502/2000... Training loss: 0.5043\n",
      "Epoch: 1502/2000... Training loss: 0.4229\n",
      "Epoch: 1502/2000... Training loss: 0.3084\n",
      "Epoch: 1502/2000... Training loss: 0.4139\n",
      "Epoch: 1502/2000... Training loss: 0.4061\n",
      "Epoch: 1502/2000... Training loss: 0.4353\n",
      "Epoch: 1502/2000... Training loss: 0.4522\n",
      "Epoch: 1502/2000... Training loss: 0.4248\n",
      "Epoch: 1502/2000... Training loss: 0.5587\n",
      "Epoch: 1502/2000... Training loss: 0.3506\n",
      "Epoch: 1502/2000... Training loss: 0.3203\n",
      "Epoch: 1502/2000... Training loss: 0.5150\n",
      "Epoch: 1502/2000... Training loss: 0.3830\n",
      "Epoch: 1502/2000... Training loss: 0.3566\n",
      "Epoch: 1502/2000... Training loss: 0.2641\n",
      "Epoch: 1502/2000... Training loss: 0.3864\n",
      "Epoch: 1502/2000... Training loss: 0.3883\n",
      "Epoch: 1502/2000... Training loss: 0.5117\n",
      "Epoch: 1502/2000... Training loss: 0.2614\n",
      "Epoch: 1502/2000... Training loss: 0.5121\n",
      "Epoch: 1502/2000... Training loss: 0.4437\n",
      "Epoch: 1502/2000... Training loss: 0.2564\n",
      "Epoch: 1502/2000... Training loss: 0.5122\n",
      "Epoch: 1502/2000... Training loss: 0.3263\n",
      "Epoch: 1503/2000... Training loss: 0.3846\n",
      "Epoch: 1503/2000... Training loss: 0.3745\n",
      "Epoch: 1503/2000... Training loss: 0.4876\n",
      "Epoch: 1503/2000... Training loss: 0.3167\n",
      "Epoch: 1503/2000... Training loss: 0.4447\n",
      "Epoch: 1503/2000... Training loss: 0.6235\n",
      "Epoch: 1503/2000... Training loss: 0.3926\n",
      "Epoch: 1503/2000... Training loss: 0.4922\n",
      "Epoch: 1503/2000... Training loss: 0.4336\n",
      "Epoch: 1503/2000... Training loss: 0.4721\n",
      "Epoch: 1503/2000... Training loss: 0.3533\n",
      "Epoch: 1503/2000... Training loss: 0.5048\n",
      "Epoch: 1503/2000... Training loss: 0.4951\n",
      "Epoch: 1503/2000... Training loss: 0.2401\n",
      "Epoch: 1503/2000... Training loss: 0.4031\n",
      "Epoch: 1503/2000... Training loss: 0.4375\n",
      "Epoch: 1503/2000... Training loss: 0.4414\n",
      "Epoch: 1503/2000... Training loss: 0.4697\n",
      "Epoch: 1503/2000... Training loss: 0.2996\n",
      "Epoch: 1503/2000... Training loss: 0.3964\n",
      "Epoch: 1503/2000... Training loss: 0.3088\n",
      "Epoch: 1503/2000... Training loss: 0.4202\n",
      "Epoch: 1503/2000... Training loss: 0.5500\n",
      "Epoch: 1503/2000... Training loss: 0.5076\n",
      "Epoch: 1503/2000... Training loss: 0.5935\n",
      "Epoch: 1503/2000... Training loss: 0.4855\n",
      "Epoch: 1503/2000... Training loss: 0.4305\n",
      "Epoch: 1503/2000... Training loss: 0.3903\n",
      "Epoch: 1503/2000... Training loss: 0.4609\n",
      "Epoch: 1503/2000... Training loss: 0.3762\n",
      "Epoch: 1503/2000... Training loss: 0.3668\n",
      "Epoch: 1504/2000... Training loss: 0.4223\n",
      "Epoch: 1504/2000... Training loss: 0.3941\n",
      "Epoch: 1504/2000... Training loss: 0.4747\n",
      "Epoch: 1504/2000... Training loss: 0.4738\n",
      "Epoch: 1504/2000... Training loss: 0.5335\n",
      "Epoch: 1504/2000... Training loss: 0.4334\n",
      "Epoch: 1504/2000... Training loss: 0.3664\n",
      "Epoch: 1504/2000... Training loss: 0.4633\n",
      "Epoch: 1504/2000... Training loss: 0.2560\n",
      "Epoch: 1504/2000... Training loss: 0.4726\n",
      "Epoch: 1504/2000... Training loss: 0.3352\n",
      "Epoch: 1504/2000... Training loss: 0.3300\n",
      "Epoch: 1504/2000... Training loss: 0.3665\n",
      "Epoch: 1504/2000... Training loss: 0.5014\n",
      "Epoch: 1504/2000... Training loss: 0.2534\n",
      "Epoch: 1504/2000... Training loss: 0.3237\n",
      "Epoch: 1504/2000... Training loss: 0.5207\n",
      "Epoch: 1504/2000... Training loss: 0.4728\n",
      "Epoch: 1504/2000... Training loss: 0.4677\n",
      "Epoch: 1504/2000... Training loss: 0.4589\n",
      "Epoch: 1504/2000... Training loss: 0.4012\n",
      "Epoch: 1504/2000... Training loss: 0.3848\n",
      "Epoch: 1504/2000... Training loss: 0.4157\n",
      "Epoch: 1504/2000... Training loss: 0.2450\n",
      "Epoch: 1504/2000... Training loss: 0.4115\n",
      "Epoch: 1504/2000... Training loss: 0.3891\n",
      "Epoch: 1504/2000... Training loss: 0.4786\n",
      "Epoch: 1504/2000... Training loss: 0.5079\n",
      "Epoch: 1504/2000... Training loss: 0.3301\n",
      "Epoch: 1504/2000... Training loss: 0.3038\n",
      "Epoch: 1504/2000... Training loss: 0.3529\n",
      "Epoch: 1505/2000... Training loss: 0.3322\n",
      "Epoch: 1505/2000... Training loss: 0.3987\n",
      "Epoch: 1505/2000... Training loss: 0.3556\n",
      "Epoch: 1505/2000... Training loss: 0.4703\n",
      "Epoch: 1505/2000... Training loss: 0.4567\n",
      "Epoch: 1505/2000... Training loss: 0.5474\n",
      "Epoch: 1505/2000... Training loss: 0.3435\n",
      "Epoch: 1505/2000... Training loss: 0.4219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1505/2000... Training loss: 0.3590\n",
      "Epoch: 1505/2000... Training loss: 0.3751\n",
      "Epoch: 1505/2000... Training loss: 0.2698\n",
      "Epoch: 1505/2000... Training loss: 0.3876\n",
      "Epoch: 1505/2000... Training loss: 0.4876\n",
      "Epoch: 1505/2000... Training loss: 0.3531\n",
      "Epoch: 1505/2000... Training loss: 0.3929\n",
      "Epoch: 1505/2000... Training loss: 0.4230\n",
      "Epoch: 1505/2000... Training loss: 0.3291\n",
      "Epoch: 1505/2000... Training loss: 0.2831\n",
      "Epoch: 1505/2000... Training loss: 0.3411\n",
      "Epoch: 1505/2000... Training loss: 0.5072\n",
      "Epoch: 1505/2000... Training loss: 0.3161\n",
      "Epoch: 1505/2000... Training loss: 0.4259\n",
      "Epoch: 1505/2000... Training loss: 0.3555\n",
      "Epoch: 1505/2000... Training loss: 0.2124\n",
      "Epoch: 1505/2000... Training loss: 0.3481\n",
      "Epoch: 1505/2000... Training loss: 0.5870\n",
      "Epoch: 1505/2000... Training loss: 0.5069\n",
      "Epoch: 1505/2000... Training loss: 0.4373\n",
      "Epoch: 1505/2000... Training loss: 0.4144\n",
      "Epoch: 1505/2000... Training loss: 0.4627\n",
      "Epoch: 1505/2000... Training loss: 0.2680\n",
      "Epoch: 1506/2000... Training loss: 0.3992\n",
      "Epoch: 1506/2000... Training loss: 0.3962\n",
      "Epoch: 1506/2000... Training loss: 0.4011\n",
      "Epoch: 1506/2000... Training loss: 0.4170\n",
      "Epoch: 1506/2000... Training loss: 0.5537\n",
      "Epoch: 1506/2000... Training loss: 0.4323\n",
      "Epoch: 1506/2000... Training loss: 0.4068\n",
      "Epoch: 1506/2000... Training loss: 0.4465\n",
      "Epoch: 1506/2000... Training loss: 0.3400\n",
      "Epoch: 1506/2000... Training loss: 0.4838\n",
      "Epoch: 1506/2000... Training loss: 0.3317\n",
      "Epoch: 1506/2000... Training loss: 0.4549\n",
      "Epoch: 1506/2000... Training loss: 0.4643\n",
      "Epoch: 1506/2000... Training loss: 0.3130\n",
      "Epoch: 1506/2000... Training loss: 0.2450\n",
      "Epoch: 1506/2000... Training loss: 0.3613\n",
      "Epoch: 1506/2000... Training loss: 0.3422\n",
      "Epoch: 1506/2000... Training loss: 0.5123\n",
      "Epoch: 1506/2000... Training loss: 0.4066\n",
      "Epoch: 1506/2000... Training loss: 0.4541\n",
      "Epoch: 1506/2000... Training loss: 0.4928\n",
      "Epoch: 1506/2000... Training loss: 0.3270\n",
      "Epoch: 1506/2000... Training loss: 0.3346\n",
      "Epoch: 1506/2000... Training loss: 0.6083\n",
      "Epoch: 1506/2000... Training loss: 0.6184\n",
      "Epoch: 1506/2000... Training loss: 0.2780\n",
      "Epoch: 1506/2000... Training loss: 0.4248\n",
      "Epoch: 1506/2000... Training loss: 0.2961\n",
      "Epoch: 1506/2000... Training loss: 0.3543\n",
      "Epoch: 1506/2000... Training loss: 0.5583\n",
      "Epoch: 1506/2000... Training loss: 0.3947\n",
      "Epoch: 1507/2000... Training loss: 0.3467\n",
      "Epoch: 1507/2000... Training loss: 0.4635\n",
      "Epoch: 1507/2000... Training loss: 0.4010\n",
      "Epoch: 1507/2000... Training loss: 0.4459\n",
      "Epoch: 1507/2000... Training loss: 0.4120\n",
      "Epoch: 1507/2000... Training loss: 0.4224\n",
      "Epoch: 1507/2000... Training loss: 0.4321\n",
      "Epoch: 1507/2000... Training loss: 0.3716\n",
      "Epoch: 1507/2000... Training loss: 0.5195\n",
      "Epoch: 1507/2000... Training loss: 0.2500\n",
      "Epoch: 1507/2000... Training loss: 0.4544\n",
      "Epoch: 1507/2000... Training loss: 0.3429\n",
      "Epoch: 1507/2000... Training loss: 0.4642\n",
      "Epoch: 1507/2000... Training loss: 0.4905\n",
      "Epoch: 1507/2000... Training loss: 0.3853\n",
      "Epoch: 1507/2000... Training loss: 0.3067\n",
      "Epoch: 1507/2000... Training loss: 0.5050\n",
      "Epoch: 1507/2000... Training loss: 0.4931\n",
      "Epoch: 1507/2000... Training loss: 0.3739\n",
      "Epoch: 1507/2000... Training loss: 0.3888\n",
      "Epoch: 1507/2000... Training loss: 0.3565\n",
      "Epoch: 1507/2000... Training loss: 0.4859\n",
      "Epoch: 1507/2000... Training loss: 0.2956\n",
      "Epoch: 1507/2000... Training loss: 0.4223\n",
      "Epoch: 1507/2000... Training loss: 0.3415\n",
      "Epoch: 1507/2000... Training loss: 0.3505\n",
      "Epoch: 1507/2000... Training loss: 0.4428\n",
      "Epoch: 1507/2000... Training loss: 0.2781\n",
      "Epoch: 1507/2000... Training loss: 0.5197\n",
      "Epoch: 1507/2000... Training loss: 0.3681\n",
      "Epoch: 1507/2000... Training loss: 0.5003\n",
      "Epoch: 1508/2000... Training loss: 0.3564\n",
      "Epoch: 1508/2000... Training loss: 0.4905\n",
      "Epoch: 1508/2000... Training loss: 0.3815\n",
      "Epoch: 1508/2000... Training loss: 0.3705\n",
      "Epoch: 1508/2000... Training loss: 0.4215\n",
      "Epoch: 1508/2000... Training loss: 0.3528\n",
      "Epoch: 1508/2000... Training loss: 0.3389\n",
      "Epoch: 1508/2000... Training loss: 0.3401\n",
      "Epoch: 1508/2000... Training loss: 0.5392\n",
      "Epoch: 1508/2000... Training loss: 0.4802\n",
      "Epoch: 1508/2000... Training loss: 0.4802\n",
      "Epoch: 1508/2000... Training loss: 0.3822\n",
      "Epoch: 1508/2000... Training loss: 0.3819\n",
      "Epoch: 1508/2000... Training loss: 0.3678\n",
      "Epoch: 1508/2000... Training loss: 0.5463\n",
      "Epoch: 1508/2000... Training loss: 0.3694\n",
      "Epoch: 1508/2000... Training loss: 0.2509\n",
      "Epoch: 1508/2000... Training loss: 0.6320\n",
      "Epoch: 1508/2000... Training loss: 0.4367\n",
      "Epoch: 1508/2000... Training loss: 0.4538\n",
      "Epoch: 1508/2000... Training loss: 0.5969\n",
      "Epoch: 1508/2000... Training loss: 0.5721\n",
      "Epoch: 1508/2000... Training loss: 0.3809\n",
      "Epoch: 1508/2000... Training loss: 0.5170\n",
      "Epoch: 1508/2000... Training loss: 0.3506\n",
      "Epoch: 1508/2000... Training loss: 0.5933\n",
      "Epoch: 1508/2000... Training loss: 0.5740\n",
      "Epoch: 1508/2000... Training loss: 0.4996\n",
      "Epoch: 1508/2000... Training loss: 0.4588\n",
      "Epoch: 1508/2000... Training loss: 0.2854\n",
      "Epoch: 1508/2000... Training loss: 0.2705\n",
      "Epoch: 1509/2000... Training loss: 0.4648\n",
      "Epoch: 1509/2000... Training loss: 0.4602\n",
      "Epoch: 1509/2000... Training loss: 0.4639\n",
      "Epoch: 1509/2000... Training loss: 0.2229\n",
      "Epoch: 1509/2000... Training loss: 0.3012\n",
      "Epoch: 1509/2000... Training loss: 0.4796\n",
      "Epoch: 1509/2000... Training loss: 0.4321\n",
      "Epoch: 1509/2000... Training loss: 0.4327\n",
      "Epoch: 1509/2000... Training loss: 0.3419\n",
      "Epoch: 1509/2000... Training loss: 0.4588\n",
      "Epoch: 1509/2000... Training loss: 0.3046\n",
      "Epoch: 1509/2000... Training loss: 0.4127\n",
      "Epoch: 1509/2000... Training loss: 0.4825\n",
      "Epoch: 1509/2000... Training loss: 0.3130\n",
      "Epoch: 1509/2000... Training loss: 0.4871\n",
      "Epoch: 1509/2000... Training loss: 0.4276\n",
      "Epoch: 1509/2000... Training loss: 0.3429\n",
      "Epoch: 1509/2000... Training loss: 0.4082\n",
      "Epoch: 1509/2000... Training loss: 0.3541\n",
      "Epoch: 1509/2000... Training loss: 0.4319\n",
      "Epoch: 1509/2000... Training loss: 0.3008\n",
      "Epoch: 1509/2000... Training loss: 0.4177\n",
      "Epoch: 1509/2000... Training loss: 0.4396\n",
      "Epoch: 1509/2000... Training loss: 0.5385\n",
      "Epoch: 1509/2000... Training loss: 0.4475\n",
      "Epoch: 1509/2000... Training loss: 0.4398\n",
      "Epoch: 1509/2000... Training loss: 0.5617\n",
      "Epoch: 1509/2000... Training loss: 0.4500\n",
      "Epoch: 1509/2000... Training loss: 0.3969\n",
      "Epoch: 1509/2000... Training loss: 0.3906\n",
      "Epoch: 1509/2000... Training loss: 0.2672\n",
      "Epoch: 1510/2000... Training loss: 0.3182\n",
      "Epoch: 1510/2000... Training loss: 0.4671\n",
      "Epoch: 1510/2000... Training loss: 0.4419\n",
      "Epoch: 1510/2000... Training loss: 0.2720\n",
      "Epoch: 1510/2000... Training loss: 0.4362\n",
      "Epoch: 1510/2000... Training loss: 0.2951\n",
      "Epoch: 1510/2000... Training loss: 0.3516\n",
      "Epoch: 1510/2000... Training loss: 0.3957\n",
      "Epoch: 1510/2000... Training loss: 0.4515\n",
      "Epoch: 1510/2000... Training loss: 0.2852\n",
      "Epoch: 1510/2000... Training loss: 0.3574\n",
      "Epoch: 1510/2000... Training loss: 0.2313\n",
      "Epoch: 1510/2000... Training loss: 0.3583\n",
      "Epoch: 1510/2000... Training loss: 0.4060\n",
      "Epoch: 1510/2000... Training loss: 0.4095\n",
      "Epoch: 1510/2000... Training loss: 0.3354\n",
      "Epoch: 1510/2000... Training loss: 0.1969\n",
      "Epoch: 1510/2000... Training loss: 0.3477\n",
      "Epoch: 1510/2000... Training loss: 0.3989\n",
      "Epoch: 1510/2000... Training loss: 0.4488\n",
      "Epoch: 1510/2000... Training loss: 0.2811\n",
      "Epoch: 1510/2000... Training loss: 0.4001\n",
      "Epoch: 1510/2000... Training loss: 0.5002\n",
      "Epoch: 1510/2000... Training loss: 0.3857\n",
      "Epoch: 1510/2000... Training loss: 0.3111\n",
      "Epoch: 1510/2000... Training loss: 0.3857\n",
      "Epoch: 1510/2000... Training loss: 0.4903\n",
      "Epoch: 1510/2000... Training loss: 0.3552\n",
      "Epoch: 1510/2000... Training loss: 0.3933\n",
      "Epoch: 1510/2000... Training loss: 0.3868\n",
      "Epoch: 1510/2000... Training loss: 0.4525\n",
      "Epoch: 1511/2000... Training loss: 0.4518\n",
      "Epoch: 1511/2000... Training loss: 0.4255\n",
      "Epoch: 1511/2000... Training loss: 0.4156\n",
      "Epoch: 1511/2000... Training loss: 0.4775\n",
      "Epoch: 1511/2000... Training loss: 0.5263\n",
      "Epoch: 1511/2000... Training loss: 0.3769\n",
      "Epoch: 1511/2000... Training loss: 0.2821\n",
      "Epoch: 1511/2000... Training loss: 0.4876\n",
      "Epoch: 1511/2000... Training loss: 0.3590\n",
      "Epoch: 1511/2000... Training loss: 0.4692\n",
      "Epoch: 1511/2000... Training loss: 0.4001\n",
      "Epoch: 1511/2000... Training loss: 0.5889\n",
      "Epoch: 1511/2000... Training loss: 0.2260\n",
      "Epoch: 1511/2000... Training loss: 0.3719\n",
      "Epoch: 1511/2000... Training loss: 0.4596\n",
      "Epoch: 1511/2000... Training loss: 0.3494\n",
      "Epoch: 1511/2000... Training loss: 0.4067\n",
      "Epoch: 1511/2000... Training loss: 0.4849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1511/2000... Training loss: 0.6502\n",
      "Epoch: 1511/2000... Training loss: 0.2095\n",
      "Epoch: 1511/2000... Training loss: 0.3149\n",
      "Epoch: 1511/2000... Training loss: 0.4098\n",
      "Epoch: 1511/2000... Training loss: 0.4465\n",
      "Epoch: 1511/2000... Training loss: 0.4699\n",
      "Epoch: 1511/2000... Training loss: 0.4353\n",
      "Epoch: 1511/2000... Training loss: 0.3630\n",
      "Epoch: 1511/2000... Training loss: 0.4431\n",
      "Epoch: 1511/2000... Training loss: 0.3407\n",
      "Epoch: 1511/2000... Training loss: 0.3918\n",
      "Epoch: 1511/2000... Training loss: 0.5930\n",
      "Epoch: 1511/2000... Training loss: 0.5288\n",
      "Epoch: 1512/2000... Training loss: 0.6044\n",
      "Epoch: 1512/2000... Training loss: 0.2912\n",
      "Epoch: 1512/2000... Training loss: 0.5520\n",
      "Epoch: 1512/2000... Training loss: 0.3713\n",
      "Epoch: 1512/2000... Training loss: 0.4908\n",
      "Epoch: 1512/2000... Training loss: 0.4869\n",
      "Epoch: 1512/2000... Training loss: 0.3295\n",
      "Epoch: 1512/2000... Training loss: 0.3361\n",
      "Epoch: 1512/2000... Training loss: 0.3371\n",
      "Epoch: 1512/2000... Training loss: 0.4117\n",
      "Epoch: 1512/2000... Training loss: 0.3989\n",
      "Epoch: 1512/2000... Training loss: 0.4329\n",
      "Epoch: 1512/2000... Training loss: 0.3354\n",
      "Epoch: 1512/2000... Training loss: 0.4589\n",
      "Epoch: 1512/2000... Training loss: 0.4581\n",
      "Epoch: 1512/2000... Training loss: 0.3611\n",
      "Epoch: 1512/2000... Training loss: 0.5491\n",
      "Epoch: 1512/2000... Training loss: 0.3537\n",
      "Epoch: 1512/2000... Training loss: 0.3781\n",
      "Epoch: 1512/2000... Training loss: 0.3414\n",
      "Epoch: 1512/2000... Training loss: 0.3970\n",
      "Epoch: 1512/2000... Training loss: 0.3843\n",
      "Epoch: 1512/2000... Training loss: 0.3280\n",
      "Epoch: 1512/2000... Training loss: 0.5429\n",
      "Epoch: 1512/2000... Training loss: 0.4674\n",
      "Epoch: 1512/2000... Training loss: 0.5829\n",
      "Epoch: 1512/2000... Training loss: 0.4129\n",
      "Epoch: 1512/2000... Training loss: 0.3128\n",
      "Epoch: 1512/2000... Training loss: 0.4117\n",
      "Epoch: 1512/2000... Training loss: 0.5139\n",
      "Epoch: 1512/2000... Training loss: 0.3478\n",
      "Epoch: 1513/2000... Training loss: 0.4025\n",
      "Epoch: 1513/2000... Training loss: 0.5292\n",
      "Epoch: 1513/2000... Training loss: 0.4355\n",
      "Epoch: 1513/2000... Training loss: 0.3656\n",
      "Epoch: 1513/2000... Training loss: 0.3970\n",
      "Epoch: 1513/2000... Training loss: 0.4005\n",
      "Epoch: 1513/2000... Training loss: 0.4212\n",
      "Epoch: 1513/2000... Training loss: 0.4078\n",
      "Epoch: 1513/2000... Training loss: 0.3031\n",
      "Epoch: 1513/2000... Training loss: 0.2878\n",
      "Epoch: 1513/2000... Training loss: 0.4045\n",
      "Epoch: 1513/2000... Training loss: 0.4090\n",
      "Epoch: 1513/2000... Training loss: 0.5689\n",
      "Epoch: 1513/2000... Training loss: 0.3990\n",
      "Epoch: 1513/2000... Training loss: 0.3881\n",
      "Epoch: 1513/2000... Training loss: 0.3876\n",
      "Epoch: 1513/2000... Training loss: 0.3787\n",
      "Epoch: 1513/2000... Training loss: 0.3764\n",
      "Epoch: 1513/2000... Training loss: 0.4197\n",
      "Epoch: 1513/2000... Training loss: 0.4048\n",
      "Epoch: 1513/2000... Training loss: 0.4406\n",
      "Epoch: 1513/2000... Training loss: 0.3949\n",
      "Epoch: 1513/2000... Training loss: 0.3861\n",
      "Epoch: 1513/2000... Training loss: 0.3899\n",
      "Epoch: 1513/2000... Training loss: 0.4298\n",
      "Epoch: 1513/2000... Training loss: 0.4974\n",
      "Epoch: 1513/2000... Training loss: 0.4612\n",
      "Epoch: 1513/2000... Training loss: 0.4422\n",
      "Epoch: 1513/2000... Training loss: 0.5008\n",
      "Epoch: 1513/2000... Training loss: 0.2863\n",
      "Epoch: 1513/2000... Training loss: 0.3720\n",
      "Epoch: 1514/2000... Training loss: 0.3922\n",
      "Epoch: 1514/2000... Training loss: 0.6610\n",
      "Epoch: 1514/2000... Training loss: 0.4325\n",
      "Epoch: 1514/2000... Training loss: 0.4438\n",
      "Epoch: 1514/2000... Training loss: 0.3810\n",
      "Epoch: 1514/2000... Training loss: 0.3399\n",
      "Epoch: 1514/2000... Training loss: 0.3297\n",
      "Epoch: 1514/2000... Training loss: 0.3713\n",
      "Epoch: 1514/2000... Training loss: 0.4407\n",
      "Epoch: 1514/2000... Training loss: 0.4915\n",
      "Epoch: 1514/2000... Training loss: 0.3547\n",
      "Epoch: 1514/2000... Training loss: 0.2752\n",
      "Epoch: 1514/2000... Training loss: 0.5123\n",
      "Epoch: 1514/2000... Training loss: 0.3175\n",
      "Epoch: 1514/2000... Training loss: 0.3863\n",
      "Epoch: 1514/2000... Training loss: 0.2586\n",
      "Epoch: 1514/2000... Training loss: 0.3128\n",
      "Epoch: 1514/2000... Training loss: 0.3006\n",
      "Epoch: 1514/2000... Training loss: 0.5178\n",
      "Epoch: 1514/2000... Training loss: 0.4186\n",
      "Epoch: 1514/2000... Training loss: 0.3207\n",
      "Epoch: 1514/2000... Training loss: 0.3760\n",
      "Epoch: 1514/2000... Training loss: 0.5758\n",
      "Epoch: 1514/2000... Training loss: 0.2896\n",
      "Epoch: 1514/2000... Training loss: 0.3581\n",
      "Epoch: 1514/2000... Training loss: 0.3858\n",
      "Epoch: 1514/2000... Training loss: 0.5553\n",
      "Epoch: 1514/2000... Training loss: 0.3005\n",
      "Epoch: 1514/2000... Training loss: 0.3507\n",
      "Epoch: 1514/2000... Training loss: 0.4348\n",
      "Epoch: 1514/2000... Training loss: 0.3864\n",
      "Epoch: 1515/2000... Training loss: 0.3214\n",
      "Epoch: 1515/2000... Training loss: 0.5188\n",
      "Epoch: 1515/2000... Training loss: 0.4391\n",
      "Epoch: 1515/2000... Training loss: 0.2047\n",
      "Epoch: 1515/2000... Training loss: 0.4522\n",
      "Epoch: 1515/2000... Training loss: 0.4193\n",
      "Epoch: 1515/2000... Training loss: 0.3335\n",
      "Epoch: 1515/2000... Training loss: 0.3655\n",
      "Epoch: 1515/2000... Training loss: 0.3038\n",
      "Epoch: 1515/2000... Training loss: 0.5500\n",
      "Epoch: 1515/2000... Training loss: 0.3570\n",
      "Epoch: 1515/2000... Training loss: 0.4363\n",
      "Epoch: 1515/2000... Training loss: 0.3265\n",
      "Epoch: 1515/2000... Training loss: 0.2870\n",
      "Epoch: 1515/2000... Training loss: 0.3476\n",
      "Epoch: 1515/2000... Training loss: 0.3822\n",
      "Epoch: 1515/2000... Training loss: 0.5251\n",
      "Epoch: 1515/2000... Training loss: 0.4496\n",
      "Epoch: 1515/2000... Training loss: 0.2508\n",
      "Epoch: 1515/2000... Training loss: 0.3199\n",
      "Epoch: 1515/2000... Training loss: 0.3962\n",
      "Epoch: 1515/2000... Training loss: 0.3184\n",
      "Epoch: 1515/2000... Training loss: 0.4099\n",
      "Epoch: 1515/2000... Training loss: 0.4116\n",
      "Epoch: 1515/2000... Training loss: 0.4856\n",
      "Epoch: 1515/2000... Training loss: 0.3030\n",
      "Epoch: 1515/2000... Training loss: 0.3062\n",
      "Epoch: 1515/2000... Training loss: 0.3334\n",
      "Epoch: 1515/2000... Training loss: 0.4929\n",
      "Epoch: 1515/2000... Training loss: 0.4752\n",
      "Epoch: 1515/2000... Training loss: 0.5092\n",
      "Epoch: 1516/2000... Training loss: 0.4749\n",
      "Epoch: 1516/2000... Training loss: 0.6089\n",
      "Epoch: 1516/2000... Training loss: 0.3869\n",
      "Epoch: 1516/2000... Training loss: 0.2238\n",
      "Epoch: 1516/2000... Training loss: 0.3876\n",
      "Epoch: 1516/2000... Training loss: 0.4383\n",
      "Epoch: 1516/2000... Training loss: 0.3197\n",
      "Epoch: 1516/2000... Training loss: 0.4387\n",
      "Epoch: 1516/2000... Training loss: 0.3890\n",
      "Epoch: 1516/2000... Training loss: 0.5843\n",
      "Epoch: 1516/2000... Training loss: 0.4399\n",
      "Epoch: 1516/2000... Training loss: 0.5222\n",
      "Epoch: 1516/2000... Training loss: 0.3056\n",
      "Epoch: 1516/2000... Training loss: 0.4450\n",
      "Epoch: 1516/2000... Training loss: 0.2766\n",
      "Epoch: 1516/2000... Training loss: 0.4536\n",
      "Epoch: 1516/2000... Training loss: 0.4733\n",
      "Epoch: 1516/2000... Training loss: 0.2856\n",
      "Epoch: 1516/2000... Training loss: 0.3105\n",
      "Epoch: 1516/2000... Training loss: 0.5580\n",
      "Epoch: 1516/2000... Training loss: 0.2865\n",
      "Epoch: 1516/2000... Training loss: 0.3211\n",
      "Epoch: 1516/2000... Training loss: 0.4977\n",
      "Epoch: 1516/2000... Training loss: 0.4724\n",
      "Epoch: 1516/2000... Training loss: 0.5694\n",
      "Epoch: 1516/2000... Training loss: 0.4320\n",
      "Epoch: 1516/2000... Training loss: 0.3287\n",
      "Epoch: 1516/2000... Training loss: 0.3560\n",
      "Epoch: 1516/2000... Training loss: 0.4524\n",
      "Epoch: 1516/2000... Training loss: 0.3940\n",
      "Epoch: 1516/2000... Training loss: 0.6081\n",
      "Epoch: 1517/2000... Training loss: 0.4347\n",
      "Epoch: 1517/2000... Training loss: 0.4147\n",
      "Epoch: 1517/2000... Training loss: 0.5344\n",
      "Epoch: 1517/2000... Training loss: 0.3220\n",
      "Epoch: 1517/2000... Training loss: 0.3579\n",
      "Epoch: 1517/2000... Training loss: 0.4725\n",
      "Epoch: 1517/2000... Training loss: 0.4706\n",
      "Epoch: 1517/2000... Training loss: 0.3361\n",
      "Epoch: 1517/2000... Training loss: 0.3039\n",
      "Epoch: 1517/2000... Training loss: 0.5430\n",
      "Epoch: 1517/2000... Training loss: 0.3911\n",
      "Epoch: 1517/2000... Training loss: 0.4746\n",
      "Epoch: 1517/2000... Training loss: 0.3972\n",
      "Epoch: 1517/2000... Training loss: 0.3762\n",
      "Epoch: 1517/2000... Training loss: 0.3985\n",
      "Epoch: 1517/2000... Training loss: 0.5167\n",
      "Epoch: 1517/2000... Training loss: 0.4513\n",
      "Epoch: 1517/2000... Training loss: 0.3939\n",
      "Epoch: 1517/2000... Training loss: 0.5171\n",
      "Epoch: 1517/2000... Training loss: 0.4369\n",
      "Epoch: 1517/2000... Training loss: 0.7210\n",
      "Epoch: 1517/2000... Training loss: 0.3754\n",
      "Epoch: 1517/2000... Training loss: 0.6437\n",
      "Epoch: 1517/2000... Training loss: 0.3370\n",
      "Epoch: 1517/2000... Training loss: 0.3850\n",
      "Epoch: 1517/2000... Training loss: 0.3432\n",
      "Epoch: 1517/2000... Training loss: 0.5071\n",
      "Epoch: 1517/2000... Training loss: 0.3863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1517/2000... Training loss: 0.3947\n",
      "Epoch: 1517/2000... Training loss: 0.4626\n",
      "Epoch: 1517/2000... Training loss: 0.3662\n",
      "Epoch: 1518/2000... Training loss: 0.5088\n",
      "Epoch: 1518/2000... Training loss: 0.3787\n",
      "Epoch: 1518/2000... Training loss: 0.4736\n",
      "Epoch: 1518/2000... Training loss: 0.2702\n",
      "Epoch: 1518/2000... Training loss: 0.4963\n",
      "Epoch: 1518/2000... Training loss: 0.4424\n",
      "Epoch: 1518/2000... Training loss: 0.2290\n",
      "Epoch: 1518/2000... Training loss: 0.4657\n",
      "Epoch: 1518/2000... Training loss: 0.3244\n",
      "Epoch: 1518/2000... Training loss: 0.3953\n",
      "Epoch: 1518/2000... Training loss: 0.3892\n",
      "Epoch: 1518/2000... Training loss: 0.2920\n",
      "Epoch: 1518/2000... Training loss: 0.2138\n",
      "Epoch: 1518/2000... Training loss: 0.5068\n",
      "Epoch: 1518/2000... Training loss: 0.4000\n",
      "Epoch: 1518/2000... Training loss: 0.3454\n",
      "Epoch: 1518/2000... Training loss: 0.5268\n",
      "Epoch: 1518/2000... Training loss: 0.3084\n",
      "Epoch: 1518/2000... Training loss: 0.4442\n",
      "Epoch: 1518/2000... Training loss: 0.3325\n",
      "Epoch: 1518/2000... Training loss: 0.3881\n",
      "Epoch: 1518/2000... Training loss: 0.4073\n",
      "Epoch: 1518/2000... Training loss: 0.5296\n",
      "Epoch: 1518/2000... Training loss: 0.3211\n",
      "Epoch: 1518/2000... Training loss: 0.4787\n",
      "Epoch: 1518/2000... Training loss: 0.4198\n",
      "Epoch: 1518/2000... Training loss: 0.4453\n",
      "Epoch: 1518/2000... Training loss: 0.6402\n",
      "Epoch: 1518/2000... Training loss: 0.3325\n",
      "Epoch: 1518/2000... Training loss: 0.3272\n",
      "Epoch: 1518/2000... Training loss: 0.4101\n",
      "Epoch: 1519/2000... Training loss: 0.3098\n",
      "Epoch: 1519/2000... Training loss: 0.3402\n",
      "Epoch: 1519/2000... Training loss: 0.4558\n",
      "Epoch: 1519/2000... Training loss: 0.5148\n",
      "Epoch: 1519/2000... Training loss: 0.4024\n",
      "Epoch: 1519/2000... Training loss: 0.2967\n",
      "Epoch: 1519/2000... Training loss: 0.3869\n",
      "Epoch: 1519/2000... Training loss: 0.3735\n",
      "Epoch: 1519/2000... Training loss: 0.3984\n",
      "Epoch: 1519/2000... Training loss: 0.3747\n",
      "Epoch: 1519/2000... Training loss: 0.3276\n",
      "Epoch: 1519/2000... Training loss: 0.5516\n",
      "Epoch: 1519/2000... Training loss: 0.3435\n",
      "Epoch: 1519/2000... Training loss: 0.5592\n",
      "Epoch: 1519/2000... Training loss: 0.2878\n",
      "Epoch: 1519/2000... Training loss: 0.4966\n",
      "Epoch: 1519/2000... Training loss: 0.3415\n",
      "Epoch: 1519/2000... Training loss: 0.3312\n",
      "Epoch: 1519/2000... Training loss: 0.4234\n",
      "Epoch: 1519/2000... Training loss: 0.3557\n",
      "Epoch: 1519/2000... Training loss: 0.5005\n",
      "Epoch: 1519/2000... Training loss: 0.3974\n",
      "Epoch: 1519/2000... Training loss: 0.3150\n",
      "Epoch: 1519/2000... Training loss: 0.4227\n",
      "Epoch: 1519/2000... Training loss: 0.5111\n",
      "Epoch: 1519/2000... Training loss: 0.6271\n",
      "Epoch: 1519/2000... Training loss: 0.3837\n",
      "Epoch: 1519/2000... Training loss: 0.4713\n",
      "Epoch: 1519/2000... Training loss: 0.5462\n",
      "Epoch: 1519/2000... Training loss: 0.4518\n",
      "Epoch: 1519/2000... Training loss: 0.5527\n",
      "Epoch: 1520/2000... Training loss: 0.4033\n",
      "Epoch: 1520/2000... Training loss: 0.2797\n",
      "Epoch: 1520/2000... Training loss: 0.3184\n",
      "Epoch: 1520/2000... Training loss: 0.4332\n",
      "Epoch: 1520/2000... Training loss: 0.2115\n",
      "Epoch: 1520/2000... Training loss: 0.2736\n",
      "Epoch: 1520/2000... Training loss: 0.4693\n",
      "Epoch: 1520/2000... Training loss: 0.3895\n",
      "Epoch: 1520/2000... Training loss: 0.4224\n",
      "Epoch: 1520/2000... Training loss: 0.6304\n",
      "Epoch: 1520/2000... Training loss: 0.6698\n",
      "Epoch: 1520/2000... Training loss: 0.2707\n",
      "Epoch: 1520/2000... Training loss: 0.5861\n",
      "Epoch: 1520/2000... Training loss: 0.3447\n",
      "Epoch: 1520/2000... Training loss: 0.2573\n",
      "Epoch: 1520/2000... Training loss: 0.5007\n",
      "Epoch: 1520/2000... Training loss: 0.4658\n",
      "Epoch: 1520/2000... Training loss: 0.2931\n",
      "Epoch: 1520/2000... Training loss: 0.4248\n",
      "Epoch: 1520/2000... Training loss: 0.4679\n",
      "Epoch: 1520/2000... Training loss: 0.6157\n",
      "Epoch: 1520/2000... Training loss: 0.5073\n",
      "Epoch: 1520/2000... Training loss: 0.4447\n",
      "Epoch: 1520/2000... Training loss: 0.5602\n",
      "Epoch: 1520/2000... Training loss: 0.3147\n",
      "Epoch: 1520/2000... Training loss: 0.4712\n",
      "Epoch: 1520/2000... Training loss: 0.3553\n",
      "Epoch: 1520/2000... Training loss: 0.5769\n",
      "Epoch: 1520/2000... Training loss: 0.5636\n",
      "Epoch: 1520/2000... Training loss: 0.4830\n",
      "Epoch: 1520/2000... Training loss: 0.4457\n",
      "Epoch: 1521/2000... Training loss: 0.2989\n",
      "Epoch: 1521/2000... Training loss: 0.5625\n",
      "Epoch: 1521/2000... Training loss: 0.4538\n",
      "Epoch: 1521/2000... Training loss: 0.4600\n",
      "Epoch: 1521/2000... Training loss: 0.4068\n",
      "Epoch: 1521/2000... Training loss: 0.3821\n",
      "Epoch: 1521/2000... Training loss: 0.5378\n",
      "Epoch: 1521/2000... Training loss: 0.4347\n",
      "Epoch: 1521/2000... Training loss: 0.5471\n",
      "Epoch: 1521/2000... Training loss: 0.3577\n",
      "Epoch: 1521/2000... Training loss: 0.3705\n",
      "Epoch: 1521/2000... Training loss: 0.3517\n",
      "Epoch: 1521/2000... Training loss: 0.3573\n",
      "Epoch: 1521/2000... Training loss: 0.3313\n",
      "Epoch: 1521/2000... Training loss: 0.3966\n",
      "Epoch: 1521/2000... Training loss: 0.3081\n",
      "Epoch: 1521/2000... Training loss: 0.4473\n",
      "Epoch: 1521/2000... Training loss: 0.5232\n",
      "Epoch: 1521/2000... Training loss: 0.3842\n",
      "Epoch: 1521/2000... Training loss: 0.3276\n",
      "Epoch: 1521/2000... Training loss: 0.3530\n",
      "Epoch: 1521/2000... Training loss: 0.3270\n",
      "Epoch: 1521/2000... Training loss: 0.2810\n",
      "Epoch: 1521/2000... Training loss: 0.4235\n",
      "Epoch: 1521/2000... Training loss: 0.3976\n",
      "Epoch: 1521/2000... Training loss: 0.4148\n",
      "Epoch: 1521/2000... Training loss: 0.2674\n",
      "Epoch: 1521/2000... Training loss: 0.4658\n",
      "Epoch: 1521/2000... Training loss: 0.3915\n",
      "Epoch: 1521/2000... Training loss: 0.3553\n",
      "Epoch: 1521/2000... Training loss: 0.2834\n",
      "Epoch: 1522/2000... Training loss: 0.4455\n",
      "Epoch: 1522/2000... Training loss: 0.5211\n",
      "Epoch: 1522/2000... Training loss: 0.2437\n",
      "Epoch: 1522/2000... Training loss: 0.5194\n",
      "Epoch: 1522/2000... Training loss: 0.6643\n",
      "Epoch: 1522/2000... Training loss: 0.5211\n",
      "Epoch: 1522/2000... Training loss: 0.2864\n",
      "Epoch: 1522/2000... Training loss: 0.4621\n",
      "Epoch: 1522/2000... Training loss: 0.4988\n",
      "Epoch: 1522/2000... Training loss: 0.4514\n",
      "Epoch: 1522/2000... Training loss: 0.3584\n",
      "Epoch: 1522/2000... Training loss: 0.5158\n",
      "Epoch: 1522/2000... Training loss: 0.4555\n",
      "Epoch: 1522/2000... Training loss: 0.5749\n",
      "Epoch: 1522/2000... Training loss: 0.4668\n",
      "Epoch: 1522/2000... Training loss: 0.5752\n",
      "Epoch: 1522/2000... Training loss: 0.3260\n",
      "Epoch: 1522/2000... Training loss: 0.4264\n",
      "Epoch: 1522/2000... Training loss: 0.4939\n",
      "Epoch: 1522/2000... Training loss: 0.3080\n",
      "Epoch: 1522/2000... Training loss: 0.3741\n",
      "Epoch: 1522/2000... Training loss: 0.4473\n",
      "Epoch: 1522/2000... Training loss: 0.2641\n",
      "Epoch: 1522/2000... Training loss: 0.4639\n",
      "Epoch: 1522/2000... Training loss: 0.6456\n",
      "Epoch: 1522/2000... Training loss: 0.3807\n",
      "Epoch: 1522/2000... Training loss: 0.4272\n",
      "Epoch: 1522/2000... Training loss: 0.5307\n",
      "Epoch: 1522/2000... Training loss: 0.4858\n",
      "Epoch: 1522/2000... Training loss: 0.5341\n",
      "Epoch: 1522/2000... Training loss: 0.4061\n",
      "Epoch: 1523/2000... Training loss: 0.2998\n",
      "Epoch: 1523/2000... Training loss: 0.3170\n",
      "Epoch: 1523/2000... Training loss: 0.5287\n",
      "Epoch: 1523/2000... Training loss: 0.3663\n",
      "Epoch: 1523/2000... Training loss: 0.3275\n",
      "Epoch: 1523/2000... Training loss: 0.5850\n",
      "Epoch: 1523/2000... Training loss: 0.3736\n",
      "Epoch: 1523/2000... Training loss: 0.3955\n",
      "Epoch: 1523/2000... Training loss: 0.4181\n",
      "Epoch: 1523/2000... Training loss: 0.3752\n",
      "Epoch: 1523/2000... Training loss: 0.4867\n",
      "Epoch: 1523/2000... Training loss: 0.4053\n",
      "Epoch: 1523/2000... Training loss: 0.5063\n",
      "Epoch: 1523/2000... Training loss: 0.2599\n",
      "Epoch: 1523/2000... Training loss: 0.4037\n",
      "Epoch: 1523/2000... Training loss: 0.3008\n",
      "Epoch: 1523/2000... Training loss: 0.3684\n",
      "Epoch: 1523/2000... Training loss: 0.5623\n",
      "Epoch: 1523/2000... Training loss: 0.5371\n",
      "Epoch: 1523/2000... Training loss: 0.3213\n",
      "Epoch: 1523/2000... Training loss: 0.4075\n",
      "Epoch: 1523/2000... Training loss: 0.2583\n",
      "Epoch: 1523/2000... Training loss: 0.4423\n",
      "Epoch: 1523/2000... Training loss: 0.4007\n",
      "Epoch: 1523/2000... Training loss: 0.3740\n",
      "Epoch: 1523/2000... Training loss: 0.6234\n",
      "Epoch: 1523/2000... Training loss: 0.3906\n",
      "Epoch: 1523/2000... Training loss: 0.2746\n",
      "Epoch: 1523/2000... Training loss: 0.3316\n",
      "Epoch: 1523/2000... Training loss: 0.4421\n",
      "Epoch: 1523/2000... Training loss: 0.3239\n",
      "Epoch: 1524/2000... Training loss: 0.3239\n",
      "Epoch: 1524/2000... Training loss: 0.5374\n",
      "Epoch: 1524/2000... Training loss: 0.5668\n",
      "Epoch: 1524/2000... Training loss: 0.4527\n",
      "Epoch: 1524/2000... Training loss: 0.4767\n",
      "Epoch: 1524/2000... Training loss: 0.4321\n",
      "Epoch: 1524/2000... Training loss: 0.3439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1524/2000... Training loss: 0.3019\n",
      "Epoch: 1524/2000... Training loss: 0.4580\n",
      "Epoch: 1524/2000... Training loss: 0.3363\n",
      "Epoch: 1524/2000... Training loss: 0.2981\n",
      "Epoch: 1524/2000... Training loss: 0.3328\n",
      "Epoch: 1524/2000... Training loss: 0.2634\n",
      "Epoch: 1524/2000... Training loss: 0.5575\n",
      "Epoch: 1524/2000... Training loss: 0.2648\n",
      "Epoch: 1524/2000... Training loss: 0.4616\n",
      "Epoch: 1524/2000... Training loss: 0.4394\n",
      "Epoch: 1524/2000... Training loss: 0.4221\n",
      "Epoch: 1524/2000... Training loss: 0.3564\n",
      "Epoch: 1524/2000... Training loss: 0.3083\n",
      "Epoch: 1524/2000... Training loss: 0.3429\n",
      "Epoch: 1524/2000... Training loss: 0.3811\n",
      "Epoch: 1524/2000... Training loss: 0.2451\n",
      "Epoch: 1524/2000... Training loss: 0.3937\n",
      "Epoch: 1524/2000... Training loss: 0.3947\n",
      "Epoch: 1524/2000... Training loss: 0.2529\n",
      "Epoch: 1524/2000... Training loss: 0.3515\n",
      "Epoch: 1524/2000... Training loss: 0.4668\n",
      "Epoch: 1524/2000... Training loss: 0.3812\n",
      "Epoch: 1524/2000... Training loss: 0.4608\n",
      "Epoch: 1524/2000... Training loss: 0.3668\n",
      "Epoch: 1525/2000... Training loss: 0.4517\n",
      "Epoch: 1525/2000... Training loss: 0.4979\n",
      "Epoch: 1525/2000... Training loss: 0.3091\n",
      "Epoch: 1525/2000... Training loss: 0.4080\n",
      "Epoch: 1525/2000... Training loss: 0.4725\n",
      "Epoch: 1525/2000... Training loss: 0.4977\n",
      "Epoch: 1525/2000... Training loss: 0.3862\n",
      "Epoch: 1525/2000... Training loss: 0.4684\n",
      "Epoch: 1525/2000... Training loss: 0.4094\n",
      "Epoch: 1525/2000... Training loss: 0.4074\n",
      "Epoch: 1525/2000... Training loss: 0.3157\n",
      "Epoch: 1525/2000... Training loss: 0.4717\n",
      "Epoch: 1525/2000... Training loss: 0.3029\n",
      "Epoch: 1525/2000... Training loss: 0.5492\n",
      "Epoch: 1525/2000... Training loss: 0.1994\n",
      "Epoch: 1525/2000... Training loss: 0.4455\n",
      "Epoch: 1525/2000... Training loss: 0.3890\n",
      "Epoch: 1525/2000... Training loss: 0.4147\n",
      "Epoch: 1525/2000... Training loss: 0.2485\n",
      "Epoch: 1525/2000... Training loss: 0.3094\n",
      "Epoch: 1525/2000... Training loss: 0.5000\n",
      "Epoch: 1525/2000... Training loss: 0.5830\n",
      "Epoch: 1525/2000... Training loss: 0.5046\n",
      "Epoch: 1525/2000... Training loss: 0.3873\n",
      "Epoch: 1525/2000... Training loss: 0.4487\n",
      "Epoch: 1525/2000... Training loss: 0.2662\n",
      "Epoch: 1525/2000... Training loss: 0.3577\n",
      "Epoch: 1525/2000... Training loss: 0.3684\n",
      "Epoch: 1525/2000... Training loss: 0.3441\n",
      "Epoch: 1525/2000... Training loss: 0.3192\n",
      "Epoch: 1525/2000... Training loss: 0.5406\n",
      "Epoch: 1526/2000... Training loss: 0.6042\n",
      "Epoch: 1526/2000... Training loss: 0.4130\n",
      "Epoch: 1526/2000... Training loss: 0.4250\n",
      "Epoch: 1526/2000... Training loss: 0.4171\n",
      "Epoch: 1526/2000... Training loss: 0.4599\n",
      "Epoch: 1526/2000... Training loss: 0.4535\n",
      "Epoch: 1526/2000... Training loss: 0.4515\n",
      "Epoch: 1526/2000... Training loss: 0.4058\n",
      "Epoch: 1526/2000... Training loss: 0.3690\n",
      "Epoch: 1526/2000... Training loss: 0.2873\n",
      "Epoch: 1526/2000... Training loss: 0.4294\n",
      "Epoch: 1526/2000... Training loss: 0.2381\n",
      "Epoch: 1526/2000... Training loss: 0.2576\n",
      "Epoch: 1526/2000... Training loss: 0.4382\n",
      "Epoch: 1526/2000... Training loss: 0.3727\n",
      "Epoch: 1526/2000... Training loss: 0.4930\n",
      "Epoch: 1526/2000... Training loss: 0.1434\n",
      "Epoch: 1526/2000... Training loss: 0.2969\n",
      "Epoch: 1526/2000... Training loss: 0.3726\n",
      "Epoch: 1526/2000... Training loss: 0.1430\n",
      "Epoch: 1526/2000... Training loss: 0.3772\n",
      "Epoch: 1526/2000... Training loss: 0.4682\n",
      "Epoch: 1526/2000... Training loss: 0.6124\n",
      "Epoch: 1526/2000... Training loss: 0.5473\n",
      "Epoch: 1526/2000... Training loss: 0.2225\n",
      "Epoch: 1526/2000... Training loss: 0.5861\n",
      "Epoch: 1526/2000... Training loss: 0.4772\n",
      "Epoch: 1526/2000... Training loss: 0.4971\n",
      "Epoch: 1526/2000... Training loss: 0.3244\n",
      "Epoch: 1526/2000... Training loss: 0.4370\n",
      "Epoch: 1526/2000... Training loss: 0.3700\n",
      "Epoch: 1527/2000... Training loss: 0.4532\n",
      "Epoch: 1527/2000... Training loss: 0.6095\n",
      "Epoch: 1527/2000... Training loss: 0.2270\n",
      "Epoch: 1527/2000... Training loss: 0.3254\n",
      "Epoch: 1527/2000... Training loss: 0.4040\n",
      "Epoch: 1527/2000... Training loss: 0.2319\n",
      "Epoch: 1527/2000... Training loss: 0.5583\n",
      "Epoch: 1527/2000... Training loss: 0.3302\n",
      "Epoch: 1527/2000... Training loss: 0.3472\n",
      "Epoch: 1527/2000... Training loss: 0.3549\n",
      "Epoch: 1527/2000... Training loss: 0.2661\n",
      "Epoch: 1527/2000... Training loss: 0.3944\n",
      "Epoch: 1527/2000... Training loss: 0.3271\n",
      "Epoch: 1527/2000... Training loss: 0.4253\n",
      "Epoch: 1527/2000... Training loss: 0.4167\n",
      "Epoch: 1527/2000... Training loss: 0.3662\n",
      "Epoch: 1527/2000... Training loss: 0.2854\n",
      "Epoch: 1527/2000... Training loss: 0.3247\n",
      "Epoch: 1527/2000... Training loss: 0.2842\n",
      "Epoch: 1527/2000... Training loss: 0.3323\n",
      "Epoch: 1527/2000... Training loss: 0.2829\n",
      "Epoch: 1527/2000... Training loss: 0.3514\n",
      "Epoch: 1527/2000... Training loss: 0.4852\n",
      "Epoch: 1527/2000... Training loss: 0.2412\n",
      "Epoch: 1527/2000... Training loss: 0.5378\n",
      "Epoch: 1527/2000... Training loss: 0.3701\n",
      "Epoch: 1527/2000... Training loss: 0.6688\n",
      "Epoch: 1527/2000... Training loss: 0.3802\n",
      "Epoch: 1527/2000... Training loss: 0.4970\n",
      "Epoch: 1527/2000... Training loss: 0.3599\n",
      "Epoch: 1527/2000... Training loss: 0.5028\n",
      "Epoch: 1528/2000... Training loss: 0.3748\n",
      "Epoch: 1528/2000... Training loss: 0.4110\n",
      "Epoch: 1528/2000... Training loss: 0.4268\n",
      "Epoch: 1528/2000... Training loss: 0.3490\n",
      "Epoch: 1528/2000... Training loss: 0.3516\n",
      "Epoch: 1528/2000... Training loss: 0.5614\n",
      "Epoch: 1528/2000... Training loss: 0.2984\n",
      "Epoch: 1528/2000... Training loss: 0.4484\n",
      "Epoch: 1528/2000... Training loss: 0.3836\n",
      "Epoch: 1528/2000... Training loss: 0.3739\n",
      "Epoch: 1528/2000... Training loss: 0.5223\n",
      "Epoch: 1528/2000... Training loss: 0.2391\n",
      "Epoch: 1528/2000... Training loss: 0.3848\n",
      "Epoch: 1528/2000... Training loss: 0.3031\n",
      "Epoch: 1528/2000... Training loss: 0.3531\n",
      "Epoch: 1528/2000... Training loss: 0.5329\n",
      "Epoch: 1528/2000... Training loss: 0.4332\n",
      "Epoch: 1528/2000... Training loss: 0.5193\n",
      "Epoch: 1528/2000... Training loss: 0.4401\n",
      "Epoch: 1528/2000... Training loss: 0.3567\n",
      "Epoch: 1528/2000... Training loss: 0.4838\n",
      "Epoch: 1528/2000... Training loss: 0.3382\n",
      "Epoch: 1528/2000... Training loss: 0.4778\n",
      "Epoch: 1528/2000... Training loss: 0.4188\n",
      "Epoch: 1528/2000... Training loss: 0.4476\n",
      "Epoch: 1528/2000... Training loss: 0.4307\n",
      "Epoch: 1528/2000... Training loss: 0.3077\n",
      "Epoch: 1528/2000... Training loss: 0.3673\n",
      "Epoch: 1528/2000... Training loss: 0.3659\n",
      "Epoch: 1528/2000... Training loss: 0.4264\n",
      "Epoch: 1528/2000... Training loss: 0.4315\n",
      "Epoch: 1529/2000... Training loss: 0.3822\n",
      "Epoch: 1529/2000... Training loss: 0.4972\n",
      "Epoch: 1529/2000... Training loss: 0.3242\n",
      "Epoch: 1529/2000... Training loss: 0.3294\n",
      "Epoch: 1529/2000... Training loss: 0.3196\n",
      "Epoch: 1529/2000... Training loss: 0.3416\n",
      "Epoch: 1529/2000... Training loss: 0.6246\n",
      "Epoch: 1529/2000... Training loss: 0.5336\n",
      "Epoch: 1529/2000... Training loss: 0.4472\n",
      "Epoch: 1529/2000... Training loss: 0.5704\n",
      "Epoch: 1529/2000... Training loss: 0.3377\n",
      "Epoch: 1529/2000... Training loss: 0.2743\n",
      "Epoch: 1529/2000... Training loss: 0.4969\n",
      "Epoch: 1529/2000... Training loss: 0.4590\n",
      "Epoch: 1529/2000... Training loss: 0.3799\n",
      "Epoch: 1529/2000... Training loss: 0.3464\n",
      "Epoch: 1529/2000... Training loss: 0.3630\n",
      "Epoch: 1529/2000... Training loss: 0.3363\n",
      "Epoch: 1529/2000... Training loss: 0.4642\n",
      "Epoch: 1529/2000... Training loss: 0.5036\n",
      "Epoch: 1529/2000... Training loss: 0.4106\n",
      "Epoch: 1529/2000... Training loss: 0.4517\n",
      "Epoch: 1529/2000... Training loss: 0.3488\n",
      "Epoch: 1529/2000... Training loss: 0.7182\n",
      "Epoch: 1529/2000... Training loss: 0.2908\n",
      "Epoch: 1529/2000... Training loss: 0.3524\n",
      "Epoch: 1529/2000... Training loss: 0.4065\n",
      "Epoch: 1529/2000... Training loss: 0.4185\n",
      "Epoch: 1529/2000... Training loss: 0.3743\n",
      "Epoch: 1529/2000... Training loss: 0.4110\n",
      "Epoch: 1529/2000... Training loss: 0.4072\n",
      "Epoch: 1530/2000... Training loss: 0.3843\n",
      "Epoch: 1530/2000... Training loss: 0.2438\n",
      "Epoch: 1530/2000... Training loss: 0.2885\n",
      "Epoch: 1530/2000... Training loss: 0.3674\n",
      "Epoch: 1530/2000... Training loss: 0.2704\n",
      "Epoch: 1530/2000... Training loss: 0.3121\n",
      "Epoch: 1530/2000... Training loss: 0.3959\n",
      "Epoch: 1530/2000... Training loss: 0.5242\n",
      "Epoch: 1530/2000... Training loss: 0.4795\n",
      "Epoch: 1530/2000... Training loss: 0.3669\n",
      "Epoch: 1530/2000... Training loss: 0.3414\n",
      "Epoch: 1530/2000... Training loss: 0.3927\n",
      "Epoch: 1530/2000... Training loss: 0.2224\n",
      "Epoch: 1530/2000... Training loss: 0.4435\n",
      "Epoch: 1530/2000... Training loss: 0.3259\n",
      "Epoch: 1530/2000... Training loss: 0.2142\n",
      "Epoch: 1530/2000... Training loss: 0.3565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1530/2000... Training loss: 0.3028\n",
      "Epoch: 1530/2000... Training loss: 0.4144\n",
      "Epoch: 1530/2000... Training loss: 0.4993\n",
      "Epoch: 1530/2000... Training loss: 0.3338\n",
      "Epoch: 1530/2000... Training loss: 0.3068\n",
      "Epoch: 1530/2000... Training loss: 0.3642\n",
      "Epoch: 1530/2000... Training loss: 0.4338\n",
      "Epoch: 1530/2000... Training loss: 0.4759\n",
      "Epoch: 1530/2000... Training loss: 0.4009\n",
      "Epoch: 1530/2000... Training loss: 0.3439\n",
      "Epoch: 1530/2000... Training loss: 0.4571\n",
      "Epoch: 1530/2000... Training loss: 0.2825\n",
      "Epoch: 1530/2000... Training loss: 0.5397\n",
      "Epoch: 1530/2000... Training loss: 0.3438\n",
      "Epoch: 1531/2000... Training loss: 0.4095\n",
      "Epoch: 1531/2000... Training loss: 0.4970\n",
      "Epoch: 1531/2000... Training loss: 0.5418\n",
      "Epoch: 1531/2000... Training loss: 0.2987\n",
      "Epoch: 1531/2000... Training loss: 0.4340\n",
      "Epoch: 1531/2000... Training loss: 0.4700\n",
      "Epoch: 1531/2000... Training loss: 0.4211\n",
      "Epoch: 1531/2000... Training loss: 0.4330\n",
      "Epoch: 1531/2000... Training loss: 0.5042\n",
      "Epoch: 1531/2000... Training loss: 0.4645\n",
      "Epoch: 1531/2000... Training loss: 0.4809\n",
      "Epoch: 1531/2000... Training loss: 0.5854\n",
      "Epoch: 1531/2000... Training loss: 0.3692\n",
      "Epoch: 1531/2000... Training loss: 0.4453\n",
      "Epoch: 1531/2000... Training loss: 0.3450\n",
      "Epoch: 1531/2000... Training loss: 0.4324\n",
      "Epoch: 1531/2000... Training loss: 0.3251\n",
      "Epoch: 1531/2000... Training loss: 0.5575\n",
      "Epoch: 1531/2000... Training loss: 0.3026\n",
      "Epoch: 1531/2000... Training loss: 0.3905\n",
      "Epoch: 1531/2000... Training loss: 0.4607\n",
      "Epoch: 1531/2000... Training loss: 0.3546\n",
      "Epoch: 1531/2000... Training loss: 0.3843\n",
      "Epoch: 1531/2000... Training loss: 0.2186\n",
      "Epoch: 1531/2000... Training loss: 0.4048\n",
      "Epoch: 1531/2000... Training loss: 0.4038\n",
      "Epoch: 1531/2000... Training loss: 0.4034\n",
      "Epoch: 1531/2000... Training loss: 0.3828\n",
      "Epoch: 1531/2000... Training loss: 0.2870\n",
      "Epoch: 1531/2000... Training loss: 0.4931\n",
      "Epoch: 1531/2000... Training loss: 0.3435\n",
      "Epoch: 1532/2000... Training loss: 0.6046\n",
      "Epoch: 1532/2000... Training loss: 0.4664\n",
      "Epoch: 1532/2000... Training loss: 0.6190\n",
      "Epoch: 1532/2000... Training loss: 0.5122\n",
      "Epoch: 1532/2000... Training loss: 0.3705\n",
      "Epoch: 1532/2000... Training loss: 0.4179\n",
      "Epoch: 1532/2000... Training loss: 0.4595\n",
      "Epoch: 1532/2000... Training loss: 0.4259\n",
      "Epoch: 1532/2000... Training loss: 0.2947\n",
      "Epoch: 1532/2000... Training loss: 0.3240\n",
      "Epoch: 1532/2000... Training loss: 0.2964\n",
      "Epoch: 1532/2000... Training loss: 0.2761\n",
      "Epoch: 1532/2000... Training loss: 0.4693\n",
      "Epoch: 1532/2000... Training loss: 0.4147\n",
      "Epoch: 1532/2000... Training loss: 0.5533\n",
      "Epoch: 1532/2000... Training loss: 0.5566\n",
      "Epoch: 1532/2000... Training loss: 0.3261\n",
      "Epoch: 1532/2000... Training loss: 0.3964\n",
      "Epoch: 1532/2000... Training loss: 0.4607\n",
      "Epoch: 1532/2000... Training loss: 0.3996\n",
      "Epoch: 1532/2000... Training loss: 0.3161\n",
      "Epoch: 1532/2000... Training loss: 0.4331\n",
      "Epoch: 1532/2000... Training loss: 0.3819\n",
      "Epoch: 1532/2000... Training loss: 0.2653\n",
      "Epoch: 1532/2000... Training loss: 0.4083\n",
      "Epoch: 1532/2000... Training loss: 0.4727\n",
      "Epoch: 1532/2000... Training loss: 0.4014\n",
      "Epoch: 1532/2000... Training loss: 0.3575\n",
      "Epoch: 1532/2000... Training loss: 0.5894\n",
      "Epoch: 1532/2000... Training loss: 0.6136\n",
      "Epoch: 1532/2000... Training loss: 0.3618\n",
      "Epoch: 1533/2000... Training loss: 0.3893\n",
      "Epoch: 1533/2000... Training loss: 0.5048\n",
      "Epoch: 1533/2000... Training loss: 0.3632\n",
      "Epoch: 1533/2000... Training loss: 0.2697\n",
      "Epoch: 1533/2000... Training loss: 0.4431\n",
      "Epoch: 1533/2000... Training loss: 0.2369\n",
      "Epoch: 1533/2000... Training loss: 0.3378\n",
      "Epoch: 1533/2000... Training loss: 0.5196\n",
      "Epoch: 1533/2000... Training loss: 0.3469\n",
      "Epoch: 1533/2000... Training loss: 0.4319\n",
      "Epoch: 1533/2000... Training loss: 0.2974\n",
      "Epoch: 1533/2000... Training loss: 0.3447\n",
      "Epoch: 1533/2000... Training loss: 0.4437\n",
      "Epoch: 1533/2000... Training loss: 0.2928\n",
      "Epoch: 1533/2000... Training loss: 0.2834\n",
      "Epoch: 1533/2000... Training loss: 0.2664\n",
      "Epoch: 1533/2000... Training loss: 0.3080\n",
      "Epoch: 1533/2000... Training loss: 0.2860\n",
      "Epoch: 1533/2000... Training loss: 0.3630\n",
      "Epoch: 1533/2000... Training loss: 0.4426\n",
      "Epoch: 1533/2000... Training loss: 0.4027\n",
      "Epoch: 1533/2000... Training loss: 0.4143\n",
      "Epoch: 1533/2000... Training loss: 0.3342\n",
      "Epoch: 1533/2000... Training loss: 0.3879\n",
      "Epoch: 1533/2000... Training loss: 0.2771\n",
      "Epoch: 1533/2000... Training loss: 0.4084\n",
      "Epoch: 1533/2000... Training loss: 0.4294\n",
      "Epoch: 1533/2000... Training loss: 0.4737\n",
      "Epoch: 1533/2000... Training loss: 0.4730\n",
      "Epoch: 1533/2000... Training loss: 0.2537\n",
      "Epoch: 1533/2000... Training loss: 0.4156\n",
      "Epoch: 1534/2000... Training loss: 0.4220\n",
      "Epoch: 1534/2000... Training loss: 0.3452\n",
      "Epoch: 1534/2000... Training loss: 0.4145\n",
      "Epoch: 1534/2000... Training loss: 0.3002\n",
      "Epoch: 1534/2000... Training loss: 0.3514\n",
      "Epoch: 1534/2000... Training loss: 0.7525\n",
      "Epoch: 1534/2000... Training loss: 0.5245\n",
      "Epoch: 1534/2000... Training loss: 0.3343\n",
      "Epoch: 1534/2000... Training loss: 0.3052\n",
      "Epoch: 1534/2000... Training loss: 0.2391\n",
      "Epoch: 1534/2000... Training loss: 0.2496\n",
      "Epoch: 1534/2000... Training loss: 0.2833\n",
      "Epoch: 1534/2000... Training loss: 0.4066\n",
      "Epoch: 1534/2000... Training loss: 0.4968\n",
      "Epoch: 1534/2000... Training loss: 0.4089\n",
      "Epoch: 1534/2000... Training loss: 0.4392\n",
      "Epoch: 1534/2000... Training loss: 0.6452\n",
      "Epoch: 1534/2000... Training loss: 0.2796\n",
      "Epoch: 1534/2000... Training loss: 0.5514\n",
      "Epoch: 1534/2000... Training loss: 0.2908\n",
      "Epoch: 1534/2000... Training loss: 0.3573\n",
      "Epoch: 1534/2000... Training loss: 0.2908\n",
      "Epoch: 1534/2000... Training loss: 0.4176\n",
      "Epoch: 1534/2000... Training loss: 0.3252\n",
      "Epoch: 1534/2000... Training loss: 0.5567\n",
      "Epoch: 1534/2000... Training loss: 0.4480\n",
      "Epoch: 1534/2000... Training loss: 0.3547\n",
      "Epoch: 1534/2000... Training loss: 0.3318\n",
      "Epoch: 1534/2000... Training loss: 0.3548\n",
      "Epoch: 1534/2000... Training loss: 0.4231\n",
      "Epoch: 1534/2000... Training loss: 0.4127\n",
      "Epoch: 1535/2000... Training loss: 0.4659\n",
      "Epoch: 1535/2000... Training loss: 0.5074\n",
      "Epoch: 1535/2000... Training loss: 0.3330\n",
      "Epoch: 1535/2000... Training loss: 0.2809\n",
      "Epoch: 1535/2000... Training loss: 0.3645\n",
      "Epoch: 1535/2000... Training loss: 0.4491\n",
      "Epoch: 1535/2000... Training loss: 0.2943\n",
      "Epoch: 1535/2000... Training loss: 0.4192\n",
      "Epoch: 1535/2000... Training loss: 0.4193\n",
      "Epoch: 1535/2000... Training loss: 0.4834\n",
      "Epoch: 1535/2000... Training loss: 0.2772\n",
      "Epoch: 1535/2000... Training loss: 0.2968\n",
      "Epoch: 1535/2000... Training loss: 0.3361\n",
      "Epoch: 1535/2000... Training loss: 0.2767\n",
      "Epoch: 1535/2000... Training loss: 0.3534\n",
      "Epoch: 1535/2000... Training loss: 0.3347\n",
      "Epoch: 1535/2000... Training loss: 0.3160\n",
      "Epoch: 1535/2000... Training loss: 0.2895\n",
      "Epoch: 1535/2000... Training loss: 0.4022\n",
      "Epoch: 1535/2000... Training loss: 0.4975\n",
      "Epoch: 1535/2000... Training loss: 0.3955\n",
      "Epoch: 1535/2000... Training loss: 0.3814\n",
      "Epoch: 1535/2000... Training loss: 0.5592\n",
      "Epoch: 1535/2000... Training loss: 0.4194\n",
      "Epoch: 1535/2000... Training loss: 0.2165\n",
      "Epoch: 1535/2000... Training loss: 0.3614\n",
      "Epoch: 1535/2000... Training loss: 0.4215\n",
      "Epoch: 1535/2000... Training loss: 0.3560\n",
      "Epoch: 1535/2000... Training loss: 0.5610\n",
      "Epoch: 1535/2000... Training loss: 0.4495\n",
      "Epoch: 1535/2000... Training loss: 0.5037\n",
      "Epoch: 1536/2000... Training loss: 0.2605\n",
      "Epoch: 1536/2000... Training loss: 0.3026\n",
      "Epoch: 1536/2000... Training loss: 0.2872\n",
      "Epoch: 1536/2000... Training loss: 0.6720\n",
      "Epoch: 1536/2000... Training loss: 0.3455\n",
      "Epoch: 1536/2000... Training loss: 0.1753\n",
      "Epoch: 1536/2000... Training loss: 0.4809\n",
      "Epoch: 1536/2000... Training loss: 0.4871\n",
      "Epoch: 1536/2000... Training loss: 0.4300\n",
      "Epoch: 1536/2000... Training loss: 0.4393\n",
      "Epoch: 1536/2000... Training loss: 0.3271\n",
      "Epoch: 1536/2000... Training loss: 0.4892\n",
      "Epoch: 1536/2000... Training loss: 0.2930\n",
      "Epoch: 1536/2000... Training loss: 0.3249\n",
      "Epoch: 1536/2000... Training loss: 0.4662\n",
      "Epoch: 1536/2000... Training loss: 0.2554\n",
      "Epoch: 1536/2000... Training loss: 0.3124\n",
      "Epoch: 1536/2000... Training loss: 0.2987\n",
      "Epoch: 1536/2000... Training loss: 0.3056\n",
      "Epoch: 1536/2000... Training loss: 0.3385\n",
      "Epoch: 1536/2000... Training loss: 0.3264\n",
      "Epoch: 1536/2000... Training loss: 0.3659\n",
      "Epoch: 1536/2000... Training loss: 0.3460\n",
      "Epoch: 1536/2000... Training loss: 0.4168\n",
      "Epoch: 1536/2000... Training loss: 0.6372\n",
      "Epoch: 1536/2000... Training loss: 0.2920\n",
      "Epoch: 1536/2000... Training loss: 0.6375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1536/2000... Training loss: 0.5097\n",
      "Epoch: 1536/2000... Training loss: 0.6491\n",
      "Epoch: 1536/2000... Training loss: 0.5751\n",
      "Epoch: 1536/2000... Training loss: 0.5963\n",
      "Epoch: 1537/2000... Training loss: 0.3870\n",
      "Epoch: 1537/2000... Training loss: 0.3082\n",
      "Epoch: 1537/2000... Training loss: 0.5508\n",
      "Epoch: 1537/2000... Training loss: 0.2918\n",
      "Epoch: 1537/2000... Training loss: 0.3359\n",
      "Epoch: 1537/2000... Training loss: 0.2076\n",
      "Epoch: 1537/2000... Training loss: 0.3703\n",
      "Epoch: 1537/2000... Training loss: 0.4617\n",
      "Epoch: 1537/2000... Training loss: 0.3099\n",
      "Epoch: 1537/2000... Training loss: 0.3717\n",
      "Epoch: 1537/2000... Training loss: 0.4777\n",
      "Epoch: 1537/2000... Training loss: 0.3737\n",
      "Epoch: 1537/2000... Training loss: 0.3707\n",
      "Epoch: 1537/2000... Training loss: 0.3943\n",
      "Epoch: 1537/2000... Training loss: 0.4655\n",
      "Epoch: 1537/2000... Training loss: 0.4482\n",
      "Epoch: 1537/2000... Training loss: 0.5657\n",
      "Epoch: 1537/2000... Training loss: 0.3648\n",
      "Epoch: 1537/2000... Training loss: 0.4642\n",
      "Epoch: 1537/2000... Training loss: 0.4056\n",
      "Epoch: 1537/2000... Training loss: 0.6326\n",
      "Epoch: 1537/2000... Training loss: 0.4175\n",
      "Epoch: 1537/2000... Training loss: 0.4737\n",
      "Epoch: 1537/2000... Training loss: 0.3898\n",
      "Epoch: 1537/2000... Training loss: 0.4316\n",
      "Epoch: 1537/2000... Training loss: 0.3708\n",
      "Epoch: 1537/2000... Training loss: 0.3473\n",
      "Epoch: 1537/2000... Training loss: 0.6445\n",
      "Epoch: 1537/2000... Training loss: 0.4422\n",
      "Epoch: 1537/2000... Training loss: 0.3467\n",
      "Epoch: 1537/2000... Training loss: 0.3799\n",
      "Epoch: 1538/2000... Training loss: 0.4649\n",
      "Epoch: 1538/2000... Training loss: 0.3883\n",
      "Epoch: 1538/2000... Training loss: 0.3538\n",
      "Epoch: 1538/2000... Training loss: 0.3279\n",
      "Epoch: 1538/2000... Training loss: 0.3804\n",
      "Epoch: 1538/2000... Training loss: 0.3316\n",
      "Epoch: 1538/2000... Training loss: 0.2563\n",
      "Epoch: 1538/2000... Training loss: 0.7229\n",
      "Epoch: 1538/2000... Training loss: 0.4365\n",
      "Epoch: 1538/2000... Training loss: 0.4692\n",
      "Epoch: 1538/2000... Training loss: 0.4195\n",
      "Epoch: 1538/2000... Training loss: 0.3887\n",
      "Epoch: 1538/2000... Training loss: 0.4112\n",
      "Epoch: 1538/2000... Training loss: 0.5518\n",
      "Epoch: 1538/2000... Training loss: 0.2956\n",
      "Epoch: 1538/2000... Training loss: 0.4674\n",
      "Epoch: 1538/2000... Training loss: 0.5168\n",
      "Epoch: 1538/2000... Training loss: 0.3467\n",
      "Epoch: 1538/2000... Training loss: 0.6883\n",
      "Epoch: 1538/2000... Training loss: 0.3944\n",
      "Epoch: 1538/2000... Training loss: 0.4218\n",
      "Epoch: 1538/2000... Training loss: 0.4022\n",
      "Epoch: 1538/2000... Training loss: 0.4164\n",
      "Epoch: 1538/2000... Training loss: 0.5570\n",
      "Epoch: 1538/2000... Training loss: 0.4793\n",
      "Epoch: 1538/2000... Training loss: 0.2319\n",
      "Epoch: 1538/2000... Training loss: 0.4123\n",
      "Epoch: 1538/2000... Training loss: 0.5115\n",
      "Epoch: 1538/2000... Training loss: 0.3348\n",
      "Epoch: 1538/2000... Training loss: 0.5290\n",
      "Epoch: 1538/2000... Training loss: 0.4905\n",
      "Epoch: 1539/2000... Training loss: 0.4895\n",
      "Epoch: 1539/2000... Training loss: 0.5715\n",
      "Epoch: 1539/2000... Training loss: 0.4585\n",
      "Epoch: 1539/2000... Training loss: 0.4699\n",
      "Epoch: 1539/2000... Training loss: 0.5567\n",
      "Epoch: 1539/2000... Training loss: 0.1786\n",
      "Epoch: 1539/2000... Training loss: 0.4232\n",
      "Epoch: 1539/2000... Training loss: 0.3556\n",
      "Epoch: 1539/2000... Training loss: 0.4712\n",
      "Epoch: 1539/2000... Training loss: 0.3848\n",
      "Epoch: 1539/2000... Training loss: 0.4089\n",
      "Epoch: 1539/2000... Training loss: 0.4680\n",
      "Epoch: 1539/2000... Training loss: 0.4104\n",
      "Epoch: 1539/2000... Training loss: 0.4084\n",
      "Epoch: 1539/2000... Training loss: 0.4936\n",
      "Epoch: 1539/2000... Training loss: 0.2667\n",
      "Epoch: 1539/2000... Training loss: 0.4701\n",
      "Epoch: 1539/2000... Training loss: 0.4232\n",
      "Epoch: 1539/2000... Training loss: 0.4376\n",
      "Epoch: 1539/2000... Training loss: 0.4404\n",
      "Epoch: 1539/2000... Training loss: 0.3108\n",
      "Epoch: 1539/2000... Training loss: 0.3784\n",
      "Epoch: 1539/2000... Training loss: 0.4470\n",
      "Epoch: 1539/2000... Training loss: 0.3757\n",
      "Epoch: 1539/2000... Training loss: 0.3432\n",
      "Epoch: 1539/2000... Training loss: 0.5567\n",
      "Epoch: 1539/2000... Training loss: 0.2830\n",
      "Epoch: 1539/2000... Training loss: 0.3746\n",
      "Epoch: 1539/2000... Training loss: 0.3589\n",
      "Epoch: 1539/2000... Training loss: 0.3160\n",
      "Epoch: 1539/2000... Training loss: 0.4103\n",
      "Epoch: 1540/2000... Training loss: 0.4251\n",
      "Epoch: 1540/2000... Training loss: 0.2639\n",
      "Epoch: 1540/2000... Training loss: 0.4921\n",
      "Epoch: 1540/2000... Training loss: 0.3737\n",
      "Epoch: 1540/2000... Training loss: 0.3350\n",
      "Epoch: 1540/2000... Training loss: 0.5597\n",
      "Epoch: 1540/2000... Training loss: 0.3266\n",
      "Epoch: 1540/2000... Training loss: 0.4589\n",
      "Epoch: 1540/2000... Training loss: 0.4916\n",
      "Epoch: 1540/2000... Training loss: 0.3933\n",
      "Epoch: 1540/2000... Training loss: 0.3952\n",
      "Epoch: 1540/2000... Training loss: 0.5187\n",
      "Epoch: 1540/2000... Training loss: 0.4432\n",
      "Epoch: 1540/2000... Training loss: 0.2746\n",
      "Epoch: 1540/2000... Training loss: 0.5102\n",
      "Epoch: 1540/2000... Training loss: 0.2907\n",
      "Epoch: 1540/2000... Training loss: 0.3582\n",
      "Epoch: 1540/2000... Training loss: 0.4538\n",
      "Epoch: 1540/2000... Training loss: 0.4870\n",
      "Epoch: 1540/2000... Training loss: 0.3165\n",
      "Epoch: 1540/2000... Training loss: 0.4832\n",
      "Epoch: 1540/2000... Training loss: 0.4985\n",
      "Epoch: 1540/2000... Training loss: 0.2504\n",
      "Epoch: 1540/2000... Training loss: 0.6748\n",
      "Epoch: 1540/2000... Training loss: 0.3345\n",
      "Epoch: 1540/2000... Training loss: 0.4757\n",
      "Epoch: 1540/2000... Training loss: 0.4310\n",
      "Epoch: 1540/2000... Training loss: 0.3603\n",
      "Epoch: 1540/2000... Training loss: 0.3043\n",
      "Epoch: 1540/2000... Training loss: 0.4321\n",
      "Epoch: 1540/2000... Training loss: 0.3128\n",
      "Epoch: 1541/2000... Training loss: 0.3097\n",
      "Epoch: 1541/2000... Training loss: 0.5109\n",
      "Epoch: 1541/2000... Training loss: 0.4438\n",
      "Epoch: 1541/2000... Training loss: 0.4785\n",
      "Epoch: 1541/2000... Training loss: 0.3817\n",
      "Epoch: 1541/2000... Training loss: 0.3601\n",
      "Epoch: 1541/2000... Training loss: 0.4063\n",
      "Epoch: 1541/2000... Training loss: 0.4233\n",
      "Epoch: 1541/2000... Training loss: 0.2521\n",
      "Epoch: 1541/2000... Training loss: 0.3918\n",
      "Epoch: 1541/2000... Training loss: 0.5119\n",
      "Epoch: 1541/2000... Training loss: 0.3840\n",
      "Epoch: 1541/2000... Training loss: 0.4516\n",
      "Epoch: 1541/2000... Training loss: 0.3508\n",
      "Epoch: 1541/2000... Training loss: 0.3696\n",
      "Epoch: 1541/2000... Training loss: 0.4210\n",
      "Epoch: 1541/2000... Training loss: 0.5117\n",
      "Epoch: 1541/2000... Training loss: 0.6094\n",
      "Epoch: 1541/2000... Training loss: 0.3393\n",
      "Epoch: 1541/2000... Training loss: 0.4354\n",
      "Epoch: 1541/2000... Training loss: 0.4502\n",
      "Epoch: 1541/2000... Training loss: 0.4679\n",
      "Epoch: 1541/2000... Training loss: 0.6141\n",
      "Epoch: 1541/2000... Training loss: 0.3836\n",
      "Epoch: 1541/2000... Training loss: 0.3849\n",
      "Epoch: 1541/2000... Training loss: 0.6051\n",
      "Epoch: 1541/2000... Training loss: 0.5364\n",
      "Epoch: 1541/2000... Training loss: 0.3745\n",
      "Epoch: 1541/2000... Training loss: 0.5827\n",
      "Epoch: 1541/2000... Training loss: 0.4262\n",
      "Epoch: 1541/2000... Training loss: 0.3491\n",
      "Epoch: 1542/2000... Training loss: 0.4443\n",
      "Epoch: 1542/2000... Training loss: 0.4284\n",
      "Epoch: 1542/2000... Training loss: 0.5774\n",
      "Epoch: 1542/2000... Training loss: 0.4639\n",
      "Epoch: 1542/2000... Training loss: 0.5020\n",
      "Epoch: 1542/2000... Training loss: 0.3831\n",
      "Epoch: 1542/2000... Training loss: 0.2120\n",
      "Epoch: 1542/2000... Training loss: 0.5232\n",
      "Epoch: 1542/2000... Training loss: 0.3705\n",
      "Epoch: 1542/2000... Training loss: 0.5486\n",
      "Epoch: 1542/2000... Training loss: 0.3993\n",
      "Epoch: 1542/2000... Training loss: 0.2838\n",
      "Epoch: 1542/2000... Training loss: 0.5237\n",
      "Epoch: 1542/2000... Training loss: 0.6344\n",
      "Epoch: 1542/2000... Training loss: 0.5901\n",
      "Epoch: 1542/2000... Training loss: 0.4089\n",
      "Epoch: 1542/2000... Training loss: 0.5921\n",
      "Epoch: 1542/2000... Training loss: 0.3303\n",
      "Epoch: 1542/2000... Training loss: 0.4136\n",
      "Epoch: 1542/2000... Training loss: 0.3771\n",
      "Epoch: 1542/2000... Training loss: 0.3750\n",
      "Epoch: 1542/2000... Training loss: 0.3798\n",
      "Epoch: 1542/2000... Training loss: 0.4364\n",
      "Epoch: 1542/2000... Training loss: 0.4013\n",
      "Epoch: 1542/2000... Training loss: 0.5506\n",
      "Epoch: 1542/2000... Training loss: 0.4650\n",
      "Epoch: 1542/2000... Training loss: 0.5281\n",
      "Epoch: 1542/2000... Training loss: 0.3487\n",
      "Epoch: 1542/2000... Training loss: 0.4157\n",
      "Epoch: 1542/2000... Training loss: 0.3870\n",
      "Epoch: 1542/2000... Training loss: 0.3748\n",
      "Epoch: 1543/2000... Training loss: 0.3450\n",
      "Epoch: 1543/2000... Training loss: 0.2850\n",
      "Epoch: 1543/2000... Training loss: 0.5009\n",
      "Epoch: 1543/2000... Training loss: 0.2350\n",
      "Epoch: 1543/2000... Training loss: 0.5833\n",
      "Epoch: 1543/2000... Training loss: 0.4058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1543/2000... Training loss: 0.3571\n",
      "Epoch: 1543/2000... Training loss: 0.3614\n",
      "Epoch: 1543/2000... Training loss: 0.2858\n",
      "Epoch: 1543/2000... Training loss: 0.4114\n",
      "Epoch: 1543/2000... Training loss: 0.4429\n",
      "Epoch: 1543/2000... Training loss: 0.2924\n",
      "Epoch: 1543/2000... Training loss: 0.3310\n",
      "Epoch: 1543/2000... Training loss: 0.3807\n",
      "Epoch: 1543/2000... Training loss: 0.2185\n",
      "Epoch: 1543/2000... Training loss: 0.6321\n",
      "Epoch: 1543/2000... Training loss: 0.7611\n",
      "Epoch: 1543/2000... Training loss: 0.3544\n",
      "Epoch: 1543/2000... Training loss: 0.2730\n",
      "Epoch: 1543/2000... Training loss: 0.4860\n",
      "Epoch: 1543/2000... Training loss: 0.4328\n",
      "Epoch: 1543/2000... Training loss: 0.5265\n",
      "Epoch: 1543/2000... Training loss: 0.3077\n",
      "Epoch: 1543/2000... Training loss: 0.3409\n",
      "Epoch: 1543/2000... Training loss: 0.5030\n",
      "Epoch: 1543/2000... Training loss: 0.2659\n",
      "Epoch: 1543/2000... Training loss: 0.4337\n",
      "Epoch: 1543/2000... Training loss: 0.3553\n",
      "Epoch: 1543/2000... Training loss: 0.2892\n",
      "Epoch: 1543/2000... Training loss: 0.3680\n",
      "Epoch: 1543/2000... Training loss: 0.3159\n",
      "Epoch: 1544/2000... Training loss: 0.3981\n",
      "Epoch: 1544/2000... Training loss: 0.4117\n",
      "Epoch: 1544/2000... Training loss: 0.4800\n",
      "Epoch: 1544/2000... Training loss: 0.4084\n",
      "Epoch: 1544/2000... Training loss: 0.2791\n",
      "Epoch: 1544/2000... Training loss: 0.4542\n",
      "Epoch: 1544/2000... Training loss: 0.3300\n",
      "Epoch: 1544/2000... Training loss: 0.3217\n",
      "Epoch: 1544/2000... Training loss: 0.5115\n",
      "Epoch: 1544/2000... Training loss: 0.3869\n",
      "Epoch: 1544/2000... Training loss: 0.4011\n",
      "Epoch: 1544/2000... Training loss: 0.3581\n",
      "Epoch: 1544/2000... Training loss: 0.4620\n",
      "Epoch: 1544/2000... Training loss: 0.5321\n",
      "Epoch: 1544/2000... Training loss: 0.4976\n",
      "Epoch: 1544/2000... Training loss: 0.3017\n",
      "Epoch: 1544/2000... Training loss: 0.3440\n",
      "Epoch: 1544/2000... Training loss: 0.3855\n",
      "Epoch: 1544/2000... Training loss: 0.4122\n",
      "Epoch: 1544/2000... Training loss: 0.3497\n",
      "Epoch: 1544/2000... Training loss: 0.4325\n",
      "Epoch: 1544/2000... Training loss: 0.3024\n",
      "Epoch: 1544/2000... Training loss: 0.4100\n",
      "Epoch: 1544/2000... Training loss: 0.2768\n",
      "Epoch: 1544/2000... Training loss: 0.6158\n",
      "Epoch: 1544/2000... Training loss: 0.3502\n",
      "Epoch: 1544/2000... Training loss: 0.4644\n",
      "Epoch: 1544/2000... Training loss: 0.6050\n",
      "Epoch: 1544/2000... Training loss: 0.4963\n",
      "Epoch: 1544/2000... Training loss: 0.5376\n",
      "Epoch: 1544/2000... Training loss: 0.4046\n",
      "Epoch: 1545/2000... Training loss: 0.6086\n",
      "Epoch: 1545/2000... Training loss: 0.5049\n",
      "Epoch: 1545/2000... Training loss: 0.4161\n",
      "Epoch: 1545/2000... Training loss: 0.2311\n",
      "Epoch: 1545/2000... Training loss: 0.4167\n",
      "Epoch: 1545/2000... Training loss: 0.3239\n",
      "Epoch: 1545/2000... Training loss: 0.3268\n",
      "Epoch: 1545/2000... Training loss: 0.4323\n",
      "Epoch: 1545/2000... Training loss: 0.4455\n",
      "Epoch: 1545/2000... Training loss: 0.4840\n",
      "Epoch: 1545/2000... Training loss: 0.5519\n",
      "Epoch: 1545/2000... Training loss: 0.3966\n",
      "Epoch: 1545/2000... Training loss: 0.3788\n",
      "Epoch: 1545/2000... Training loss: 0.4900\n",
      "Epoch: 1545/2000... Training loss: 0.4635\n",
      "Epoch: 1545/2000... Training loss: 0.3031\n",
      "Epoch: 1545/2000... Training loss: 0.4211\n",
      "Epoch: 1545/2000... Training loss: 0.3569\n",
      "Epoch: 1545/2000... Training loss: 0.4674\n",
      "Epoch: 1545/2000... Training loss: 0.2103\n",
      "Epoch: 1545/2000... Training loss: 0.2852\n",
      "Epoch: 1545/2000... Training loss: 0.3238\n",
      "Epoch: 1545/2000... Training loss: 0.4595\n",
      "Epoch: 1545/2000... Training loss: 0.5109\n",
      "Epoch: 1545/2000... Training loss: 0.3793\n",
      "Epoch: 1545/2000... Training loss: 0.3319\n",
      "Epoch: 1545/2000... Training loss: 0.4047\n",
      "Epoch: 1545/2000... Training loss: 0.3120\n",
      "Epoch: 1545/2000... Training loss: 0.4002\n",
      "Epoch: 1545/2000... Training loss: 0.3312\n",
      "Epoch: 1545/2000... Training loss: 0.2940\n",
      "Epoch: 1546/2000... Training loss: 0.3985\n",
      "Epoch: 1546/2000... Training loss: 0.3307\n",
      "Epoch: 1546/2000... Training loss: 0.3274\n",
      "Epoch: 1546/2000... Training loss: 0.3686\n",
      "Epoch: 1546/2000... Training loss: 0.4019\n",
      "Epoch: 1546/2000... Training loss: 0.3823\n",
      "Epoch: 1546/2000... Training loss: 0.4045\n",
      "Epoch: 1546/2000... Training loss: 0.4969\n",
      "Epoch: 1546/2000... Training loss: 0.3453\n",
      "Epoch: 1546/2000... Training loss: 0.3477\n",
      "Epoch: 1546/2000... Training loss: 0.2669\n",
      "Epoch: 1546/2000... Training loss: 0.2369\n",
      "Epoch: 1546/2000... Training loss: 0.3327\n",
      "Epoch: 1546/2000... Training loss: 0.3034\n",
      "Epoch: 1546/2000... Training loss: 0.4251\n",
      "Epoch: 1546/2000... Training loss: 0.3289\n",
      "Epoch: 1546/2000... Training loss: 0.4912\n",
      "Epoch: 1546/2000... Training loss: 0.4226\n",
      "Epoch: 1546/2000... Training loss: 0.4972\n",
      "Epoch: 1546/2000... Training loss: 0.4801\n",
      "Epoch: 1546/2000... Training loss: 0.4728\n",
      "Epoch: 1546/2000... Training loss: 0.3547\n",
      "Epoch: 1546/2000... Training loss: 0.2910\n",
      "Epoch: 1546/2000... Training loss: 0.4362\n",
      "Epoch: 1546/2000... Training loss: 0.3955\n",
      "Epoch: 1546/2000... Training loss: 0.5753\n",
      "Epoch: 1546/2000... Training loss: 0.4867\n",
      "Epoch: 1546/2000... Training loss: 0.3784\n",
      "Epoch: 1546/2000... Training loss: 0.3465\n",
      "Epoch: 1546/2000... Training loss: 0.5300\n",
      "Epoch: 1546/2000... Training loss: 0.3081\n",
      "Epoch: 1547/2000... Training loss: 0.5176\n",
      "Epoch: 1547/2000... Training loss: 0.2754\n",
      "Epoch: 1547/2000... Training loss: 0.3274\n",
      "Epoch: 1547/2000... Training loss: 0.3924\n",
      "Epoch: 1547/2000... Training loss: 0.5111\n",
      "Epoch: 1547/2000... Training loss: 0.5329\n",
      "Epoch: 1547/2000... Training loss: 0.3873\n",
      "Epoch: 1547/2000... Training loss: 0.4925\n",
      "Epoch: 1547/2000... Training loss: 0.3948\n",
      "Epoch: 1547/2000... Training loss: 0.4556\n",
      "Epoch: 1547/2000... Training loss: 0.3874\n",
      "Epoch: 1547/2000... Training loss: 0.4854\n",
      "Epoch: 1547/2000... Training loss: 0.4842\n",
      "Epoch: 1547/2000... Training loss: 0.3971\n",
      "Epoch: 1547/2000... Training loss: 0.4278\n",
      "Epoch: 1547/2000... Training loss: 0.3889\n",
      "Epoch: 1547/2000... Training loss: 0.5238\n",
      "Epoch: 1547/2000... Training loss: 0.5257\n",
      "Epoch: 1547/2000... Training loss: 0.4577\n",
      "Epoch: 1547/2000... Training loss: 0.4324\n",
      "Epoch: 1547/2000... Training loss: 0.5149\n",
      "Epoch: 1547/2000... Training loss: 0.2802\n",
      "Epoch: 1547/2000... Training loss: 0.2193\n",
      "Epoch: 1547/2000... Training loss: 0.4388\n",
      "Epoch: 1547/2000... Training loss: 0.2638\n",
      "Epoch: 1547/2000... Training loss: 0.4049\n",
      "Epoch: 1547/2000... Training loss: 0.5030\n",
      "Epoch: 1547/2000... Training loss: 0.4640\n",
      "Epoch: 1547/2000... Training loss: 0.3770\n",
      "Epoch: 1547/2000... Training loss: 0.4134\n",
      "Epoch: 1547/2000... Training loss: 0.3235\n",
      "Epoch: 1548/2000... Training loss: 0.5823\n",
      "Epoch: 1548/2000... Training loss: 0.5100\n",
      "Epoch: 1548/2000... Training loss: 0.4898\n",
      "Epoch: 1548/2000... Training loss: 0.4865\n",
      "Epoch: 1548/2000... Training loss: 0.3937\n",
      "Epoch: 1548/2000... Training loss: 0.3461\n",
      "Epoch: 1548/2000... Training loss: 0.4557\n",
      "Epoch: 1548/2000... Training loss: 0.3752\n",
      "Epoch: 1548/2000... Training loss: 0.4324\n",
      "Epoch: 1548/2000... Training loss: 0.3594\n",
      "Epoch: 1548/2000... Training loss: 0.3096\n",
      "Epoch: 1548/2000... Training loss: 0.2313\n",
      "Epoch: 1548/2000... Training loss: 0.3261\n",
      "Epoch: 1548/2000... Training loss: 0.1978\n",
      "Epoch: 1548/2000... Training loss: 0.4591\n",
      "Epoch: 1548/2000... Training loss: 0.3829\n",
      "Epoch: 1548/2000... Training loss: 0.3900\n",
      "Epoch: 1548/2000... Training loss: 0.4270\n",
      "Epoch: 1548/2000... Training loss: 0.4455\n",
      "Epoch: 1548/2000... Training loss: 0.3403\n",
      "Epoch: 1548/2000... Training loss: 0.4267\n",
      "Epoch: 1548/2000... Training loss: 0.2484\n",
      "Epoch: 1548/2000... Training loss: 0.3739\n",
      "Epoch: 1548/2000... Training loss: 0.3615\n",
      "Epoch: 1548/2000... Training loss: 0.3265\n",
      "Epoch: 1548/2000... Training loss: 0.4115\n",
      "Epoch: 1548/2000... Training loss: 0.4887\n",
      "Epoch: 1548/2000... Training loss: 0.5746\n",
      "Epoch: 1548/2000... Training loss: 0.4199\n",
      "Epoch: 1548/2000... Training loss: 0.3365\n",
      "Epoch: 1548/2000... Training loss: 0.3545\n",
      "Epoch: 1549/2000... Training loss: 0.6026\n",
      "Epoch: 1549/2000... Training loss: 0.3294\n",
      "Epoch: 1549/2000... Training loss: 0.6382\n",
      "Epoch: 1549/2000... Training loss: 0.3553\n",
      "Epoch: 1549/2000... Training loss: 0.5598\n",
      "Epoch: 1549/2000... Training loss: 0.4773\n",
      "Epoch: 1549/2000... Training loss: 0.2974\n",
      "Epoch: 1549/2000... Training loss: 0.4397\n",
      "Epoch: 1549/2000... Training loss: 0.3037\n",
      "Epoch: 1549/2000... Training loss: 0.5548\n",
      "Epoch: 1549/2000... Training loss: 0.5101\n",
      "Epoch: 1549/2000... Training loss: 0.3498\n",
      "Epoch: 1549/2000... Training loss: 0.3181\n",
      "Epoch: 1549/2000... Training loss: 0.3339\n",
      "Epoch: 1549/2000... Training loss: 0.4037\n",
      "Epoch: 1549/2000... Training loss: 0.3349\n",
      "Epoch: 1549/2000... Training loss: 0.3833\n",
      "Epoch: 1549/2000... Training loss: 0.3361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1549/2000... Training loss: 0.3930\n",
      "Epoch: 1549/2000... Training loss: 0.4306\n",
      "Epoch: 1549/2000... Training loss: 0.2784\n",
      "Epoch: 1549/2000... Training loss: 0.3832\n",
      "Epoch: 1549/2000... Training loss: 0.4627\n",
      "Epoch: 1549/2000... Training loss: 0.3785\n",
      "Epoch: 1549/2000... Training loss: 0.3892\n",
      "Epoch: 1549/2000... Training loss: 0.3251\n",
      "Epoch: 1549/2000... Training loss: 0.4722\n",
      "Epoch: 1549/2000... Training loss: 0.3056\n",
      "Epoch: 1549/2000... Training loss: 0.5057\n",
      "Epoch: 1549/2000... Training loss: 0.3964\n",
      "Epoch: 1549/2000... Training loss: 0.3988\n",
      "Epoch: 1550/2000... Training loss: 0.4248\n",
      "Epoch: 1550/2000... Training loss: 0.5220\n",
      "Epoch: 1550/2000... Training loss: 0.3623\n",
      "Epoch: 1550/2000... Training loss: 0.4276\n",
      "Epoch: 1550/2000... Training loss: 0.2889\n",
      "Epoch: 1550/2000... Training loss: 0.4601\n",
      "Epoch: 1550/2000... Training loss: 0.4414\n",
      "Epoch: 1550/2000... Training loss: 0.3586\n",
      "Epoch: 1550/2000... Training loss: 0.4323\n",
      "Epoch: 1550/2000... Training loss: 0.6024\n",
      "Epoch: 1550/2000... Training loss: 0.2617\n",
      "Epoch: 1550/2000... Training loss: 0.5498\n",
      "Epoch: 1550/2000... Training loss: 0.3419\n",
      "Epoch: 1550/2000... Training loss: 0.6240\n",
      "Epoch: 1550/2000... Training loss: 0.3171\n",
      "Epoch: 1550/2000... Training loss: 0.5732\n",
      "Epoch: 1550/2000... Training loss: 0.5654\n",
      "Epoch: 1550/2000... Training loss: 0.3974\n",
      "Epoch: 1550/2000... Training loss: 0.5138\n",
      "Epoch: 1550/2000... Training loss: 0.3647\n",
      "Epoch: 1550/2000... Training loss: 0.3482\n",
      "Epoch: 1550/2000... Training loss: 0.3287\n",
      "Epoch: 1550/2000... Training loss: 0.4158\n",
      "Epoch: 1550/2000... Training loss: 0.4770\n",
      "Epoch: 1550/2000... Training loss: 0.4205\n",
      "Epoch: 1550/2000... Training loss: 0.4717\n",
      "Epoch: 1550/2000... Training loss: 0.3342\n",
      "Epoch: 1550/2000... Training loss: 0.2905\n",
      "Epoch: 1550/2000... Training loss: 0.4385\n",
      "Epoch: 1550/2000... Training loss: 0.5395\n",
      "Epoch: 1550/2000... Training loss: 0.4436\n",
      "Epoch: 1551/2000... Training loss: 0.2076\n",
      "Epoch: 1551/2000... Training loss: 0.4462\n",
      "Epoch: 1551/2000... Training loss: 0.5223\n",
      "Epoch: 1551/2000... Training loss: 0.4526\n",
      "Epoch: 1551/2000... Training loss: 0.3330\n",
      "Epoch: 1551/2000... Training loss: 0.5179\n",
      "Epoch: 1551/2000... Training loss: 0.4687\n",
      "Epoch: 1551/2000... Training loss: 0.4397\n",
      "Epoch: 1551/2000... Training loss: 0.4613\n",
      "Epoch: 1551/2000... Training loss: 0.5891\n",
      "Epoch: 1551/2000... Training loss: 0.4612\n",
      "Epoch: 1551/2000... Training loss: 0.2336\n",
      "Epoch: 1551/2000... Training loss: 0.2284\n",
      "Epoch: 1551/2000... Training loss: 0.6343\n",
      "Epoch: 1551/2000... Training loss: 0.3177\n",
      "Epoch: 1551/2000... Training loss: 0.5553\n",
      "Epoch: 1551/2000... Training loss: 0.5106\n",
      "Epoch: 1551/2000... Training loss: 0.3802\n",
      "Epoch: 1551/2000... Training loss: 0.3519\n",
      "Epoch: 1551/2000... Training loss: 0.3494\n",
      "Epoch: 1551/2000... Training loss: 0.2753\n",
      "Epoch: 1551/2000... Training loss: 0.5078\n",
      "Epoch: 1551/2000... Training loss: 0.3757\n",
      "Epoch: 1551/2000... Training loss: 0.4956\n",
      "Epoch: 1551/2000... Training loss: 0.3247\n",
      "Epoch: 1551/2000... Training loss: 0.4544\n",
      "Epoch: 1551/2000... Training loss: 0.4031\n",
      "Epoch: 1551/2000... Training loss: 0.4924\n",
      "Epoch: 1551/2000... Training loss: 0.5139\n",
      "Epoch: 1551/2000... Training loss: 0.4413\n",
      "Epoch: 1551/2000... Training loss: 0.3874\n",
      "Epoch: 1552/2000... Training loss: 0.4543\n",
      "Epoch: 1552/2000... Training loss: 0.5665\n",
      "Epoch: 1552/2000... Training loss: 0.4989\n",
      "Epoch: 1552/2000... Training loss: 0.3800\n",
      "Epoch: 1552/2000... Training loss: 0.5313\n",
      "Epoch: 1552/2000... Training loss: 0.2882\n",
      "Epoch: 1552/2000... Training loss: 0.3155\n",
      "Epoch: 1552/2000... Training loss: 0.3571\n",
      "Epoch: 1552/2000... Training loss: 0.3524\n",
      "Epoch: 1552/2000... Training loss: 0.5868\n",
      "Epoch: 1552/2000... Training loss: 0.4845\n",
      "Epoch: 1552/2000... Training loss: 0.3446\n",
      "Epoch: 1552/2000... Training loss: 0.3931\n",
      "Epoch: 1552/2000... Training loss: 0.4051\n",
      "Epoch: 1552/2000... Training loss: 0.2633\n",
      "Epoch: 1552/2000... Training loss: 0.3006\n",
      "Epoch: 1552/2000... Training loss: 0.3378\n",
      "Epoch: 1552/2000... Training loss: 0.4506\n",
      "Epoch: 1552/2000... Training loss: 0.5028\n",
      "Epoch: 1552/2000... Training loss: 0.3870\n",
      "Epoch: 1552/2000... Training loss: 0.4179\n",
      "Epoch: 1552/2000... Training loss: 0.4711\n",
      "Epoch: 1552/2000... Training loss: 0.3581\n",
      "Epoch: 1552/2000... Training loss: 0.3699\n",
      "Epoch: 1552/2000... Training loss: 0.3109\n",
      "Epoch: 1552/2000... Training loss: 0.5956\n",
      "Epoch: 1552/2000... Training loss: 0.7099\n",
      "Epoch: 1552/2000... Training loss: 0.5093\n",
      "Epoch: 1552/2000... Training loss: 0.3395\n",
      "Epoch: 1552/2000... Training loss: 0.4839\n",
      "Epoch: 1552/2000... Training loss: 0.4171\n",
      "Epoch: 1553/2000... Training loss: 0.3481\n",
      "Epoch: 1553/2000... Training loss: 0.4704\n",
      "Epoch: 1553/2000... Training loss: 0.4711\n",
      "Epoch: 1553/2000... Training loss: 0.3798\n",
      "Epoch: 1553/2000... Training loss: 0.4602\n",
      "Epoch: 1553/2000... Training loss: 0.5579\n",
      "Epoch: 1553/2000... Training loss: 0.2022\n",
      "Epoch: 1553/2000... Training loss: 0.2738\n",
      "Epoch: 1553/2000... Training loss: 0.3842\n",
      "Epoch: 1553/2000... Training loss: 0.4574\n",
      "Epoch: 1553/2000... Training loss: 0.3404\n",
      "Epoch: 1553/2000... Training loss: 0.3458\n",
      "Epoch: 1553/2000... Training loss: 0.4656\n",
      "Epoch: 1553/2000... Training loss: 0.3582\n",
      "Epoch: 1553/2000... Training loss: 0.4282\n",
      "Epoch: 1553/2000... Training loss: 0.2897\n",
      "Epoch: 1553/2000... Training loss: 0.3904\n",
      "Epoch: 1553/2000... Training loss: 0.4561\n",
      "Epoch: 1553/2000... Training loss: 0.3990\n",
      "Epoch: 1553/2000... Training loss: 0.2599\n",
      "Epoch: 1553/2000... Training loss: 0.4056\n",
      "Epoch: 1553/2000... Training loss: 0.5324\n",
      "Epoch: 1553/2000... Training loss: 0.3612\n",
      "Epoch: 1553/2000... Training loss: 0.4857\n",
      "Epoch: 1553/2000... Training loss: 0.4165\n",
      "Epoch: 1553/2000... Training loss: 0.3760\n",
      "Epoch: 1553/2000... Training loss: 0.5682\n",
      "Epoch: 1553/2000... Training loss: 0.3445\n",
      "Epoch: 1553/2000... Training loss: 0.3848\n",
      "Epoch: 1553/2000... Training loss: 0.4518\n",
      "Epoch: 1553/2000... Training loss: 0.3211\n",
      "Epoch: 1554/2000... Training loss: 0.3724\n",
      "Epoch: 1554/2000... Training loss: 0.5606\n",
      "Epoch: 1554/2000... Training loss: 0.4912\n",
      "Epoch: 1554/2000... Training loss: 0.4161\n",
      "Epoch: 1554/2000... Training loss: 0.4287\n",
      "Epoch: 1554/2000... Training loss: 0.4578\n",
      "Epoch: 1554/2000... Training loss: 0.3287\n",
      "Epoch: 1554/2000... Training loss: 0.3507\n",
      "Epoch: 1554/2000... Training loss: 0.4103\n",
      "Epoch: 1554/2000... Training loss: 0.4428\n",
      "Epoch: 1554/2000... Training loss: 0.4872\n",
      "Epoch: 1554/2000... Training loss: 0.3329\n",
      "Epoch: 1554/2000... Training loss: 0.3956\n",
      "Epoch: 1554/2000... Training loss: 0.4357\n",
      "Epoch: 1554/2000... Training loss: 0.4528\n",
      "Epoch: 1554/2000... Training loss: 0.2743\n",
      "Epoch: 1554/2000... Training loss: 0.4157\n",
      "Epoch: 1554/2000... Training loss: 0.4422\n",
      "Epoch: 1554/2000... Training loss: 0.3172\n",
      "Epoch: 1554/2000... Training loss: 0.3424\n",
      "Epoch: 1554/2000... Training loss: 0.4027\n",
      "Epoch: 1554/2000... Training loss: 0.3653\n",
      "Epoch: 1554/2000... Training loss: 0.3633\n",
      "Epoch: 1554/2000... Training loss: 0.2972\n",
      "Epoch: 1554/2000... Training loss: 0.4218\n",
      "Epoch: 1554/2000... Training loss: 0.4276\n",
      "Epoch: 1554/2000... Training loss: 0.4220\n",
      "Epoch: 1554/2000... Training loss: 0.3626\n",
      "Epoch: 1554/2000... Training loss: 0.4681\n",
      "Epoch: 1554/2000... Training loss: 0.3717\n",
      "Epoch: 1554/2000... Training loss: 0.4813\n",
      "Epoch: 1555/2000... Training loss: 0.4653\n",
      "Epoch: 1555/2000... Training loss: 0.5749\n",
      "Epoch: 1555/2000... Training loss: 0.4108\n",
      "Epoch: 1555/2000... Training loss: 0.4233\n",
      "Epoch: 1555/2000... Training loss: 0.4892\n",
      "Epoch: 1555/2000... Training loss: 0.4390\n",
      "Epoch: 1555/2000... Training loss: 0.4557\n",
      "Epoch: 1555/2000... Training loss: 0.3891\n",
      "Epoch: 1555/2000... Training loss: 0.5100\n",
      "Epoch: 1555/2000... Training loss: 0.1796\n",
      "Epoch: 1555/2000... Training loss: 0.4775\n",
      "Epoch: 1555/2000... Training loss: 0.4237\n",
      "Epoch: 1555/2000... Training loss: 0.4495\n",
      "Epoch: 1555/2000... Training loss: 0.5552\n",
      "Epoch: 1555/2000... Training loss: 0.4851\n",
      "Epoch: 1555/2000... Training loss: 0.1555\n",
      "Epoch: 1555/2000... Training loss: 0.4534\n",
      "Epoch: 1555/2000... Training loss: 0.4675\n",
      "Epoch: 1555/2000... Training loss: 0.3754\n",
      "Epoch: 1555/2000... Training loss: 0.1884\n",
      "Epoch: 1555/2000... Training loss: 0.3509\n",
      "Epoch: 1555/2000... Training loss: 0.4105\n",
      "Epoch: 1555/2000... Training loss: 0.4341\n",
      "Epoch: 1555/2000... Training loss: 0.4918\n",
      "Epoch: 1555/2000... Training loss: 0.3177\n",
      "Epoch: 1555/2000... Training loss: 0.3882\n",
      "Epoch: 1555/2000... Training loss: 0.4896\n",
      "Epoch: 1555/2000... Training loss: 0.3997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1555/2000... Training loss: 0.3880\n",
      "Epoch: 1555/2000... Training loss: 0.5852\n",
      "Epoch: 1555/2000... Training loss: 0.5014\n",
      "Epoch: 1556/2000... Training loss: 0.4503\n",
      "Epoch: 1556/2000... Training loss: 0.3258\n",
      "Epoch: 1556/2000... Training loss: 0.6794\n",
      "Epoch: 1556/2000... Training loss: 0.2695\n",
      "Epoch: 1556/2000... Training loss: 0.5785\n",
      "Epoch: 1556/2000... Training loss: 0.3479\n",
      "Epoch: 1556/2000... Training loss: 0.3759\n",
      "Epoch: 1556/2000... Training loss: 0.4563\n",
      "Epoch: 1556/2000... Training loss: 0.2942\n",
      "Epoch: 1556/2000... Training loss: 0.3444\n",
      "Epoch: 1556/2000... Training loss: 0.4014\n",
      "Epoch: 1556/2000... Training loss: 0.2891\n",
      "Epoch: 1556/2000... Training loss: 0.2168\n",
      "Epoch: 1556/2000... Training loss: 0.3706\n",
      "Epoch: 1556/2000... Training loss: 0.3864\n",
      "Epoch: 1556/2000... Training loss: 0.5048\n",
      "Epoch: 1556/2000... Training loss: 0.4051\n",
      "Epoch: 1556/2000... Training loss: 0.3034\n",
      "Epoch: 1556/2000... Training loss: 0.4956\n",
      "Epoch: 1556/2000... Training loss: 0.4908\n",
      "Epoch: 1556/2000... Training loss: 0.3838\n",
      "Epoch: 1556/2000... Training loss: 0.4669\n",
      "Epoch: 1556/2000... Training loss: 0.5959\n",
      "Epoch: 1556/2000... Training loss: 0.5603\n",
      "Epoch: 1556/2000... Training loss: 0.4111\n",
      "Epoch: 1556/2000... Training loss: 0.5569\n",
      "Epoch: 1556/2000... Training loss: 0.3726\n",
      "Epoch: 1556/2000... Training loss: 0.5016\n",
      "Epoch: 1556/2000... Training loss: 0.4398\n",
      "Epoch: 1556/2000... Training loss: 0.4570\n",
      "Epoch: 1556/2000... Training loss: 0.4918\n",
      "Epoch: 1557/2000... Training loss: 0.5026\n",
      "Epoch: 1557/2000... Training loss: 0.4238\n",
      "Epoch: 1557/2000... Training loss: 0.3706\n",
      "Epoch: 1557/2000... Training loss: 0.2752\n",
      "Epoch: 1557/2000... Training loss: 0.2215\n",
      "Epoch: 1557/2000... Training loss: 0.3446\n",
      "Epoch: 1557/2000... Training loss: 0.4199\n",
      "Epoch: 1557/2000... Training loss: 0.4626\n",
      "Epoch: 1557/2000... Training loss: 0.3533\n",
      "Epoch: 1557/2000... Training loss: 0.3340\n",
      "Epoch: 1557/2000... Training loss: 0.4188\n",
      "Epoch: 1557/2000... Training loss: 0.4526\n",
      "Epoch: 1557/2000... Training loss: 0.4444\n",
      "Epoch: 1557/2000... Training loss: 0.4043\n",
      "Epoch: 1557/2000... Training loss: 0.3822\n",
      "Epoch: 1557/2000... Training loss: 0.6191\n",
      "Epoch: 1557/2000... Training loss: 0.4000\n",
      "Epoch: 1557/2000... Training loss: 0.2287\n",
      "Epoch: 1557/2000... Training loss: 0.4007\n",
      "Epoch: 1557/2000... Training loss: 0.4541\n",
      "Epoch: 1557/2000... Training loss: 0.3325\n",
      "Epoch: 1557/2000... Training loss: 0.2738\n",
      "Epoch: 1557/2000... Training loss: 0.3872\n",
      "Epoch: 1557/2000... Training loss: 0.5035\n",
      "Epoch: 1557/2000... Training loss: 0.4493\n",
      "Epoch: 1557/2000... Training loss: 0.5271\n",
      "Epoch: 1557/2000... Training loss: 0.5020\n",
      "Epoch: 1557/2000... Training loss: 0.2875\n",
      "Epoch: 1557/2000... Training loss: 0.4004\n",
      "Epoch: 1557/2000... Training loss: 0.4193\n",
      "Epoch: 1557/2000... Training loss: 0.4363\n",
      "Epoch: 1558/2000... Training loss: 0.4073\n",
      "Epoch: 1558/2000... Training loss: 0.2442\n",
      "Epoch: 1558/2000... Training loss: 0.3660\n",
      "Epoch: 1558/2000... Training loss: 0.3583\n",
      "Epoch: 1558/2000... Training loss: 0.4197\n",
      "Epoch: 1558/2000... Training loss: 0.5853\n",
      "Epoch: 1558/2000... Training loss: 0.6266\n",
      "Epoch: 1558/2000... Training loss: 0.3424\n",
      "Epoch: 1558/2000... Training loss: 0.4889\n",
      "Epoch: 1558/2000... Training loss: 0.5241\n",
      "Epoch: 1558/2000... Training loss: 0.5890\n",
      "Epoch: 1558/2000... Training loss: 0.2299\n",
      "Epoch: 1558/2000... Training loss: 0.4815\n",
      "Epoch: 1558/2000... Training loss: 0.5684\n",
      "Epoch: 1558/2000... Training loss: 0.4198\n",
      "Epoch: 1558/2000... Training loss: 0.2893\n",
      "Epoch: 1558/2000... Training loss: 0.3575\n",
      "Epoch: 1558/2000... Training loss: 0.4180\n",
      "Epoch: 1558/2000... Training loss: 0.3815\n",
      "Epoch: 1558/2000... Training loss: 0.6022\n",
      "Epoch: 1558/2000... Training loss: 0.3689\n",
      "Epoch: 1558/2000... Training loss: 0.4531\n",
      "Epoch: 1558/2000... Training loss: 0.4267\n",
      "Epoch: 1558/2000... Training loss: 0.2434\n",
      "Epoch: 1558/2000... Training loss: 0.3543\n",
      "Epoch: 1558/2000... Training loss: 0.3552\n",
      "Epoch: 1558/2000... Training loss: 0.3589\n",
      "Epoch: 1558/2000... Training loss: 0.4396\n",
      "Epoch: 1558/2000... Training loss: 0.4205\n",
      "Epoch: 1558/2000... Training loss: 0.2915\n",
      "Epoch: 1558/2000... Training loss: 0.3667\n",
      "Epoch: 1559/2000... Training loss: 0.4120\n",
      "Epoch: 1559/2000... Training loss: 0.5621\n",
      "Epoch: 1559/2000... Training loss: 0.3601\n",
      "Epoch: 1559/2000... Training loss: 0.4196\n",
      "Epoch: 1559/2000... Training loss: 0.4094\n",
      "Epoch: 1559/2000... Training loss: 0.3275\n",
      "Epoch: 1559/2000... Training loss: 0.3957\n",
      "Epoch: 1559/2000... Training loss: 0.4888\n",
      "Epoch: 1559/2000... Training loss: 0.2216\n",
      "Epoch: 1559/2000... Training loss: 0.4801\n",
      "Epoch: 1559/2000... Training loss: 0.6266\n",
      "Epoch: 1559/2000... Training loss: 0.3669\n",
      "Epoch: 1559/2000... Training loss: 0.4138\n",
      "Epoch: 1559/2000... Training loss: 0.3817\n",
      "Epoch: 1559/2000... Training loss: 0.3468\n",
      "Epoch: 1559/2000... Training loss: 0.4261\n",
      "Epoch: 1559/2000... Training loss: 0.4694\n",
      "Epoch: 1559/2000... Training loss: 0.3720\n",
      "Epoch: 1559/2000... Training loss: 0.4143\n",
      "Epoch: 1559/2000... Training loss: 0.2966\n",
      "Epoch: 1559/2000... Training loss: 0.3289\n",
      "Epoch: 1559/2000... Training loss: 0.4564\n",
      "Epoch: 1559/2000... Training loss: 0.3933\n",
      "Epoch: 1559/2000... Training loss: 0.4990\n",
      "Epoch: 1559/2000... Training loss: 0.3442\n",
      "Epoch: 1559/2000... Training loss: 0.5778\n",
      "Epoch: 1559/2000... Training loss: 0.4592\n",
      "Epoch: 1559/2000... Training loss: 0.3681\n",
      "Epoch: 1559/2000... Training loss: 0.5353\n",
      "Epoch: 1559/2000... Training loss: 0.5077\n",
      "Epoch: 1559/2000... Training loss: 0.2301\n",
      "Epoch: 1560/2000... Training loss: 0.4730\n",
      "Epoch: 1560/2000... Training loss: 0.7656\n",
      "Epoch: 1560/2000... Training loss: 0.3744\n",
      "Epoch: 1560/2000... Training loss: 0.4622\n",
      "Epoch: 1560/2000... Training loss: 0.3960\n",
      "Epoch: 1560/2000... Training loss: 0.3174\n",
      "Epoch: 1560/2000... Training loss: 0.2537\n",
      "Epoch: 1560/2000... Training loss: 0.4296\n",
      "Epoch: 1560/2000... Training loss: 0.4081\n",
      "Epoch: 1560/2000... Training loss: 0.3829\n",
      "Epoch: 1560/2000... Training loss: 0.2942\n",
      "Epoch: 1560/2000... Training loss: 0.3337\n",
      "Epoch: 1560/2000... Training loss: 0.4772\n",
      "Epoch: 1560/2000... Training loss: 0.4282\n",
      "Epoch: 1560/2000... Training loss: 0.3024\n",
      "Epoch: 1560/2000... Training loss: 0.3779\n",
      "Epoch: 1560/2000... Training loss: 0.5010\n",
      "Epoch: 1560/2000... Training loss: 0.5275\n",
      "Epoch: 1560/2000... Training loss: 0.4084\n",
      "Epoch: 1560/2000... Training loss: 0.4615\n",
      "Epoch: 1560/2000... Training loss: 0.2739\n",
      "Epoch: 1560/2000... Training loss: 0.3358\n",
      "Epoch: 1560/2000... Training loss: 0.4712\n",
      "Epoch: 1560/2000... Training loss: 0.4342\n",
      "Epoch: 1560/2000... Training loss: 0.4120\n",
      "Epoch: 1560/2000... Training loss: 0.3494\n",
      "Epoch: 1560/2000... Training loss: 0.5551\n",
      "Epoch: 1560/2000... Training loss: 0.3074\n",
      "Epoch: 1560/2000... Training loss: 0.6407\n",
      "Epoch: 1560/2000... Training loss: 0.4324\n",
      "Epoch: 1560/2000... Training loss: 0.3209\n",
      "Epoch: 1561/2000... Training loss: 0.3689\n",
      "Epoch: 1561/2000... Training loss: 0.4005\n",
      "Epoch: 1561/2000... Training loss: 0.3995\n",
      "Epoch: 1561/2000... Training loss: 0.3599\n",
      "Epoch: 1561/2000... Training loss: 0.2595\n",
      "Epoch: 1561/2000... Training loss: 0.3596\n",
      "Epoch: 1561/2000... Training loss: 0.4622\n",
      "Epoch: 1561/2000... Training loss: 0.3763\n",
      "Epoch: 1561/2000... Training loss: 0.4877\n",
      "Epoch: 1561/2000... Training loss: 0.3998\n",
      "Epoch: 1561/2000... Training loss: 0.4385\n",
      "Epoch: 1561/2000... Training loss: 0.3802\n",
      "Epoch: 1561/2000... Training loss: 0.4309\n",
      "Epoch: 1561/2000... Training loss: 0.4892\n",
      "Epoch: 1561/2000... Training loss: 0.3712\n",
      "Epoch: 1561/2000... Training loss: 0.3799\n",
      "Epoch: 1561/2000... Training loss: 0.4634\n",
      "Epoch: 1561/2000... Training loss: 0.5739\n",
      "Epoch: 1561/2000... Training loss: 0.3919\n",
      "Epoch: 1561/2000... Training loss: 0.4698\n",
      "Epoch: 1561/2000... Training loss: 0.3024\n",
      "Epoch: 1561/2000... Training loss: 0.4896\n",
      "Epoch: 1561/2000... Training loss: 0.3717\n",
      "Epoch: 1561/2000... Training loss: 0.5644\n",
      "Epoch: 1561/2000... Training loss: 0.6008\n",
      "Epoch: 1561/2000... Training loss: 0.4242\n",
      "Epoch: 1561/2000... Training loss: 0.5312\n",
      "Epoch: 1561/2000... Training loss: 0.5082\n",
      "Epoch: 1561/2000... Training loss: 0.3847\n",
      "Epoch: 1561/2000... Training loss: 0.2403\n",
      "Epoch: 1561/2000... Training loss: 0.2787\n",
      "Epoch: 1562/2000... Training loss: 0.5060\n",
      "Epoch: 1562/2000... Training loss: 0.5466\n",
      "Epoch: 1562/2000... Training loss: 0.3037\n",
      "Epoch: 1562/2000... Training loss: 0.3995\n",
      "Epoch: 1562/2000... Training loss: 0.4530\n",
      "Epoch: 1562/2000... Training loss: 0.3678\n",
      "Epoch: 1562/2000... Training loss: 0.3415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1562/2000... Training loss: 0.4174\n",
      "Epoch: 1562/2000... Training loss: 0.4037\n",
      "Epoch: 1562/2000... Training loss: 0.3802\n",
      "Epoch: 1562/2000... Training loss: 0.2754\n",
      "Epoch: 1562/2000... Training loss: 0.3124\n",
      "Epoch: 1562/2000... Training loss: 0.4150\n",
      "Epoch: 1562/2000... Training loss: 0.2583\n",
      "Epoch: 1562/2000... Training loss: 0.4979\n",
      "Epoch: 1562/2000... Training loss: 0.3755\n",
      "Epoch: 1562/2000... Training loss: 0.4520\n",
      "Epoch: 1562/2000... Training loss: 0.3105\n",
      "Epoch: 1562/2000... Training loss: 0.3115\n",
      "Epoch: 1562/2000... Training loss: 0.2103\n",
      "Epoch: 1562/2000... Training loss: 0.5265\n",
      "Epoch: 1562/2000... Training loss: 0.2707\n",
      "Epoch: 1562/2000... Training loss: 0.4189\n",
      "Epoch: 1562/2000... Training loss: 0.3591\n",
      "Epoch: 1562/2000... Training loss: 0.3689\n",
      "Epoch: 1562/2000... Training loss: 0.4356\n",
      "Epoch: 1562/2000... Training loss: 0.3199\n",
      "Epoch: 1562/2000... Training loss: 0.2801\n",
      "Epoch: 1562/2000... Training loss: 0.4392\n",
      "Epoch: 1562/2000... Training loss: 0.5213\n",
      "Epoch: 1562/2000... Training loss: 0.4738\n",
      "Epoch: 1563/2000... Training loss: 0.3651\n",
      "Epoch: 1563/2000... Training loss: 0.4018\n",
      "Epoch: 1563/2000... Training loss: 0.5940\n",
      "Epoch: 1563/2000... Training loss: 0.3445\n",
      "Epoch: 1563/2000... Training loss: 0.3230\n",
      "Epoch: 1563/2000... Training loss: 0.4060\n",
      "Epoch: 1563/2000... Training loss: 0.3420\n",
      "Epoch: 1563/2000... Training loss: 0.3931\n",
      "Epoch: 1563/2000... Training loss: 0.3676\n",
      "Epoch: 1563/2000... Training loss: 0.3534\n",
      "Epoch: 1563/2000... Training loss: 0.4801\n",
      "Epoch: 1563/2000... Training loss: 0.5049\n",
      "Epoch: 1563/2000... Training loss: 0.3462\n",
      "Epoch: 1563/2000... Training loss: 0.4228\n",
      "Epoch: 1563/2000... Training loss: 0.3681\n",
      "Epoch: 1563/2000... Training loss: 0.4704\n",
      "Epoch: 1563/2000... Training loss: 0.4162\n",
      "Epoch: 1563/2000... Training loss: 0.3482\n",
      "Epoch: 1563/2000... Training loss: 0.5997\n",
      "Epoch: 1563/2000... Training loss: 0.4294\n",
      "Epoch: 1563/2000... Training loss: 0.3993\n",
      "Epoch: 1563/2000... Training loss: 0.2541\n",
      "Epoch: 1563/2000... Training loss: 0.4010\n",
      "Epoch: 1563/2000... Training loss: 0.4830\n",
      "Epoch: 1563/2000... Training loss: 0.4637\n",
      "Epoch: 1563/2000... Training loss: 0.3708\n",
      "Epoch: 1563/2000... Training loss: 0.3591\n",
      "Epoch: 1563/2000... Training loss: 0.3337\n",
      "Epoch: 1563/2000... Training loss: 0.2920\n",
      "Epoch: 1563/2000... Training loss: 0.3192\n",
      "Epoch: 1563/2000... Training loss: 0.4411\n",
      "Epoch: 1564/2000... Training loss: 0.4401\n",
      "Epoch: 1564/2000... Training loss: 0.4035\n",
      "Epoch: 1564/2000... Training loss: 0.4771\n",
      "Epoch: 1564/2000... Training loss: 0.3798\n",
      "Epoch: 1564/2000... Training loss: 0.5150\n",
      "Epoch: 1564/2000... Training loss: 0.3880\n",
      "Epoch: 1564/2000... Training loss: 0.3308\n",
      "Epoch: 1564/2000... Training loss: 0.4673\n",
      "Epoch: 1564/2000... Training loss: 0.4422\n",
      "Epoch: 1564/2000... Training loss: 0.3996\n",
      "Epoch: 1564/2000... Training loss: 0.4012\n",
      "Epoch: 1564/2000... Training loss: 0.3678\n",
      "Epoch: 1564/2000... Training loss: 0.4151\n",
      "Epoch: 1564/2000... Training loss: 0.3523\n",
      "Epoch: 1564/2000... Training loss: 0.4857\n",
      "Epoch: 1564/2000... Training loss: 0.5340\n",
      "Epoch: 1564/2000... Training loss: 0.4769\n",
      "Epoch: 1564/2000... Training loss: 0.5494\n",
      "Epoch: 1564/2000... Training loss: 0.4176\n",
      "Epoch: 1564/2000... Training loss: 0.3944\n",
      "Epoch: 1564/2000... Training loss: 0.2874\n",
      "Epoch: 1564/2000... Training loss: 0.4302\n",
      "Epoch: 1564/2000... Training loss: 0.3870\n",
      "Epoch: 1564/2000... Training loss: 0.3681\n",
      "Epoch: 1564/2000... Training loss: 0.3589\n",
      "Epoch: 1564/2000... Training loss: 0.4114\n",
      "Epoch: 1564/2000... Training loss: 0.6359\n",
      "Epoch: 1564/2000... Training loss: 0.3094\n",
      "Epoch: 1564/2000... Training loss: 0.4504\n",
      "Epoch: 1564/2000... Training loss: 0.4327\n",
      "Epoch: 1564/2000... Training loss: 0.2716\n",
      "Epoch: 1565/2000... Training loss: 0.3303\n",
      "Epoch: 1565/2000... Training loss: 0.3943\n",
      "Epoch: 1565/2000... Training loss: 0.4460\n",
      "Epoch: 1565/2000... Training loss: 0.3130\n",
      "Epoch: 1565/2000... Training loss: 0.2797\n",
      "Epoch: 1565/2000... Training loss: 0.3425\n",
      "Epoch: 1565/2000... Training loss: 0.2405\n",
      "Epoch: 1565/2000... Training loss: 0.3413\n",
      "Epoch: 1565/2000... Training loss: 0.3322\n",
      "Epoch: 1565/2000... Training loss: 0.3177\n",
      "Epoch: 1565/2000... Training loss: 0.4254\n",
      "Epoch: 1565/2000... Training loss: 0.2992\n",
      "Epoch: 1565/2000... Training loss: 0.3070\n",
      "Epoch: 1565/2000... Training loss: 0.4276\n",
      "Epoch: 1565/2000... Training loss: 0.4685\n",
      "Epoch: 1565/2000... Training loss: 0.2617\n",
      "Epoch: 1565/2000... Training loss: 0.4748\n",
      "Epoch: 1565/2000... Training loss: 0.4054\n",
      "Epoch: 1565/2000... Training loss: 0.3226\n",
      "Epoch: 1565/2000... Training loss: 0.5202\n",
      "Epoch: 1565/2000... Training loss: 0.4802\n",
      "Epoch: 1565/2000... Training loss: 0.3738\n",
      "Epoch: 1565/2000... Training loss: 0.2867\n",
      "Epoch: 1565/2000... Training loss: 0.2468\n",
      "Epoch: 1565/2000... Training loss: 0.3858\n",
      "Epoch: 1565/2000... Training loss: 0.4175\n",
      "Epoch: 1565/2000... Training loss: 0.3759\n",
      "Epoch: 1565/2000... Training loss: 0.3378\n",
      "Epoch: 1565/2000... Training loss: 0.3673\n",
      "Epoch: 1565/2000... Training loss: 0.3650\n",
      "Epoch: 1565/2000... Training loss: 0.5308\n",
      "Epoch: 1566/2000... Training loss: 0.3616\n",
      "Epoch: 1566/2000... Training loss: 0.3763\n",
      "Epoch: 1566/2000... Training loss: 0.4030\n",
      "Epoch: 1566/2000... Training loss: 0.3720\n",
      "Epoch: 1566/2000... Training loss: 0.3368\n",
      "Epoch: 1566/2000... Training loss: 0.4734\n",
      "Epoch: 1566/2000... Training loss: 0.3286\n",
      "Epoch: 1566/2000... Training loss: 0.4491\n",
      "Epoch: 1566/2000... Training loss: 0.4029\n",
      "Epoch: 1566/2000... Training loss: 0.3112\n",
      "Epoch: 1566/2000... Training loss: 0.2877\n",
      "Epoch: 1566/2000... Training loss: 0.3152\n",
      "Epoch: 1566/2000... Training loss: 0.2352\n",
      "Epoch: 1566/2000... Training loss: 0.4580\n",
      "Epoch: 1566/2000... Training loss: 0.2609\n",
      "Epoch: 1566/2000... Training loss: 0.3259\n",
      "Epoch: 1566/2000... Training loss: 0.4012\n",
      "Epoch: 1566/2000... Training loss: 0.2795\n",
      "Epoch: 1566/2000... Training loss: 0.4028\n",
      "Epoch: 1566/2000... Training loss: 0.2790\n",
      "Epoch: 1566/2000... Training loss: 0.3219\n",
      "Epoch: 1566/2000... Training loss: 0.4370\n",
      "Epoch: 1566/2000... Training loss: 0.4357\n",
      "Epoch: 1566/2000... Training loss: 0.5125\n",
      "Epoch: 1566/2000... Training loss: 0.4201\n",
      "Epoch: 1566/2000... Training loss: 0.4628\n",
      "Epoch: 1566/2000... Training loss: 0.4802\n",
      "Epoch: 1566/2000... Training loss: 0.3179\n",
      "Epoch: 1566/2000... Training loss: 0.3289\n",
      "Epoch: 1566/2000... Training loss: 0.5012\n",
      "Epoch: 1566/2000... Training loss: 0.2792\n",
      "Epoch: 1567/2000... Training loss: 0.4339\n",
      "Epoch: 1567/2000... Training loss: 0.4218\n",
      "Epoch: 1567/2000... Training loss: 0.3126\n",
      "Epoch: 1567/2000... Training loss: 0.4559\n",
      "Epoch: 1567/2000... Training loss: 0.2891\n",
      "Epoch: 1567/2000... Training loss: 0.4697\n",
      "Epoch: 1567/2000... Training loss: 0.4083\n",
      "Epoch: 1567/2000... Training loss: 0.2844\n",
      "Epoch: 1567/2000... Training loss: 0.3449\n",
      "Epoch: 1567/2000... Training loss: 0.4254\n",
      "Epoch: 1567/2000... Training loss: 0.3771\n",
      "Epoch: 1567/2000... Training loss: 0.3158\n",
      "Epoch: 1567/2000... Training loss: 0.2363\n",
      "Epoch: 1567/2000... Training loss: 0.5106\n",
      "Epoch: 1567/2000... Training loss: 0.4081\n",
      "Epoch: 1567/2000... Training loss: 0.5358\n",
      "Epoch: 1567/2000... Training loss: 0.1741\n",
      "Epoch: 1567/2000... Training loss: 0.3869\n",
      "Epoch: 1567/2000... Training loss: 0.2368\n",
      "Epoch: 1567/2000... Training loss: 0.4364\n",
      "Epoch: 1567/2000... Training loss: 0.4691\n",
      "Epoch: 1567/2000... Training loss: 0.1896\n",
      "Epoch: 1567/2000... Training loss: 0.5082\n",
      "Epoch: 1567/2000... Training loss: 0.3613\n",
      "Epoch: 1567/2000... Training loss: 0.4846\n",
      "Epoch: 1567/2000... Training loss: 0.3835\n",
      "Epoch: 1567/2000... Training loss: 0.4614\n",
      "Epoch: 1567/2000... Training loss: 0.3156\n",
      "Epoch: 1567/2000... Training loss: 0.3859\n",
      "Epoch: 1567/2000... Training loss: 0.2470\n",
      "Epoch: 1567/2000... Training loss: 0.5169\n",
      "Epoch: 1568/2000... Training loss: 0.4935\n",
      "Epoch: 1568/2000... Training loss: 0.3701\n",
      "Epoch: 1568/2000... Training loss: 0.4187\n",
      "Epoch: 1568/2000... Training loss: 0.3381\n",
      "Epoch: 1568/2000... Training loss: 0.4539\n",
      "Epoch: 1568/2000... Training loss: 0.4215\n",
      "Epoch: 1568/2000... Training loss: 0.5021\n",
      "Epoch: 1568/2000... Training loss: 0.5118\n",
      "Epoch: 1568/2000... Training loss: 0.3580\n",
      "Epoch: 1568/2000... Training loss: 0.4732\n",
      "Epoch: 1568/2000... Training loss: 0.4695\n",
      "Epoch: 1568/2000... Training loss: 0.4190\n",
      "Epoch: 1568/2000... Training loss: 0.2985\n",
      "Epoch: 1568/2000... Training loss: 0.4143\n",
      "Epoch: 1568/2000... Training loss: 0.4513\n",
      "Epoch: 1568/2000... Training loss: 0.4966\n",
      "Epoch: 1568/2000... Training loss: 0.5345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1568/2000... Training loss: 0.3790\n",
      "Epoch: 1568/2000... Training loss: 0.3909\n",
      "Epoch: 1568/2000... Training loss: 0.4742\n",
      "Epoch: 1568/2000... Training loss: 0.2973\n",
      "Epoch: 1568/2000... Training loss: 0.3224\n",
      "Epoch: 1568/2000... Training loss: 0.3088\n",
      "Epoch: 1568/2000... Training loss: 0.2892\n",
      "Epoch: 1568/2000... Training loss: 0.4670\n",
      "Epoch: 1568/2000... Training loss: 0.3107\n",
      "Epoch: 1568/2000... Training loss: 0.4314\n",
      "Epoch: 1568/2000... Training loss: 0.2641\n",
      "Epoch: 1568/2000... Training loss: 0.3848\n",
      "Epoch: 1568/2000... Training loss: 0.3728\n",
      "Epoch: 1568/2000... Training loss: 0.5354\n",
      "Epoch: 1569/2000... Training loss: 0.2141\n",
      "Epoch: 1569/2000... Training loss: 0.5239\n",
      "Epoch: 1569/2000... Training loss: 0.4114\n",
      "Epoch: 1569/2000... Training loss: 0.4982\n",
      "Epoch: 1569/2000... Training loss: 0.3178\n",
      "Epoch: 1569/2000... Training loss: 0.4571\n",
      "Epoch: 1569/2000... Training loss: 0.5795\n",
      "Epoch: 1569/2000... Training loss: 0.4129\n",
      "Epoch: 1569/2000... Training loss: 0.3219\n",
      "Epoch: 1569/2000... Training loss: 0.3273\n",
      "Epoch: 1569/2000... Training loss: 0.3386\n",
      "Epoch: 1569/2000... Training loss: 0.3868\n",
      "Epoch: 1569/2000... Training loss: 0.4719\n",
      "Epoch: 1569/2000... Training loss: 0.3198\n",
      "Epoch: 1569/2000... Training loss: 0.4033\n",
      "Epoch: 1569/2000... Training loss: 0.2313\n",
      "Epoch: 1569/2000... Training loss: 0.3455\n",
      "Epoch: 1569/2000... Training loss: 0.4809\n",
      "Epoch: 1569/2000... Training loss: 0.4972\n",
      "Epoch: 1569/2000... Training loss: 0.4120\n",
      "Epoch: 1569/2000... Training loss: 0.3743\n",
      "Epoch: 1569/2000... Training loss: 0.4657\n",
      "Epoch: 1569/2000... Training loss: 0.3160\n",
      "Epoch: 1569/2000... Training loss: 0.3564\n",
      "Epoch: 1569/2000... Training loss: 0.3910\n",
      "Epoch: 1569/2000... Training loss: 0.3602\n",
      "Epoch: 1569/2000... Training loss: 0.3356\n",
      "Epoch: 1569/2000... Training loss: 0.3961\n",
      "Epoch: 1569/2000... Training loss: 0.5744\n",
      "Epoch: 1569/2000... Training loss: 0.4709\n",
      "Epoch: 1569/2000... Training loss: 0.2947\n",
      "Epoch: 1570/2000... Training loss: 0.4194\n",
      "Epoch: 1570/2000... Training loss: 0.4309\n",
      "Epoch: 1570/2000... Training loss: 0.4799\n",
      "Epoch: 1570/2000... Training loss: 0.5164\n",
      "Epoch: 1570/2000... Training loss: 0.3981\n",
      "Epoch: 1570/2000... Training loss: 0.2217\n",
      "Epoch: 1570/2000... Training loss: 0.3708\n",
      "Epoch: 1570/2000... Training loss: 0.4108\n",
      "Epoch: 1570/2000... Training loss: 0.6260\n",
      "Epoch: 1570/2000... Training loss: 0.4684\n",
      "Epoch: 1570/2000... Training loss: 0.4098\n",
      "Epoch: 1570/2000... Training loss: 0.3254\n",
      "Epoch: 1570/2000... Training loss: 0.3364\n",
      "Epoch: 1570/2000... Training loss: 0.3996\n",
      "Epoch: 1570/2000... Training loss: 0.3830\n",
      "Epoch: 1570/2000... Training loss: 0.4813\n",
      "Epoch: 1570/2000... Training loss: 0.4861\n",
      "Epoch: 1570/2000... Training loss: 0.2826\n",
      "Epoch: 1570/2000... Training loss: 0.5562\n",
      "Epoch: 1570/2000... Training loss: 0.6122\n",
      "Epoch: 1570/2000... Training loss: 0.4138\n",
      "Epoch: 1570/2000... Training loss: 0.3755\n",
      "Epoch: 1570/2000... Training loss: 0.4986\n",
      "Epoch: 1570/2000... Training loss: 0.3050\n",
      "Epoch: 1570/2000... Training loss: 0.3215\n",
      "Epoch: 1570/2000... Training loss: 0.3305\n",
      "Epoch: 1570/2000... Training loss: 0.5106\n",
      "Epoch: 1570/2000... Training loss: 0.5175\n",
      "Epoch: 1570/2000... Training loss: 0.4278\n",
      "Epoch: 1570/2000... Training loss: 0.5538\n",
      "Epoch: 1570/2000... Training loss: 0.4249\n",
      "Epoch: 1571/2000... Training loss: 0.4748\n",
      "Epoch: 1571/2000... Training loss: 0.3848\n",
      "Epoch: 1571/2000... Training loss: 0.4416\n",
      "Epoch: 1571/2000... Training loss: 0.5817\n",
      "Epoch: 1571/2000... Training loss: 0.5721\n",
      "Epoch: 1571/2000... Training loss: 0.3507\n",
      "Epoch: 1571/2000... Training loss: 0.2867\n",
      "Epoch: 1571/2000... Training loss: 0.3562\n",
      "Epoch: 1571/2000... Training loss: 0.4415\n",
      "Epoch: 1571/2000... Training loss: 0.4145\n",
      "Epoch: 1571/2000... Training loss: 0.4833\n",
      "Epoch: 1571/2000... Training loss: 0.2886\n",
      "Epoch: 1571/2000... Training loss: 0.4026\n",
      "Epoch: 1571/2000... Training loss: 0.4898\n",
      "Epoch: 1571/2000... Training loss: 0.4055\n",
      "Epoch: 1571/2000... Training loss: 0.2971\n",
      "Epoch: 1571/2000... Training loss: 0.3102\n",
      "Epoch: 1571/2000... Training loss: 0.3230\n",
      "Epoch: 1571/2000... Training loss: 0.3393\n",
      "Epoch: 1571/2000... Training loss: 0.4522\n",
      "Epoch: 1571/2000... Training loss: 0.5408\n",
      "Epoch: 1571/2000... Training loss: 0.4083\n",
      "Epoch: 1571/2000... Training loss: 0.6001\n",
      "Epoch: 1571/2000... Training loss: 0.5074\n",
      "Epoch: 1571/2000... Training loss: 0.4789\n",
      "Epoch: 1571/2000... Training loss: 0.3576\n",
      "Epoch: 1571/2000... Training loss: 0.4361\n",
      "Epoch: 1571/2000... Training loss: 0.5247\n",
      "Epoch: 1571/2000... Training loss: 0.3121\n",
      "Epoch: 1571/2000... Training loss: 0.4384\n",
      "Epoch: 1571/2000... Training loss: 0.4660\n",
      "Epoch: 1572/2000... Training loss: 0.5112\n",
      "Epoch: 1572/2000... Training loss: 0.4720\n",
      "Epoch: 1572/2000... Training loss: 0.1846\n",
      "Epoch: 1572/2000... Training loss: 0.3589\n",
      "Epoch: 1572/2000... Training loss: 0.2736\n",
      "Epoch: 1572/2000... Training loss: 0.3752\n",
      "Epoch: 1572/2000... Training loss: 0.3247\n",
      "Epoch: 1572/2000... Training loss: 0.4381\n",
      "Epoch: 1572/2000... Training loss: 0.3487\n",
      "Epoch: 1572/2000... Training loss: 0.3765\n",
      "Epoch: 1572/2000... Training loss: 0.2881\n",
      "Epoch: 1572/2000... Training loss: 0.4402\n",
      "Epoch: 1572/2000... Training loss: 0.4881\n",
      "Epoch: 1572/2000... Training loss: 0.4465\n",
      "Epoch: 1572/2000... Training loss: 0.3412\n",
      "Epoch: 1572/2000... Training loss: 0.4248\n",
      "Epoch: 1572/2000... Training loss: 0.4077\n",
      "Epoch: 1572/2000... Training loss: 0.4056\n",
      "Epoch: 1572/2000... Training loss: 0.4235\n",
      "Epoch: 1572/2000... Training loss: 0.3365\n",
      "Epoch: 1572/2000... Training loss: 0.2756\n",
      "Epoch: 1572/2000... Training loss: 0.4971\n",
      "Epoch: 1572/2000... Training loss: 0.3763\n",
      "Epoch: 1572/2000... Training loss: 0.2581\n",
      "Epoch: 1572/2000... Training loss: 0.5732\n",
      "Epoch: 1572/2000... Training loss: 0.4912\n",
      "Epoch: 1572/2000... Training loss: 0.4994\n",
      "Epoch: 1572/2000... Training loss: 0.4700\n",
      "Epoch: 1572/2000... Training loss: 0.4105\n",
      "Epoch: 1572/2000... Training loss: 0.5275\n",
      "Epoch: 1572/2000... Training loss: 0.3322\n",
      "Epoch: 1573/2000... Training loss: 0.3910\n",
      "Epoch: 1573/2000... Training loss: 0.5444\n",
      "Epoch: 1573/2000... Training loss: 0.4935\n",
      "Epoch: 1573/2000... Training loss: 0.3530\n",
      "Epoch: 1573/2000... Training loss: 0.4542\n",
      "Epoch: 1573/2000... Training loss: 0.3951\n",
      "Epoch: 1573/2000... Training loss: 0.2118\n",
      "Epoch: 1573/2000... Training loss: 0.4040\n",
      "Epoch: 1573/2000... Training loss: 0.3607\n",
      "Epoch: 1573/2000... Training loss: 0.4676\n",
      "Epoch: 1573/2000... Training loss: 0.5375\n",
      "Epoch: 1573/2000... Training loss: 0.3048\n",
      "Epoch: 1573/2000... Training loss: 0.3494\n",
      "Epoch: 1573/2000... Training loss: 0.5283\n",
      "Epoch: 1573/2000... Training loss: 0.4154\n",
      "Epoch: 1573/2000... Training loss: 0.3274\n",
      "Epoch: 1573/2000... Training loss: 0.6869\n",
      "Epoch: 1573/2000... Training loss: 0.2556\n",
      "Epoch: 1573/2000... Training loss: 0.6088\n",
      "Epoch: 1573/2000... Training loss: 0.4794\n",
      "Epoch: 1573/2000... Training loss: 0.3558\n",
      "Epoch: 1573/2000... Training loss: 0.4076\n",
      "Epoch: 1573/2000... Training loss: 0.3311\n",
      "Epoch: 1573/2000... Training loss: 0.3979\n",
      "Epoch: 1573/2000... Training loss: 0.4348\n",
      "Epoch: 1573/2000... Training loss: 0.3584\n",
      "Epoch: 1573/2000... Training loss: 0.6066\n",
      "Epoch: 1573/2000... Training loss: 0.4693\n",
      "Epoch: 1573/2000... Training loss: 0.3840\n",
      "Epoch: 1573/2000... Training loss: 0.3411\n",
      "Epoch: 1573/2000... Training loss: 0.4477\n",
      "Epoch: 1574/2000... Training loss: 0.5618\n",
      "Epoch: 1574/2000... Training loss: 0.3570\n",
      "Epoch: 1574/2000... Training loss: 0.4280\n",
      "Epoch: 1574/2000... Training loss: 0.3065\n",
      "Epoch: 1574/2000... Training loss: 0.3988\n",
      "Epoch: 1574/2000... Training loss: 0.5620\n",
      "Epoch: 1574/2000... Training loss: 0.5430\n",
      "Epoch: 1574/2000... Training loss: 0.5143\n",
      "Epoch: 1574/2000... Training loss: 0.4002\n",
      "Epoch: 1574/2000... Training loss: 0.4193\n",
      "Epoch: 1574/2000... Training loss: 0.2817\n",
      "Epoch: 1574/2000... Training loss: 0.4256\n",
      "Epoch: 1574/2000... Training loss: 0.4803\n",
      "Epoch: 1574/2000... Training loss: 0.4766\n",
      "Epoch: 1574/2000... Training loss: 0.3653\n",
      "Epoch: 1574/2000... Training loss: 0.5038\n",
      "Epoch: 1574/2000... Training loss: 0.3881\n",
      "Epoch: 1574/2000... Training loss: 0.6152\n",
      "Epoch: 1574/2000... Training loss: 0.4934\n",
      "Epoch: 1574/2000... Training loss: 0.5696\n",
      "Epoch: 1574/2000... Training loss: 0.2546\n",
      "Epoch: 1574/2000... Training loss: 0.3304\n",
      "Epoch: 1574/2000... Training loss: 0.6317\n",
      "Epoch: 1574/2000... Training loss: 0.4067\n",
      "Epoch: 1574/2000... Training loss: 0.6066\n",
      "Epoch: 1574/2000... Training loss: 0.4252\n",
      "Epoch: 1574/2000... Training loss: 0.3948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1574/2000... Training loss: 0.4256\n",
      "Epoch: 1574/2000... Training loss: 0.4547\n",
      "Epoch: 1574/2000... Training loss: 0.5534\n",
      "Epoch: 1574/2000... Training loss: 0.3025\n",
      "Epoch: 1575/2000... Training loss: 0.2968\n",
      "Epoch: 1575/2000... Training loss: 0.4411\n",
      "Epoch: 1575/2000... Training loss: 0.3827\n",
      "Epoch: 1575/2000... Training loss: 0.4538\n",
      "Epoch: 1575/2000... Training loss: 0.3633\n",
      "Epoch: 1575/2000... Training loss: 0.3733\n",
      "Epoch: 1575/2000... Training loss: 0.3192\n",
      "Epoch: 1575/2000... Training loss: 0.3205\n",
      "Epoch: 1575/2000... Training loss: 0.2405\n",
      "Epoch: 1575/2000... Training loss: 0.4512\n",
      "Epoch: 1575/2000... Training loss: 0.3060\n",
      "Epoch: 1575/2000... Training loss: 0.3653\n",
      "Epoch: 1575/2000... Training loss: 0.3949\n",
      "Epoch: 1575/2000... Training loss: 0.3033\n",
      "Epoch: 1575/2000... Training loss: 0.4540\n",
      "Epoch: 1575/2000... Training loss: 0.3952\n",
      "Epoch: 1575/2000... Training loss: 0.3769\n",
      "Epoch: 1575/2000... Training loss: 0.3236\n",
      "Epoch: 1575/2000... Training loss: 0.3209\n",
      "Epoch: 1575/2000... Training loss: 0.4297\n",
      "Epoch: 1575/2000... Training loss: 0.3566\n",
      "Epoch: 1575/2000... Training loss: 0.4703\n",
      "Epoch: 1575/2000... Training loss: 0.5120\n",
      "Epoch: 1575/2000... Training loss: 0.4824\n",
      "Epoch: 1575/2000... Training loss: 0.3038\n",
      "Epoch: 1575/2000... Training loss: 0.4904\n",
      "Epoch: 1575/2000... Training loss: 0.1910\n",
      "Epoch: 1575/2000... Training loss: 0.4518\n",
      "Epoch: 1575/2000... Training loss: 0.3478\n",
      "Epoch: 1575/2000... Training loss: 0.3962\n",
      "Epoch: 1575/2000... Training loss: 0.5175\n",
      "Epoch: 1576/2000... Training loss: 0.2911\n",
      "Epoch: 1576/2000... Training loss: 0.4593\n",
      "Epoch: 1576/2000... Training loss: 0.4424\n",
      "Epoch: 1576/2000... Training loss: 0.4664\n",
      "Epoch: 1576/2000... Training loss: 0.2740\n",
      "Epoch: 1576/2000... Training loss: 0.3959\n",
      "Epoch: 1576/2000... Training loss: 0.3619\n",
      "Epoch: 1576/2000... Training loss: 0.3013\n",
      "Epoch: 1576/2000... Training loss: 0.3551\n",
      "Epoch: 1576/2000... Training loss: 0.3914\n",
      "Epoch: 1576/2000... Training loss: 0.4058\n",
      "Epoch: 1576/2000... Training loss: 0.5332\n",
      "Epoch: 1576/2000... Training loss: 0.3807\n",
      "Epoch: 1576/2000... Training loss: 0.4509\n",
      "Epoch: 1576/2000... Training loss: 0.3293\n",
      "Epoch: 1576/2000... Training loss: 0.3673\n",
      "Epoch: 1576/2000... Training loss: 0.4250\n",
      "Epoch: 1576/2000... Training loss: 0.4210\n",
      "Epoch: 1576/2000... Training loss: 0.3312\n",
      "Epoch: 1576/2000... Training loss: 0.3129\n",
      "Epoch: 1576/2000... Training loss: 0.5200\n",
      "Epoch: 1576/2000... Training loss: 0.3410\n",
      "Epoch: 1576/2000... Training loss: 0.4722\n",
      "Epoch: 1576/2000... Training loss: 0.3930\n",
      "Epoch: 1576/2000... Training loss: 0.2148\n",
      "Epoch: 1576/2000... Training loss: 0.3136\n",
      "Epoch: 1576/2000... Training loss: 0.3879\n",
      "Epoch: 1576/2000... Training loss: 0.3898\n",
      "Epoch: 1576/2000... Training loss: 0.2499\n",
      "Epoch: 1576/2000... Training loss: 0.3290\n",
      "Epoch: 1576/2000... Training loss: 0.3905\n",
      "Epoch: 1577/2000... Training loss: 0.6385\n",
      "Epoch: 1577/2000... Training loss: 0.3010\n",
      "Epoch: 1577/2000... Training loss: 0.4270\n",
      "Epoch: 1577/2000... Training loss: 0.4438\n",
      "Epoch: 1577/2000... Training loss: 0.4575\n",
      "Epoch: 1577/2000... Training loss: 0.3067\n",
      "Epoch: 1577/2000... Training loss: 0.5265\n",
      "Epoch: 1577/2000... Training loss: 0.5094\n",
      "Epoch: 1577/2000... Training loss: 0.4830\n",
      "Epoch: 1577/2000... Training loss: 0.3969\n",
      "Epoch: 1577/2000... Training loss: 0.3860\n",
      "Epoch: 1577/2000... Training loss: 0.3453\n",
      "Epoch: 1577/2000... Training loss: 0.3704\n",
      "Epoch: 1577/2000... Training loss: 0.4849\n",
      "Epoch: 1577/2000... Training loss: 0.3999\n",
      "Epoch: 1577/2000... Training loss: 0.4286\n",
      "Epoch: 1577/2000... Training loss: 0.2654\n",
      "Epoch: 1577/2000... Training loss: 0.5007\n",
      "Epoch: 1577/2000... Training loss: 0.2306\n",
      "Epoch: 1577/2000... Training loss: 0.5401\n",
      "Epoch: 1577/2000... Training loss: 0.5583\n",
      "Epoch: 1577/2000... Training loss: 0.6274\n",
      "Epoch: 1577/2000... Training loss: 0.6280\n",
      "Epoch: 1577/2000... Training loss: 0.3662\n",
      "Epoch: 1577/2000... Training loss: 0.3420\n",
      "Epoch: 1577/2000... Training loss: 0.4683\n",
      "Epoch: 1577/2000... Training loss: 0.4834\n",
      "Epoch: 1577/2000... Training loss: 0.4452\n",
      "Epoch: 1577/2000... Training loss: 0.6072\n",
      "Epoch: 1577/2000... Training loss: 0.4389\n",
      "Epoch: 1577/2000... Training loss: 0.4196\n",
      "Epoch: 1578/2000... Training loss: 0.5484\n",
      "Epoch: 1578/2000... Training loss: 0.4819\n",
      "Epoch: 1578/2000... Training loss: 0.3786\n",
      "Epoch: 1578/2000... Training loss: 0.4761\n",
      "Epoch: 1578/2000... Training loss: 0.3822\n",
      "Epoch: 1578/2000... Training loss: 0.3876\n",
      "Epoch: 1578/2000... Training loss: 0.3328\n",
      "Epoch: 1578/2000... Training loss: 0.3274\n",
      "Epoch: 1578/2000... Training loss: 0.4963\n",
      "Epoch: 1578/2000... Training loss: 0.5838\n",
      "Epoch: 1578/2000... Training loss: 0.4180\n",
      "Epoch: 1578/2000... Training loss: 0.2733\n",
      "Epoch: 1578/2000... Training loss: 0.4847\n",
      "Epoch: 1578/2000... Training loss: 0.3802\n",
      "Epoch: 1578/2000... Training loss: 0.3973\n",
      "Epoch: 1578/2000... Training loss: 0.2768\n",
      "Epoch: 1578/2000... Training loss: 0.4790\n",
      "Epoch: 1578/2000... Training loss: 0.3860\n",
      "Epoch: 1578/2000... Training loss: 0.6320\n",
      "Epoch: 1578/2000... Training loss: 0.3568\n",
      "Epoch: 1578/2000... Training loss: 0.4493\n",
      "Epoch: 1578/2000... Training loss: 0.3246\n",
      "Epoch: 1578/2000... Training loss: 0.4771\n",
      "Epoch: 1578/2000... Training loss: 0.5365\n",
      "Epoch: 1578/2000... Training loss: 0.4413\n",
      "Epoch: 1578/2000... Training loss: 0.4313\n",
      "Epoch: 1578/2000... Training loss: 0.3356\n",
      "Epoch: 1578/2000... Training loss: 0.2837\n",
      "Epoch: 1578/2000... Training loss: 0.3411\n",
      "Epoch: 1578/2000... Training loss: 0.3817\n",
      "Epoch: 1578/2000... Training loss: 0.4466\n",
      "Epoch: 1579/2000... Training loss: 0.4569\n",
      "Epoch: 1579/2000... Training loss: 0.2289\n",
      "Epoch: 1579/2000... Training loss: 0.4768\n",
      "Epoch: 1579/2000... Training loss: 0.4430\n",
      "Epoch: 1579/2000... Training loss: 0.3212\n",
      "Epoch: 1579/2000... Training loss: 0.3578\n",
      "Epoch: 1579/2000... Training loss: 0.3684\n",
      "Epoch: 1579/2000... Training loss: 0.3777\n",
      "Epoch: 1579/2000... Training loss: 0.5787\n",
      "Epoch: 1579/2000... Training loss: 0.3660\n",
      "Epoch: 1579/2000... Training loss: 0.4449\n",
      "Epoch: 1579/2000... Training loss: 0.3801\n",
      "Epoch: 1579/2000... Training loss: 0.3326\n",
      "Epoch: 1579/2000... Training loss: 0.3595\n",
      "Epoch: 1579/2000... Training loss: 0.5076\n",
      "Epoch: 1579/2000... Training loss: 0.4620\n",
      "Epoch: 1579/2000... Training loss: 0.4082\n",
      "Epoch: 1579/2000... Training loss: 0.5314\n",
      "Epoch: 1579/2000... Training loss: 0.7066\n",
      "Epoch: 1579/2000... Training loss: 0.4032\n",
      "Epoch: 1579/2000... Training loss: 0.2760\n",
      "Epoch: 1579/2000... Training loss: 0.3787\n",
      "Epoch: 1579/2000... Training loss: 0.3384\n",
      "Epoch: 1579/2000... Training loss: 0.3441\n",
      "Epoch: 1579/2000... Training loss: 0.5129\n",
      "Epoch: 1579/2000... Training loss: 0.3984\n",
      "Epoch: 1579/2000... Training loss: 0.4627\n",
      "Epoch: 1579/2000... Training loss: 0.3070\n",
      "Epoch: 1579/2000... Training loss: 0.4834\n",
      "Epoch: 1579/2000... Training loss: 0.4969\n",
      "Epoch: 1579/2000... Training loss: 0.2443\n",
      "Epoch: 1580/2000... Training loss: 0.3810\n",
      "Epoch: 1580/2000... Training loss: 0.4358\n",
      "Epoch: 1580/2000... Training loss: 0.3288\n",
      "Epoch: 1580/2000... Training loss: 0.3934\n",
      "Epoch: 1580/2000... Training loss: 0.2693\n",
      "Epoch: 1580/2000... Training loss: 0.4672\n",
      "Epoch: 1580/2000... Training loss: 0.3375\n",
      "Epoch: 1580/2000... Training loss: 0.2836\n",
      "Epoch: 1580/2000... Training loss: 0.3919\n",
      "Epoch: 1580/2000... Training loss: 0.4733\n",
      "Epoch: 1580/2000... Training loss: 0.3749\n",
      "Epoch: 1580/2000... Training loss: 0.3220\n",
      "Epoch: 1580/2000... Training loss: 0.3254\n",
      "Epoch: 1580/2000... Training loss: 0.5079\n",
      "Epoch: 1580/2000... Training loss: 0.4324\n",
      "Epoch: 1580/2000... Training loss: 0.3726\n",
      "Epoch: 1580/2000... Training loss: 0.4218\n",
      "Epoch: 1580/2000... Training loss: 0.3430\n",
      "Epoch: 1580/2000... Training loss: 0.5291\n",
      "Epoch: 1580/2000... Training loss: 0.4411\n",
      "Epoch: 1580/2000... Training loss: 0.3622\n",
      "Epoch: 1580/2000... Training loss: 0.7233\n",
      "Epoch: 1580/2000... Training loss: 0.4877\n",
      "Epoch: 1580/2000... Training loss: 0.2417\n",
      "Epoch: 1580/2000... Training loss: 0.3148\n",
      "Epoch: 1580/2000... Training loss: 0.4158\n",
      "Epoch: 1580/2000... Training loss: 0.4324\n",
      "Epoch: 1580/2000... Training loss: 0.2987\n",
      "Epoch: 1580/2000... Training loss: 0.3370\n",
      "Epoch: 1580/2000... Training loss: 0.4148\n",
      "Epoch: 1580/2000... Training loss: 0.3631\n",
      "Epoch: 1581/2000... Training loss: 0.5099\n",
      "Epoch: 1581/2000... Training loss: 0.3400\n",
      "Epoch: 1581/2000... Training loss: 0.3880\n",
      "Epoch: 1581/2000... Training loss: 0.2828\n",
      "Epoch: 1581/2000... Training loss: 0.4829\n",
      "Epoch: 1581/2000... Training loss: 0.2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1581/2000... Training loss: 0.3994\n",
      "Epoch: 1581/2000... Training loss: 0.5127\n",
      "Epoch: 1581/2000... Training loss: 0.3871\n",
      "Epoch: 1581/2000... Training loss: 0.4799\n",
      "Epoch: 1581/2000... Training loss: 0.4429\n",
      "Epoch: 1581/2000... Training loss: 0.4816\n",
      "Epoch: 1581/2000... Training loss: 0.4796\n",
      "Epoch: 1581/2000... Training loss: 0.5484\n",
      "Epoch: 1581/2000... Training loss: 0.4123\n",
      "Epoch: 1581/2000... Training loss: 0.4575\n",
      "Epoch: 1581/2000... Training loss: 0.6750\n",
      "Epoch: 1581/2000... Training loss: 0.3274\n",
      "Epoch: 1581/2000... Training loss: 0.3025\n",
      "Epoch: 1581/2000... Training loss: 0.4950\n",
      "Epoch: 1581/2000... Training loss: 0.5312\n",
      "Epoch: 1581/2000... Training loss: 0.5043\n",
      "Epoch: 1581/2000... Training loss: 0.3653\n",
      "Epoch: 1581/2000... Training loss: 0.3835\n",
      "Epoch: 1581/2000... Training loss: 0.3081\n",
      "Epoch: 1581/2000... Training loss: 0.4415\n",
      "Epoch: 1581/2000... Training loss: 0.3800\n",
      "Epoch: 1581/2000... Training loss: 0.4108\n",
      "Epoch: 1581/2000... Training loss: 0.3714\n",
      "Epoch: 1581/2000... Training loss: 0.3998\n",
      "Epoch: 1581/2000... Training loss: 0.4636\n",
      "Epoch: 1582/2000... Training loss: 0.3930\n",
      "Epoch: 1582/2000... Training loss: 0.3984\n",
      "Epoch: 1582/2000... Training loss: 0.5347\n",
      "Epoch: 1582/2000... Training loss: 0.3156\n",
      "Epoch: 1582/2000... Training loss: 0.2706\n",
      "Epoch: 1582/2000... Training loss: 0.3708\n",
      "Epoch: 1582/2000... Training loss: 0.6678\n",
      "Epoch: 1582/2000... Training loss: 0.4539\n",
      "Epoch: 1582/2000... Training loss: 0.2833\n",
      "Epoch: 1582/2000... Training loss: 0.4656\n",
      "Epoch: 1582/2000... Training loss: 0.3155\n",
      "Epoch: 1582/2000... Training loss: 0.4912\n",
      "Epoch: 1582/2000... Training loss: 0.2422\n",
      "Epoch: 1582/2000... Training loss: 0.5343\n",
      "Epoch: 1582/2000... Training loss: 0.4212\n",
      "Epoch: 1582/2000... Training loss: 0.4929\n",
      "Epoch: 1582/2000... Training loss: 0.5629\n",
      "Epoch: 1582/2000... Training loss: 0.2522\n",
      "Epoch: 1582/2000... Training loss: 0.6502\n",
      "Epoch: 1582/2000... Training loss: 0.3336\n",
      "Epoch: 1582/2000... Training loss: 0.4653\n",
      "Epoch: 1582/2000... Training loss: 0.2521\n",
      "Epoch: 1582/2000... Training loss: 0.3316\n",
      "Epoch: 1582/2000... Training loss: 0.4181\n",
      "Epoch: 1582/2000... Training loss: 0.5313\n",
      "Epoch: 1582/2000... Training loss: 0.5329\n",
      "Epoch: 1582/2000... Training loss: 0.4878\n",
      "Epoch: 1582/2000... Training loss: 0.3819\n",
      "Epoch: 1582/2000... Training loss: 0.2621\n",
      "Epoch: 1582/2000... Training loss: 0.4448\n",
      "Epoch: 1582/2000... Training loss: 0.4513\n",
      "Epoch: 1583/2000... Training loss: 0.3695\n",
      "Epoch: 1583/2000... Training loss: 0.5011\n",
      "Epoch: 1583/2000... Training loss: 0.3374\n",
      "Epoch: 1583/2000... Training loss: 0.3717\n",
      "Epoch: 1583/2000... Training loss: 0.3546\n",
      "Epoch: 1583/2000... Training loss: 0.4253\n",
      "Epoch: 1583/2000... Training loss: 0.4788\n",
      "Epoch: 1583/2000... Training loss: 0.3722\n",
      "Epoch: 1583/2000... Training loss: 0.2490\n",
      "Epoch: 1583/2000... Training loss: 0.2895\n",
      "Epoch: 1583/2000... Training loss: 0.3829\n",
      "Epoch: 1583/2000... Training loss: 0.3453\n",
      "Epoch: 1583/2000... Training loss: 0.4628\n",
      "Epoch: 1583/2000... Training loss: 0.4237\n",
      "Epoch: 1583/2000... Training loss: 0.4064\n",
      "Epoch: 1583/2000... Training loss: 0.2615\n",
      "Epoch: 1583/2000... Training loss: 0.4126\n",
      "Epoch: 1583/2000... Training loss: 0.5271\n",
      "Epoch: 1583/2000... Training loss: 0.3962\n",
      "Epoch: 1583/2000... Training loss: 0.4193\n",
      "Epoch: 1583/2000... Training loss: 0.2017\n",
      "Epoch: 1583/2000... Training loss: 0.4776\n",
      "Epoch: 1583/2000... Training loss: 0.3977\n",
      "Epoch: 1583/2000... Training loss: 0.3727\n",
      "Epoch: 1583/2000... Training loss: 0.3711\n",
      "Epoch: 1583/2000... Training loss: 0.5024\n",
      "Epoch: 1583/2000... Training loss: 0.4957\n",
      "Epoch: 1583/2000... Training loss: 0.6154\n",
      "Epoch: 1583/2000... Training loss: 0.6158\n",
      "Epoch: 1583/2000... Training loss: 0.4222\n",
      "Epoch: 1583/2000... Training loss: 0.6387\n",
      "Epoch: 1584/2000... Training loss: 0.5329\n",
      "Epoch: 1584/2000... Training loss: 0.5361\n",
      "Epoch: 1584/2000... Training loss: 0.3122\n",
      "Epoch: 1584/2000... Training loss: 0.4395\n",
      "Epoch: 1584/2000... Training loss: 0.4104\n",
      "Epoch: 1584/2000... Training loss: 0.3120\n",
      "Epoch: 1584/2000... Training loss: 0.4310\n",
      "Epoch: 1584/2000... Training loss: 0.3288\n",
      "Epoch: 1584/2000... Training loss: 0.3948\n",
      "Epoch: 1584/2000... Training loss: 0.3376\n",
      "Epoch: 1584/2000... Training loss: 0.3104\n",
      "Epoch: 1584/2000... Training loss: 0.2326\n",
      "Epoch: 1584/2000... Training loss: 0.4054\n",
      "Epoch: 1584/2000... Training loss: 0.4305\n",
      "Epoch: 1584/2000... Training loss: 0.3516\n",
      "Epoch: 1584/2000... Training loss: 0.4633\n",
      "Epoch: 1584/2000... Training loss: 0.2370\n",
      "Epoch: 1584/2000... Training loss: 0.4898\n",
      "Epoch: 1584/2000... Training loss: 0.6279\n",
      "Epoch: 1584/2000... Training loss: 0.4181\n",
      "Epoch: 1584/2000... Training loss: 0.4168\n",
      "Epoch: 1584/2000... Training loss: 0.3474\n",
      "Epoch: 1584/2000... Training loss: 0.2953\n",
      "Epoch: 1584/2000... Training loss: 0.3728\n",
      "Epoch: 1584/2000... Training loss: 0.2852\n",
      "Epoch: 1584/2000... Training loss: 0.2794\n",
      "Epoch: 1584/2000... Training loss: 0.3394\n",
      "Epoch: 1584/2000... Training loss: 0.4117\n",
      "Epoch: 1584/2000... Training loss: 0.4309\n",
      "Epoch: 1584/2000... Training loss: 0.2883\n",
      "Epoch: 1584/2000... Training loss: 0.3350\n",
      "Epoch: 1585/2000... Training loss: 0.2371\n",
      "Epoch: 1585/2000... Training loss: 0.3910\n",
      "Epoch: 1585/2000... Training loss: 0.3307\n",
      "Epoch: 1585/2000... Training loss: 0.4041\n",
      "Epoch: 1585/2000... Training loss: 0.2691\n",
      "Epoch: 1585/2000... Training loss: 0.6919\n",
      "Epoch: 1585/2000... Training loss: 0.3841\n",
      "Epoch: 1585/2000... Training loss: 0.4550\n",
      "Epoch: 1585/2000... Training loss: 0.2424\n",
      "Epoch: 1585/2000... Training loss: 0.3172\n",
      "Epoch: 1585/2000... Training loss: 0.3927\n",
      "Epoch: 1585/2000... Training loss: 0.3684\n",
      "Epoch: 1585/2000... Training loss: 0.5911\n",
      "Epoch: 1585/2000... Training loss: 0.3276\n",
      "Epoch: 1585/2000... Training loss: 0.4560\n",
      "Epoch: 1585/2000... Training loss: 0.3543\n",
      "Epoch: 1585/2000... Training loss: 0.4981\n",
      "Epoch: 1585/2000... Training loss: 0.4016\n",
      "Epoch: 1585/2000... Training loss: 0.2365\n",
      "Epoch: 1585/2000... Training loss: 0.4596\n",
      "Epoch: 1585/2000... Training loss: 0.3219\n",
      "Epoch: 1585/2000... Training loss: 0.3608\n",
      "Epoch: 1585/2000... Training loss: 0.3952\n",
      "Epoch: 1585/2000... Training loss: 0.4199\n",
      "Epoch: 1585/2000... Training loss: 0.2408\n",
      "Epoch: 1585/2000... Training loss: 0.3465\n",
      "Epoch: 1585/2000... Training loss: 0.4668\n",
      "Epoch: 1585/2000... Training loss: 0.3173\n",
      "Epoch: 1585/2000... Training loss: 0.3797\n",
      "Epoch: 1585/2000... Training loss: 0.3576\n",
      "Epoch: 1585/2000... Training loss: 0.3355\n",
      "Epoch: 1586/2000... Training loss: 0.3872\n",
      "Epoch: 1586/2000... Training loss: 0.3523\n",
      "Epoch: 1586/2000... Training loss: 0.3987\n",
      "Epoch: 1586/2000... Training loss: 0.5174\n",
      "Epoch: 1586/2000... Training loss: 0.3771\n",
      "Epoch: 1586/2000... Training loss: 0.2549\n",
      "Epoch: 1586/2000... Training loss: 0.4162\n",
      "Epoch: 1586/2000... Training loss: 0.2947\n",
      "Epoch: 1586/2000... Training loss: 0.5332\n",
      "Epoch: 1586/2000... Training loss: 0.3992\n",
      "Epoch: 1586/2000... Training loss: 0.6495\n",
      "Epoch: 1586/2000... Training loss: 0.4220\n",
      "Epoch: 1586/2000... Training loss: 0.4606\n",
      "Epoch: 1586/2000... Training loss: 0.3351\n",
      "Epoch: 1586/2000... Training loss: 0.4624\n",
      "Epoch: 1586/2000... Training loss: 0.4577\n",
      "Epoch: 1586/2000... Training loss: 0.3817\n",
      "Epoch: 1586/2000... Training loss: 0.5028\n",
      "Epoch: 1586/2000... Training loss: 0.3685\n",
      "Epoch: 1586/2000... Training loss: 0.3385\n",
      "Epoch: 1586/2000... Training loss: 0.3332\n",
      "Epoch: 1586/2000... Training loss: 0.2215\n",
      "Epoch: 1586/2000... Training loss: 0.3830\n",
      "Epoch: 1586/2000... Training loss: 0.3963\n",
      "Epoch: 1586/2000... Training loss: 0.4126\n",
      "Epoch: 1586/2000... Training loss: 0.4761\n",
      "Epoch: 1586/2000... Training loss: 0.5382\n",
      "Epoch: 1586/2000... Training loss: 0.2794\n",
      "Epoch: 1586/2000... Training loss: 0.4424\n",
      "Epoch: 1586/2000... Training loss: 0.5095\n",
      "Epoch: 1586/2000... Training loss: 0.4675\n",
      "Epoch: 1587/2000... Training loss: 0.3523\n",
      "Epoch: 1587/2000... Training loss: 0.4087\n",
      "Epoch: 1587/2000... Training loss: 0.3830\n",
      "Epoch: 1587/2000... Training loss: 0.4169\n",
      "Epoch: 1587/2000... Training loss: 0.3683\n",
      "Epoch: 1587/2000... Training loss: 0.3051\n",
      "Epoch: 1587/2000... Training loss: 0.4720\n",
      "Epoch: 1587/2000... Training loss: 0.4047\n",
      "Epoch: 1587/2000... Training loss: 0.4175\n",
      "Epoch: 1587/2000... Training loss: 0.2610\n",
      "Epoch: 1587/2000... Training loss: 0.5069\n",
      "Epoch: 1587/2000... Training loss: 0.3527\n",
      "Epoch: 1587/2000... Training loss: 0.4175\n",
      "Epoch: 1587/2000... Training loss: 0.5214\n",
      "Epoch: 1587/2000... Training loss: 0.4140\n",
      "Epoch: 1587/2000... Training loss: 0.5177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1587/2000... Training loss: 0.4045\n",
      "Epoch: 1587/2000... Training loss: 0.3962\n",
      "Epoch: 1587/2000... Training loss: 0.4990\n",
      "Epoch: 1587/2000... Training loss: 0.2915\n",
      "Epoch: 1587/2000... Training loss: 0.3705\n",
      "Epoch: 1587/2000... Training loss: 0.4349\n",
      "Epoch: 1587/2000... Training loss: 0.4023\n",
      "Epoch: 1587/2000... Training loss: 0.2725\n",
      "Epoch: 1587/2000... Training loss: 0.2852\n",
      "Epoch: 1587/2000... Training loss: 0.4138\n",
      "Epoch: 1587/2000... Training loss: 0.4770\n",
      "Epoch: 1587/2000... Training loss: 0.3973\n",
      "Epoch: 1587/2000... Training loss: 0.3695\n",
      "Epoch: 1587/2000... Training loss: 0.2918\n",
      "Epoch: 1587/2000... Training loss: 0.4801\n",
      "Epoch: 1588/2000... Training loss: 0.6619\n",
      "Epoch: 1588/2000... Training loss: 0.3721\n",
      "Epoch: 1588/2000... Training loss: 0.3165\n",
      "Epoch: 1588/2000... Training loss: 0.2994\n",
      "Epoch: 1588/2000... Training loss: 0.5776\n",
      "Epoch: 1588/2000... Training loss: 0.5279\n",
      "Epoch: 1588/2000... Training loss: 0.2627\n",
      "Epoch: 1588/2000... Training loss: 0.3029\n",
      "Epoch: 1588/2000... Training loss: 0.3293\n",
      "Epoch: 1588/2000... Training loss: 0.3152\n",
      "Epoch: 1588/2000... Training loss: 0.4081\n",
      "Epoch: 1588/2000... Training loss: 0.4073\n",
      "Epoch: 1588/2000... Training loss: 0.2865\n",
      "Epoch: 1588/2000... Training loss: 0.4179\n",
      "Epoch: 1588/2000... Training loss: 0.4280\n",
      "Epoch: 1588/2000... Training loss: 0.2316\n",
      "Epoch: 1588/2000... Training loss: 0.2824\n",
      "Epoch: 1588/2000... Training loss: 0.3571\n",
      "Epoch: 1588/2000... Training loss: 0.3463\n",
      "Epoch: 1588/2000... Training loss: 0.4707\n",
      "Epoch: 1588/2000... Training loss: 0.5473\n",
      "Epoch: 1588/2000... Training loss: 0.4293\n",
      "Epoch: 1588/2000... Training loss: 0.3962\n",
      "Epoch: 1588/2000... Training loss: 0.4012\n",
      "Epoch: 1588/2000... Training loss: 0.3947\n",
      "Epoch: 1588/2000... Training loss: 0.4383\n",
      "Epoch: 1588/2000... Training loss: 0.5885\n",
      "Epoch: 1588/2000... Training loss: 0.3891\n",
      "Epoch: 1588/2000... Training loss: 0.3776\n",
      "Epoch: 1588/2000... Training loss: 0.3888\n",
      "Epoch: 1588/2000... Training loss: 0.4272\n",
      "Epoch: 1589/2000... Training loss: 0.2553\n",
      "Epoch: 1589/2000... Training loss: 0.4125\n",
      "Epoch: 1589/2000... Training loss: 0.4121\n",
      "Epoch: 1589/2000... Training loss: 0.3434\n",
      "Epoch: 1589/2000... Training loss: 0.3570\n",
      "Epoch: 1589/2000... Training loss: 0.4421\n",
      "Epoch: 1589/2000... Training loss: 0.2371\n",
      "Epoch: 1589/2000... Training loss: 0.3182\n",
      "Epoch: 1589/2000... Training loss: 0.5467\n",
      "Epoch: 1589/2000... Training loss: 0.2035\n",
      "Epoch: 1589/2000... Training loss: 0.5204\n",
      "Epoch: 1589/2000... Training loss: 0.3562\n",
      "Epoch: 1589/2000... Training loss: 0.3290\n",
      "Epoch: 1589/2000... Training loss: 0.3009\n",
      "Epoch: 1589/2000... Training loss: 0.3250\n",
      "Epoch: 1589/2000... Training loss: 0.4561\n",
      "Epoch: 1589/2000... Training loss: 0.4543\n",
      "Epoch: 1589/2000... Training loss: 0.2642\n",
      "Epoch: 1589/2000... Training loss: 0.3279\n",
      "Epoch: 1589/2000... Training loss: 0.2849\n",
      "Epoch: 1589/2000... Training loss: 0.3137\n",
      "Epoch: 1589/2000... Training loss: 0.3163\n",
      "Epoch: 1589/2000... Training loss: 0.2707\n",
      "Epoch: 1589/2000... Training loss: 0.3830\n",
      "Epoch: 1589/2000... Training loss: 0.3861\n",
      "Epoch: 1589/2000... Training loss: 0.5095\n",
      "Epoch: 1589/2000... Training loss: 0.3778\n",
      "Epoch: 1589/2000... Training loss: 0.3161\n",
      "Epoch: 1589/2000... Training loss: 0.3235\n",
      "Epoch: 1589/2000... Training loss: 0.3559\n",
      "Epoch: 1589/2000... Training loss: 0.2139\n",
      "Epoch: 1590/2000... Training loss: 0.3227\n",
      "Epoch: 1590/2000... Training loss: 0.5210\n",
      "Epoch: 1590/2000... Training loss: 0.3048\n",
      "Epoch: 1590/2000... Training loss: 0.4314\n",
      "Epoch: 1590/2000... Training loss: 0.3588\n",
      "Epoch: 1590/2000... Training loss: 0.3973\n",
      "Epoch: 1590/2000... Training loss: 0.4278\n",
      "Epoch: 1590/2000... Training loss: 0.2941\n",
      "Epoch: 1590/2000... Training loss: 0.3987\n",
      "Epoch: 1590/2000... Training loss: 0.5299\n",
      "Epoch: 1590/2000... Training loss: 0.3220\n",
      "Epoch: 1590/2000... Training loss: 0.4537\n",
      "Epoch: 1590/2000... Training loss: 0.5644\n",
      "Epoch: 1590/2000... Training loss: 0.6044\n",
      "Epoch: 1590/2000... Training loss: 0.4154\n",
      "Epoch: 1590/2000... Training loss: 0.4150\n",
      "Epoch: 1590/2000... Training loss: 0.6056\n",
      "Epoch: 1590/2000... Training loss: 0.2969\n",
      "Epoch: 1590/2000... Training loss: 0.3995\n",
      "Epoch: 1590/2000... Training loss: 0.5829\n",
      "Epoch: 1590/2000... Training loss: 0.4004\n",
      "Epoch: 1590/2000... Training loss: 0.4202\n",
      "Epoch: 1590/2000... Training loss: 0.5491\n",
      "Epoch: 1590/2000... Training loss: 0.4578\n",
      "Epoch: 1590/2000... Training loss: 0.3974\n",
      "Epoch: 1590/2000... Training loss: 0.5608\n",
      "Epoch: 1590/2000... Training loss: 0.4029\n",
      "Epoch: 1590/2000... Training loss: 0.5911\n",
      "Epoch: 1590/2000... Training loss: 0.4012\n",
      "Epoch: 1590/2000... Training loss: 0.4056\n",
      "Epoch: 1590/2000... Training loss: 0.4128\n",
      "Epoch: 1591/2000... Training loss: 0.4050\n",
      "Epoch: 1591/2000... Training loss: 0.4650\n",
      "Epoch: 1591/2000... Training loss: 0.2936\n",
      "Epoch: 1591/2000... Training loss: 0.4862\n",
      "Epoch: 1591/2000... Training loss: 0.4147\n",
      "Epoch: 1591/2000... Training loss: 0.4635\n",
      "Epoch: 1591/2000... Training loss: 0.3826\n",
      "Epoch: 1591/2000... Training loss: 0.4228\n",
      "Epoch: 1591/2000... Training loss: 0.5047\n",
      "Epoch: 1591/2000... Training loss: 0.4310\n",
      "Epoch: 1591/2000... Training loss: 0.3496\n",
      "Epoch: 1591/2000... Training loss: 0.4263\n",
      "Epoch: 1591/2000... Training loss: 0.3011\n",
      "Epoch: 1591/2000... Training loss: 0.4311\n",
      "Epoch: 1591/2000... Training loss: 0.4084\n",
      "Epoch: 1591/2000... Training loss: 0.4660\n",
      "Epoch: 1591/2000... Training loss: 0.3053\n",
      "Epoch: 1591/2000... Training loss: 0.3398\n",
      "Epoch: 1591/2000... Training loss: 0.3924\n",
      "Epoch: 1591/2000... Training loss: 0.4518\n",
      "Epoch: 1591/2000... Training loss: 0.3336\n",
      "Epoch: 1591/2000... Training loss: 0.3308\n",
      "Epoch: 1591/2000... Training loss: 0.4121\n",
      "Epoch: 1591/2000... Training loss: 0.4142\n",
      "Epoch: 1591/2000... Training loss: 0.4118\n",
      "Epoch: 1591/2000... Training loss: 0.5053\n",
      "Epoch: 1591/2000... Training loss: 0.4628\n",
      "Epoch: 1591/2000... Training loss: 0.3070\n",
      "Epoch: 1591/2000... Training loss: 0.4452\n",
      "Epoch: 1591/2000... Training loss: 0.5975\n",
      "Epoch: 1591/2000... Training loss: 0.4120\n",
      "Epoch: 1592/2000... Training loss: 0.4627\n",
      "Epoch: 1592/2000... Training loss: 0.4486\n",
      "Epoch: 1592/2000... Training loss: 0.6237\n",
      "Epoch: 1592/2000... Training loss: 0.3412\n",
      "Epoch: 1592/2000... Training loss: 0.4976\n",
      "Epoch: 1592/2000... Training loss: 0.3775\n",
      "Epoch: 1592/2000... Training loss: 0.2386\n",
      "Epoch: 1592/2000... Training loss: 0.3553\n",
      "Epoch: 1592/2000... Training loss: 0.3074\n",
      "Epoch: 1592/2000... Training loss: 0.4239\n",
      "Epoch: 1592/2000... Training loss: 0.3618\n",
      "Epoch: 1592/2000... Training loss: 0.2934\n",
      "Epoch: 1592/2000... Training loss: 0.3036\n",
      "Epoch: 1592/2000... Training loss: 0.4755\n",
      "Epoch: 1592/2000... Training loss: 0.4540\n",
      "Epoch: 1592/2000... Training loss: 0.5560\n",
      "Epoch: 1592/2000... Training loss: 0.4499\n",
      "Epoch: 1592/2000... Training loss: 0.4619\n",
      "Epoch: 1592/2000... Training loss: 0.4543\n",
      "Epoch: 1592/2000... Training loss: 0.4395\n",
      "Epoch: 1592/2000... Training loss: 0.3660\n",
      "Epoch: 1592/2000... Training loss: 0.5549\n",
      "Epoch: 1592/2000... Training loss: 0.4095\n",
      "Epoch: 1592/2000... Training loss: 0.2956\n",
      "Epoch: 1592/2000... Training loss: 0.4652\n",
      "Epoch: 1592/2000... Training loss: 0.3214\n",
      "Epoch: 1592/2000... Training loss: 0.3890\n",
      "Epoch: 1592/2000... Training loss: 0.4655\n",
      "Epoch: 1592/2000... Training loss: 0.6675\n",
      "Epoch: 1592/2000... Training loss: 0.3377\n",
      "Epoch: 1592/2000... Training loss: 0.4486\n",
      "Epoch: 1593/2000... Training loss: 0.2341\n",
      "Epoch: 1593/2000... Training loss: 0.2964\n",
      "Epoch: 1593/2000... Training loss: 0.2280\n",
      "Epoch: 1593/2000... Training loss: 0.4768\n",
      "Epoch: 1593/2000... Training loss: 0.5186\n",
      "Epoch: 1593/2000... Training loss: 0.3439\n",
      "Epoch: 1593/2000... Training loss: 0.3392\n",
      "Epoch: 1593/2000... Training loss: 0.3092\n",
      "Epoch: 1593/2000... Training loss: 0.4949\n",
      "Epoch: 1593/2000... Training loss: 0.3813\n",
      "Epoch: 1593/2000... Training loss: 0.4809\n",
      "Epoch: 1593/2000... Training loss: 0.3366\n",
      "Epoch: 1593/2000... Training loss: 0.2591\n",
      "Epoch: 1593/2000... Training loss: 0.4610\n",
      "Epoch: 1593/2000... Training loss: 0.3416\n",
      "Epoch: 1593/2000... Training loss: 0.3859\n",
      "Epoch: 1593/2000... Training loss: 0.4619\n",
      "Epoch: 1593/2000... Training loss: 0.3044\n",
      "Epoch: 1593/2000... Training loss: 0.2303\n",
      "Epoch: 1593/2000... Training loss: 0.3759\n",
      "Epoch: 1593/2000... Training loss: 0.5244\n",
      "Epoch: 1593/2000... Training loss: 0.3834\n",
      "Epoch: 1593/2000... Training loss: 0.5411\n",
      "Epoch: 1593/2000... Training loss: 0.2678\n",
      "Epoch: 1593/2000... Training loss: 0.4220\n",
      "Epoch: 1593/2000... Training loss: 0.4836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1593/2000... Training loss: 0.3360\n",
      "Epoch: 1593/2000... Training loss: 0.2902\n",
      "Epoch: 1593/2000... Training loss: 0.4947\n",
      "Epoch: 1593/2000... Training loss: 0.6071\n",
      "Epoch: 1593/2000... Training loss: 0.3687\n",
      "Epoch: 1594/2000... Training loss: 0.4592\n",
      "Epoch: 1594/2000... Training loss: 0.4246\n",
      "Epoch: 1594/2000... Training loss: 0.5299\n",
      "Epoch: 1594/2000... Training loss: 0.4667\n",
      "Epoch: 1594/2000... Training loss: 0.3832\n",
      "Epoch: 1594/2000... Training loss: 0.3654\n",
      "Epoch: 1594/2000... Training loss: 0.4113\n",
      "Epoch: 1594/2000... Training loss: 0.4340\n",
      "Epoch: 1594/2000... Training loss: 0.1973\n",
      "Epoch: 1594/2000... Training loss: 0.4143\n",
      "Epoch: 1594/2000... Training loss: 0.3367\n",
      "Epoch: 1594/2000... Training loss: 0.3213\n",
      "Epoch: 1594/2000... Training loss: 0.5095\n",
      "Epoch: 1594/2000... Training loss: 0.3648\n",
      "Epoch: 1594/2000... Training loss: 0.4522\n",
      "Epoch: 1594/2000... Training loss: 0.3616\n",
      "Epoch: 1594/2000... Training loss: 0.4396\n",
      "Epoch: 1594/2000... Training loss: 0.3806\n",
      "Epoch: 1594/2000... Training loss: 0.4356\n",
      "Epoch: 1594/2000... Training loss: 0.4624\n",
      "Epoch: 1594/2000... Training loss: 0.3409\n",
      "Epoch: 1594/2000... Training loss: 0.3259\n",
      "Epoch: 1594/2000... Training loss: 0.6060\n",
      "Epoch: 1594/2000... Training loss: 0.2722\n",
      "Epoch: 1594/2000... Training loss: 0.3450\n",
      "Epoch: 1594/2000... Training loss: 0.5297\n",
      "Epoch: 1594/2000... Training loss: 0.7087\n",
      "Epoch: 1594/2000... Training loss: 0.4075\n",
      "Epoch: 1594/2000... Training loss: 0.3935\n",
      "Epoch: 1594/2000... Training loss: 0.4685\n",
      "Epoch: 1594/2000... Training loss: 0.3034\n",
      "Epoch: 1595/2000... Training loss: 0.4233\n",
      "Epoch: 1595/2000... Training loss: 0.5156\n",
      "Epoch: 1595/2000... Training loss: 0.4673\n",
      "Epoch: 1595/2000... Training loss: 0.4076\n",
      "Epoch: 1595/2000... Training loss: 0.2552\n",
      "Epoch: 1595/2000... Training loss: 0.4963\n",
      "Epoch: 1595/2000... Training loss: 0.3860\n",
      "Epoch: 1595/2000... Training loss: 0.4314\n",
      "Epoch: 1595/2000... Training loss: 0.3941\n",
      "Epoch: 1595/2000... Training loss: 0.4768\n",
      "Epoch: 1595/2000... Training loss: 0.5751\n",
      "Epoch: 1595/2000... Training loss: 0.4559\n",
      "Epoch: 1595/2000... Training loss: 0.3219\n",
      "Epoch: 1595/2000... Training loss: 0.2896\n",
      "Epoch: 1595/2000... Training loss: 0.3461\n",
      "Epoch: 1595/2000... Training loss: 0.3000\n",
      "Epoch: 1595/2000... Training loss: 0.5119\n",
      "Epoch: 1595/2000... Training loss: 0.4214\n",
      "Epoch: 1595/2000... Training loss: 0.4329\n",
      "Epoch: 1595/2000... Training loss: 0.3124\n",
      "Epoch: 1595/2000... Training loss: 0.4583\n",
      "Epoch: 1595/2000... Training loss: 0.2681\n",
      "Epoch: 1595/2000... Training loss: 0.3008\n",
      "Epoch: 1595/2000... Training loss: 0.4645\n",
      "Epoch: 1595/2000... Training loss: 0.6219\n",
      "Epoch: 1595/2000... Training loss: 0.4282\n",
      "Epoch: 1595/2000... Training loss: 0.2511\n",
      "Epoch: 1595/2000... Training loss: 0.3235\n",
      "Epoch: 1595/2000... Training loss: 0.3752\n",
      "Epoch: 1595/2000... Training loss: 0.5155\n",
      "Epoch: 1595/2000... Training loss: 0.3711\n",
      "Epoch: 1596/2000... Training loss: 0.5148\n",
      "Epoch: 1596/2000... Training loss: 0.2895\n",
      "Epoch: 1596/2000... Training loss: 0.2495\n",
      "Epoch: 1596/2000... Training loss: 0.3172\n",
      "Epoch: 1596/2000... Training loss: 0.3468\n",
      "Epoch: 1596/2000... Training loss: 0.5116\n",
      "Epoch: 1596/2000... Training loss: 0.2974\n",
      "Epoch: 1596/2000... Training loss: 0.4581\n",
      "Epoch: 1596/2000... Training loss: 0.4130\n",
      "Epoch: 1596/2000... Training loss: 0.3681\n",
      "Epoch: 1596/2000... Training loss: 0.4728\n",
      "Epoch: 1596/2000... Training loss: 0.3422\n",
      "Epoch: 1596/2000... Training loss: 0.4976\n",
      "Epoch: 1596/2000... Training loss: 0.4917\n",
      "Epoch: 1596/2000... Training loss: 0.3288\n",
      "Epoch: 1596/2000... Training loss: 0.3075\n",
      "Epoch: 1596/2000... Training loss: 0.3333\n",
      "Epoch: 1596/2000... Training loss: 0.4374\n",
      "Epoch: 1596/2000... Training loss: 0.5546\n",
      "Epoch: 1596/2000... Training loss: 0.3455\n",
      "Epoch: 1596/2000... Training loss: 0.4312\n",
      "Epoch: 1596/2000... Training loss: 0.3872\n",
      "Epoch: 1596/2000... Training loss: 0.4152\n",
      "Epoch: 1596/2000... Training loss: 0.3657\n",
      "Epoch: 1596/2000... Training loss: 0.4197\n",
      "Epoch: 1596/2000... Training loss: 0.2752\n",
      "Epoch: 1596/2000... Training loss: 0.5727\n",
      "Epoch: 1596/2000... Training loss: 0.6576\n",
      "Epoch: 1596/2000... Training loss: 0.4023\n",
      "Epoch: 1596/2000... Training loss: 0.6299\n",
      "Epoch: 1596/2000... Training loss: 0.3865\n",
      "Epoch: 1597/2000... Training loss: 0.2599\n",
      "Epoch: 1597/2000... Training loss: 0.4951\n",
      "Epoch: 1597/2000... Training loss: 0.3754\n",
      "Epoch: 1597/2000... Training loss: 0.5231\n",
      "Epoch: 1597/2000... Training loss: 0.5318\n",
      "Epoch: 1597/2000... Training loss: 0.4461\n",
      "Epoch: 1597/2000... Training loss: 0.2744\n",
      "Epoch: 1597/2000... Training loss: 0.3771\n",
      "Epoch: 1597/2000... Training loss: 0.2537\n",
      "Epoch: 1597/2000... Training loss: 0.3754\n",
      "Epoch: 1597/2000... Training loss: 0.2247\n",
      "Epoch: 1597/2000... Training loss: 0.4056\n",
      "Epoch: 1597/2000... Training loss: 0.4167\n",
      "Epoch: 1597/2000... Training loss: 0.3956\n",
      "Epoch: 1597/2000... Training loss: 0.4932\n",
      "Epoch: 1597/2000... Training loss: 0.4161\n",
      "Epoch: 1597/2000... Training loss: 0.3223\n",
      "Epoch: 1597/2000... Training loss: 0.5644\n",
      "Epoch: 1597/2000... Training loss: 0.2292\n",
      "Epoch: 1597/2000... Training loss: 0.3295\n",
      "Epoch: 1597/2000... Training loss: 0.3485\n",
      "Epoch: 1597/2000... Training loss: 0.3851\n",
      "Epoch: 1597/2000... Training loss: 0.2841\n",
      "Epoch: 1597/2000... Training loss: 0.4832\n",
      "Epoch: 1597/2000... Training loss: 0.2919\n",
      "Epoch: 1597/2000... Training loss: 0.3071\n",
      "Epoch: 1597/2000... Training loss: 0.3757\n",
      "Epoch: 1597/2000... Training loss: 0.3323\n",
      "Epoch: 1597/2000... Training loss: 0.5421\n",
      "Epoch: 1597/2000... Training loss: 0.3315\n",
      "Epoch: 1597/2000... Training loss: 0.4504\n",
      "Epoch: 1598/2000... Training loss: 0.3239\n",
      "Epoch: 1598/2000... Training loss: 0.3826\n",
      "Epoch: 1598/2000... Training loss: 0.3686\n",
      "Epoch: 1598/2000... Training loss: 0.4991\n",
      "Epoch: 1598/2000... Training loss: 0.4817\n",
      "Epoch: 1598/2000... Training loss: 0.4407\n",
      "Epoch: 1598/2000... Training loss: 0.2644\n",
      "Epoch: 1598/2000... Training loss: 0.2825\n",
      "Epoch: 1598/2000... Training loss: 0.4345\n",
      "Epoch: 1598/2000... Training loss: 0.4988\n",
      "Epoch: 1598/2000... Training loss: 0.3455\n",
      "Epoch: 1598/2000... Training loss: 0.3147\n",
      "Epoch: 1598/2000... Training loss: 0.5400\n",
      "Epoch: 1598/2000... Training loss: 0.3138\n",
      "Epoch: 1598/2000... Training loss: 0.5057\n",
      "Epoch: 1598/2000... Training loss: 0.3377\n",
      "Epoch: 1598/2000... Training loss: 0.4634\n",
      "Epoch: 1598/2000... Training loss: 0.2473\n",
      "Epoch: 1598/2000... Training loss: 0.4859\n",
      "Epoch: 1598/2000... Training loss: 0.3083\n",
      "Epoch: 1598/2000... Training loss: 0.2338\n",
      "Epoch: 1598/2000... Training loss: 0.3173\n",
      "Epoch: 1598/2000... Training loss: 0.3799\n",
      "Epoch: 1598/2000... Training loss: 0.2591\n",
      "Epoch: 1598/2000... Training loss: 0.4721\n",
      "Epoch: 1598/2000... Training loss: 0.7490\n",
      "Epoch: 1598/2000... Training loss: 0.4225\n",
      "Epoch: 1598/2000... Training loss: 0.3813\n",
      "Epoch: 1598/2000... Training loss: 0.2004\n",
      "Epoch: 1598/2000... Training loss: 0.4396\n",
      "Epoch: 1598/2000... Training loss: 0.3287\n",
      "Epoch: 1599/2000... Training loss: 0.4839\n",
      "Epoch: 1599/2000... Training loss: 0.4658\n",
      "Epoch: 1599/2000... Training loss: 0.3055\n",
      "Epoch: 1599/2000... Training loss: 0.4055\n",
      "Epoch: 1599/2000... Training loss: 0.3659\n",
      "Epoch: 1599/2000... Training loss: 0.3635\n",
      "Epoch: 1599/2000... Training loss: 0.3930\n",
      "Epoch: 1599/2000... Training loss: 0.4267\n",
      "Epoch: 1599/2000... Training loss: 0.2901\n",
      "Epoch: 1599/2000... Training loss: 0.3282\n",
      "Epoch: 1599/2000... Training loss: 0.4199\n",
      "Epoch: 1599/2000... Training loss: 0.3182\n",
      "Epoch: 1599/2000... Training loss: 0.4533\n",
      "Epoch: 1599/2000... Training loss: 0.3008\n",
      "Epoch: 1599/2000... Training loss: 0.3956\n",
      "Epoch: 1599/2000... Training loss: 0.3132\n",
      "Epoch: 1599/2000... Training loss: 0.3995\n",
      "Epoch: 1599/2000... Training loss: 0.3662\n",
      "Epoch: 1599/2000... Training loss: 0.4219\n",
      "Epoch: 1599/2000... Training loss: 0.5800\n",
      "Epoch: 1599/2000... Training loss: 0.3892\n",
      "Epoch: 1599/2000... Training loss: 0.2438\n",
      "Epoch: 1599/2000... Training loss: 0.2981\n",
      "Epoch: 1599/2000... Training loss: 0.5151\n",
      "Epoch: 1599/2000... Training loss: 0.3350\n",
      "Epoch: 1599/2000... Training loss: 0.3512\n",
      "Epoch: 1599/2000... Training loss: 0.3779\n",
      "Epoch: 1599/2000... Training loss: 0.2988\n",
      "Epoch: 1599/2000... Training loss: 0.5074\n",
      "Epoch: 1599/2000... Training loss: 0.4310\n",
      "Epoch: 1599/2000... Training loss: 0.3192\n",
      "Epoch: 1600/2000... Training loss: 0.3564\n",
      "Epoch: 1600/2000... Training loss: 0.5613\n",
      "Epoch: 1600/2000... Training loss: 0.4301\n",
      "Epoch: 1600/2000... Training loss: 0.5581\n",
      "Epoch: 1600/2000... Training loss: 0.4041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1600/2000... Training loss: 0.4796\n",
      "Epoch: 1600/2000... Training loss: 0.4099\n",
      "Epoch: 1600/2000... Training loss: 0.2721\n",
      "Epoch: 1600/2000... Training loss: 0.4153\n",
      "Epoch: 1600/2000... Training loss: 0.3465\n",
      "Epoch: 1600/2000... Training loss: 0.4172\n",
      "Epoch: 1600/2000... Training loss: 0.3918\n",
      "Epoch: 1600/2000... Training loss: 0.4885\n",
      "Epoch: 1600/2000... Training loss: 0.2231\n",
      "Epoch: 1600/2000... Training loss: 0.4355\n",
      "Epoch: 1600/2000... Training loss: 0.3361\n",
      "Epoch: 1600/2000... Training loss: 0.2905\n",
      "Epoch: 1600/2000... Training loss: 0.2561\n",
      "Epoch: 1600/2000... Training loss: 0.4488\n",
      "Epoch: 1600/2000... Training loss: 0.3705\n",
      "Epoch: 1600/2000... Training loss: 0.3531\n",
      "Epoch: 1600/2000... Training loss: 0.5653\n",
      "Epoch: 1600/2000... Training loss: 0.5370\n",
      "Epoch: 1600/2000... Training loss: 0.3226\n",
      "Epoch: 1600/2000... Training loss: 0.2706\n",
      "Epoch: 1600/2000... Training loss: 0.3500\n",
      "Epoch: 1600/2000... Training loss: 0.4575\n",
      "Epoch: 1600/2000... Training loss: 0.5782\n",
      "Epoch: 1600/2000... Training loss: 0.4377\n",
      "Epoch: 1600/2000... Training loss: 0.5035\n",
      "Epoch: 1600/2000... Training loss: 0.4149\n",
      "Epoch: 1601/2000... Training loss: 0.2836\n",
      "Epoch: 1601/2000... Training loss: 0.5360\n",
      "Epoch: 1601/2000... Training loss: 0.4319\n",
      "Epoch: 1601/2000... Training loss: 0.4988\n",
      "Epoch: 1601/2000... Training loss: 0.3702\n",
      "Epoch: 1601/2000... Training loss: 0.4114\n",
      "Epoch: 1601/2000... Training loss: 0.3691\n",
      "Epoch: 1601/2000... Training loss: 0.4462\n",
      "Epoch: 1601/2000... Training loss: 0.5809\n",
      "Epoch: 1601/2000... Training loss: 0.3490\n",
      "Epoch: 1601/2000... Training loss: 0.6268\n",
      "Epoch: 1601/2000... Training loss: 0.4438\n",
      "Epoch: 1601/2000... Training loss: 0.2784\n",
      "Epoch: 1601/2000... Training loss: 0.4657\n",
      "Epoch: 1601/2000... Training loss: 0.3031\n",
      "Epoch: 1601/2000... Training loss: 0.3325\n",
      "Epoch: 1601/2000... Training loss: 0.4911\n",
      "Epoch: 1601/2000... Training loss: 0.4304\n",
      "Epoch: 1601/2000... Training loss: 0.3810\n",
      "Epoch: 1601/2000... Training loss: 0.4568\n",
      "Epoch: 1601/2000... Training loss: 0.4972\n",
      "Epoch: 1601/2000... Training loss: 0.3698\n",
      "Epoch: 1601/2000... Training loss: 0.4049\n",
      "Epoch: 1601/2000... Training loss: 0.3996\n",
      "Epoch: 1601/2000... Training loss: 0.4784\n",
      "Epoch: 1601/2000... Training loss: 0.3979\n",
      "Epoch: 1601/2000... Training loss: 0.4175\n",
      "Epoch: 1601/2000... Training loss: 0.3837\n",
      "Epoch: 1601/2000... Training loss: 0.2453\n",
      "Epoch: 1601/2000... Training loss: 0.4841\n",
      "Epoch: 1601/2000... Training loss: 0.4449\n",
      "Epoch: 1602/2000... Training loss: 0.4229\n",
      "Epoch: 1602/2000... Training loss: 0.5777\n",
      "Epoch: 1602/2000... Training loss: 0.4770\n",
      "Epoch: 1602/2000... Training loss: 0.4922\n",
      "Epoch: 1602/2000... Training loss: 0.4750\n",
      "Epoch: 1602/2000... Training loss: 0.4666\n",
      "Epoch: 1602/2000... Training loss: 0.4353\n",
      "Epoch: 1602/2000... Training loss: 0.3570\n",
      "Epoch: 1602/2000... Training loss: 0.4605\n",
      "Epoch: 1602/2000... Training loss: 0.2159\n",
      "Epoch: 1602/2000... Training loss: 0.4532\n",
      "Epoch: 1602/2000... Training loss: 0.2507\n",
      "Epoch: 1602/2000... Training loss: 0.5382\n",
      "Epoch: 1602/2000... Training loss: 0.2085\n",
      "Epoch: 1602/2000... Training loss: 0.4258\n",
      "Epoch: 1602/2000... Training loss: 0.3357\n",
      "Epoch: 1602/2000... Training loss: 0.3514\n",
      "Epoch: 1602/2000... Training loss: 0.4011\n",
      "Epoch: 1602/2000... Training loss: 0.3789\n",
      "Epoch: 1602/2000... Training loss: 0.4896\n",
      "Epoch: 1602/2000... Training loss: 0.3478\n",
      "Epoch: 1602/2000... Training loss: 0.4002\n",
      "Epoch: 1602/2000... Training loss: 0.4460\n",
      "Epoch: 1602/2000... Training loss: 0.3626\n",
      "Epoch: 1602/2000... Training loss: 0.6463\n",
      "Epoch: 1602/2000... Training loss: 0.5060\n",
      "Epoch: 1602/2000... Training loss: 0.2644\n",
      "Epoch: 1602/2000... Training loss: 0.3788\n",
      "Epoch: 1602/2000... Training loss: 0.3857\n",
      "Epoch: 1602/2000... Training loss: 0.2552\n",
      "Epoch: 1602/2000... Training loss: 0.3953\n",
      "Epoch: 1603/2000... Training loss: 0.4040\n",
      "Epoch: 1603/2000... Training loss: 0.5193\n",
      "Epoch: 1603/2000... Training loss: 0.4817\n",
      "Epoch: 1603/2000... Training loss: 0.4077\n",
      "Epoch: 1603/2000... Training loss: 0.2637\n",
      "Epoch: 1603/2000... Training loss: 0.4576\n",
      "Epoch: 1603/2000... Training loss: 0.3259\n",
      "Epoch: 1603/2000... Training loss: 0.3346\n",
      "Epoch: 1603/2000... Training loss: 0.5280\n",
      "Epoch: 1603/2000... Training loss: 0.3705\n",
      "Epoch: 1603/2000... Training loss: 0.4560\n",
      "Epoch: 1603/2000... Training loss: 0.4548\n",
      "Epoch: 1603/2000... Training loss: 0.4207\n",
      "Epoch: 1603/2000... Training loss: 0.4419\n",
      "Epoch: 1603/2000... Training loss: 0.4464\n",
      "Epoch: 1603/2000... Training loss: 0.4592\n",
      "Epoch: 1603/2000... Training loss: 0.3926\n",
      "Epoch: 1603/2000... Training loss: 0.5467\n",
      "Epoch: 1603/2000... Training loss: 0.3378\n",
      "Epoch: 1603/2000... Training loss: 0.3653\n",
      "Epoch: 1603/2000... Training loss: 0.4340\n",
      "Epoch: 1603/2000... Training loss: 0.2646\n",
      "Epoch: 1603/2000... Training loss: 0.2603\n",
      "Epoch: 1603/2000... Training loss: 0.4574\n",
      "Epoch: 1603/2000... Training loss: 0.4432\n",
      "Epoch: 1603/2000... Training loss: 0.3189\n",
      "Epoch: 1603/2000... Training loss: 0.4296\n",
      "Epoch: 1603/2000... Training loss: 0.4316\n",
      "Epoch: 1603/2000... Training loss: 0.5364\n",
      "Epoch: 1603/2000... Training loss: 0.2669\n",
      "Epoch: 1603/2000... Training loss: 0.5045\n",
      "Epoch: 1604/2000... Training loss: 0.4473\n",
      "Epoch: 1604/2000... Training loss: 0.5110\n",
      "Epoch: 1604/2000... Training loss: 0.2575\n",
      "Epoch: 1604/2000... Training loss: 0.3475\n",
      "Epoch: 1604/2000... Training loss: 0.5797\n",
      "Epoch: 1604/2000... Training loss: 0.4119\n",
      "Epoch: 1604/2000... Training loss: 0.4371\n",
      "Epoch: 1604/2000... Training loss: 0.5659\n",
      "Epoch: 1604/2000... Training loss: 0.5015\n",
      "Epoch: 1604/2000... Training loss: 0.3104\n",
      "Epoch: 1604/2000... Training loss: 0.3997\n",
      "Epoch: 1604/2000... Training loss: 0.4549\n",
      "Epoch: 1604/2000... Training loss: 0.3988\n",
      "Epoch: 1604/2000... Training loss: 0.4697\n",
      "Epoch: 1604/2000... Training loss: 0.3981\n",
      "Epoch: 1604/2000... Training loss: 0.3497\n",
      "Epoch: 1604/2000... Training loss: 0.3163\n",
      "Epoch: 1604/2000... Training loss: 0.3950\n",
      "Epoch: 1604/2000... Training loss: 0.3001\n",
      "Epoch: 1604/2000... Training loss: 0.5281\n",
      "Epoch: 1604/2000... Training loss: 0.4033\n",
      "Epoch: 1604/2000... Training loss: 0.2187\n",
      "Epoch: 1604/2000... Training loss: 0.5175\n",
      "Epoch: 1604/2000... Training loss: 0.4922\n",
      "Epoch: 1604/2000... Training loss: 0.3897\n",
      "Epoch: 1604/2000... Training loss: 0.4505\n",
      "Epoch: 1604/2000... Training loss: 0.4482\n",
      "Epoch: 1604/2000... Training loss: 0.3773\n",
      "Epoch: 1604/2000... Training loss: 0.6878\n",
      "Epoch: 1604/2000... Training loss: 0.3531\n",
      "Epoch: 1604/2000... Training loss: 0.2592\n",
      "Epoch: 1605/2000... Training loss: 0.4943\n",
      "Epoch: 1605/2000... Training loss: 0.3752\n",
      "Epoch: 1605/2000... Training loss: 0.4636\n",
      "Epoch: 1605/2000... Training loss: 0.4887\n",
      "Epoch: 1605/2000... Training loss: 0.3298\n",
      "Epoch: 1605/2000... Training loss: 0.2798\n",
      "Epoch: 1605/2000... Training loss: 0.3951\n",
      "Epoch: 1605/2000... Training loss: 0.3513\n",
      "Epoch: 1605/2000... Training loss: 0.5898\n",
      "Epoch: 1605/2000... Training loss: 0.3117\n",
      "Epoch: 1605/2000... Training loss: 0.3945\n",
      "Epoch: 1605/2000... Training loss: 0.4871\n",
      "Epoch: 1605/2000... Training loss: 0.2713\n",
      "Epoch: 1605/2000... Training loss: 0.5542\n",
      "Epoch: 1605/2000... Training loss: 0.4150\n",
      "Epoch: 1605/2000... Training loss: 0.3749\n",
      "Epoch: 1605/2000... Training loss: 0.3517\n",
      "Epoch: 1605/2000... Training loss: 0.4533\n",
      "Epoch: 1605/2000... Training loss: 0.3827\n",
      "Epoch: 1605/2000... Training loss: 0.2449\n",
      "Epoch: 1605/2000... Training loss: 0.6832\n",
      "Epoch: 1605/2000... Training loss: 0.3537\n",
      "Epoch: 1605/2000... Training loss: 0.4538\n",
      "Epoch: 1605/2000... Training loss: 0.3241\n",
      "Epoch: 1605/2000... Training loss: 0.5553\n",
      "Epoch: 1605/2000... Training loss: 0.5486\n",
      "Epoch: 1605/2000... Training loss: 0.5422\n",
      "Epoch: 1605/2000... Training loss: 0.3147\n",
      "Epoch: 1605/2000... Training loss: 0.4606\n",
      "Epoch: 1605/2000... Training loss: 0.4209\n",
      "Epoch: 1605/2000... Training loss: 0.4172\n",
      "Epoch: 1606/2000... Training loss: 0.2794\n",
      "Epoch: 1606/2000... Training loss: 0.4567\n",
      "Epoch: 1606/2000... Training loss: 0.4760\n",
      "Epoch: 1606/2000... Training loss: 0.4493\n",
      "Epoch: 1606/2000... Training loss: 0.5972\n",
      "Epoch: 1606/2000... Training loss: 0.4968\n",
      "Epoch: 1606/2000... Training loss: 0.4348\n",
      "Epoch: 1606/2000... Training loss: 0.3404\n",
      "Epoch: 1606/2000... Training loss: 0.5876\n",
      "Epoch: 1606/2000... Training loss: 0.3710\n",
      "Epoch: 1606/2000... Training loss: 0.4453\n",
      "Epoch: 1606/2000... Training loss: 0.3227\n",
      "Epoch: 1606/2000... Training loss: 0.4719\n",
      "Epoch: 1606/2000... Training loss: 0.4113\n",
      "Epoch: 1606/2000... Training loss: 0.5859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1606/2000... Training loss: 0.3338\n",
      "Epoch: 1606/2000... Training loss: 0.4572\n",
      "Epoch: 1606/2000... Training loss: 0.3281\n",
      "Epoch: 1606/2000... Training loss: 0.1982\n",
      "Epoch: 1606/2000... Training loss: 0.2624\n",
      "Epoch: 1606/2000... Training loss: 0.3378\n",
      "Epoch: 1606/2000... Training loss: 0.4997\n",
      "Epoch: 1606/2000... Training loss: 0.5176\n",
      "Epoch: 1606/2000... Training loss: 0.4008\n",
      "Epoch: 1606/2000... Training loss: 0.4652\n",
      "Epoch: 1606/2000... Training loss: 0.2925\n",
      "Epoch: 1606/2000... Training loss: 0.4978\n",
      "Epoch: 1606/2000... Training loss: 0.4494\n",
      "Epoch: 1606/2000... Training loss: 0.3258\n",
      "Epoch: 1606/2000... Training loss: 0.7562\n",
      "Epoch: 1606/2000... Training loss: 0.4654\n",
      "Epoch: 1607/2000... Training loss: 0.4154\n",
      "Epoch: 1607/2000... Training loss: 0.5347\n",
      "Epoch: 1607/2000... Training loss: 0.4039\n",
      "Epoch: 1607/2000... Training loss: 0.4170\n",
      "Epoch: 1607/2000... Training loss: 0.4285\n",
      "Epoch: 1607/2000... Training loss: 0.3742\n",
      "Epoch: 1607/2000... Training loss: 0.5987\n",
      "Epoch: 1607/2000... Training loss: 0.5113\n",
      "Epoch: 1607/2000... Training loss: 0.4869\n",
      "Epoch: 1607/2000... Training loss: 0.3748\n",
      "Epoch: 1607/2000... Training loss: 0.3023\n",
      "Epoch: 1607/2000... Training loss: 0.2846\n",
      "Epoch: 1607/2000... Training loss: 0.4524\n",
      "Epoch: 1607/2000... Training loss: 0.4866\n",
      "Epoch: 1607/2000... Training loss: 0.3838\n",
      "Epoch: 1607/2000... Training loss: 0.3425\n",
      "Epoch: 1607/2000... Training loss: 0.2376\n",
      "Epoch: 1607/2000... Training loss: 0.4237\n",
      "Epoch: 1607/2000... Training loss: 0.3964\n",
      "Epoch: 1607/2000... Training loss: 0.5927\n",
      "Epoch: 1607/2000... Training loss: 0.3082\n",
      "Epoch: 1607/2000... Training loss: 0.4097\n",
      "Epoch: 1607/2000... Training loss: 0.4098\n",
      "Epoch: 1607/2000... Training loss: 0.4099\n",
      "Epoch: 1607/2000... Training loss: 0.4814\n",
      "Epoch: 1607/2000... Training loss: 0.4571\n",
      "Epoch: 1607/2000... Training loss: 0.3574\n",
      "Epoch: 1607/2000... Training loss: 0.3927\n",
      "Epoch: 1607/2000... Training loss: 0.3884\n",
      "Epoch: 1607/2000... Training loss: 0.2884\n",
      "Epoch: 1607/2000... Training loss: 0.5197\n",
      "Epoch: 1608/2000... Training loss: 0.4733\n",
      "Epoch: 1608/2000... Training loss: 0.3074\n",
      "Epoch: 1608/2000... Training loss: 0.5607\n",
      "Epoch: 1608/2000... Training loss: 0.2111\n",
      "Epoch: 1608/2000... Training loss: 0.3978\n",
      "Epoch: 1608/2000... Training loss: 0.3859\n",
      "Epoch: 1608/2000... Training loss: 0.5223\n",
      "Epoch: 1608/2000... Training loss: 0.5585\n",
      "Epoch: 1608/2000... Training loss: 0.4109\n",
      "Epoch: 1608/2000... Training loss: 0.4650\n",
      "Epoch: 1608/2000... Training loss: 0.4905\n",
      "Epoch: 1608/2000... Training loss: 0.2439\n",
      "Epoch: 1608/2000... Training loss: 0.3968\n",
      "Epoch: 1608/2000... Training loss: 0.3710\n",
      "Epoch: 1608/2000... Training loss: 0.3793\n",
      "Epoch: 1608/2000... Training loss: 0.4175\n",
      "Epoch: 1608/2000... Training loss: 0.2703\n",
      "Epoch: 1608/2000... Training loss: 0.6045\n",
      "Epoch: 1608/2000... Training loss: 0.3543\n",
      "Epoch: 1608/2000... Training loss: 0.2675\n",
      "Epoch: 1608/2000... Training loss: 0.3988\n",
      "Epoch: 1608/2000... Training loss: 0.3606\n",
      "Epoch: 1608/2000... Training loss: 0.3231\n",
      "Epoch: 1608/2000... Training loss: 0.3561\n",
      "Epoch: 1608/2000... Training loss: 0.3011\n",
      "Epoch: 1608/2000... Training loss: 0.2193\n",
      "Epoch: 1608/2000... Training loss: 0.4771\n",
      "Epoch: 1608/2000... Training loss: 0.3513\n",
      "Epoch: 1608/2000... Training loss: 0.3738\n",
      "Epoch: 1608/2000... Training loss: 0.3909\n",
      "Epoch: 1608/2000... Training loss: 0.3747\n",
      "Epoch: 1609/2000... Training loss: 0.5404\n",
      "Epoch: 1609/2000... Training loss: 0.3489\n",
      "Epoch: 1609/2000... Training loss: 0.3540\n",
      "Epoch: 1609/2000... Training loss: 0.4003\n",
      "Epoch: 1609/2000... Training loss: 0.3233\n",
      "Epoch: 1609/2000... Training loss: 0.4242\n",
      "Epoch: 1609/2000... Training loss: 0.3929\n",
      "Epoch: 1609/2000... Training loss: 0.1641\n",
      "Epoch: 1609/2000... Training loss: 0.2129\n",
      "Epoch: 1609/2000... Training loss: 0.3857\n",
      "Epoch: 1609/2000... Training loss: 0.2390\n",
      "Epoch: 1609/2000... Training loss: 0.5248\n",
      "Epoch: 1609/2000... Training loss: 0.3268\n",
      "Epoch: 1609/2000... Training loss: 0.4328\n",
      "Epoch: 1609/2000... Training loss: 0.4491\n",
      "Epoch: 1609/2000... Training loss: 0.2832\n",
      "Epoch: 1609/2000... Training loss: 0.4374\n",
      "Epoch: 1609/2000... Training loss: 0.2880\n",
      "Epoch: 1609/2000... Training loss: 0.3381\n",
      "Epoch: 1609/2000... Training loss: 0.3318\n",
      "Epoch: 1609/2000... Training loss: 0.4706\n",
      "Epoch: 1609/2000... Training loss: 0.4312\n",
      "Epoch: 1609/2000... Training loss: 0.4976\n",
      "Epoch: 1609/2000... Training loss: 0.2972\n",
      "Epoch: 1609/2000... Training loss: 0.2354\n",
      "Epoch: 1609/2000... Training loss: 0.6800\n",
      "Epoch: 1609/2000... Training loss: 0.2645\n",
      "Epoch: 1609/2000... Training loss: 0.4536\n",
      "Epoch: 1609/2000... Training loss: 0.5284\n",
      "Epoch: 1609/2000... Training loss: 0.3709\n",
      "Epoch: 1609/2000... Training loss: 0.4022\n",
      "Epoch: 1610/2000... Training loss: 0.4874\n",
      "Epoch: 1610/2000... Training loss: 0.3922\n",
      "Epoch: 1610/2000... Training loss: 0.3040\n",
      "Epoch: 1610/2000... Training loss: 0.3909\n",
      "Epoch: 1610/2000... Training loss: 0.4415\n",
      "Epoch: 1610/2000... Training loss: 0.5326\n",
      "Epoch: 1610/2000... Training loss: 0.3973\n",
      "Epoch: 1610/2000... Training loss: 0.3238\n",
      "Epoch: 1610/2000... Training loss: 0.4242\n",
      "Epoch: 1610/2000... Training loss: 0.4565\n",
      "Epoch: 1610/2000... Training loss: 0.4294\n",
      "Epoch: 1610/2000... Training loss: 0.5042\n",
      "Epoch: 1610/2000... Training loss: 0.3003\n",
      "Epoch: 1610/2000... Training loss: 0.4058\n",
      "Epoch: 1610/2000... Training loss: 0.5644\n",
      "Epoch: 1610/2000... Training loss: 0.4127\n",
      "Epoch: 1610/2000... Training loss: 0.1569\n",
      "Epoch: 1610/2000... Training loss: 0.3803\n",
      "Epoch: 1610/2000... Training loss: 0.3177\n",
      "Epoch: 1610/2000... Training loss: 0.3379\n",
      "Epoch: 1610/2000... Training loss: 0.2734\n",
      "Epoch: 1610/2000... Training loss: 0.5869\n",
      "Epoch: 1610/2000... Training loss: 0.4541\n",
      "Epoch: 1610/2000... Training loss: 0.3444\n",
      "Epoch: 1610/2000... Training loss: 0.3315\n",
      "Epoch: 1610/2000... Training loss: 0.3178\n",
      "Epoch: 1610/2000... Training loss: 0.4708\n",
      "Epoch: 1610/2000... Training loss: 0.3252\n",
      "Epoch: 1610/2000... Training loss: 0.3729\n",
      "Epoch: 1610/2000... Training loss: 0.3566\n",
      "Epoch: 1610/2000... Training loss: 0.5961\n",
      "Epoch: 1611/2000... Training loss: 0.4642\n",
      "Epoch: 1611/2000... Training loss: 0.6566\n",
      "Epoch: 1611/2000... Training loss: 0.3930\n",
      "Epoch: 1611/2000... Training loss: 0.5146\n",
      "Epoch: 1611/2000... Training loss: 0.3339\n",
      "Epoch: 1611/2000... Training loss: 0.4256\n",
      "Epoch: 1611/2000... Training loss: 0.4841\n",
      "Epoch: 1611/2000... Training loss: 0.3291\n",
      "Epoch: 1611/2000... Training loss: 0.5473\n",
      "Epoch: 1611/2000... Training loss: 0.2347\n",
      "Epoch: 1611/2000... Training loss: 0.3165\n",
      "Epoch: 1611/2000... Training loss: 0.3782\n",
      "Epoch: 1611/2000... Training loss: 0.3088\n",
      "Epoch: 1611/2000... Training loss: 0.4732\n",
      "Epoch: 1611/2000... Training loss: 0.3463\n",
      "Epoch: 1611/2000... Training loss: 0.2828\n",
      "Epoch: 1611/2000... Training loss: 0.4306\n",
      "Epoch: 1611/2000... Training loss: 0.3387\n",
      "Epoch: 1611/2000... Training loss: 0.4379\n",
      "Epoch: 1611/2000... Training loss: 0.3493\n",
      "Epoch: 1611/2000... Training loss: 0.3456\n",
      "Epoch: 1611/2000... Training loss: 0.4718\n",
      "Epoch: 1611/2000... Training loss: 0.3459\n",
      "Epoch: 1611/2000... Training loss: 0.3246\n",
      "Epoch: 1611/2000... Training loss: 0.3608\n",
      "Epoch: 1611/2000... Training loss: 0.5768\n",
      "Epoch: 1611/2000... Training loss: 0.3759\n",
      "Epoch: 1611/2000... Training loss: 0.3561\n",
      "Epoch: 1611/2000... Training loss: 0.2974\n",
      "Epoch: 1611/2000... Training loss: 0.4030\n",
      "Epoch: 1611/2000... Training loss: 0.4641\n",
      "Epoch: 1612/2000... Training loss: 0.4361\n",
      "Epoch: 1612/2000... Training loss: 0.3885\n",
      "Epoch: 1612/2000... Training loss: 0.3234\n",
      "Epoch: 1612/2000... Training loss: 0.5576\n",
      "Epoch: 1612/2000... Training loss: 0.6041\n",
      "Epoch: 1612/2000... Training loss: 0.4381\n",
      "Epoch: 1612/2000... Training loss: 0.3717\n",
      "Epoch: 1612/2000... Training loss: 0.4825\n",
      "Epoch: 1612/2000... Training loss: 0.4132\n",
      "Epoch: 1612/2000... Training loss: 0.3640\n",
      "Epoch: 1612/2000... Training loss: 0.3348\n",
      "Epoch: 1612/2000... Training loss: 0.3730\n",
      "Epoch: 1612/2000... Training loss: 0.4224\n",
      "Epoch: 1612/2000... Training loss: 0.3922\n",
      "Epoch: 1612/2000... Training loss: 0.3674\n",
      "Epoch: 1612/2000... Training loss: 0.6200\n",
      "Epoch: 1612/2000... Training loss: 0.4996\n",
      "Epoch: 1612/2000... Training loss: 0.2361\n",
      "Epoch: 1612/2000... Training loss: 0.5728\n",
      "Epoch: 1612/2000... Training loss: 0.2811\n",
      "Epoch: 1612/2000... Training loss: 0.2951\n",
      "Epoch: 1612/2000... Training loss: 0.3205\n",
      "Epoch: 1612/2000... Training loss: 0.4639\n",
      "Epoch: 1612/2000... Training loss: 0.4185\n",
      "Epoch: 1612/2000... Training loss: 0.3441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1612/2000... Training loss: 0.4088\n",
      "Epoch: 1612/2000... Training loss: 0.3548\n",
      "Epoch: 1612/2000... Training loss: 0.4160\n",
      "Epoch: 1612/2000... Training loss: 0.4164\n",
      "Epoch: 1612/2000... Training loss: 0.4680\n",
      "Epoch: 1612/2000... Training loss: 0.3633\n",
      "Epoch: 1613/2000... Training loss: 0.3091\n",
      "Epoch: 1613/2000... Training loss: 0.4128\n",
      "Epoch: 1613/2000... Training loss: 0.4185\n",
      "Epoch: 1613/2000... Training loss: 0.4584\n",
      "Epoch: 1613/2000... Training loss: 0.4119\n",
      "Epoch: 1613/2000... Training loss: 0.3403\n",
      "Epoch: 1613/2000... Training loss: 0.2932\n",
      "Epoch: 1613/2000... Training loss: 0.3876\n",
      "Epoch: 1613/2000... Training loss: 0.5951\n",
      "Epoch: 1613/2000... Training loss: 0.4789\n",
      "Epoch: 1613/2000... Training loss: 0.3924\n",
      "Epoch: 1613/2000... Training loss: 0.3031\n",
      "Epoch: 1613/2000... Training loss: 0.5478\n",
      "Epoch: 1613/2000... Training loss: 0.3751\n",
      "Epoch: 1613/2000... Training loss: 0.3254\n",
      "Epoch: 1613/2000... Training loss: 0.3474\n",
      "Epoch: 1613/2000... Training loss: 0.4216\n",
      "Epoch: 1613/2000... Training loss: 0.6133\n",
      "Epoch: 1613/2000... Training loss: 0.4729\n",
      "Epoch: 1613/2000... Training loss: 0.3739\n",
      "Epoch: 1613/2000... Training loss: 0.3438\n",
      "Epoch: 1613/2000... Training loss: 0.4294\n",
      "Epoch: 1613/2000... Training loss: 0.4590\n",
      "Epoch: 1613/2000... Training loss: 0.4091\n",
      "Epoch: 1613/2000... Training loss: 0.2669\n",
      "Epoch: 1613/2000... Training loss: 0.4001\n",
      "Epoch: 1613/2000... Training loss: 0.4539\n",
      "Epoch: 1613/2000... Training loss: 0.6373\n",
      "Epoch: 1613/2000... Training loss: 0.5554\n",
      "Epoch: 1613/2000... Training loss: 0.4870\n",
      "Epoch: 1613/2000... Training loss: 0.3523\n",
      "Epoch: 1614/2000... Training loss: 0.3112\n",
      "Epoch: 1614/2000... Training loss: 0.4229\n",
      "Epoch: 1614/2000... Training loss: 0.4670\n",
      "Epoch: 1614/2000... Training loss: 0.3019\n",
      "Epoch: 1614/2000... Training loss: 0.2965\n",
      "Epoch: 1614/2000... Training loss: 0.4116\n",
      "Epoch: 1614/2000... Training loss: 0.4570\n",
      "Epoch: 1614/2000... Training loss: 0.4111\n",
      "Epoch: 1614/2000... Training loss: 0.4731\n",
      "Epoch: 1614/2000... Training loss: 0.5554\n",
      "Epoch: 1614/2000... Training loss: 0.3807\n",
      "Epoch: 1614/2000... Training loss: 0.4370\n",
      "Epoch: 1614/2000... Training loss: 0.3016\n",
      "Epoch: 1614/2000... Training loss: 0.4366\n",
      "Epoch: 1614/2000... Training loss: 0.4247\n",
      "Epoch: 1614/2000... Training loss: 0.2401\n",
      "Epoch: 1614/2000... Training loss: 0.2591\n",
      "Epoch: 1614/2000... Training loss: 0.2120\n",
      "Epoch: 1614/2000... Training loss: 0.3957\n",
      "Epoch: 1614/2000... Training loss: 0.3815\n",
      "Epoch: 1614/2000... Training loss: 0.4806\n",
      "Epoch: 1614/2000... Training loss: 0.3463\n",
      "Epoch: 1614/2000... Training loss: 0.3487\n",
      "Epoch: 1614/2000... Training loss: 0.3154\n",
      "Epoch: 1614/2000... Training loss: 0.4751\n",
      "Epoch: 1614/2000... Training loss: 0.3177\n",
      "Epoch: 1614/2000... Training loss: 0.4108\n",
      "Epoch: 1614/2000... Training loss: 0.3315\n",
      "Epoch: 1614/2000... Training loss: 0.2709\n",
      "Epoch: 1614/2000... Training loss: 0.4626\n",
      "Epoch: 1614/2000... Training loss: 0.4816\n",
      "Epoch: 1615/2000... Training loss: 0.2551\n",
      "Epoch: 1615/2000... Training loss: 0.2859\n",
      "Epoch: 1615/2000... Training loss: 0.3472\n",
      "Epoch: 1615/2000... Training loss: 0.5045\n",
      "Epoch: 1615/2000... Training loss: 0.3292\n",
      "Epoch: 1615/2000... Training loss: 0.4163\n",
      "Epoch: 1615/2000... Training loss: 0.4438\n",
      "Epoch: 1615/2000... Training loss: 0.3791\n",
      "Epoch: 1615/2000... Training loss: 0.4228\n",
      "Epoch: 1615/2000... Training loss: 0.4191\n",
      "Epoch: 1615/2000... Training loss: 0.3472\n",
      "Epoch: 1615/2000... Training loss: 0.4879\n",
      "Epoch: 1615/2000... Training loss: 0.3723\n",
      "Epoch: 1615/2000... Training loss: 0.3709\n",
      "Epoch: 1615/2000... Training loss: 0.5461\n",
      "Epoch: 1615/2000... Training loss: 0.4313\n",
      "Epoch: 1615/2000... Training loss: 0.3762\n",
      "Epoch: 1615/2000... Training loss: 0.5704\n",
      "Epoch: 1615/2000... Training loss: 0.3398\n",
      "Epoch: 1615/2000... Training loss: 0.3149\n",
      "Epoch: 1615/2000... Training loss: 0.3915\n",
      "Epoch: 1615/2000... Training loss: 0.3979\n",
      "Epoch: 1615/2000... Training loss: 0.4418\n",
      "Epoch: 1615/2000... Training loss: 0.4290\n",
      "Epoch: 1615/2000... Training loss: 0.3622\n",
      "Epoch: 1615/2000... Training loss: 0.5312\n",
      "Epoch: 1615/2000... Training loss: 0.6053\n",
      "Epoch: 1615/2000... Training loss: 0.2790\n",
      "Epoch: 1615/2000... Training loss: 0.3374\n",
      "Epoch: 1615/2000... Training loss: 0.3832\n",
      "Epoch: 1615/2000... Training loss: 0.4763\n",
      "Epoch: 1616/2000... Training loss: 0.4071\n",
      "Epoch: 1616/2000... Training loss: 0.2635\n",
      "Epoch: 1616/2000... Training loss: 0.3438\n",
      "Epoch: 1616/2000... Training loss: 0.4474\n",
      "Epoch: 1616/2000... Training loss: 0.3325\n",
      "Epoch: 1616/2000... Training loss: 0.3229\n",
      "Epoch: 1616/2000... Training loss: 0.3228\n",
      "Epoch: 1616/2000... Training loss: 0.4220\n",
      "Epoch: 1616/2000... Training loss: 0.4192\n",
      "Epoch: 1616/2000... Training loss: 0.4051\n",
      "Epoch: 1616/2000... Training loss: 0.3252\n",
      "Epoch: 1616/2000... Training loss: 0.3300\n",
      "Epoch: 1616/2000... Training loss: 0.4450\n",
      "Epoch: 1616/2000... Training loss: 0.5617\n",
      "Epoch: 1616/2000... Training loss: 0.4281\n",
      "Epoch: 1616/2000... Training loss: 0.3262\n",
      "Epoch: 1616/2000... Training loss: 0.2293\n",
      "Epoch: 1616/2000... Training loss: 0.3840\n",
      "Epoch: 1616/2000... Training loss: 0.3721\n",
      "Epoch: 1616/2000... Training loss: 0.4239\n",
      "Epoch: 1616/2000... Training loss: 0.5755\n",
      "Epoch: 1616/2000... Training loss: 0.3171\n",
      "Epoch: 1616/2000... Training loss: 0.3316\n",
      "Epoch: 1616/2000... Training loss: 0.3229\n",
      "Epoch: 1616/2000... Training loss: 0.3235\n",
      "Epoch: 1616/2000... Training loss: 0.3280\n",
      "Epoch: 1616/2000... Training loss: 0.3890\n",
      "Epoch: 1616/2000... Training loss: 0.2642\n",
      "Epoch: 1616/2000... Training loss: 0.3762\n",
      "Epoch: 1616/2000... Training loss: 0.3860\n",
      "Epoch: 1616/2000... Training loss: 0.3371\n",
      "Epoch: 1617/2000... Training loss: 0.2089\n",
      "Epoch: 1617/2000... Training loss: 0.3998\n",
      "Epoch: 1617/2000... Training loss: 0.2930\n",
      "Epoch: 1617/2000... Training loss: 0.2517\n",
      "Epoch: 1617/2000... Training loss: 0.3030\n",
      "Epoch: 1617/2000... Training loss: 0.6791\n",
      "Epoch: 1617/2000... Training loss: 0.4308\n",
      "Epoch: 1617/2000... Training loss: 0.4257\n",
      "Epoch: 1617/2000... Training loss: 0.3065\n",
      "Epoch: 1617/2000... Training loss: 0.3463\n",
      "Epoch: 1617/2000... Training loss: 0.3670\n",
      "Epoch: 1617/2000... Training loss: 0.4212\n",
      "Epoch: 1617/2000... Training loss: 0.2961\n",
      "Epoch: 1617/2000... Training loss: 0.3233\n",
      "Epoch: 1617/2000... Training loss: 0.4110\n",
      "Epoch: 1617/2000... Training loss: 0.2530\n",
      "Epoch: 1617/2000... Training loss: 0.6105\n",
      "Epoch: 1617/2000... Training loss: 0.3510\n",
      "Epoch: 1617/2000... Training loss: 0.4909\n",
      "Epoch: 1617/2000... Training loss: 0.4324\n",
      "Epoch: 1617/2000... Training loss: 0.3293\n",
      "Epoch: 1617/2000... Training loss: 0.3670\n",
      "Epoch: 1617/2000... Training loss: 0.3152\n",
      "Epoch: 1617/2000... Training loss: 0.3423\n",
      "Epoch: 1617/2000... Training loss: 0.3390\n",
      "Epoch: 1617/2000... Training loss: 0.3396\n",
      "Epoch: 1617/2000... Training loss: 0.2917\n",
      "Epoch: 1617/2000... Training loss: 0.3419\n",
      "Epoch: 1617/2000... Training loss: 0.2896\n",
      "Epoch: 1617/2000... Training loss: 0.3252\n",
      "Epoch: 1617/2000... Training loss: 0.6299\n",
      "Epoch: 1618/2000... Training loss: 0.3550\n",
      "Epoch: 1618/2000... Training loss: 0.3739\n",
      "Epoch: 1618/2000... Training loss: 0.3420\n",
      "Epoch: 1618/2000... Training loss: 0.2839\n",
      "Epoch: 1618/2000... Training loss: 0.3182\n",
      "Epoch: 1618/2000... Training loss: 0.3153\n",
      "Epoch: 1618/2000... Training loss: 0.3206\n",
      "Epoch: 1618/2000... Training loss: 0.4198\n",
      "Epoch: 1618/2000... Training loss: 0.4402\n",
      "Epoch: 1618/2000... Training loss: 0.5569\n",
      "Epoch: 1618/2000... Training loss: 0.2941\n",
      "Epoch: 1618/2000... Training loss: 0.3191\n",
      "Epoch: 1618/2000... Training loss: 0.3950\n",
      "Epoch: 1618/2000... Training loss: 0.3316\n",
      "Epoch: 1618/2000... Training loss: 0.4421\n",
      "Epoch: 1618/2000... Training loss: 0.5449\n",
      "Epoch: 1618/2000... Training loss: 0.5074\n",
      "Epoch: 1618/2000... Training loss: 0.3175\n",
      "Epoch: 1618/2000... Training loss: 0.2736\n",
      "Epoch: 1618/2000... Training loss: 0.3460\n",
      "Epoch: 1618/2000... Training loss: 0.2914\n",
      "Epoch: 1618/2000... Training loss: 0.4299\n",
      "Epoch: 1618/2000... Training loss: 0.3269\n",
      "Epoch: 1618/2000... Training loss: 0.4302\n",
      "Epoch: 1618/2000... Training loss: 0.4402\n",
      "Epoch: 1618/2000... Training loss: 0.3652\n",
      "Epoch: 1618/2000... Training loss: 0.5028\n",
      "Epoch: 1618/2000... Training loss: 0.4790\n",
      "Epoch: 1618/2000... Training loss: 0.3860\n",
      "Epoch: 1618/2000... Training loss: 0.4428\n",
      "Epoch: 1618/2000... Training loss: 0.3790\n",
      "Epoch: 1619/2000... Training loss: 0.3411\n",
      "Epoch: 1619/2000... Training loss: 0.5323\n",
      "Epoch: 1619/2000... Training loss: 0.5006\n",
      "Epoch: 1619/2000... Training loss: 0.3747\n",
      "Epoch: 1619/2000... Training loss: 0.4084\n",
      "Epoch: 1619/2000... Training loss: 0.2495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1619/2000... Training loss: 0.5276\n",
      "Epoch: 1619/2000... Training loss: 0.5182\n",
      "Epoch: 1619/2000... Training loss: 0.4176\n",
      "Epoch: 1619/2000... Training loss: 0.3132\n",
      "Epoch: 1619/2000... Training loss: 0.3752\n",
      "Epoch: 1619/2000... Training loss: 0.3685\n",
      "Epoch: 1619/2000... Training loss: 0.3741\n",
      "Epoch: 1619/2000... Training loss: 0.3302\n",
      "Epoch: 1619/2000... Training loss: 0.3694\n",
      "Epoch: 1619/2000... Training loss: 0.2305\n",
      "Epoch: 1619/2000... Training loss: 0.3651\n",
      "Epoch: 1619/2000... Training loss: 0.3573\n",
      "Epoch: 1619/2000... Training loss: 0.4240\n",
      "Epoch: 1619/2000... Training loss: 0.3091\n",
      "Epoch: 1619/2000... Training loss: 0.2760\n",
      "Epoch: 1619/2000... Training loss: 0.4334\n",
      "Epoch: 1619/2000... Training loss: 0.4469\n",
      "Epoch: 1619/2000... Training loss: 0.4401\n",
      "Epoch: 1619/2000... Training loss: 0.4931\n",
      "Epoch: 1619/2000... Training loss: 0.3464\n",
      "Epoch: 1619/2000... Training loss: 0.3628\n",
      "Epoch: 1619/2000... Training loss: 0.3356\n",
      "Epoch: 1619/2000... Training loss: 0.5111\n",
      "Epoch: 1619/2000... Training loss: 0.4104\n",
      "Epoch: 1619/2000... Training loss: 0.3857\n",
      "Epoch: 1620/2000... Training loss: 0.4461\n",
      "Epoch: 1620/2000... Training loss: 0.3481\n",
      "Epoch: 1620/2000... Training loss: 0.4092\n",
      "Epoch: 1620/2000... Training loss: 0.3062\n",
      "Epoch: 1620/2000... Training loss: 0.4250\n",
      "Epoch: 1620/2000... Training loss: 0.4263\n",
      "Epoch: 1620/2000... Training loss: 0.3617\n",
      "Epoch: 1620/2000... Training loss: 0.5798\n",
      "Epoch: 1620/2000... Training loss: 0.4032\n",
      "Epoch: 1620/2000... Training loss: 0.5036\n",
      "Epoch: 1620/2000... Training loss: 0.5121\n",
      "Epoch: 1620/2000... Training loss: 0.5669\n",
      "Epoch: 1620/2000... Training loss: 0.4576\n",
      "Epoch: 1620/2000... Training loss: 0.3768\n",
      "Epoch: 1620/2000... Training loss: 0.3866\n",
      "Epoch: 1620/2000... Training loss: 0.4610\n",
      "Epoch: 1620/2000... Training loss: 0.4255\n",
      "Epoch: 1620/2000... Training loss: 0.4955\n",
      "Epoch: 1620/2000... Training loss: 0.4070\n",
      "Epoch: 1620/2000... Training loss: 0.3314\n",
      "Epoch: 1620/2000... Training loss: 0.2962\n",
      "Epoch: 1620/2000... Training loss: 0.2117\n",
      "Epoch: 1620/2000... Training loss: 0.5803\n",
      "Epoch: 1620/2000... Training loss: 0.3382\n",
      "Epoch: 1620/2000... Training loss: 0.3950\n",
      "Epoch: 1620/2000... Training loss: 0.5160\n",
      "Epoch: 1620/2000... Training loss: 0.3620\n",
      "Epoch: 1620/2000... Training loss: 0.4598\n",
      "Epoch: 1620/2000... Training loss: 0.2512\n",
      "Epoch: 1620/2000... Training loss: 0.3652\n",
      "Epoch: 1620/2000... Training loss: 0.3893\n",
      "Epoch: 1621/2000... Training loss: 0.3811\n",
      "Epoch: 1621/2000... Training loss: 0.4742\n",
      "Epoch: 1621/2000... Training loss: 0.4352\n",
      "Epoch: 1621/2000... Training loss: 0.5319\n",
      "Epoch: 1621/2000... Training loss: 0.4569\n",
      "Epoch: 1621/2000... Training loss: 0.2182\n",
      "Epoch: 1621/2000... Training loss: 0.4335\n",
      "Epoch: 1621/2000... Training loss: 0.2522\n",
      "Epoch: 1621/2000... Training loss: 0.3177\n",
      "Epoch: 1621/2000... Training loss: 0.4626\n",
      "Epoch: 1621/2000... Training loss: 0.3612\n",
      "Epoch: 1621/2000... Training loss: 0.3917\n",
      "Epoch: 1621/2000... Training loss: 0.3688\n",
      "Epoch: 1621/2000... Training loss: 0.5617\n",
      "Epoch: 1621/2000... Training loss: 0.4576\n",
      "Epoch: 1621/2000... Training loss: 0.4448\n",
      "Epoch: 1621/2000... Training loss: 0.3962\n",
      "Epoch: 1621/2000... Training loss: 0.2913\n",
      "Epoch: 1621/2000... Training loss: 0.3558\n",
      "Epoch: 1621/2000... Training loss: 0.4263\n",
      "Epoch: 1621/2000... Training loss: 0.2663\n",
      "Epoch: 1621/2000... Training loss: 0.5739\n",
      "Epoch: 1621/2000... Training loss: 0.3897\n",
      "Epoch: 1621/2000... Training loss: 0.1740\n",
      "Epoch: 1621/2000... Training loss: 0.3187\n",
      "Epoch: 1621/2000... Training loss: 0.2990\n",
      "Epoch: 1621/2000... Training loss: 0.3911\n",
      "Epoch: 1621/2000... Training loss: 0.2725\n",
      "Epoch: 1621/2000... Training loss: 0.5052\n",
      "Epoch: 1621/2000... Training loss: 0.3339\n",
      "Epoch: 1621/2000... Training loss: 0.5585\n",
      "Epoch: 1622/2000... Training loss: 0.5061\n",
      "Epoch: 1622/2000... Training loss: 0.5369\n",
      "Epoch: 1622/2000... Training loss: 0.3732\n",
      "Epoch: 1622/2000... Training loss: 0.2745\n",
      "Epoch: 1622/2000... Training loss: 0.4024\n",
      "Epoch: 1622/2000... Training loss: 0.3537\n",
      "Epoch: 1622/2000... Training loss: 0.3955\n",
      "Epoch: 1622/2000... Training loss: 0.4397\n",
      "Epoch: 1622/2000... Training loss: 0.4011\n",
      "Epoch: 1622/2000... Training loss: 0.4056\n",
      "Epoch: 1622/2000... Training loss: 0.3772\n",
      "Epoch: 1622/2000... Training loss: 0.2512\n",
      "Epoch: 1622/2000... Training loss: 0.5812\n",
      "Epoch: 1622/2000... Training loss: 0.5570\n",
      "Epoch: 1622/2000... Training loss: 0.4086\n",
      "Epoch: 1622/2000... Training loss: 0.5041\n",
      "Epoch: 1622/2000... Training loss: 0.2761\n",
      "Epoch: 1622/2000... Training loss: 0.3942\n",
      "Epoch: 1622/2000... Training loss: 0.5727\n",
      "Epoch: 1622/2000... Training loss: 0.4839\n",
      "Epoch: 1622/2000... Training loss: 0.4579\n",
      "Epoch: 1622/2000... Training loss: 0.3677\n",
      "Epoch: 1622/2000... Training loss: 0.5568\n",
      "Epoch: 1622/2000... Training loss: 0.6171\n",
      "Epoch: 1622/2000... Training loss: 0.4846\n",
      "Epoch: 1622/2000... Training loss: 0.5280\n",
      "Epoch: 1622/2000... Training loss: 0.5864\n",
      "Epoch: 1622/2000... Training loss: 0.3985\n",
      "Epoch: 1622/2000... Training loss: 0.4282\n",
      "Epoch: 1622/2000... Training loss: 0.3887\n",
      "Epoch: 1622/2000... Training loss: 0.3774\n",
      "Epoch: 1623/2000... Training loss: 0.3934\n",
      "Epoch: 1623/2000... Training loss: 0.4930\n",
      "Epoch: 1623/2000... Training loss: 0.3904\n",
      "Epoch: 1623/2000... Training loss: 0.4532\n",
      "Epoch: 1623/2000... Training loss: 0.3927\n",
      "Epoch: 1623/2000... Training loss: 0.4092\n",
      "Epoch: 1623/2000... Training loss: 0.4586\n",
      "Epoch: 1623/2000... Training loss: 0.2876\n",
      "Epoch: 1623/2000... Training loss: 0.6653\n",
      "Epoch: 1623/2000... Training loss: 0.4397\n",
      "Epoch: 1623/2000... Training loss: 0.3697\n",
      "Epoch: 1623/2000... Training loss: 0.3267\n",
      "Epoch: 1623/2000... Training loss: 0.3603\n",
      "Epoch: 1623/2000... Training loss: 0.4830\n",
      "Epoch: 1623/2000... Training loss: 0.5877\n",
      "Epoch: 1623/2000... Training loss: 0.4110\n",
      "Epoch: 1623/2000... Training loss: 0.4576\n",
      "Epoch: 1623/2000... Training loss: 0.4620\n",
      "Epoch: 1623/2000... Training loss: 0.2891\n",
      "Epoch: 1623/2000... Training loss: 0.1881\n",
      "Epoch: 1623/2000... Training loss: 0.4819\n",
      "Epoch: 1623/2000... Training loss: 0.4420\n",
      "Epoch: 1623/2000... Training loss: 0.4198\n",
      "Epoch: 1623/2000... Training loss: 0.3413\n",
      "Epoch: 1623/2000... Training loss: 0.3187\n",
      "Epoch: 1623/2000... Training loss: 0.4331\n",
      "Epoch: 1623/2000... Training loss: 0.3286\n",
      "Epoch: 1623/2000... Training loss: 0.5329\n",
      "Epoch: 1623/2000... Training loss: 0.4199\n",
      "Epoch: 1623/2000... Training loss: 0.3936\n",
      "Epoch: 1623/2000... Training loss: 0.4310\n",
      "Epoch: 1624/2000... Training loss: 0.4289\n",
      "Epoch: 1624/2000... Training loss: 0.4918\n",
      "Epoch: 1624/2000... Training loss: 0.4320\n",
      "Epoch: 1624/2000... Training loss: 0.2350\n",
      "Epoch: 1624/2000... Training loss: 0.3805\n",
      "Epoch: 1624/2000... Training loss: 0.3419\n",
      "Epoch: 1624/2000... Training loss: 0.4536\n",
      "Epoch: 1624/2000... Training loss: 0.3645\n",
      "Epoch: 1624/2000... Training loss: 0.3638\n",
      "Epoch: 1624/2000... Training loss: 0.2180\n",
      "Epoch: 1624/2000... Training loss: 0.3306\n",
      "Epoch: 1624/2000... Training loss: 0.4542\n",
      "Epoch: 1624/2000... Training loss: 0.4049\n",
      "Epoch: 1624/2000... Training loss: 0.3213\n",
      "Epoch: 1624/2000... Training loss: 0.2832\n",
      "Epoch: 1624/2000... Training loss: 0.3539\n",
      "Epoch: 1624/2000... Training loss: 0.3107\n",
      "Epoch: 1624/2000... Training loss: 0.3333\n",
      "Epoch: 1624/2000... Training loss: 0.3775\n",
      "Epoch: 1624/2000... Training loss: 0.3421\n",
      "Epoch: 1624/2000... Training loss: 0.3698\n",
      "Epoch: 1624/2000... Training loss: 0.2313\n",
      "Epoch: 1624/2000... Training loss: 0.3443\n",
      "Epoch: 1624/2000... Training loss: 0.4475\n",
      "Epoch: 1624/2000... Training loss: 0.3738\n",
      "Epoch: 1624/2000... Training loss: 0.4194\n",
      "Epoch: 1624/2000... Training loss: 0.4000\n",
      "Epoch: 1624/2000... Training loss: 0.6136\n",
      "Epoch: 1624/2000... Training loss: 0.3309\n",
      "Epoch: 1624/2000... Training loss: 0.3385\n",
      "Epoch: 1624/2000... Training loss: 0.4354\n",
      "Epoch: 1625/2000... Training loss: 0.4352\n",
      "Epoch: 1625/2000... Training loss: 0.4923\n",
      "Epoch: 1625/2000... Training loss: 0.4354\n",
      "Epoch: 1625/2000... Training loss: 0.3632\n",
      "Epoch: 1625/2000... Training loss: 0.3613\n",
      "Epoch: 1625/2000... Training loss: 0.4685\n",
      "Epoch: 1625/2000... Training loss: 0.4838\n",
      "Epoch: 1625/2000... Training loss: 0.3695\n",
      "Epoch: 1625/2000... Training loss: 0.3729\n",
      "Epoch: 1625/2000... Training loss: 0.3140\n",
      "Epoch: 1625/2000... Training loss: 0.2973\n",
      "Epoch: 1625/2000... Training loss: 0.3912\n",
      "Epoch: 1625/2000... Training loss: 0.3336\n",
      "Epoch: 1625/2000... Training loss: 0.2281\n",
      "Epoch: 1625/2000... Training loss: 0.4776\n",
      "Epoch: 1625/2000... Training loss: 0.3518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1625/2000... Training loss: 0.4084\n",
      "Epoch: 1625/2000... Training loss: 0.3049\n",
      "Epoch: 1625/2000... Training loss: 0.2373\n",
      "Epoch: 1625/2000... Training loss: 0.3278\n",
      "Epoch: 1625/2000... Training loss: 0.2892\n",
      "Epoch: 1625/2000... Training loss: 0.5371\n",
      "Epoch: 1625/2000... Training loss: 0.4411\n",
      "Epoch: 1625/2000... Training loss: 0.4836\n",
      "Epoch: 1625/2000... Training loss: 0.2823\n",
      "Epoch: 1625/2000... Training loss: 0.4668\n",
      "Epoch: 1625/2000... Training loss: 0.2383\n",
      "Epoch: 1625/2000... Training loss: 0.2687\n",
      "Epoch: 1625/2000... Training loss: 0.4098\n",
      "Epoch: 1625/2000... Training loss: 0.3518\n",
      "Epoch: 1625/2000... Training loss: 0.4248\n",
      "Epoch: 1626/2000... Training loss: 0.2666\n",
      "Epoch: 1626/2000... Training loss: 0.4603\n",
      "Epoch: 1626/2000... Training loss: 0.4230\n",
      "Epoch: 1626/2000... Training loss: 0.3151\n",
      "Epoch: 1626/2000... Training loss: 0.4284\n",
      "Epoch: 1626/2000... Training loss: 0.4141\n",
      "Epoch: 1626/2000... Training loss: 0.4290\n",
      "Epoch: 1626/2000... Training loss: 0.3518\n",
      "Epoch: 1626/2000... Training loss: 0.6798\n",
      "Epoch: 1626/2000... Training loss: 0.4642\n",
      "Epoch: 1626/2000... Training loss: 0.4535\n",
      "Epoch: 1626/2000... Training loss: 0.2881\n",
      "Epoch: 1626/2000... Training loss: 0.4991\n",
      "Epoch: 1626/2000... Training loss: 0.4719\n",
      "Epoch: 1626/2000... Training loss: 0.4762\n",
      "Epoch: 1626/2000... Training loss: 0.5352\n",
      "Epoch: 1626/2000... Training loss: 0.4564\n",
      "Epoch: 1626/2000... Training loss: 0.3996\n",
      "Epoch: 1626/2000... Training loss: 0.3777\n",
      "Epoch: 1626/2000... Training loss: 0.3738\n",
      "Epoch: 1626/2000... Training loss: 0.2458\n",
      "Epoch: 1626/2000... Training loss: 0.3117\n",
      "Epoch: 1626/2000... Training loss: 0.3585\n",
      "Epoch: 1626/2000... Training loss: 0.3706\n",
      "Epoch: 1626/2000... Training loss: 0.3947\n",
      "Epoch: 1626/2000... Training loss: 0.5004\n",
      "Epoch: 1626/2000... Training loss: 0.4325\n",
      "Epoch: 1626/2000... Training loss: 0.2935\n",
      "Epoch: 1626/2000... Training loss: 0.5035\n",
      "Epoch: 1626/2000... Training loss: 0.3907\n",
      "Epoch: 1626/2000... Training loss: 0.4078\n",
      "Epoch: 1627/2000... Training loss: 0.4617\n",
      "Epoch: 1627/2000... Training loss: 0.5029\n",
      "Epoch: 1627/2000... Training loss: 0.6359\n",
      "Epoch: 1627/2000... Training loss: 0.3659\n",
      "Epoch: 1627/2000... Training loss: 0.3260\n",
      "Epoch: 1627/2000... Training loss: 0.3654\n",
      "Epoch: 1627/2000... Training loss: 0.4076\n",
      "Epoch: 1627/2000... Training loss: 0.3431\n",
      "Epoch: 1627/2000... Training loss: 0.4380\n",
      "Epoch: 1627/2000... Training loss: 0.3716\n",
      "Epoch: 1627/2000... Training loss: 0.2967\n",
      "Epoch: 1627/2000... Training loss: 0.3281\n",
      "Epoch: 1627/2000... Training loss: 0.4617\n",
      "Epoch: 1627/2000... Training loss: 0.3463\n",
      "Epoch: 1627/2000... Training loss: 0.2540\n",
      "Epoch: 1627/2000... Training loss: 0.3543\n",
      "Epoch: 1627/2000... Training loss: 0.5862\n",
      "Epoch: 1627/2000... Training loss: 0.2541\n",
      "Epoch: 1627/2000... Training loss: 0.3103\n",
      "Epoch: 1627/2000... Training loss: 0.2277\n",
      "Epoch: 1627/2000... Training loss: 0.4016\n",
      "Epoch: 1627/2000... Training loss: 0.3831\n",
      "Epoch: 1627/2000... Training loss: 0.3642\n",
      "Epoch: 1627/2000... Training loss: 0.4375\n",
      "Epoch: 1627/2000... Training loss: 0.3409\n",
      "Epoch: 1627/2000... Training loss: 0.5027\n",
      "Epoch: 1627/2000... Training loss: 0.4822\n",
      "Epoch: 1627/2000... Training loss: 0.5474\n",
      "Epoch: 1627/2000... Training loss: 0.2576\n",
      "Epoch: 1627/2000... Training loss: 0.4355\n",
      "Epoch: 1627/2000... Training loss: 0.2442\n",
      "Epoch: 1628/2000... Training loss: 0.2389\n",
      "Epoch: 1628/2000... Training loss: 0.3735\n",
      "Epoch: 1628/2000... Training loss: 0.4840\n",
      "Epoch: 1628/2000... Training loss: 0.4504\n",
      "Epoch: 1628/2000... Training loss: 0.3667\n",
      "Epoch: 1628/2000... Training loss: 0.3024\n",
      "Epoch: 1628/2000... Training loss: 0.5612\n",
      "Epoch: 1628/2000... Training loss: 0.3155\n",
      "Epoch: 1628/2000... Training loss: 0.1690\n",
      "Epoch: 1628/2000... Training loss: 0.4928\n",
      "Epoch: 1628/2000... Training loss: 0.4282\n",
      "Epoch: 1628/2000... Training loss: 0.4029\n",
      "Epoch: 1628/2000... Training loss: 0.3987\n",
      "Epoch: 1628/2000... Training loss: 0.5206\n",
      "Epoch: 1628/2000... Training loss: 0.5008\n",
      "Epoch: 1628/2000... Training loss: 0.4111\n",
      "Epoch: 1628/2000... Training loss: 0.3116\n",
      "Epoch: 1628/2000... Training loss: 0.3397\n",
      "Epoch: 1628/2000... Training loss: 0.4922\n",
      "Epoch: 1628/2000... Training loss: 0.3043\n",
      "Epoch: 1628/2000... Training loss: 0.3554\n",
      "Epoch: 1628/2000... Training loss: 0.3206\n",
      "Epoch: 1628/2000... Training loss: 0.1704\n",
      "Epoch: 1628/2000... Training loss: 0.4615\n",
      "Epoch: 1628/2000... Training loss: 0.4538\n",
      "Epoch: 1628/2000... Training loss: 0.5435\n",
      "Epoch: 1628/2000... Training loss: 0.2925\n",
      "Epoch: 1628/2000... Training loss: 0.4019\n",
      "Epoch: 1628/2000... Training loss: 0.5045\n",
      "Epoch: 1628/2000... Training loss: 0.3638\n",
      "Epoch: 1628/2000... Training loss: 0.3945\n",
      "Epoch: 1629/2000... Training loss: 0.4166\n",
      "Epoch: 1629/2000... Training loss: 0.3098\n",
      "Epoch: 1629/2000... Training loss: 0.4824\n",
      "Epoch: 1629/2000... Training loss: 0.4215\n",
      "Epoch: 1629/2000... Training loss: 0.3821\n",
      "Epoch: 1629/2000... Training loss: 0.4099\n",
      "Epoch: 1629/2000... Training loss: 0.4054\n",
      "Epoch: 1629/2000... Training loss: 0.4028\n",
      "Epoch: 1629/2000... Training loss: 0.5206\n",
      "Epoch: 1629/2000... Training loss: 0.2822\n",
      "Epoch: 1629/2000... Training loss: 0.2966\n",
      "Epoch: 1629/2000... Training loss: 0.5359\n",
      "Epoch: 1629/2000... Training loss: 0.3727\n",
      "Epoch: 1629/2000... Training loss: 0.4216\n",
      "Epoch: 1629/2000... Training loss: 0.2467\n",
      "Epoch: 1629/2000... Training loss: 0.3962\n",
      "Epoch: 1629/2000... Training loss: 0.3366\n",
      "Epoch: 1629/2000... Training loss: 0.3837\n",
      "Epoch: 1629/2000... Training loss: 0.2901\n",
      "Epoch: 1629/2000... Training loss: 0.4131\n",
      "Epoch: 1629/2000... Training loss: 0.2957\n",
      "Epoch: 1629/2000... Training loss: 0.4896\n",
      "Epoch: 1629/2000... Training loss: 0.5186\n",
      "Epoch: 1629/2000... Training loss: 0.3158\n",
      "Epoch: 1629/2000... Training loss: 0.3100\n",
      "Epoch: 1629/2000... Training loss: 0.4697\n",
      "Epoch: 1629/2000... Training loss: 0.5581\n",
      "Epoch: 1629/2000... Training loss: 0.3271\n",
      "Epoch: 1629/2000... Training loss: 0.3885\n",
      "Epoch: 1629/2000... Training loss: 0.4895\n",
      "Epoch: 1629/2000... Training loss: 0.5020\n",
      "Epoch: 1630/2000... Training loss: 0.3689\n",
      "Epoch: 1630/2000... Training loss: 0.4653\n",
      "Epoch: 1630/2000... Training loss: 0.3668\n",
      "Epoch: 1630/2000... Training loss: 0.2762\n",
      "Epoch: 1630/2000... Training loss: 0.4133\n",
      "Epoch: 1630/2000... Training loss: 0.4827\n",
      "Epoch: 1630/2000... Training loss: 0.4844\n",
      "Epoch: 1630/2000... Training loss: 0.3382\n",
      "Epoch: 1630/2000... Training loss: 0.2972\n",
      "Epoch: 1630/2000... Training loss: 0.4962\n",
      "Epoch: 1630/2000... Training loss: 0.3406\n",
      "Epoch: 1630/2000... Training loss: 0.3248\n",
      "Epoch: 1630/2000... Training loss: 0.3176\n",
      "Epoch: 1630/2000... Training loss: 0.3778\n",
      "Epoch: 1630/2000... Training loss: 0.4116\n",
      "Epoch: 1630/2000... Training loss: 0.6262\n",
      "Epoch: 1630/2000... Training loss: 0.5162\n",
      "Epoch: 1630/2000... Training loss: 0.3957\n",
      "Epoch: 1630/2000... Training loss: 0.4640\n",
      "Epoch: 1630/2000... Training loss: 0.4044\n",
      "Epoch: 1630/2000... Training loss: 0.4939\n",
      "Epoch: 1630/2000... Training loss: 0.4032\n",
      "Epoch: 1630/2000... Training loss: 0.3398\n",
      "Epoch: 1630/2000... Training loss: 0.3842\n",
      "Epoch: 1630/2000... Training loss: 0.4550\n",
      "Epoch: 1630/2000... Training loss: 0.4013\n",
      "Epoch: 1630/2000... Training loss: 0.3556\n",
      "Epoch: 1630/2000... Training loss: 0.3376\n",
      "Epoch: 1630/2000... Training loss: 0.4365\n",
      "Epoch: 1630/2000... Training loss: 0.3344\n",
      "Epoch: 1630/2000... Training loss: 0.4141\n",
      "Epoch: 1631/2000... Training loss: 0.2959\n",
      "Epoch: 1631/2000... Training loss: 0.3987\n",
      "Epoch: 1631/2000... Training loss: 0.4062\n",
      "Epoch: 1631/2000... Training loss: 0.3738\n",
      "Epoch: 1631/2000... Training loss: 0.3896\n",
      "Epoch: 1631/2000... Training loss: 0.3829\n",
      "Epoch: 1631/2000... Training loss: 0.3716\n",
      "Epoch: 1631/2000... Training loss: 0.3432\n",
      "Epoch: 1631/2000... Training loss: 0.4191\n",
      "Epoch: 1631/2000... Training loss: 0.6429\n",
      "Epoch: 1631/2000... Training loss: 0.3263\n",
      "Epoch: 1631/2000... Training loss: 0.3633\n",
      "Epoch: 1631/2000... Training loss: 0.4395\n",
      "Epoch: 1631/2000... Training loss: 0.5895\n",
      "Epoch: 1631/2000... Training loss: 0.4997\n",
      "Epoch: 1631/2000... Training loss: 0.3387\n",
      "Epoch: 1631/2000... Training loss: 0.5111\n",
      "Epoch: 1631/2000... Training loss: 0.3359\n",
      "Epoch: 1631/2000... Training loss: 0.4816\n",
      "Epoch: 1631/2000... Training loss: 0.2245\n",
      "Epoch: 1631/2000... Training loss: 0.4779\n",
      "Epoch: 1631/2000... Training loss: 0.2633\n",
      "Epoch: 1631/2000... Training loss: 0.3561\n",
      "Epoch: 1631/2000... Training loss: 0.5170\n",
      "Epoch: 1631/2000... Training loss: 0.3324\n",
      "Epoch: 1631/2000... Training loss: 0.3055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1631/2000... Training loss: 0.2890\n",
      "Epoch: 1631/2000... Training loss: 0.3089\n",
      "Epoch: 1631/2000... Training loss: 0.3583\n",
      "Epoch: 1631/2000... Training loss: 0.2744\n",
      "Epoch: 1631/2000... Training loss: 0.3858\n",
      "Epoch: 1632/2000... Training loss: 0.4053\n",
      "Epoch: 1632/2000... Training loss: 0.3810\n",
      "Epoch: 1632/2000... Training loss: 0.3735\n",
      "Epoch: 1632/2000... Training loss: 0.4575\n",
      "Epoch: 1632/2000... Training loss: 0.2082\n",
      "Epoch: 1632/2000... Training loss: 0.3046\n",
      "Epoch: 1632/2000... Training loss: 0.5172\n",
      "Epoch: 1632/2000... Training loss: 0.4476\n",
      "Epoch: 1632/2000... Training loss: 0.4106\n",
      "Epoch: 1632/2000... Training loss: 0.2412\n",
      "Epoch: 1632/2000... Training loss: 0.2840\n",
      "Epoch: 1632/2000... Training loss: 0.3911\n",
      "Epoch: 1632/2000... Training loss: 0.2888\n",
      "Epoch: 1632/2000... Training loss: 0.4017\n",
      "Epoch: 1632/2000... Training loss: 0.5538\n",
      "Epoch: 1632/2000... Training loss: 0.3939\n",
      "Epoch: 1632/2000... Training loss: 0.3582\n",
      "Epoch: 1632/2000... Training loss: 0.4389\n",
      "Epoch: 1632/2000... Training loss: 0.3644\n",
      "Epoch: 1632/2000... Training loss: 0.3509\n",
      "Epoch: 1632/2000... Training loss: 0.4182\n",
      "Epoch: 1632/2000... Training loss: 0.3973\n",
      "Epoch: 1632/2000... Training loss: 0.5059\n",
      "Epoch: 1632/2000... Training loss: 0.6044\n",
      "Epoch: 1632/2000... Training loss: 0.3289\n",
      "Epoch: 1632/2000... Training loss: 0.4363\n",
      "Epoch: 1632/2000... Training loss: 0.3038\n",
      "Epoch: 1632/2000... Training loss: 0.4524\n",
      "Epoch: 1632/2000... Training loss: 0.5265\n",
      "Epoch: 1632/2000... Training loss: 0.4827\n",
      "Epoch: 1632/2000... Training loss: 0.5442\n",
      "Epoch: 1633/2000... Training loss: 0.2795\n",
      "Epoch: 1633/2000... Training loss: 0.2752\n",
      "Epoch: 1633/2000... Training loss: 0.4009\n",
      "Epoch: 1633/2000... Training loss: 0.4549\n",
      "Epoch: 1633/2000... Training loss: 0.3766\n",
      "Epoch: 1633/2000... Training loss: 0.2521\n",
      "Epoch: 1633/2000... Training loss: 0.4065\n",
      "Epoch: 1633/2000... Training loss: 0.4254\n",
      "Epoch: 1633/2000... Training loss: 0.3844\n",
      "Epoch: 1633/2000... Training loss: 0.2501\n",
      "Epoch: 1633/2000... Training loss: 0.2759\n",
      "Epoch: 1633/2000... Training loss: 0.3380\n",
      "Epoch: 1633/2000... Training loss: 0.3565\n",
      "Epoch: 1633/2000... Training loss: 0.5003\n",
      "Epoch: 1633/2000... Training loss: 0.3966\n",
      "Epoch: 1633/2000... Training loss: 0.4137\n",
      "Epoch: 1633/2000... Training loss: 0.3750\n",
      "Epoch: 1633/2000... Training loss: 0.3436\n",
      "Epoch: 1633/2000... Training loss: 0.4319\n",
      "Epoch: 1633/2000... Training loss: 0.3207\n",
      "Epoch: 1633/2000... Training loss: 0.3569\n",
      "Epoch: 1633/2000... Training loss: 0.4898\n",
      "Epoch: 1633/2000... Training loss: 0.3994\n",
      "Epoch: 1633/2000... Training loss: 0.4808\n",
      "Epoch: 1633/2000... Training loss: 0.3678\n",
      "Epoch: 1633/2000... Training loss: 0.2202\n",
      "Epoch: 1633/2000... Training loss: 0.3878\n",
      "Epoch: 1633/2000... Training loss: 0.3407\n",
      "Epoch: 1633/2000... Training loss: 0.4118\n",
      "Epoch: 1633/2000... Training loss: 0.4151\n",
      "Epoch: 1633/2000... Training loss: 0.3874\n",
      "Epoch: 1634/2000... Training loss: 0.3296\n",
      "Epoch: 1634/2000... Training loss: 0.4410\n",
      "Epoch: 1634/2000... Training loss: 0.4100\n",
      "Epoch: 1634/2000... Training loss: 0.3178\n",
      "Epoch: 1634/2000... Training loss: 0.3368\n",
      "Epoch: 1634/2000... Training loss: 0.3495\n",
      "Epoch: 1634/2000... Training loss: 0.5187\n",
      "Epoch: 1634/2000... Training loss: 0.2782\n",
      "Epoch: 1634/2000... Training loss: 0.2879\n",
      "Epoch: 1634/2000... Training loss: 0.4351\n",
      "Epoch: 1634/2000... Training loss: 0.2841\n",
      "Epoch: 1634/2000... Training loss: 0.2923\n",
      "Epoch: 1634/2000... Training loss: 0.6037\n",
      "Epoch: 1634/2000... Training loss: 0.4876\n",
      "Epoch: 1634/2000... Training loss: 0.2614\n",
      "Epoch: 1634/2000... Training loss: 0.3758\n",
      "Epoch: 1634/2000... Training loss: 0.4006\n",
      "Epoch: 1634/2000... Training loss: 0.5603\n",
      "Epoch: 1634/2000... Training loss: 0.4795\n",
      "Epoch: 1634/2000... Training loss: 0.4473\n",
      "Epoch: 1634/2000... Training loss: 0.5030\n",
      "Epoch: 1634/2000... Training loss: 0.3356\n",
      "Epoch: 1634/2000... Training loss: 0.2817\n",
      "Epoch: 1634/2000... Training loss: 0.3325\n",
      "Epoch: 1634/2000... Training loss: 0.3538\n",
      "Epoch: 1634/2000... Training loss: 0.4735\n",
      "Epoch: 1634/2000... Training loss: 0.5809\n",
      "Epoch: 1634/2000... Training loss: 0.2498\n",
      "Epoch: 1634/2000... Training loss: 0.5107\n",
      "Epoch: 1634/2000... Training loss: 0.4033\n",
      "Epoch: 1634/2000... Training loss: 0.5279\n",
      "Epoch: 1635/2000... Training loss: 0.5200\n",
      "Epoch: 1635/2000... Training loss: 0.3323\n",
      "Epoch: 1635/2000... Training loss: 0.4657\n",
      "Epoch: 1635/2000... Training loss: 0.3488\n",
      "Epoch: 1635/2000... Training loss: 0.3811\n",
      "Epoch: 1635/2000... Training loss: 0.3640\n",
      "Epoch: 1635/2000... Training loss: 0.4223\n",
      "Epoch: 1635/2000... Training loss: 0.4287\n",
      "Epoch: 1635/2000... Training loss: 0.5790\n",
      "Epoch: 1635/2000... Training loss: 0.2987\n",
      "Epoch: 1635/2000... Training loss: 0.4342\n",
      "Epoch: 1635/2000... Training loss: 0.3630\n",
      "Epoch: 1635/2000... Training loss: 0.3073\n",
      "Epoch: 1635/2000... Training loss: 0.5025\n",
      "Epoch: 1635/2000... Training loss: 0.4534\n",
      "Epoch: 1635/2000... Training loss: 0.3788\n",
      "Epoch: 1635/2000... Training loss: 0.4248\n",
      "Epoch: 1635/2000... Training loss: 0.3145\n",
      "Epoch: 1635/2000... Training loss: 0.3353\n",
      "Epoch: 1635/2000... Training loss: 0.2824\n",
      "Epoch: 1635/2000... Training loss: 0.3127\n",
      "Epoch: 1635/2000... Training loss: 0.5113\n",
      "Epoch: 1635/2000... Training loss: 0.3941\n",
      "Epoch: 1635/2000... Training loss: 0.3565\n",
      "Epoch: 1635/2000... Training loss: 0.4596\n",
      "Epoch: 1635/2000... Training loss: 0.4861\n",
      "Epoch: 1635/2000... Training loss: 0.3881\n",
      "Epoch: 1635/2000... Training loss: 0.3920\n",
      "Epoch: 1635/2000... Training loss: 0.3339\n",
      "Epoch: 1635/2000... Training loss: 0.3203\n",
      "Epoch: 1635/2000... Training loss: 0.3321\n",
      "Epoch: 1636/2000... Training loss: 0.3444\n",
      "Epoch: 1636/2000... Training loss: 0.4064\n",
      "Epoch: 1636/2000... Training loss: 0.4034\n",
      "Epoch: 1636/2000... Training loss: 0.5888\n",
      "Epoch: 1636/2000... Training loss: 0.3201\n",
      "Epoch: 1636/2000... Training loss: 0.3793\n",
      "Epoch: 1636/2000... Training loss: 0.3378\n",
      "Epoch: 1636/2000... Training loss: 0.2432\n",
      "Epoch: 1636/2000... Training loss: 0.3644\n",
      "Epoch: 1636/2000... Training loss: 0.4096\n",
      "Epoch: 1636/2000... Training loss: 0.4402\n",
      "Epoch: 1636/2000... Training loss: 0.3808\n",
      "Epoch: 1636/2000... Training loss: 0.3746\n",
      "Epoch: 1636/2000... Training loss: 0.4006\n",
      "Epoch: 1636/2000... Training loss: 0.3374\n",
      "Epoch: 1636/2000... Training loss: 0.3418\n",
      "Epoch: 1636/2000... Training loss: 0.3687\n",
      "Epoch: 1636/2000... Training loss: 0.3100\n",
      "Epoch: 1636/2000... Training loss: 0.5610\n",
      "Epoch: 1636/2000... Training loss: 0.3909\n",
      "Epoch: 1636/2000... Training loss: 0.2419\n",
      "Epoch: 1636/2000... Training loss: 0.4265\n",
      "Epoch: 1636/2000... Training loss: 0.3927\n",
      "Epoch: 1636/2000... Training loss: 0.4103\n",
      "Epoch: 1636/2000... Training loss: 0.4509\n",
      "Epoch: 1636/2000... Training loss: 0.4187\n",
      "Epoch: 1636/2000... Training loss: 0.4891\n",
      "Epoch: 1636/2000... Training loss: 0.3462\n",
      "Epoch: 1636/2000... Training loss: 0.5594\n",
      "Epoch: 1636/2000... Training loss: 0.5333\n",
      "Epoch: 1636/2000... Training loss: 0.3545\n",
      "Epoch: 1637/2000... Training loss: 0.4307\n",
      "Epoch: 1637/2000... Training loss: 0.4961\n",
      "Epoch: 1637/2000... Training loss: 0.4313\n",
      "Epoch: 1637/2000... Training loss: 0.4018\n",
      "Epoch: 1637/2000... Training loss: 0.4141\n",
      "Epoch: 1637/2000... Training loss: 0.2341\n",
      "Epoch: 1637/2000... Training loss: 0.4504\n",
      "Epoch: 1637/2000... Training loss: 0.5185\n",
      "Epoch: 1637/2000... Training loss: 0.3717\n",
      "Epoch: 1637/2000... Training loss: 0.2376\n",
      "Epoch: 1637/2000... Training loss: 0.4933\n",
      "Epoch: 1637/2000... Training loss: 0.5522\n",
      "Epoch: 1637/2000... Training loss: 0.4552\n",
      "Epoch: 1637/2000... Training loss: 0.4847\n",
      "Epoch: 1637/2000... Training loss: 0.3116\n",
      "Epoch: 1637/2000... Training loss: 0.3642\n",
      "Epoch: 1637/2000... Training loss: 0.2835\n",
      "Epoch: 1637/2000... Training loss: 0.4341\n",
      "Epoch: 1637/2000... Training loss: 0.6476\n",
      "Epoch: 1637/2000... Training loss: 0.2704\n",
      "Epoch: 1637/2000... Training loss: 0.2701\n",
      "Epoch: 1637/2000... Training loss: 0.3090\n",
      "Epoch: 1637/2000... Training loss: 0.3760\n",
      "Epoch: 1637/2000... Training loss: 0.4249\n",
      "Epoch: 1637/2000... Training loss: 0.3199\n",
      "Epoch: 1637/2000... Training loss: 0.4876\n",
      "Epoch: 1637/2000... Training loss: 0.4104\n",
      "Epoch: 1637/2000... Training loss: 0.3793\n",
      "Epoch: 1637/2000... Training loss: 0.4514\n",
      "Epoch: 1637/2000... Training loss: 0.4246\n",
      "Epoch: 1637/2000... Training loss: 0.4281\n",
      "Epoch: 1638/2000... Training loss: 0.4372\n",
      "Epoch: 1638/2000... Training loss: 0.6184\n",
      "Epoch: 1638/2000... Training loss: 0.4042\n",
      "Epoch: 1638/2000... Training loss: 0.6389\n",
      "Epoch: 1638/2000... Training loss: 0.4885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1638/2000... Training loss: 0.4167\n",
      "Epoch: 1638/2000... Training loss: 0.3018\n",
      "Epoch: 1638/2000... Training loss: 0.3825\n",
      "Epoch: 1638/2000... Training loss: 0.6661\n",
      "Epoch: 1638/2000... Training loss: 0.4463\n",
      "Epoch: 1638/2000... Training loss: 0.4791\n",
      "Epoch: 1638/2000... Training loss: 0.2395\n",
      "Epoch: 1638/2000... Training loss: 0.5968\n",
      "Epoch: 1638/2000... Training loss: 0.4435\n",
      "Epoch: 1638/2000... Training loss: 0.3123\n",
      "Epoch: 1638/2000... Training loss: 0.5989\n",
      "Epoch: 1638/2000... Training loss: 0.4164\n",
      "Epoch: 1638/2000... Training loss: 0.3737\n",
      "Epoch: 1638/2000... Training loss: 0.4956\n",
      "Epoch: 1638/2000... Training loss: 0.4999\n",
      "Epoch: 1638/2000... Training loss: 0.4470\n",
      "Epoch: 1638/2000... Training loss: 0.2828\n",
      "Epoch: 1638/2000... Training loss: 0.4193\n",
      "Epoch: 1638/2000... Training loss: 0.3934\n",
      "Epoch: 1638/2000... Training loss: 0.3517\n",
      "Epoch: 1638/2000... Training loss: 0.2929\n",
      "Epoch: 1638/2000... Training loss: 0.3740\n",
      "Epoch: 1638/2000... Training loss: 0.3659\n",
      "Epoch: 1638/2000... Training loss: 0.2945\n",
      "Epoch: 1638/2000... Training loss: 0.3063\n",
      "Epoch: 1638/2000... Training loss: 0.5233\n",
      "Epoch: 1639/2000... Training loss: 0.5180\n",
      "Epoch: 1639/2000... Training loss: 0.4737\n",
      "Epoch: 1639/2000... Training loss: 0.2362\n",
      "Epoch: 1639/2000... Training loss: 0.2793\n",
      "Epoch: 1639/2000... Training loss: 0.4749\n",
      "Epoch: 1639/2000... Training loss: 0.2721\n",
      "Epoch: 1639/2000... Training loss: 0.4283\n",
      "Epoch: 1639/2000... Training loss: 0.4210\n",
      "Epoch: 1639/2000... Training loss: 0.2417\n",
      "Epoch: 1639/2000... Training loss: 0.3936\n",
      "Epoch: 1639/2000... Training loss: 0.2949\n",
      "Epoch: 1639/2000... Training loss: 0.3375\n",
      "Epoch: 1639/2000... Training loss: 0.4818\n",
      "Epoch: 1639/2000... Training loss: 0.3125\n",
      "Epoch: 1639/2000... Training loss: 0.4299\n",
      "Epoch: 1639/2000... Training loss: 0.3599\n",
      "Epoch: 1639/2000... Training loss: 0.4805\n",
      "Epoch: 1639/2000... Training loss: 0.5024\n",
      "Epoch: 1639/2000... Training loss: 0.3854\n",
      "Epoch: 1639/2000... Training loss: 0.4252\n",
      "Epoch: 1639/2000... Training loss: 0.4653\n",
      "Epoch: 1639/2000... Training loss: 0.4329\n",
      "Epoch: 1639/2000... Training loss: 0.3828\n",
      "Epoch: 1639/2000... Training loss: 0.4303\n",
      "Epoch: 1639/2000... Training loss: 0.4771\n",
      "Epoch: 1639/2000... Training loss: 0.3809\n",
      "Epoch: 1639/2000... Training loss: 0.3237\n",
      "Epoch: 1639/2000... Training loss: 0.6376\n",
      "Epoch: 1639/2000... Training loss: 0.5098\n",
      "Epoch: 1639/2000... Training loss: 0.6368\n",
      "Epoch: 1639/2000... Training loss: 0.6089\n",
      "Epoch: 1640/2000... Training loss: 0.2335\n",
      "Epoch: 1640/2000... Training loss: 0.2551\n",
      "Epoch: 1640/2000... Training loss: 0.3815\n",
      "Epoch: 1640/2000... Training loss: 0.4351\n",
      "Epoch: 1640/2000... Training loss: 0.3215\n",
      "Epoch: 1640/2000... Training loss: 0.3334\n",
      "Epoch: 1640/2000... Training loss: 0.5681\n",
      "Epoch: 1640/2000... Training loss: 0.3674\n",
      "Epoch: 1640/2000... Training loss: 0.3886\n",
      "Epoch: 1640/2000... Training loss: 0.3961\n",
      "Epoch: 1640/2000... Training loss: 0.4696\n",
      "Epoch: 1640/2000... Training loss: 0.4725\n",
      "Epoch: 1640/2000... Training loss: 0.3810\n",
      "Epoch: 1640/2000... Training loss: 0.2298\n",
      "Epoch: 1640/2000... Training loss: 0.3243\n",
      "Epoch: 1640/2000... Training loss: 0.3627\n",
      "Epoch: 1640/2000... Training loss: 0.4615\n",
      "Epoch: 1640/2000... Training loss: 0.3629\n",
      "Epoch: 1640/2000... Training loss: 0.5368\n",
      "Epoch: 1640/2000... Training loss: 0.4494\n",
      "Epoch: 1640/2000... Training loss: 0.5616\n",
      "Epoch: 1640/2000... Training loss: 0.5365\n",
      "Epoch: 1640/2000... Training loss: 0.3457\n",
      "Epoch: 1640/2000... Training loss: 0.3804\n",
      "Epoch: 1640/2000... Training loss: 0.4661\n",
      "Epoch: 1640/2000... Training loss: 0.4487\n",
      "Epoch: 1640/2000... Training loss: 0.2893\n",
      "Epoch: 1640/2000... Training loss: 0.2874\n",
      "Epoch: 1640/2000... Training loss: 0.5490\n",
      "Epoch: 1640/2000... Training loss: 0.3968\n",
      "Epoch: 1640/2000... Training loss: 0.3221\n",
      "Epoch: 1641/2000... Training loss: 0.4003\n",
      "Epoch: 1641/2000... Training loss: 0.3715\n",
      "Epoch: 1641/2000... Training loss: 0.4591\n",
      "Epoch: 1641/2000... Training loss: 0.3162\n",
      "Epoch: 1641/2000... Training loss: 0.4173\n",
      "Epoch: 1641/2000... Training loss: 0.3828\n",
      "Epoch: 1641/2000... Training loss: 0.5077\n",
      "Epoch: 1641/2000... Training loss: 0.2237\n",
      "Epoch: 1641/2000... Training loss: 0.5609\n",
      "Epoch: 1641/2000... Training loss: 0.5284\n",
      "Epoch: 1641/2000... Training loss: 0.2431\n",
      "Epoch: 1641/2000... Training loss: 0.3316\n",
      "Epoch: 1641/2000... Training loss: 0.2935\n",
      "Epoch: 1641/2000... Training loss: 0.3251\n",
      "Epoch: 1641/2000... Training loss: 0.5366\n",
      "Epoch: 1641/2000... Training loss: 0.4368\n",
      "Epoch: 1641/2000... Training loss: 0.3429\n",
      "Epoch: 1641/2000... Training loss: 0.3236\n",
      "Epoch: 1641/2000... Training loss: 0.2694\n",
      "Epoch: 1641/2000... Training loss: 0.2525\n",
      "Epoch: 1641/2000... Training loss: 0.3249\n",
      "Epoch: 1641/2000... Training loss: 0.3414\n",
      "Epoch: 1641/2000... Training loss: 0.5127\n",
      "Epoch: 1641/2000... Training loss: 0.2963\n",
      "Epoch: 1641/2000... Training loss: 0.4480\n",
      "Epoch: 1641/2000... Training loss: 0.3602\n",
      "Epoch: 1641/2000... Training loss: 0.3360\n",
      "Epoch: 1641/2000... Training loss: 0.3943\n",
      "Epoch: 1641/2000... Training loss: 0.3453\n",
      "Epoch: 1641/2000... Training loss: 0.4270\n",
      "Epoch: 1641/2000... Training loss: 0.4955\n",
      "Epoch: 1642/2000... Training loss: 0.2928\n",
      "Epoch: 1642/2000... Training loss: 0.4980\n",
      "Epoch: 1642/2000... Training loss: 0.5772\n",
      "Epoch: 1642/2000... Training loss: 0.3414\n",
      "Epoch: 1642/2000... Training loss: 0.4836\n",
      "Epoch: 1642/2000... Training loss: 0.4580\n",
      "Epoch: 1642/2000... Training loss: 0.4572\n",
      "Epoch: 1642/2000... Training loss: 0.4588\n",
      "Epoch: 1642/2000... Training loss: 0.2969\n",
      "Epoch: 1642/2000... Training loss: 0.2949\n",
      "Epoch: 1642/2000... Training loss: 0.2633\n",
      "Epoch: 1642/2000... Training loss: 0.4254\n",
      "Epoch: 1642/2000... Training loss: 0.3253\n",
      "Epoch: 1642/2000... Training loss: 0.4461\n",
      "Epoch: 1642/2000... Training loss: 0.2593\n",
      "Epoch: 1642/2000... Training loss: 0.4719\n",
      "Epoch: 1642/2000... Training loss: 0.4185\n",
      "Epoch: 1642/2000... Training loss: 0.3064\n",
      "Epoch: 1642/2000... Training loss: 0.5414\n",
      "Epoch: 1642/2000... Training loss: 0.4118\n",
      "Epoch: 1642/2000... Training loss: 0.3603\n",
      "Epoch: 1642/2000... Training loss: 0.3674\n",
      "Epoch: 1642/2000... Training loss: 0.5576\n",
      "Epoch: 1642/2000... Training loss: 0.2763\n",
      "Epoch: 1642/2000... Training loss: 0.4067\n",
      "Epoch: 1642/2000... Training loss: 0.4983\n",
      "Epoch: 1642/2000... Training loss: 0.4432\n",
      "Epoch: 1642/2000... Training loss: 0.3664\n",
      "Epoch: 1642/2000... Training loss: 0.3755\n",
      "Epoch: 1642/2000... Training loss: 0.3756\n",
      "Epoch: 1642/2000... Training loss: 0.3537\n",
      "Epoch: 1643/2000... Training loss: 0.4598\n",
      "Epoch: 1643/2000... Training loss: 0.4670\n",
      "Epoch: 1643/2000... Training loss: 0.4915\n",
      "Epoch: 1643/2000... Training loss: 0.4278\n",
      "Epoch: 1643/2000... Training loss: 0.2813\n",
      "Epoch: 1643/2000... Training loss: 0.4071\n",
      "Epoch: 1643/2000... Training loss: 0.4742\n",
      "Epoch: 1643/2000... Training loss: 0.3815\n",
      "Epoch: 1643/2000... Training loss: 0.4537\n",
      "Epoch: 1643/2000... Training loss: 0.3363\n",
      "Epoch: 1643/2000... Training loss: 0.4025\n",
      "Epoch: 1643/2000... Training loss: 0.2730\n",
      "Epoch: 1643/2000... Training loss: 0.4975\n",
      "Epoch: 1643/2000... Training loss: 0.5302\n",
      "Epoch: 1643/2000... Training loss: 0.4111\n",
      "Epoch: 1643/2000... Training loss: 0.2388\n",
      "Epoch: 1643/2000... Training loss: 0.3416\n",
      "Epoch: 1643/2000... Training loss: 0.4502\n",
      "Epoch: 1643/2000... Training loss: 0.4600\n",
      "Epoch: 1643/2000... Training loss: 0.2301\n",
      "Epoch: 1643/2000... Training loss: 0.3991\n",
      "Epoch: 1643/2000... Training loss: 0.3982\n",
      "Epoch: 1643/2000... Training loss: 0.2759\n",
      "Epoch: 1643/2000... Training loss: 0.2715\n",
      "Epoch: 1643/2000... Training loss: 0.3147\n",
      "Epoch: 1643/2000... Training loss: 0.3209\n",
      "Epoch: 1643/2000... Training loss: 0.4440\n",
      "Epoch: 1643/2000... Training loss: 0.2648\n",
      "Epoch: 1643/2000... Training loss: 0.3811\n",
      "Epoch: 1643/2000... Training loss: 0.4780\n",
      "Epoch: 1643/2000... Training loss: 0.2375\n",
      "Epoch: 1644/2000... Training loss: 0.3272\n",
      "Epoch: 1644/2000... Training loss: 0.4885\n",
      "Epoch: 1644/2000... Training loss: 0.3668\n",
      "Epoch: 1644/2000... Training loss: 0.5467\n",
      "Epoch: 1644/2000... Training loss: 0.4649\n",
      "Epoch: 1644/2000... Training loss: 0.4291\n",
      "Epoch: 1644/2000... Training loss: 0.4895\n",
      "Epoch: 1644/2000... Training loss: 0.3191\n",
      "Epoch: 1644/2000... Training loss: 0.4196\n",
      "Epoch: 1644/2000... Training loss: 0.3787\n",
      "Epoch: 1644/2000... Training loss: 0.2557\n",
      "Epoch: 1644/2000... Training loss: 0.3279\n",
      "Epoch: 1644/2000... Training loss: 0.3055\n",
      "Epoch: 1644/2000... Training loss: 0.4003\n",
      "Epoch: 1644/2000... Training loss: 0.4435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1644/2000... Training loss: 0.4775\n",
      "Epoch: 1644/2000... Training loss: 0.2755\n",
      "Epoch: 1644/2000... Training loss: 0.5579\n",
      "Epoch: 1644/2000... Training loss: 0.4336\n",
      "Epoch: 1644/2000... Training loss: 0.3542\n",
      "Epoch: 1644/2000... Training loss: 0.3657\n",
      "Epoch: 1644/2000... Training loss: 0.3861\n",
      "Epoch: 1644/2000... Training loss: 0.4414\n",
      "Epoch: 1644/2000... Training loss: 0.4217\n",
      "Epoch: 1644/2000... Training loss: 0.3059\n",
      "Epoch: 1644/2000... Training loss: 0.4516\n",
      "Epoch: 1644/2000... Training loss: 0.6950\n",
      "Epoch: 1644/2000... Training loss: 0.5505\n",
      "Epoch: 1644/2000... Training loss: 0.3781\n",
      "Epoch: 1644/2000... Training loss: 0.3069\n",
      "Epoch: 1644/2000... Training loss: 0.3915\n",
      "Epoch: 1645/2000... Training loss: 0.4193\n",
      "Epoch: 1645/2000... Training loss: 0.4937\n",
      "Epoch: 1645/2000... Training loss: 0.2738\n",
      "Epoch: 1645/2000... Training loss: 0.3984\n",
      "Epoch: 1645/2000... Training loss: 0.3723\n",
      "Epoch: 1645/2000... Training loss: 0.5340\n",
      "Epoch: 1645/2000... Training loss: 0.4145\n",
      "Epoch: 1645/2000... Training loss: 0.4706\n",
      "Epoch: 1645/2000... Training loss: 0.3186\n",
      "Epoch: 1645/2000... Training loss: 0.4335\n",
      "Epoch: 1645/2000... Training loss: 0.4795\n",
      "Epoch: 1645/2000... Training loss: 0.2559\n",
      "Epoch: 1645/2000... Training loss: 0.2667\n",
      "Epoch: 1645/2000... Training loss: 0.3327\n",
      "Epoch: 1645/2000... Training loss: 0.5266\n",
      "Epoch: 1645/2000... Training loss: 0.4899\n",
      "Epoch: 1645/2000... Training loss: 0.4873\n",
      "Epoch: 1645/2000... Training loss: 0.5061\n",
      "Epoch: 1645/2000... Training loss: 0.4356\n",
      "Epoch: 1645/2000... Training loss: 0.4089\n",
      "Epoch: 1645/2000... Training loss: 0.3019\n",
      "Epoch: 1645/2000... Training loss: 0.4881\n",
      "Epoch: 1645/2000... Training loss: 0.4597\n",
      "Epoch: 1645/2000... Training loss: 0.2919\n",
      "Epoch: 1645/2000... Training loss: 0.3358\n",
      "Epoch: 1645/2000... Training loss: 0.2521\n",
      "Epoch: 1645/2000... Training loss: 0.4273\n",
      "Epoch: 1645/2000... Training loss: 0.5311\n",
      "Epoch: 1645/2000... Training loss: 0.2908\n",
      "Epoch: 1645/2000... Training loss: 0.4021\n",
      "Epoch: 1645/2000... Training loss: 0.4461\n",
      "Epoch: 1646/2000... Training loss: 0.3714\n",
      "Epoch: 1646/2000... Training loss: 0.4626\n",
      "Epoch: 1646/2000... Training loss: 0.2677\n",
      "Epoch: 1646/2000... Training loss: 0.3202\n",
      "Epoch: 1646/2000... Training loss: 0.3580\n",
      "Epoch: 1646/2000... Training loss: 0.3040\n",
      "Epoch: 1646/2000... Training loss: 0.3208\n",
      "Epoch: 1646/2000... Training loss: 0.3161\n",
      "Epoch: 1646/2000... Training loss: 0.4301\n",
      "Epoch: 1646/2000... Training loss: 0.3747\n",
      "Epoch: 1646/2000... Training loss: 0.3209\n",
      "Epoch: 1646/2000... Training loss: 0.3885\n",
      "Epoch: 1646/2000... Training loss: 0.3390\n",
      "Epoch: 1646/2000... Training loss: 0.2460\n",
      "Epoch: 1646/2000... Training loss: 0.3282\n",
      "Epoch: 1646/2000... Training loss: 0.3931\n",
      "Epoch: 1646/2000... Training loss: 0.3578\n",
      "Epoch: 1646/2000... Training loss: 0.2465\n",
      "Epoch: 1646/2000... Training loss: 0.6933\n",
      "Epoch: 1646/2000... Training loss: 0.4359\n",
      "Epoch: 1646/2000... Training loss: 0.4459\n",
      "Epoch: 1646/2000... Training loss: 0.4247\n",
      "Epoch: 1646/2000... Training loss: 0.2144\n",
      "Epoch: 1646/2000... Training loss: 0.4795\n",
      "Epoch: 1646/2000... Training loss: 0.3313\n",
      "Epoch: 1646/2000... Training loss: 0.6538\n",
      "Epoch: 1646/2000... Training loss: 0.3680\n",
      "Epoch: 1646/2000... Training loss: 0.3836\n",
      "Epoch: 1646/2000... Training loss: 0.3908\n",
      "Epoch: 1646/2000... Training loss: 0.4407\n",
      "Epoch: 1646/2000... Training loss: 0.2423\n",
      "Epoch: 1647/2000... Training loss: 0.5148\n",
      "Epoch: 1647/2000... Training loss: 0.4921\n",
      "Epoch: 1647/2000... Training loss: 0.4759\n",
      "Epoch: 1647/2000... Training loss: 0.4733\n",
      "Epoch: 1647/2000... Training loss: 0.4781\n",
      "Epoch: 1647/2000... Training loss: 0.5684\n",
      "Epoch: 1647/2000... Training loss: 0.5613\n",
      "Epoch: 1647/2000... Training loss: 0.4648\n",
      "Epoch: 1647/2000... Training loss: 0.4301\n",
      "Epoch: 1647/2000... Training loss: 0.3140\n",
      "Epoch: 1647/2000... Training loss: 0.4586\n",
      "Epoch: 1647/2000... Training loss: 0.4375\n",
      "Epoch: 1647/2000... Training loss: 0.5177\n",
      "Epoch: 1647/2000... Training loss: 0.4303\n",
      "Epoch: 1647/2000... Training loss: 0.4554\n",
      "Epoch: 1647/2000... Training loss: 0.4306\n",
      "Epoch: 1647/2000... Training loss: 0.3602\n",
      "Epoch: 1647/2000... Training loss: 0.3922\n",
      "Epoch: 1647/2000... Training loss: 0.4800\n",
      "Epoch: 1647/2000... Training loss: 0.3770\n",
      "Epoch: 1647/2000... Training loss: 0.2980\n",
      "Epoch: 1647/2000... Training loss: 0.3768\n",
      "Epoch: 1647/2000... Training loss: 0.2899\n",
      "Epoch: 1647/2000... Training loss: 0.3436\n",
      "Epoch: 1647/2000... Training loss: 0.2302\n",
      "Epoch: 1647/2000... Training loss: 0.3472\n",
      "Epoch: 1647/2000... Training loss: 0.4379\n",
      "Epoch: 1647/2000... Training loss: 0.2671\n",
      "Epoch: 1647/2000... Training loss: 0.5289\n",
      "Epoch: 1647/2000... Training loss: 0.3501\n",
      "Epoch: 1647/2000... Training loss: 0.5169\n",
      "Epoch: 1648/2000... Training loss: 0.3803\n",
      "Epoch: 1648/2000... Training loss: 0.3250\n",
      "Epoch: 1648/2000... Training loss: 0.5663\n",
      "Epoch: 1648/2000... Training loss: 0.5107\n",
      "Epoch: 1648/2000... Training loss: 0.4597\n",
      "Epoch: 1648/2000... Training loss: 0.3582\n",
      "Epoch: 1648/2000... Training loss: 0.7116\n",
      "Epoch: 1648/2000... Training loss: 0.3363\n",
      "Epoch: 1648/2000... Training loss: 0.3860\n",
      "Epoch: 1648/2000... Training loss: 0.4539\n",
      "Epoch: 1648/2000... Training loss: 0.2908\n",
      "Epoch: 1648/2000... Training loss: 0.4981\n",
      "Epoch: 1648/2000... Training loss: 0.2766\n",
      "Epoch: 1648/2000... Training loss: 0.5340\n",
      "Epoch: 1648/2000... Training loss: 0.4435\n",
      "Epoch: 1648/2000... Training loss: 0.2481\n",
      "Epoch: 1648/2000... Training loss: 0.5579\n",
      "Epoch: 1648/2000... Training loss: 0.4359\n",
      "Epoch: 1648/2000... Training loss: 0.5075\n",
      "Epoch: 1648/2000... Training loss: 0.5463\n",
      "Epoch: 1648/2000... Training loss: 0.2661\n",
      "Epoch: 1648/2000... Training loss: 0.5113\n",
      "Epoch: 1648/2000... Training loss: 0.2473\n",
      "Epoch: 1648/2000... Training loss: 0.3030\n",
      "Epoch: 1648/2000... Training loss: 0.3623\n",
      "Epoch: 1648/2000... Training loss: 0.2874\n",
      "Epoch: 1648/2000... Training loss: 0.4622\n",
      "Epoch: 1648/2000... Training loss: 0.5422\n",
      "Epoch: 1648/2000... Training loss: 0.4182\n",
      "Epoch: 1648/2000... Training loss: 0.3487\n",
      "Epoch: 1648/2000... Training loss: 0.3375\n",
      "Epoch: 1649/2000... Training loss: 0.3961\n",
      "Epoch: 1649/2000... Training loss: 0.3864\n",
      "Epoch: 1649/2000... Training loss: 0.5104\n",
      "Epoch: 1649/2000... Training loss: 0.2852\n",
      "Epoch: 1649/2000... Training loss: 0.3081\n",
      "Epoch: 1649/2000... Training loss: 0.2070\n",
      "Epoch: 1649/2000... Training loss: 0.6334\n",
      "Epoch: 1649/2000... Training loss: 0.4438\n",
      "Epoch: 1649/2000... Training loss: 0.3898\n",
      "Epoch: 1649/2000... Training loss: 0.2479\n",
      "Epoch: 1649/2000... Training loss: 0.4735\n",
      "Epoch: 1649/2000... Training loss: 0.2567\n",
      "Epoch: 1649/2000... Training loss: 0.5729\n",
      "Epoch: 1649/2000... Training loss: 0.4167\n",
      "Epoch: 1649/2000... Training loss: 0.5462\n",
      "Epoch: 1649/2000... Training loss: 0.4952\n",
      "Epoch: 1649/2000... Training loss: 0.2610\n",
      "Epoch: 1649/2000... Training loss: 0.4051\n",
      "Epoch: 1649/2000... Training loss: 0.3891\n",
      "Epoch: 1649/2000... Training loss: 0.3554\n",
      "Epoch: 1649/2000... Training loss: 0.4665\n",
      "Epoch: 1649/2000... Training loss: 0.3536\n",
      "Epoch: 1649/2000... Training loss: 0.5964\n",
      "Epoch: 1649/2000... Training loss: 0.3262\n",
      "Epoch: 1649/2000... Training loss: 0.5345\n",
      "Epoch: 1649/2000... Training loss: 0.2835\n",
      "Epoch: 1649/2000... Training loss: 0.3279\n",
      "Epoch: 1649/2000... Training loss: 0.3258\n",
      "Epoch: 1649/2000... Training loss: 0.3383\n",
      "Epoch: 1649/2000... Training loss: 0.2044\n",
      "Epoch: 1649/2000... Training loss: 0.2490\n",
      "Epoch: 1650/2000... Training loss: 0.4713\n",
      "Epoch: 1650/2000... Training loss: 0.3427\n",
      "Epoch: 1650/2000... Training loss: 0.4128\n",
      "Epoch: 1650/2000... Training loss: 0.4768\n",
      "Epoch: 1650/2000... Training loss: 0.4476\n",
      "Epoch: 1650/2000... Training loss: 0.5152\n",
      "Epoch: 1650/2000... Training loss: 0.3295\n",
      "Epoch: 1650/2000... Training loss: 0.3756\n",
      "Epoch: 1650/2000... Training loss: 0.5345\n",
      "Epoch: 1650/2000... Training loss: 0.5019\n",
      "Epoch: 1650/2000... Training loss: 0.4474\n",
      "Epoch: 1650/2000... Training loss: 0.3889\n",
      "Epoch: 1650/2000... Training loss: 0.2047\n",
      "Epoch: 1650/2000... Training loss: 0.3702\n",
      "Epoch: 1650/2000... Training loss: 0.4044\n",
      "Epoch: 1650/2000... Training loss: 0.5158\n",
      "Epoch: 1650/2000... Training loss: 0.3939\n",
      "Epoch: 1650/2000... Training loss: 0.4262\n",
      "Epoch: 1650/2000... Training loss: 0.3079\n",
      "Epoch: 1650/2000... Training loss: 0.3555\n",
      "Epoch: 1650/2000... Training loss: 0.3084\n",
      "Epoch: 1650/2000... Training loss: 0.3488\n",
      "Epoch: 1650/2000... Training loss: 0.3785\n",
      "Epoch: 1650/2000... Training loss: 0.3931\n",
      "Epoch: 1650/2000... Training loss: 0.4306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1650/2000... Training loss: 0.4350\n",
      "Epoch: 1650/2000... Training loss: 0.4003\n",
      "Epoch: 1650/2000... Training loss: 0.3310\n",
      "Epoch: 1650/2000... Training loss: 0.4098\n",
      "Epoch: 1650/2000... Training loss: 0.3994\n",
      "Epoch: 1650/2000... Training loss: 0.3904\n",
      "Epoch: 1651/2000... Training loss: 0.3347\n",
      "Epoch: 1651/2000... Training loss: 0.3585\n",
      "Epoch: 1651/2000... Training loss: 0.2715\n",
      "Epoch: 1651/2000... Training loss: 0.3721\n",
      "Epoch: 1651/2000... Training loss: 0.3266\n",
      "Epoch: 1651/2000... Training loss: 0.4042\n",
      "Epoch: 1651/2000... Training loss: 0.4661\n",
      "Epoch: 1651/2000... Training loss: 0.2178\n",
      "Epoch: 1651/2000... Training loss: 0.3135\n",
      "Epoch: 1651/2000... Training loss: 0.3283\n",
      "Epoch: 1651/2000... Training loss: 0.5658\n",
      "Epoch: 1651/2000... Training loss: 0.2683\n",
      "Epoch: 1651/2000... Training loss: 0.3450\n",
      "Epoch: 1651/2000... Training loss: 0.5409\n",
      "Epoch: 1651/2000... Training loss: 0.4784\n",
      "Epoch: 1651/2000... Training loss: 0.3545\n",
      "Epoch: 1651/2000... Training loss: 0.2118\n",
      "Epoch: 1651/2000... Training loss: 0.4658\n",
      "Epoch: 1651/2000... Training loss: 0.3557\n",
      "Epoch: 1651/2000... Training loss: 0.3765\n",
      "Epoch: 1651/2000... Training loss: 0.3286\n",
      "Epoch: 1651/2000... Training loss: 0.3510\n",
      "Epoch: 1651/2000... Training loss: 0.3984\n",
      "Epoch: 1651/2000... Training loss: 0.3495\n",
      "Epoch: 1651/2000... Training loss: 0.2588\n",
      "Epoch: 1651/2000... Training loss: 0.2628\n",
      "Epoch: 1651/2000... Training loss: 0.4121\n",
      "Epoch: 1651/2000... Training loss: 0.3573\n",
      "Epoch: 1651/2000... Training loss: 0.3552\n",
      "Epoch: 1651/2000... Training loss: 0.4069\n",
      "Epoch: 1651/2000... Training loss: 0.3670\n",
      "Epoch: 1652/2000... Training loss: 0.4947\n",
      "Epoch: 1652/2000... Training loss: 0.3108\n",
      "Epoch: 1652/2000... Training loss: 0.4491\n",
      "Epoch: 1652/2000... Training loss: 0.4486\n",
      "Epoch: 1652/2000... Training loss: 0.5034\n",
      "Epoch: 1652/2000... Training loss: 0.2650\n",
      "Epoch: 1652/2000... Training loss: 0.3538\n",
      "Epoch: 1652/2000... Training loss: 0.2782\n",
      "Epoch: 1652/2000... Training loss: 0.4403\n",
      "Epoch: 1652/2000... Training loss: 0.2770\n",
      "Epoch: 1652/2000... Training loss: 0.5622\n",
      "Epoch: 1652/2000... Training loss: 0.4334\n",
      "Epoch: 1652/2000... Training loss: 0.3705\n",
      "Epoch: 1652/2000... Training loss: 0.4152\n",
      "Epoch: 1652/2000... Training loss: 0.4357\n",
      "Epoch: 1652/2000... Training loss: 0.3177\n",
      "Epoch: 1652/2000... Training loss: 0.3839\n",
      "Epoch: 1652/2000... Training loss: 0.4060\n",
      "Epoch: 1652/2000... Training loss: 0.3752\n",
      "Epoch: 1652/2000... Training loss: 0.2879\n",
      "Epoch: 1652/2000... Training loss: 0.3517\n",
      "Epoch: 1652/2000... Training loss: 0.5099\n",
      "Epoch: 1652/2000... Training loss: 0.3155\n",
      "Epoch: 1652/2000... Training loss: 0.3221\n",
      "Epoch: 1652/2000... Training loss: 0.3974\n",
      "Epoch: 1652/2000... Training loss: 0.5072\n",
      "Epoch: 1652/2000... Training loss: 0.3671\n",
      "Epoch: 1652/2000... Training loss: 0.3639\n",
      "Epoch: 1652/2000... Training loss: 0.4453\n",
      "Epoch: 1652/2000... Training loss: 0.4330\n",
      "Epoch: 1652/2000... Training loss: 0.4585\n",
      "Epoch: 1653/2000... Training loss: 0.4693\n",
      "Epoch: 1653/2000... Training loss: 0.6020\n",
      "Epoch: 1653/2000... Training loss: 0.3093\n",
      "Epoch: 1653/2000... Training loss: 0.3071\n",
      "Epoch: 1653/2000... Training loss: 0.3462\n",
      "Epoch: 1653/2000... Training loss: 0.4551\n",
      "Epoch: 1653/2000... Training loss: 0.4824\n",
      "Epoch: 1653/2000... Training loss: 0.4608\n",
      "Epoch: 1653/2000... Training loss: 0.3130\n",
      "Epoch: 1653/2000... Training loss: 0.2409\n",
      "Epoch: 1653/2000... Training loss: 0.4009\n",
      "Epoch: 1653/2000... Training loss: 0.3928\n",
      "Epoch: 1653/2000... Training loss: 0.3300\n",
      "Epoch: 1653/2000... Training loss: 0.4559\n",
      "Epoch: 1653/2000... Training loss: 0.3600\n",
      "Epoch: 1653/2000... Training loss: 0.3257\n",
      "Epoch: 1653/2000... Training loss: 0.4511\n",
      "Epoch: 1653/2000... Training loss: 0.3699\n",
      "Epoch: 1653/2000... Training loss: 0.3421\n",
      "Epoch: 1653/2000... Training loss: 0.4218\n",
      "Epoch: 1653/2000... Training loss: 0.3578\n",
      "Epoch: 1653/2000... Training loss: 0.3489\n",
      "Epoch: 1653/2000... Training loss: 0.4412\n",
      "Epoch: 1653/2000... Training loss: 0.2638\n",
      "Epoch: 1653/2000... Training loss: 0.3688\n",
      "Epoch: 1653/2000... Training loss: 0.4826\n",
      "Epoch: 1653/2000... Training loss: 0.5203\n",
      "Epoch: 1653/2000... Training loss: 0.5300\n",
      "Epoch: 1653/2000... Training loss: 0.4242\n",
      "Epoch: 1653/2000... Training loss: 0.4282\n",
      "Epoch: 1653/2000... Training loss: 0.4693\n",
      "Epoch: 1654/2000... Training loss: 0.3736\n",
      "Epoch: 1654/2000... Training loss: 0.3773\n",
      "Epoch: 1654/2000... Training loss: 0.4467\n",
      "Epoch: 1654/2000... Training loss: 0.4186\n",
      "Epoch: 1654/2000... Training loss: 0.3311\n",
      "Epoch: 1654/2000... Training loss: 0.3318\n",
      "Epoch: 1654/2000... Training loss: 0.2410\n",
      "Epoch: 1654/2000... Training loss: 0.3598\n",
      "Epoch: 1654/2000... Training loss: 0.2810\n",
      "Epoch: 1654/2000... Training loss: 0.3300\n",
      "Epoch: 1654/2000... Training loss: 0.3319\n",
      "Epoch: 1654/2000... Training loss: 0.3372\n",
      "Epoch: 1654/2000... Training loss: 0.3671\n",
      "Epoch: 1654/2000... Training loss: 0.2434\n",
      "Epoch: 1654/2000... Training loss: 0.5399\n",
      "Epoch: 1654/2000... Training loss: 0.1872\n",
      "Epoch: 1654/2000... Training loss: 0.4502\n",
      "Epoch: 1654/2000... Training loss: 0.5035\n",
      "Epoch: 1654/2000... Training loss: 0.4302\n",
      "Epoch: 1654/2000... Training loss: 0.2946\n",
      "Epoch: 1654/2000... Training loss: 0.3146\n",
      "Epoch: 1654/2000... Training loss: 0.4198\n",
      "Epoch: 1654/2000... Training loss: 0.3489\n",
      "Epoch: 1654/2000... Training loss: 0.5740\n",
      "Epoch: 1654/2000... Training loss: 0.2448\n",
      "Epoch: 1654/2000... Training loss: 0.5791\n",
      "Epoch: 1654/2000... Training loss: 0.3414\n",
      "Epoch: 1654/2000... Training loss: 0.2715\n",
      "Epoch: 1654/2000... Training loss: 0.3626\n",
      "Epoch: 1654/2000... Training loss: 0.3784\n",
      "Epoch: 1654/2000... Training loss: 0.3622\n",
      "Epoch: 1655/2000... Training loss: 0.3652\n",
      "Epoch: 1655/2000... Training loss: 0.4389\n",
      "Epoch: 1655/2000... Training loss: 0.3764\n",
      "Epoch: 1655/2000... Training loss: 0.3880\n",
      "Epoch: 1655/2000... Training loss: 0.3951\n",
      "Epoch: 1655/2000... Training loss: 0.2794\n",
      "Epoch: 1655/2000... Training loss: 0.5886\n",
      "Epoch: 1655/2000... Training loss: 0.3421\n",
      "Epoch: 1655/2000... Training loss: 0.6019\n",
      "Epoch: 1655/2000... Training loss: 0.4594\n",
      "Epoch: 1655/2000... Training loss: 0.4009\n",
      "Epoch: 1655/2000... Training loss: 0.3366\n",
      "Epoch: 1655/2000... Training loss: 0.3316\n",
      "Epoch: 1655/2000... Training loss: 0.3031\n",
      "Epoch: 1655/2000... Training loss: 0.4640\n",
      "Epoch: 1655/2000... Training loss: 0.4394\n",
      "Epoch: 1655/2000... Training loss: 0.4486\n",
      "Epoch: 1655/2000... Training loss: 0.3330\n",
      "Epoch: 1655/2000... Training loss: 0.4596\n",
      "Epoch: 1655/2000... Training loss: 0.5001\n",
      "Epoch: 1655/2000... Training loss: 0.3628\n",
      "Epoch: 1655/2000... Training loss: 0.2976\n",
      "Epoch: 1655/2000... Training loss: 0.4120\n",
      "Epoch: 1655/2000... Training loss: 0.3430\n",
      "Epoch: 1655/2000... Training loss: 0.3216\n",
      "Epoch: 1655/2000... Training loss: 0.3389\n",
      "Epoch: 1655/2000... Training loss: 0.5476\n",
      "Epoch: 1655/2000... Training loss: 0.3870\n",
      "Epoch: 1655/2000... Training loss: 0.4390\n",
      "Epoch: 1655/2000... Training loss: 0.2802\n",
      "Epoch: 1655/2000... Training loss: 0.5074\n",
      "Epoch: 1656/2000... Training loss: 0.4704\n",
      "Epoch: 1656/2000... Training loss: 0.4319\n",
      "Epoch: 1656/2000... Training loss: 0.4880\n",
      "Epoch: 1656/2000... Training loss: 0.2416\n",
      "Epoch: 1656/2000... Training loss: 0.4968\n",
      "Epoch: 1656/2000... Training loss: 0.4701\n",
      "Epoch: 1656/2000... Training loss: 0.4817\n",
      "Epoch: 1656/2000... Training loss: 0.2958\n",
      "Epoch: 1656/2000... Training loss: 0.3080\n",
      "Epoch: 1656/2000... Training loss: 0.3452\n",
      "Epoch: 1656/2000... Training loss: 0.2916\n",
      "Epoch: 1656/2000... Training loss: 0.3118\n",
      "Epoch: 1656/2000... Training loss: 0.6216\n",
      "Epoch: 1656/2000... Training loss: 0.4584\n",
      "Epoch: 1656/2000... Training loss: 0.3508\n",
      "Epoch: 1656/2000... Training loss: 0.4051\n",
      "Epoch: 1656/2000... Training loss: 0.4069\n",
      "Epoch: 1656/2000... Training loss: 0.5800\n",
      "Epoch: 1656/2000... Training loss: 0.3978\n",
      "Epoch: 1656/2000... Training loss: 0.2876\n",
      "Epoch: 1656/2000... Training loss: 0.3910\n",
      "Epoch: 1656/2000... Training loss: 0.4911\n",
      "Epoch: 1656/2000... Training loss: 0.5916\n",
      "Epoch: 1656/2000... Training loss: 0.5410\n",
      "Epoch: 1656/2000... Training loss: 0.4390\n",
      "Epoch: 1656/2000... Training loss: 0.5392\n",
      "Epoch: 1656/2000... Training loss: 0.2175\n",
      "Epoch: 1656/2000... Training loss: 0.3354\n",
      "Epoch: 1656/2000... Training loss: 0.5033\n",
      "Epoch: 1656/2000... Training loss: 0.4250\n",
      "Epoch: 1656/2000... Training loss: 0.3056\n",
      "Epoch: 1657/2000... Training loss: 0.3152\n",
      "Epoch: 1657/2000... Training loss: 0.4195\n",
      "Epoch: 1657/2000... Training loss: 0.6759\n",
      "Epoch: 1657/2000... Training loss: 0.5611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1657/2000... Training loss: 0.2737\n",
      "Epoch: 1657/2000... Training loss: 0.4288\n",
      "Epoch: 1657/2000... Training loss: 0.5394\n",
      "Epoch: 1657/2000... Training loss: 0.4288\n",
      "Epoch: 1657/2000... Training loss: 0.4240\n",
      "Epoch: 1657/2000... Training loss: 0.3889\n",
      "Epoch: 1657/2000... Training loss: 0.4068\n",
      "Epoch: 1657/2000... Training loss: 0.1303\n",
      "Epoch: 1657/2000... Training loss: 0.2854\n",
      "Epoch: 1657/2000... Training loss: 0.4210\n",
      "Epoch: 1657/2000... Training loss: 0.2641\n",
      "Epoch: 1657/2000... Training loss: 0.2528\n",
      "Epoch: 1657/2000... Training loss: 0.4968\n",
      "Epoch: 1657/2000... Training loss: 0.4801\n",
      "Epoch: 1657/2000... Training loss: 0.3220\n",
      "Epoch: 1657/2000... Training loss: 0.4600\n",
      "Epoch: 1657/2000... Training loss: 0.6857\n",
      "Epoch: 1657/2000... Training loss: 0.3762\n",
      "Epoch: 1657/2000... Training loss: 0.2644\n",
      "Epoch: 1657/2000... Training loss: 0.3439\n",
      "Epoch: 1657/2000... Training loss: 0.3817\n",
      "Epoch: 1657/2000... Training loss: 0.3742\n",
      "Epoch: 1657/2000... Training loss: 0.6083\n",
      "Epoch: 1657/2000... Training loss: 0.4801\n",
      "Epoch: 1657/2000... Training loss: 0.2840\n",
      "Epoch: 1657/2000... Training loss: 0.4677\n",
      "Epoch: 1657/2000... Training loss: 0.3445\n",
      "Epoch: 1658/2000... Training loss: 0.6233\n",
      "Epoch: 1658/2000... Training loss: 0.2875\n",
      "Epoch: 1658/2000... Training loss: 0.4235\n",
      "Epoch: 1658/2000... Training loss: 0.4608\n",
      "Epoch: 1658/2000... Training loss: 0.5333\n",
      "Epoch: 1658/2000... Training loss: 0.4386\n",
      "Epoch: 1658/2000... Training loss: 0.6536\n",
      "Epoch: 1658/2000... Training loss: 0.5436\n",
      "Epoch: 1658/2000... Training loss: 0.4630\n",
      "Epoch: 1658/2000... Training loss: 0.5652\n",
      "Epoch: 1658/2000... Training loss: 0.3914\n",
      "Epoch: 1658/2000... Training loss: 0.2089\n",
      "Epoch: 1658/2000... Training loss: 0.3893\n",
      "Epoch: 1658/2000... Training loss: 0.4308\n",
      "Epoch: 1658/2000... Training loss: 0.2506\n",
      "Epoch: 1658/2000... Training loss: 0.3787\n",
      "Epoch: 1658/2000... Training loss: 0.2783\n",
      "Epoch: 1658/2000... Training loss: 0.3023\n",
      "Epoch: 1658/2000... Training loss: 0.5304\n",
      "Epoch: 1658/2000... Training loss: 0.3022\n",
      "Epoch: 1658/2000... Training loss: 0.2831\n",
      "Epoch: 1658/2000... Training loss: 0.4256\n",
      "Epoch: 1658/2000... Training loss: 0.2550\n",
      "Epoch: 1658/2000... Training loss: 0.2242\n",
      "Epoch: 1658/2000... Training loss: 0.4532\n",
      "Epoch: 1658/2000... Training loss: 0.3975\n",
      "Epoch: 1658/2000... Training loss: 0.3604\n",
      "Epoch: 1658/2000... Training loss: 0.4373\n",
      "Epoch: 1658/2000... Training loss: 0.3949\n",
      "Epoch: 1658/2000... Training loss: 0.4265\n",
      "Epoch: 1658/2000... Training loss: 0.7041\n",
      "Epoch: 1659/2000... Training loss: 0.2239\n",
      "Epoch: 1659/2000... Training loss: 0.4012\n",
      "Epoch: 1659/2000... Training loss: 0.2935\n",
      "Epoch: 1659/2000... Training loss: 0.3367\n",
      "Epoch: 1659/2000... Training loss: 0.3849\n",
      "Epoch: 1659/2000... Training loss: 0.3199\n",
      "Epoch: 1659/2000... Training loss: 0.3712\n",
      "Epoch: 1659/2000... Training loss: 0.4613\n",
      "Epoch: 1659/2000... Training loss: 0.3669\n",
      "Epoch: 1659/2000... Training loss: 0.3384\n",
      "Epoch: 1659/2000... Training loss: 0.5515\n",
      "Epoch: 1659/2000... Training loss: 0.4571\n",
      "Epoch: 1659/2000... Training loss: 0.3544\n",
      "Epoch: 1659/2000... Training loss: 0.4839\n",
      "Epoch: 1659/2000... Training loss: 0.3755\n",
      "Epoch: 1659/2000... Training loss: 0.3773\n",
      "Epoch: 1659/2000... Training loss: 0.4081\n",
      "Epoch: 1659/2000... Training loss: 0.2534\n",
      "Epoch: 1659/2000... Training loss: 0.3583\n",
      "Epoch: 1659/2000... Training loss: 0.2737\n",
      "Epoch: 1659/2000... Training loss: 0.3524\n",
      "Epoch: 1659/2000... Training loss: 0.2346\n",
      "Epoch: 1659/2000... Training loss: 0.3415\n",
      "Epoch: 1659/2000... Training loss: 0.4369\n",
      "Epoch: 1659/2000... Training loss: 0.4789\n",
      "Epoch: 1659/2000... Training loss: 0.4285\n",
      "Epoch: 1659/2000... Training loss: 0.3442\n",
      "Epoch: 1659/2000... Training loss: 0.3010\n",
      "Epoch: 1659/2000... Training loss: 0.3097\n",
      "Epoch: 1659/2000... Training loss: 0.3511\n",
      "Epoch: 1659/2000... Training loss: 0.3164\n",
      "Epoch: 1660/2000... Training loss: 0.4675\n",
      "Epoch: 1660/2000... Training loss: 0.3806\n",
      "Epoch: 1660/2000... Training loss: 0.4132\n",
      "Epoch: 1660/2000... Training loss: 0.4733\n",
      "Epoch: 1660/2000... Training loss: 0.3834\n",
      "Epoch: 1660/2000... Training loss: 0.3074\n",
      "Epoch: 1660/2000... Training loss: 0.4148\n",
      "Epoch: 1660/2000... Training loss: 0.4943\n",
      "Epoch: 1660/2000... Training loss: 0.4149\n",
      "Epoch: 1660/2000... Training loss: 0.2781\n",
      "Epoch: 1660/2000... Training loss: 0.3383\n",
      "Epoch: 1660/2000... Training loss: 0.4229\n",
      "Epoch: 1660/2000... Training loss: 0.4817\n",
      "Epoch: 1660/2000... Training loss: 0.4626\n",
      "Epoch: 1660/2000... Training loss: 0.4818\n",
      "Epoch: 1660/2000... Training loss: 0.2141\n",
      "Epoch: 1660/2000... Training loss: 0.5743\n",
      "Epoch: 1660/2000... Training loss: 0.5304\n",
      "Epoch: 1660/2000... Training loss: 0.4995\n",
      "Epoch: 1660/2000... Training loss: 0.3683\n",
      "Epoch: 1660/2000... Training loss: 0.3605\n",
      "Epoch: 1660/2000... Training loss: 0.4009\n",
      "Epoch: 1660/2000... Training loss: 0.4703\n",
      "Epoch: 1660/2000... Training loss: 0.3944\n",
      "Epoch: 1660/2000... Training loss: 0.3990\n",
      "Epoch: 1660/2000... Training loss: 0.3998\n",
      "Epoch: 1660/2000... Training loss: 0.4832\n",
      "Epoch: 1660/2000... Training loss: 0.3773\n",
      "Epoch: 1660/2000... Training loss: 0.3654\n",
      "Epoch: 1660/2000... Training loss: 0.6174\n",
      "Epoch: 1660/2000... Training loss: 0.4528\n",
      "Epoch: 1661/2000... Training loss: 0.4381\n",
      "Epoch: 1661/2000... Training loss: 0.3427\n",
      "Epoch: 1661/2000... Training loss: 0.4085\n",
      "Epoch: 1661/2000... Training loss: 0.4293\n",
      "Epoch: 1661/2000... Training loss: 0.5533\n",
      "Epoch: 1661/2000... Training loss: 0.4523\n",
      "Epoch: 1661/2000... Training loss: 0.3650\n",
      "Epoch: 1661/2000... Training loss: 0.3429\n",
      "Epoch: 1661/2000... Training loss: 0.3838\n",
      "Epoch: 1661/2000... Training loss: 0.2837\n",
      "Epoch: 1661/2000... Training loss: 0.3923\n",
      "Epoch: 1661/2000... Training loss: 0.3172\n",
      "Epoch: 1661/2000... Training loss: 0.3108\n",
      "Epoch: 1661/2000... Training loss: 0.3241\n",
      "Epoch: 1661/2000... Training loss: 0.3895\n",
      "Epoch: 1661/2000... Training loss: 0.4875\n",
      "Epoch: 1661/2000... Training loss: 0.4361\n",
      "Epoch: 1661/2000... Training loss: 0.5328\n",
      "Epoch: 1661/2000... Training loss: 0.6111\n",
      "Epoch: 1661/2000... Training loss: 0.2929\n",
      "Epoch: 1661/2000... Training loss: 0.4634\n",
      "Epoch: 1661/2000... Training loss: 0.4857\n",
      "Epoch: 1661/2000... Training loss: 0.3517\n",
      "Epoch: 1661/2000... Training loss: 0.3168\n",
      "Epoch: 1661/2000... Training loss: 0.3699\n",
      "Epoch: 1661/2000... Training loss: 0.3798\n",
      "Epoch: 1661/2000... Training loss: 0.3373\n",
      "Epoch: 1661/2000... Training loss: 0.5137\n",
      "Epoch: 1661/2000... Training loss: 0.3343\n",
      "Epoch: 1661/2000... Training loss: 0.6623\n",
      "Epoch: 1661/2000... Training loss: 0.3348\n",
      "Epoch: 1662/2000... Training loss: 0.2743\n",
      "Epoch: 1662/2000... Training loss: 0.4774\n",
      "Epoch: 1662/2000... Training loss: 0.4551\n",
      "Epoch: 1662/2000... Training loss: 0.3831\n",
      "Epoch: 1662/2000... Training loss: 0.3330\n",
      "Epoch: 1662/2000... Training loss: 0.3194\n",
      "Epoch: 1662/2000... Training loss: 0.4969\n",
      "Epoch: 1662/2000... Training loss: 0.3515\n",
      "Epoch: 1662/2000... Training loss: 0.3112\n",
      "Epoch: 1662/2000... Training loss: 0.3546\n",
      "Epoch: 1662/2000... Training loss: 0.2019\n",
      "Epoch: 1662/2000... Training loss: 0.4797\n",
      "Epoch: 1662/2000... Training loss: 0.4720\n",
      "Epoch: 1662/2000... Training loss: 0.4045\n",
      "Epoch: 1662/2000... Training loss: 0.3611\n",
      "Epoch: 1662/2000... Training loss: 0.3417\n",
      "Epoch: 1662/2000... Training loss: 0.2994\n",
      "Epoch: 1662/2000... Training loss: 0.3962\n",
      "Epoch: 1662/2000... Training loss: 0.4089\n",
      "Epoch: 1662/2000... Training loss: 0.6290\n",
      "Epoch: 1662/2000... Training loss: 0.3677\n",
      "Epoch: 1662/2000... Training loss: 0.3614\n",
      "Epoch: 1662/2000... Training loss: 0.1967\n",
      "Epoch: 1662/2000... Training loss: 0.2701\n",
      "Epoch: 1662/2000... Training loss: 0.4970\n",
      "Epoch: 1662/2000... Training loss: 0.3628\n",
      "Epoch: 1662/2000... Training loss: 0.3481\n",
      "Epoch: 1662/2000... Training loss: 0.4334\n",
      "Epoch: 1662/2000... Training loss: 0.4224\n",
      "Epoch: 1662/2000... Training loss: 0.5588\n",
      "Epoch: 1662/2000... Training loss: 0.3944\n",
      "Epoch: 1663/2000... Training loss: 0.4081\n",
      "Epoch: 1663/2000... Training loss: 0.3907\n",
      "Epoch: 1663/2000... Training loss: 0.4399\n",
      "Epoch: 1663/2000... Training loss: 0.3731\n",
      "Epoch: 1663/2000... Training loss: 0.2828\n",
      "Epoch: 1663/2000... Training loss: 0.3894\n",
      "Epoch: 1663/2000... Training loss: 0.3288\n",
      "Epoch: 1663/2000... Training loss: 0.4564\n",
      "Epoch: 1663/2000... Training loss: 0.3840\n",
      "Epoch: 1663/2000... Training loss: 0.3726\n",
      "Epoch: 1663/2000... Training loss: 0.3143\n",
      "Epoch: 1663/2000... Training loss: 0.5362\n",
      "Epoch: 1663/2000... Training loss: 0.5609\n",
      "Epoch: 1663/2000... Training loss: 0.3477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1663/2000... Training loss: 0.2690\n",
      "Epoch: 1663/2000... Training loss: 0.3196\n",
      "Epoch: 1663/2000... Training loss: 0.5800\n",
      "Epoch: 1663/2000... Training loss: 0.5202\n",
      "Epoch: 1663/2000... Training loss: 0.4308\n",
      "Epoch: 1663/2000... Training loss: 0.3883\n",
      "Epoch: 1663/2000... Training loss: 0.3649\n",
      "Epoch: 1663/2000... Training loss: 0.2977\n",
      "Epoch: 1663/2000... Training loss: 0.3691\n",
      "Epoch: 1663/2000... Training loss: 0.4612\n",
      "Epoch: 1663/2000... Training loss: 0.3884\n",
      "Epoch: 1663/2000... Training loss: 0.3818\n",
      "Epoch: 1663/2000... Training loss: 0.4067\n",
      "Epoch: 1663/2000... Training loss: 0.3880\n",
      "Epoch: 1663/2000... Training loss: 0.3928\n",
      "Epoch: 1663/2000... Training loss: 0.2842\n",
      "Epoch: 1663/2000... Training loss: 0.3403\n",
      "Epoch: 1664/2000... Training loss: 0.5133\n",
      "Epoch: 1664/2000... Training loss: 0.3864\n",
      "Epoch: 1664/2000... Training loss: 0.4244\n",
      "Epoch: 1664/2000... Training loss: 0.3140\n",
      "Epoch: 1664/2000... Training loss: 0.4097\n",
      "Epoch: 1664/2000... Training loss: 0.4377\n",
      "Epoch: 1664/2000... Training loss: 0.4466\n",
      "Epoch: 1664/2000... Training loss: 0.4222\n",
      "Epoch: 1664/2000... Training loss: 0.6148\n",
      "Epoch: 1664/2000... Training loss: 0.2713\n",
      "Epoch: 1664/2000... Training loss: 0.2984\n",
      "Epoch: 1664/2000... Training loss: 0.4176\n",
      "Epoch: 1664/2000... Training loss: 0.4268\n",
      "Epoch: 1664/2000... Training loss: 0.2963\n",
      "Epoch: 1664/2000... Training loss: 0.3133\n",
      "Epoch: 1664/2000... Training loss: 0.2550\n",
      "Epoch: 1664/2000... Training loss: 0.4754\n",
      "Epoch: 1664/2000... Training loss: 0.3761\n",
      "Epoch: 1664/2000... Training loss: 0.3575\n",
      "Epoch: 1664/2000... Training loss: 0.2614\n",
      "Epoch: 1664/2000... Training loss: 0.5304\n",
      "Epoch: 1664/2000... Training loss: 0.2398\n",
      "Epoch: 1664/2000... Training loss: 0.5232\n",
      "Epoch: 1664/2000... Training loss: 0.5083\n",
      "Epoch: 1664/2000... Training loss: 0.3454\n",
      "Epoch: 1664/2000... Training loss: 0.4460\n",
      "Epoch: 1664/2000... Training loss: 0.4836\n",
      "Epoch: 1664/2000... Training loss: 0.3236\n",
      "Epoch: 1664/2000... Training loss: 0.5851\n",
      "Epoch: 1664/2000... Training loss: 0.4637\n",
      "Epoch: 1664/2000... Training loss: 0.5378\n",
      "Epoch: 1665/2000... Training loss: 0.5215\n",
      "Epoch: 1665/2000... Training loss: 0.4156\n",
      "Epoch: 1665/2000... Training loss: 0.4105\n",
      "Epoch: 1665/2000... Training loss: 0.4338\n",
      "Epoch: 1665/2000... Training loss: 0.4537\n",
      "Epoch: 1665/2000... Training loss: 0.3095\n",
      "Epoch: 1665/2000... Training loss: 0.3282\n",
      "Epoch: 1665/2000... Training loss: 0.3816\n",
      "Epoch: 1665/2000... Training loss: 0.5111\n",
      "Epoch: 1665/2000... Training loss: 0.5498\n",
      "Epoch: 1665/2000... Training loss: 0.4562\n",
      "Epoch: 1665/2000... Training loss: 0.4346\n",
      "Epoch: 1665/2000... Training loss: 0.4734\n",
      "Epoch: 1665/2000... Training loss: 0.3544\n",
      "Epoch: 1665/2000... Training loss: 0.4552\n",
      "Epoch: 1665/2000... Training loss: 0.2898\n",
      "Epoch: 1665/2000... Training loss: 0.4706\n",
      "Epoch: 1665/2000... Training loss: 0.3989\n",
      "Epoch: 1665/2000... Training loss: 0.3761\n",
      "Epoch: 1665/2000... Training loss: 0.4153\n",
      "Epoch: 1665/2000... Training loss: 0.2935\n",
      "Epoch: 1665/2000... Training loss: 0.4563\n",
      "Epoch: 1665/2000... Training loss: 0.4829\n",
      "Epoch: 1665/2000... Training loss: 0.2426\n",
      "Epoch: 1665/2000... Training loss: 0.3652\n",
      "Epoch: 1665/2000... Training loss: 0.4056\n",
      "Epoch: 1665/2000... Training loss: 0.5156\n",
      "Epoch: 1665/2000... Training loss: 0.3532\n",
      "Epoch: 1665/2000... Training loss: 0.4361\n",
      "Epoch: 1665/2000... Training loss: 0.4203\n",
      "Epoch: 1665/2000... Training loss: 0.4947\n",
      "Epoch: 1666/2000... Training loss: 0.3989\n",
      "Epoch: 1666/2000... Training loss: 0.5411\n",
      "Epoch: 1666/2000... Training loss: 0.4034\n",
      "Epoch: 1666/2000... Training loss: 0.3373\n",
      "Epoch: 1666/2000... Training loss: 0.5042\n",
      "Epoch: 1666/2000... Training loss: 0.3298\n",
      "Epoch: 1666/2000... Training loss: 0.3883\n",
      "Epoch: 1666/2000... Training loss: 0.4347\n",
      "Epoch: 1666/2000... Training loss: 0.5917\n",
      "Epoch: 1666/2000... Training loss: 0.3955\n",
      "Epoch: 1666/2000... Training loss: 0.3250\n",
      "Epoch: 1666/2000... Training loss: 0.4193\n",
      "Epoch: 1666/2000... Training loss: 0.5088\n",
      "Epoch: 1666/2000... Training loss: 0.4517\n",
      "Epoch: 1666/2000... Training loss: 0.4371\n",
      "Epoch: 1666/2000... Training loss: 0.2471\n",
      "Epoch: 1666/2000... Training loss: 0.3271\n",
      "Epoch: 1666/2000... Training loss: 0.4299\n",
      "Epoch: 1666/2000... Training loss: 0.5848\n",
      "Epoch: 1666/2000... Training loss: 0.4932\n",
      "Epoch: 1666/2000... Training loss: 0.3167\n",
      "Epoch: 1666/2000... Training loss: 0.4088\n",
      "Epoch: 1666/2000... Training loss: 0.5591\n",
      "Epoch: 1666/2000... Training loss: 0.3511\n",
      "Epoch: 1666/2000... Training loss: 0.4516\n",
      "Epoch: 1666/2000... Training loss: 0.3319\n",
      "Epoch: 1666/2000... Training loss: 0.4340\n",
      "Epoch: 1666/2000... Training loss: 0.5299\n",
      "Epoch: 1666/2000... Training loss: 0.4989\n",
      "Epoch: 1666/2000... Training loss: 0.4167\n",
      "Epoch: 1666/2000... Training loss: 0.3511\n",
      "Epoch: 1667/2000... Training loss: 0.4182\n",
      "Epoch: 1667/2000... Training loss: 0.3476\n",
      "Epoch: 1667/2000... Training loss: 0.5981\n",
      "Epoch: 1667/2000... Training loss: 0.3461\n",
      "Epoch: 1667/2000... Training loss: 0.4015\n",
      "Epoch: 1667/2000... Training loss: 0.4237\n",
      "Epoch: 1667/2000... Training loss: 0.4089\n",
      "Epoch: 1667/2000... Training loss: 0.4931\n",
      "Epoch: 1667/2000... Training loss: 0.4478\n",
      "Epoch: 1667/2000... Training loss: 0.3578\n",
      "Epoch: 1667/2000... Training loss: 0.4088\n",
      "Epoch: 1667/2000... Training loss: 0.5468\n",
      "Epoch: 1667/2000... Training loss: 0.2915\n",
      "Epoch: 1667/2000... Training loss: 0.3389\n",
      "Epoch: 1667/2000... Training loss: 0.2160\n",
      "Epoch: 1667/2000... Training loss: 0.4066\n",
      "Epoch: 1667/2000... Training loss: 0.3955\n",
      "Epoch: 1667/2000... Training loss: 0.3243\n",
      "Epoch: 1667/2000... Training loss: 0.2985\n",
      "Epoch: 1667/2000... Training loss: 0.2614\n",
      "Epoch: 1667/2000... Training loss: 0.3530\n",
      "Epoch: 1667/2000... Training loss: 0.2932\n",
      "Epoch: 1667/2000... Training loss: 0.4013\n",
      "Epoch: 1667/2000... Training loss: 0.2871\n",
      "Epoch: 1667/2000... Training loss: 0.4033\n",
      "Epoch: 1667/2000... Training loss: 0.4349\n",
      "Epoch: 1667/2000... Training loss: 0.4176\n",
      "Epoch: 1667/2000... Training loss: 0.4589\n",
      "Epoch: 1667/2000... Training loss: 0.2653\n",
      "Epoch: 1667/2000... Training loss: 0.3904\n",
      "Epoch: 1667/2000... Training loss: 0.3837\n",
      "Epoch: 1668/2000... Training loss: 0.4130\n",
      "Epoch: 1668/2000... Training loss: 0.2847\n",
      "Epoch: 1668/2000... Training loss: 0.5778\n",
      "Epoch: 1668/2000... Training loss: 0.3544\n",
      "Epoch: 1668/2000... Training loss: 0.4341\n",
      "Epoch: 1668/2000... Training loss: 0.4071\n",
      "Epoch: 1668/2000... Training loss: 0.3964\n",
      "Epoch: 1668/2000... Training loss: 0.3607\n",
      "Epoch: 1668/2000... Training loss: 0.3740\n",
      "Epoch: 1668/2000... Training loss: 0.3963\n",
      "Epoch: 1668/2000... Training loss: 0.4299\n",
      "Epoch: 1668/2000... Training loss: 0.2233\n",
      "Epoch: 1668/2000... Training loss: 0.3174\n",
      "Epoch: 1668/2000... Training loss: 0.4484\n",
      "Epoch: 1668/2000... Training loss: 0.4880\n",
      "Epoch: 1668/2000... Training loss: 0.4826\n",
      "Epoch: 1668/2000... Training loss: 0.3497\n",
      "Epoch: 1668/2000... Training loss: 0.5130\n",
      "Epoch: 1668/2000... Training loss: 0.4965\n",
      "Epoch: 1668/2000... Training loss: 0.3991\n",
      "Epoch: 1668/2000... Training loss: 0.4441\n",
      "Epoch: 1668/2000... Training loss: 0.5719\n",
      "Epoch: 1668/2000... Training loss: 0.2446\n",
      "Epoch: 1668/2000... Training loss: 0.3736\n",
      "Epoch: 1668/2000... Training loss: 0.4478\n",
      "Epoch: 1668/2000... Training loss: 0.3039\n",
      "Epoch: 1668/2000... Training loss: 0.2902\n",
      "Epoch: 1668/2000... Training loss: 0.3403\n",
      "Epoch: 1668/2000... Training loss: 0.4496\n",
      "Epoch: 1668/2000... Training loss: 0.4130\n",
      "Epoch: 1668/2000... Training loss: 0.4346\n",
      "Epoch: 1669/2000... Training loss: 0.3601\n",
      "Epoch: 1669/2000... Training loss: 0.5325\n",
      "Epoch: 1669/2000... Training loss: 0.4032\n",
      "Epoch: 1669/2000... Training loss: 0.2804\n",
      "Epoch: 1669/2000... Training loss: 0.3507\n",
      "Epoch: 1669/2000... Training loss: 0.3169\n",
      "Epoch: 1669/2000... Training loss: 0.5255\n",
      "Epoch: 1669/2000... Training loss: 0.4752\n",
      "Epoch: 1669/2000... Training loss: 0.3967\n",
      "Epoch: 1669/2000... Training loss: 0.3484\n",
      "Epoch: 1669/2000... Training loss: 0.4813\n",
      "Epoch: 1669/2000... Training loss: 0.3358\n",
      "Epoch: 1669/2000... Training loss: 0.3437\n",
      "Epoch: 1669/2000... Training loss: 0.4254\n",
      "Epoch: 1669/2000... Training loss: 0.3113\n",
      "Epoch: 1669/2000... Training loss: 0.4939\n",
      "Epoch: 1669/2000... Training loss: 0.4050\n",
      "Epoch: 1669/2000... Training loss: 0.4036\n",
      "Epoch: 1669/2000... Training loss: 0.5815\n",
      "Epoch: 1669/2000... Training loss: 0.3983\n",
      "Epoch: 1669/2000... Training loss: 0.2357\n",
      "Epoch: 1669/2000... Training loss: 0.4303\n",
      "Epoch: 1669/2000... Training loss: 0.4113\n",
      "Epoch: 1669/2000... Training loss: 0.5439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1669/2000... Training loss: 0.4633\n",
      "Epoch: 1669/2000... Training loss: 0.2587\n",
      "Epoch: 1669/2000... Training loss: 0.3497\n",
      "Epoch: 1669/2000... Training loss: 0.4853\n",
      "Epoch: 1669/2000... Training loss: 0.5056\n",
      "Epoch: 1669/2000... Training loss: 0.3178\n",
      "Epoch: 1669/2000... Training loss: 0.3850\n",
      "Epoch: 1670/2000... Training loss: 0.5943\n",
      "Epoch: 1670/2000... Training loss: 0.3420\n",
      "Epoch: 1670/2000... Training loss: 0.4188\n",
      "Epoch: 1670/2000... Training loss: 0.3894\n",
      "Epoch: 1670/2000... Training loss: 0.3314\n",
      "Epoch: 1670/2000... Training loss: 0.4549\n",
      "Epoch: 1670/2000... Training loss: 0.3676\n",
      "Epoch: 1670/2000... Training loss: 0.2459\n",
      "Epoch: 1670/2000... Training loss: 0.3979\n",
      "Epoch: 1670/2000... Training loss: 0.3008\n",
      "Epoch: 1670/2000... Training loss: 0.2649\n",
      "Epoch: 1670/2000... Training loss: 0.1847\n",
      "Epoch: 1670/2000... Training loss: 0.5960\n",
      "Epoch: 1670/2000... Training loss: 0.2920\n",
      "Epoch: 1670/2000... Training loss: 0.3726\n",
      "Epoch: 1670/2000... Training loss: 0.2026\n",
      "Epoch: 1670/2000... Training loss: 0.4346\n",
      "Epoch: 1670/2000... Training loss: 0.3847\n",
      "Epoch: 1670/2000... Training loss: 0.4596\n",
      "Epoch: 1670/2000... Training loss: 0.3250\n",
      "Epoch: 1670/2000... Training loss: 0.1685\n",
      "Epoch: 1670/2000... Training loss: 0.3081\n",
      "Epoch: 1670/2000... Training loss: 0.4453\n",
      "Epoch: 1670/2000... Training loss: 0.4247\n",
      "Epoch: 1670/2000... Training loss: 0.4291\n",
      "Epoch: 1670/2000... Training loss: 0.4043\n",
      "Epoch: 1670/2000... Training loss: 0.3882\n",
      "Epoch: 1670/2000... Training loss: 0.5535\n",
      "Epoch: 1670/2000... Training loss: 0.4556\n",
      "Epoch: 1670/2000... Training loss: 0.4205\n",
      "Epoch: 1670/2000... Training loss: 0.3470\n",
      "Epoch: 1671/2000... Training loss: 0.2879\n",
      "Epoch: 1671/2000... Training loss: 0.4402\n",
      "Epoch: 1671/2000... Training loss: 0.5751\n",
      "Epoch: 1671/2000... Training loss: 0.4390\n",
      "Epoch: 1671/2000... Training loss: 0.2535\n",
      "Epoch: 1671/2000... Training loss: 0.5121\n",
      "Epoch: 1671/2000... Training loss: 0.3499\n",
      "Epoch: 1671/2000... Training loss: 0.3794\n",
      "Epoch: 1671/2000... Training loss: 0.4738\n",
      "Epoch: 1671/2000... Training loss: 0.2115\n",
      "Epoch: 1671/2000... Training loss: 0.5257\n",
      "Epoch: 1671/2000... Training loss: 0.3455\n",
      "Epoch: 1671/2000... Training loss: 0.4482\n",
      "Epoch: 1671/2000... Training loss: 0.2118\n",
      "Epoch: 1671/2000... Training loss: 0.4903\n",
      "Epoch: 1671/2000... Training loss: 0.3960\n",
      "Epoch: 1671/2000... Training loss: 0.3685\n",
      "Epoch: 1671/2000... Training loss: 0.3589\n",
      "Epoch: 1671/2000... Training loss: 0.3589\n",
      "Epoch: 1671/2000... Training loss: 0.1747\n",
      "Epoch: 1671/2000... Training loss: 0.5708\n",
      "Epoch: 1671/2000... Training loss: 0.4391\n",
      "Epoch: 1671/2000... Training loss: 0.3055\n",
      "Epoch: 1671/2000... Training loss: 0.3313\n",
      "Epoch: 1671/2000... Training loss: 0.2924\n",
      "Epoch: 1671/2000... Training loss: 0.6289\n",
      "Epoch: 1671/2000... Training loss: 0.4585\n",
      "Epoch: 1671/2000... Training loss: 0.3360\n",
      "Epoch: 1671/2000... Training loss: 0.5202\n",
      "Epoch: 1671/2000... Training loss: 0.3748\n",
      "Epoch: 1671/2000... Training loss: 0.3094\n",
      "Epoch: 1672/2000... Training loss: 0.3369\n",
      "Epoch: 1672/2000... Training loss: 0.2835\n",
      "Epoch: 1672/2000... Training loss: 0.4771\n",
      "Epoch: 1672/2000... Training loss: 0.4354\n",
      "Epoch: 1672/2000... Training loss: 0.4355\n",
      "Epoch: 1672/2000... Training loss: 0.4627\n",
      "Epoch: 1672/2000... Training loss: 0.2964\n",
      "Epoch: 1672/2000... Training loss: 0.3427\n",
      "Epoch: 1672/2000... Training loss: 0.2629\n",
      "Epoch: 1672/2000... Training loss: 0.4581\n",
      "Epoch: 1672/2000... Training loss: 0.4166\n",
      "Epoch: 1672/2000... Training loss: 0.3576\n",
      "Epoch: 1672/2000... Training loss: 0.4346\n",
      "Epoch: 1672/2000... Training loss: 0.4782\n",
      "Epoch: 1672/2000... Training loss: 0.5194\n",
      "Epoch: 1672/2000... Training loss: 0.4221\n",
      "Epoch: 1672/2000... Training loss: 0.3780\n",
      "Epoch: 1672/2000... Training loss: 0.3504\n",
      "Epoch: 1672/2000... Training loss: 0.3853\n",
      "Epoch: 1672/2000... Training loss: 0.5220\n",
      "Epoch: 1672/2000... Training loss: 0.2835\n",
      "Epoch: 1672/2000... Training loss: 0.3008\n",
      "Epoch: 1672/2000... Training loss: 0.4288\n",
      "Epoch: 1672/2000... Training loss: 0.3284\n",
      "Epoch: 1672/2000... Training loss: 0.3598\n",
      "Epoch: 1672/2000... Training loss: 0.3119\n",
      "Epoch: 1672/2000... Training loss: 0.3976\n",
      "Epoch: 1672/2000... Training loss: 0.3357\n",
      "Epoch: 1672/2000... Training loss: 0.3659\n",
      "Epoch: 1672/2000... Training loss: 0.4765\n",
      "Epoch: 1672/2000... Training loss: 0.4051\n",
      "Epoch: 1673/2000... Training loss: 0.4948\n",
      "Epoch: 1673/2000... Training loss: 0.4498\n",
      "Epoch: 1673/2000... Training loss: 0.5081\n",
      "Epoch: 1673/2000... Training loss: 0.4170\n",
      "Epoch: 1673/2000... Training loss: 0.3709\n",
      "Epoch: 1673/2000... Training loss: 0.4089\n",
      "Epoch: 1673/2000... Training loss: 0.3632\n",
      "Epoch: 1673/2000... Training loss: 0.2933\n",
      "Epoch: 1673/2000... Training loss: 0.4036\n",
      "Epoch: 1673/2000... Training loss: 0.4926\n",
      "Epoch: 1673/2000... Training loss: 0.3061\n",
      "Epoch: 1673/2000... Training loss: 0.3308\n",
      "Epoch: 1673/2000... Training loss: 0.5201\n",
      "Epoch: 1673/2000... Training loss: 0.5713\n",
      "Epoch: 1673/2000... Training loss: 0.5125\n",
      "Epoch: 1673/2000... Training loss: 0.2976\n",
      "Epoch: 1673/2000... Training loss: 0.2746\n",
      "Epoch: 1673/2000... Training loss: 0.5511\n",
      "Epoch: 1673/2000... Training loss: 0.3087\n",
      "Epoch: 1673/2000... Training loss: 0.3684\n",
      "Epoch: 1673/2000... Training loss: 0.2875\n",
      "Epoch: 1673/2000... Training loss: 0.3965\n",
      "Epoch: 1673/2000... Training loss: 0.3910\n",
      "Epoch: 1673/2000... Training loss: 0.5396\n",
      "Epoch: 1673/2000... Training loss: 0.4488\n",
      "Epoch: 1673/2000... Training loss: 0.4394\n",
      "Epoch: 1673/2000... Training loss: 0.3202\n",
      "Epoch: 1673/2000... Training loss: 0.3286\n",
      "Epoch: 1673/2000... Training loss: 0.3493\n",
      "Epoch: 1673/2000... Training loss: 0.5503\n",
      "Epoch: 1673/2000... Training loss: 0.3514\n",
      "Epoch: 1674/2000... Training loss: 0.4294\n",
      "Epoch: 1674/2000... Training loss: 0.2654\n",
      "Epoch: 1674/2000... Training loss: 0.5230\n",
      "Epoch: 1674/2000... Training loss: 0.4453\n",
      "Epoch: 1674/2000... Training loss: 0.3293\n",
      "Epoch: 1674/2000... Training loss: 0.3828\n",
      "Epoch: 1674/2000... Training loss: 0.2655\n",
      "Epoch: 1674/2000... Training loss: 0.4135\n",
      "Epoch: 1674/2000... Training loss: 0.4744\n",
      "Epoch: 1674/2000... Training loss: 0.4446\n",
      "Epoch: 1674/2000... Training loss: 0.3450\n",
      "Epoch: 1674/2000... Training loss: 0.2877\n",
      "Epoch: 1674/2000... Training loss: 0.3541\n",
      "Epoch: 1674/2000... Training loss: 0.4130\n",
      "Epoch: 1674/2000... Training loss: 0.3871\n",
      "Epoch: 1674/2000... Training loss: 0.3787\n",
      "Epoch: 1674/2000... Training loss: 0.3943\n",
      "Epoch: 1674/2000... Training loss: 0.4355\n",
      "Epoch: 1674/2000... Training loss: 0.4685\n",
      "Epoch: 1674/2000... Training loss: 0.4305\n",
      "Epoch: 1674/2000... Training loss: 0.5203\n",
      "Epoch: 1674/2000... Training loss: 0.2274\n",
      "Epoch: 1674/2000... Training loss: 0.4053\n",
      "Epoch: 1674/2000... Training loss: 0.3791\n",
      "Epoch: 1674/2000... Training loss: 0.6686\n",
      "Epoch: 1674/2000... Training loss: 0.3400\n",
      "Epoch: 1674/2000... Training loss: 0.2649\n",
      "Epoch: 1674/2000... Training loss: 0.3084\n",
      "Epoch: 1674/2000... Training loss: 0.4743\n",
      "Epoch: 1674/2000... Training loss: 0.3659\n",
      "Epoch: 1674/2000... Training loss: 0.6191\n",
      "Epoch: 1675/2000... Training loss: 0.2904\n",
      "Epoch: 1675/2000... Training loss: 0.3073\n",
      "Epoch: 1675/2000... Training loss: 0.4310\n",
      "Epoch: 1675/2000... Training loss: 0.4964\n",
      "Epoch: 1675/2000... Training loss: 0.5079\n",
      "Epoch: 1675/2000... Training loss: 0.3859\n",
      "Epoch: 1675/2000... Training loss: 0.4861\n",
      "Epoch: 1675/2000... Training loss: 0.2391\n",
      "Epoch: 1675/2000... Training loss: 0.3860\n",
      "Epoch: 1675/2000... Training loss: 0.2993\n",
      "Epoch: 1675/2000... Training loss: 0.4212\n",
      "Epoch: 1675/2000... Training loss: 0.4760\n",
      "Epoch: 1675/2000... Training loss: 0.3022\n",
      "Epoch: 1675/2000... Training loss: 0.4292\n",
      "Epoch: 1675/2000... Training loss: 0.5327\n",
      "Epoch: 1675/2000... Training loss: 0.3565\n",
      "Epoch: 1675/2000... Training loss: 0.4597\n",
      "Epoch: 1675/2000... Training loss: 0.5362\n",
      "Epoch: 1675/2000... Training loss: 0.5338\n",
      "Epoch: 1675/2000... Training loss: 0.3955\n",
      "Epoch: 1675/2000... Training loss: 0.3185\n",
      "Epoch: 1675/2000... Training loss: 0.4561\n",
      "Epoch: 1675/2000... Training loss: 0.3816\n",
      "Epoch: 1675/2000... Training loss: 0.3285\n",
      "Epoch: 1675/2000... Training loss: 0.3238\n",
      "Epoch: 1675/2000... Training loss: 0.3908\n",
      "Epoch: 1675/2000... Training loss: 0.4002\n",
      "Epoch: 1675/2000... Training loss: 0.5466\n",
      "Epoch: 1675/2000... Training loss: 0.2666\n",
      "Epoch: 1675/2000... Training loss: 0.3996\n",
      "Epoch: 1675/2000... Training loss: 0.2161\n",
      "Epoch: 1676/2000... Training loss: 0.2157\n",
      "Epoch: 1676/2000... Training loss: 0.5751\n",
      "Epoch: 1676/2000... Training loss: 0.5485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1676/2000... Training loss: 0.3569\n",
      "Epoch: 1676/2000... Training loss: 0.3600\n",
      "Epoch: 1676/2000... Training loss: 0.4118\n",
      "Epoch: 1676/2000... Training loss: 0.3501\n",
      "Epoch: 1676/2000... Training loss: 0.2526\n",
      "Epoch: 1676/2000... Training loss: 0.3341\n",
      "Epoch: 1676/2000... Training loss: 0.4280\n",
      "Epoch: 1676/2000... Training loss: 0.4110\n",
      "Epoch: 1676/2000... Training loss: 0.4416\n",
      "Epoch: 1676/2000... Training loss: 0.4339\n",
      "Epoch: 1676/2000... Training loss: 0.4072\n",
      "Epoch: 1676/2000... Training loss: 0.3999\n",
      "Epoch: 1676/2000... Training loss: 0.3174\n",
      "Epoch: 1676/2000... Training loss: 0.4386\n",
      "Epoch: 1676/2000... Training loss: 0.3113\n",
      "Epoch: 1676/2000... Training loss: 0.4407\n",
      "Epoch: 1676/2000... Training loss: 0.2874\n",
      "Epoch: 1676/2000... Training loss: 0.3745\n",
      "Epoch: 1676/2000... Training loss: 0.4179\n",
      "Epoch: 1676/2000... Training loss: 0.5340\n",
      "Epoch: 1676/2000... Training loss: 0.4400\n",
      "Epoch: 1676/2000... Training loss: 0.5104\n",
      "Epoch: 1676/2000... Training loss: 0.4648\n",
      "Epoch: 1676/2000... Training loss: 0.2915\n",
      "Epoch: 1676/2000... Training loss: 0.4087\n",
      "Epoch: 1676/2000... Training loss: 0.3262\n",
      "Epoch: 1676/2000... Training loss: 0.3767\n",
      "Epoch: 1676/2000... Training loss: 0.5361\n",
      "Epoch: 1677/2000... Training loss: 0.4006\n",
      "Epoch: 1677/2000... Training loss: 0.3232\n",
      "Epoch: 1677/2000... Training loss: 0.5316\n",
      "Epoch: 1677/2000... Training loss: 0.4822\n",
      "Epoch: 1677/2000... Training loss: 0.5313\n",
      "Epoch: 1677/2000... Training loss: 0.3258\n",
      "Epoch: 1677/2000... Training loss: 0.2504\n",
      "Epoch: 1677/2000... Training loss: 0.3494\n",
      "Epoch: 1677/2000... Training loss: 0.4116\n",
      "Epoch: 1677/2000... Training loss: 0.1377\n",
      "Epoch: 1677/2000... Training loss: 0.4337\n",
      "Epoch: 1677/2000... Training loss: 0.3368\n",
      "Epoch: 1677/2000... Training loss: 0.4445\n",
      "Epoch: 1677/2000... Training loss: 0.3215\n",
      "Epoch: 1677/2000... Training loss: 0.2753\n",
      "Epoch: 1677/2000... Training loss: 0.3947\n",
      "Epoch: 1677/2000... Training loss: 0.3002\n",
      "Epoch: 1677/2000... Training loss: 0.3981\n",
      "Epoch: 1677/2000... Training loss: 0.6129\n",
      "Epoch: 1677/2000... Training loss: 0.3587\n",
      "Epoch: 1677/2000... Training loss: 0.3365\n",
      "Epoch: 1677/2000... Training loss: 0.3007\n",
      "Epoch: 1677/2000... Training loss: 0.3942\n",
      "Epoch: 1677/2000... Training loss: 0.4908\n",
      "Epoch: 1677/2000... Training loss: 0.3364\n",
      "Epoch: 1677/2000... Training loss: 0.4535\n",
      "Epoch: 1677/2000... Training loss: 0.4636\n",
      "Epoch: 1677/2000... Training loss: 0.2769\n",
      "Epoch: 1677/2000... Training loss: 0.4583\n",
      "Epoch: 1677/2000... Training loss: 0.4671\n",
      "Epoch: 1677/2000... Training loss: 0.5521\n",
      "Epoch: 1678/2000... Training loss: 0.4693\n",
      "Epoch: 1678/2000... Training loss: 0.3299\n",
      "Epoch: 1678/2000... Training loss: 0.3209\n",
      "Epoch: 1678/2000... Training loss: 0.4219\n",
      "Epoch: 1678/2000... Training loss: 0.3094\n",
      "Epoch: 1678/2000... Training loss: 0.3653\n",
      "Epoch: 1678/2000... Training loss: 0.3635\n",
      "Epoch: 1678/2000... Training loss: 0.3777\n",
      "Epoch: 1678/2000... Training loss: 0.3144\n",
      "Epoch: 1678/2000... Training loss: 0.3278\n",
      "Epoch: 1678/2000... Training loss: 0.5699\n",
      "Epoch: 1678/2000... Training loss: 0.2647\n",
      "Epoch: 1678/2000... Training loss: 0.5349\n",
      "Epoch: 1678/2000... Training loss: 0.3552\n",
      "Epoch: 1678/2000... Training loss: 0.4943\n",
      "Epoch: 1678/2000... Training loss: 0.2771\n",
      "Epoch: 1678/2000... Training loss: 0.3410\n",
      "Epoch: 1678/2000... Training loss: 0.3579\n",
      "Epoch: 1678/2000... Training loss: 0.3228\n",
      "Epoch: 1678/2000... Training loss: 0.4319\n",
      "Epoch: 1678/2000... Training loss: 0.3339\n",
      "Epoch: 1678/2000... Training loss: 0.4693\n",
      "Epoch: 1678/2000... Training loss: 0.2099\n",
      "Epoch: 1678/2000... Training loss: 0.3529\n",
      "Epoch: 1678/2000... Training loss: 0.2577\n",
      "Epoch: 1678/2000... Training loss: 0.3970\n",
      "Epoch: 1678/2000... Training loss: 0.1979\n",
      "Epoch: 1678/2000... Training loss: 0.4591\n",
      "Epoch: 1678/2000... Training loss: 0.3296\n",
      "Epoch: 1678/2000... Training loss: 0.3422\n",
      "Epoch: 1678/2000... Training loss: 0.4415\n",
      "Epoch: 1679/2000... Training loss: 0.5085\n",
      "Epoch: 1679/2000... Training loss: 0.3074\n",
      "Epoch: 1679/2000... Training loss: 0.4362\n",
      "Epoch: 1679/2000... Training loss: 0.4457\n",
      "Epoch: 1679/2000... Training loss: 0.3735\n",
      "Epoch: 1679/2000... Training loss: 0.4027\n",
      "Epoch: 1679/2000... Training loss: 0.4013\n",
      "Epoch: 1679/2000... Training loss: 0.3348\n",
      "Epoch: 1679/2000... Training loss: 0.2346\n",
      "Epoch: 1679/2000... Training loss: 0.3229\n",
      "Epoch: 1679/2000... Training loss: 0.5234\n",
      "Epoch: 1679/2000... Training loss: 0.2758\n",
      "Epoch: 1679/2000... Training loss: 0.3428\n",
      "Epoch: 1679/2000... Training loss: 0.4123\n",
      "Epoch: 1679/2000... Training loss: 0.4540\n",
      "Epoch: 1679/2000... Training loss: 0.4011\n",
      "Epoch: 1679/2000... Training loss: 0.4572\n",
      "Epoch: 1679/2000... Training loss: 0.3185\n",
      "Epoch: 1679/2000... Training loss: 0.2933\n",
      "Epoch: 1679/2000... Training loss: 0.4837\n",
      "Epoch: 1679/2000... Training loss: 0.4597\n",
      "Epoch: 1679/2000... Training loss: 0.2960\n",
      "Epoch: 1679/2000... Training loss: 0.2883\n",
      "Epoch: 1679/2000... Training loss: 0.4927\n",
      "Epoch: 1679/2000... Training loss: 0.2175\n",
      "Epoch: 1679/2000... Training loss: 0.5434\n",
      "Epoch: 1679/2000... Training loss: 0.3941\n",
      "Epoch: 1679/2000... Training loss: 0.3696\n",
      "Epoch: 1679/2000... Training loss: 0.4480\n",
      "Epoch: 1679/2000... Training loss: 0.4345\n",
      "Epoch: 1679/2000... Training loss: 0.2823\n",
      "Epoch: 1680/2000... Training loss: 0.3643\n",
      "Epoch: 1680/2000... Training loss: 0.4642\n",
      "Epoch: 1680/2000... Training loss: 0.4146\n",
      "Epoch: 1680/2000... Training loss: 0.4522\n",
      "Epoch: 1680/2000... Training loss: 0.5160\n",
      "Epoch: 1680/2000... Training loss: 0.4813\n",
      "Epoch: 1680/2000... Training loss: 0.3015\n",
      "Epoch: 1680/2000... Training loss: 0.3349\n",
      "Epoch: 1680/2000... Training loss: 0.1713\n",
      "Epoch: 1680/2000... Training loss: 0.3233\n",
      "Epoch: 1680/2000... Training loss: 0.5126\n",
      "Epoch: 1680/2000... Training loss: 0.3713\n",
      "Epoch: 1680/2000... Training loss: 0.4259\n",
      "Epoch: 1680/2000... Training loss: 0.5025\n",
      "Epoch: 1680/2000... Training loss: 0.3808\n",
      "Epoch: 1680/2000... Training loss: 0.2443\n",
      "Epoch: 1680/2000... Training loss: 0.3882\n",
      "Epoch: 1680/2000... Training loss: 0.2977\n",
      "Epoch: 1680/2000... Training loss: 0.3752\n",
      "Epoch: 1680/2000... Training loss: 0.2478\n",
      "Epoch: 1680/2000... Training loss: 0.4223\n",
      "Epoch: 1680/2000... Training loss: 0.5121\n",
      "Epoch: 1680/2000... Training loss: 0.3315\n",
      "Epoch: 1680/2000... Training loss: 0.3472\n",
      "Epoch: 1680/2000... Training loss: 0.4899\n",
      "Epoch: 1680/2000... Training loss: 0.2481\n",
      "Epoch: 1680/2000... Training loss: 0.6329\n",
      "Epoch: 1680/2000... Training loss: 0.2408\n",
      "Epoch: 1680/2000... Training loss: 0.4181\n",
      "Epoch: 1680/2000... Training loss: 0.4436\n",
      "Epoch: 1680/2000... Training loss: 0.2673\n",
      "Epoch: 1681/2000... Training loss: 0.3419\n",
      "Epoch: 1681/2000... Training loss: 0.4936\n",
      "Epoch: 1681/2000... Training loss: 0.2508\n",
      "Epoch: 1681/2000... Training loss: 0.3672\n",
      "Epoch: 1681/2000... Training loss: 0.3990\n",
      "Epoch: 1681/2000... Training loss: 0.3245\n",
      "Epoch: 1681/2000... Training loss: 0.4840\n",
      "Epoch: 1681/2000... Training loss: 0.4494\n",
      "Epoch: 1681/2000... Training loss: 0.4283\n",
      "Epoch: 1681/2000... Training loss: 0.4977\n",
      "Epoch: 1681/2000... Training loss: 0.4366\n",
      "Epoch: 1681/2000... Training loss: 0.4335\n",
      "Epoch: 1681/2000... Training loss: 0.2304\n",
      "Epoch: 1681/2000... Training loss: 0.4478\n",
      "Epoch: 1681/2000... Training loss: 0.3950\n",
      "Epoch: 1681/2000... Training loss: 0.3969\n",
      "Epoch: 1681/2000... Training loss: 0.5418\n",
      "Epoch: 1681/2000... Training loss: 0.3571\n",
      "Epoch: 1681/2000... Training loss: 0.3184\n",
      "Epoch: 1681/2000... Training loss: 0.5234\n",
      "Epoch: 1681/2000... Training loss: 0.2656\n",
      "Epoch: 1681/2000... Training loss: 0.4454\n",
      "Epoch: 1681/2000... Training loss: 0.4933\n",
      "Epoch: 1681/2000... Training loss: 0.3944\n",
      "Epoch: 1681/2000... Training loss: 0.3882\n",
      "Epoch: 1681/2000... Training loss: 0.4946\n",
      "Epoch: 1681/2000... Training loss: 0.3325\n",
      "Epoch: 1681/2000... Training loss: 0.3455\n",
      "Epoch: 1681/2000... Training loss: 0.3470\n",
      "Epoch: 1681/2000... Training loss: 0.3969\n",
      "Epoch: 1681/2000... Training loss: 0.4535\n",
      "Epoch: 1682/2000... Training loss: 0.4247\n",
      "Epoch: 1682/2000... Training loss: 0.3367\n",
      "Epoch: 1682/2000... Training loss: 0.2235\n",
      "Epoch: 1682/2000... Training loss: 0.4508\n",
      "Epoch: 1682/2000... Training loss: 0.3087\n",
      "Epoch: 1682/2000... Training loss: 0.2638\n",
      "Epoch: 1682/2000... Training loss: 0.4481\n",
      "Epoch: 1682/2000... Training loss: 0.4160\n",
      "Epoch: 1682/2000... Training loss: 0.4797\n",
      "Epoch: 1682/2000... Training loss: 0.3187\n",
      "Epoch: 1682/2000... Training loss: 0.2763\n",
      "Epoch: 1682/2000... Training loss: 0.2600\n",
      "Epoch: 1682/2000... Training loss: 0.3102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1682/2000... Training loss: 0.3555\n",
      "Epoch: 1682/2000... Training loss: 0.5724\n",
      "Epoch: 1682/2000... Training loss: 0.3041\n",
      "Epoch: 1682/2000... Training loss: 0.4104\n",
      "Epoch: 1682/2000... Training loss: 0.3840\n",
      "Epoch: 1682/2000... Training loss: 0.5540\n",
      "Epoch: 1682/2000... Training loss: 0.2327\n",
      "Epoch: 1682/2000... Training loss: 0.2272\n",
      "Epoch: 1682/2000... Training loss: 0.2951\n",
      "Epoch: 1682/2000... Training loss: 0.2319\n",
      "Epoch: 1682/2000... Training loss: 0.3861\n",
      "Epoch: 1682/2000... Training loss: 0.4331\n",
      "Epoch: 1682/2000... Training loss: 0.2709\n",
      "Epoch: 1682/2000... Training loss: 0.3333\n",
      "Epoch: 1682/2000... Training loss: 0.3070\n",
      "Epoch: 1682/2000... Training loss: 0.4685\n",
      "Epoch: 1682/2000... Training loss: 0.3582\n",
      "Epoch: 1682/2000... Training loss: 0.3377\n",
      "Epoch: 1683/2000... Training loss: 0.3503\n",
      "Epoch: 1683/2000... Training loss: 0.3482\n",
      "Epoch: 1683/2000... Training loss: 0.5749\n",
      "Epoch: 1683/2000... Training loss: 0.3627\n",
      "Epoch: 1683/2000... Training loss: 0.3240\n",
      "Epoch: 1683/2000... Training loss: 0.3637\n",
      "Epoch: 1683/2000... Training loss: 0.4530\n",
      "Epoch: 1683/2000... Training loss: 0.3883\n",
      "Epoch: 1683/2000... Training loss: 0.4375\n",
      "Epoch: 1683/2000... Training loss: 0.2426\n",
      "Epoch: 1683/2000... Training loss: 0.5042\n",
      "Epoch: 1683/2000... Training loss: 0.4723\n",
      "Epoch: 1683/2000... Training loss: 0.2885\n",
      "Epoch: 1683/2000... Training loss: 0.4843\n",
      "Epoch: 1683/2000... Training loss: 0.5185\n",
      "Epoch: 1683/2000... Training loss: 0.3876\n",
      "Epoch: 1683/2000... Training loss: 0.3223\n",
      "Epoch: 1683/2000... Training loss: 0.4334\n",
      "Epoch: 1683/2000... Training loss: 0.3884\n",
      "Epoch: 1683/2000... Training loss: 0.4141\n",
      "Epoch: 1683/2000... Training loss: 0.5392\n",
      "Epoch: 1683/2000... Training loss: 0.3638\n",
      "Epoch: 1683/2000... Training loss: 0.4747\n",
      "Epoch: 1683/2000... Training loss: 0.4828\n",
      "Epoch: 1683/2000... Training loss: 0.5091\n",
      "Epoch: 1683/2000... Training loss: 0.6001\n",
      "Epoch: 1683/2000... Training loss: 0.2642\n",
      "Epoch: 1683/2000... Training loss: 0.3442\n",
      "Epoch: 1683/2000... Training loss: 0.2724\n",
      "Epoch: 1683/2000... Training loss: 0.3367\n",
      "Epoch: 1683/2000... Training loss: 0.4042\n",
      "Epoch: 1684/2000... Training loss: 0.3836\n",
      "Epoch: 1684/2000... Training loss: 0.3357\n",
      "Epoch: 1684/2000... Training loss: 0.4018\n",
      "Epoch: 1684/2000... Training loss: 0.6592\n",
      "Epoch: 1684/2000... Training loss: 0.3556\n",
      "Epoch: 1684/2000... Training loss: 0.4342\n",
      "Epoch: 1684/2000... Training loss: 0.3016\n",
      "Epoch: 1684/2000... Training loss: 0.3600\n",
      "Epoch: 1684/2000... Training loss: 0.4420\n",
      "Epoch: 1684/2000... Training loss: 0.3642\n",
      "Epoch: 1684/2000... Training loss: 0.5962\n",
      "Epoch: 1684/2000... Training loss: 0.2564\n",
      "Epoch: 1684/2000... Training loss: 0.4378\n",
      "Epoch: 1684/2000... Training loss: 0.4699\n",
      "Epoch: 1684/2000... Training loss: 0.4958\n",
      "Epoch: 1684/2000... Training loss: 0.4479\n",
      "Epoch: 1684/2000... Training loss: 0.7499\n",
      "Epoch: 1684/2000... Training loss: 0.4698\n",
      "Epoch: 1684/2000... Training loss: 0.4496\n",
      "Epoch: 1684/2000... Training loss: 0.3222\n",
      "Epoch: 1684/2000... Training loss: 0.3456\n",
      "Epoch: 1684/2000... Training loss: 0.4225\n",
      "Epoch: 1684/2000... Training loss: 0.5182\n",
      "Epoch: 1684/2000... Training loss: 0.3895\n",
      "Epoch: 1684/2000... Training loss: 0.3662\n",
      "Epoch: 1684/2000... Training loss: 0.4116\n",
      "Epoch: 1684/2000... Training loss: 0.3746\n",
      "Epoch: 1684/2000... Training loss: 0.4085\n",
      "Epoch: 1684/2000... Training loss: 0.3417\n",
      "Epoch: 1684/2000... Training loss: 0.4042\n",
      "Epoch: 1684/2000... Training loss: 0.2854\n",
      "Epoch: 1685/2000... Training loss: 0.4476\n",
      "Epoch: 1685/2000... Training loss: 0.2490\n",
      "Epoch: 1685/2000... Training loss: 0.5041\n",
      "Epoch: 1685/2000... Training loss: 0.4100\n",
      "Epoch: 1685/2000... Training loss: 0.4955\n",
      "Epoch: 1685/2000... Training loss: 0.5117\n",
      "Epoch: 1685/2000... Training loss: 0.5786\n",
      "Epoch: 1685/2000... Training loss: 0.4875\n",
      "Epoch: 1685/2000... Training loss: 0.3651\n",
      "Epoch: 1685/2000... Training loss: 0.3442\n",
      "Epoch: 1685/2000... Training loss: 0.3429\n",
      "Epoch: 1685/2000... Training loss: 0.3572\n",
      "Epoch: 1685/2000... Training loss: 0.3030\n",
      "Epoch: 1685/2000... Training loss: 0.3392\n",
      "Epoch: 1685/2000... Training loss: 0.5005\n",
      "Epoch: 1685/2000... Training loss: 0.5653\n",
      "Epoch: 1685/2000... Training loss: 0.4078\n",
      "Epoch: 1685/2000... Training loss: 0.3868\n",
      "Epoch: 1685/2000... Training loss: 0.3291\n",
      "Epoch: 1685/2000... Training loss: 0.4133\n",
      "Epoch: 1685/2000... Training loss: 0.3160\n",
      "Epoch: 1685/2000... Training loss: 0.3064\n",
      "Epoch: 1685/2000... Training loss: 0.4272\n",
      "Epoch: 1685/2000... Training loss: 0.4373\n",
      "Epoch: 1685/2000... Training loss: 0.3223\n",
      "Epoch: 1685/2000... Training loss: 0.2936\n",
      "Epoch: 1685/2000... Training loss: 0.3657\n",
      "Epoch: 1685/2000... Training loss: 0.3547\n",
      "Epoch: 1685/2000... Training loss: 0.3807\n",
      "Epoch: 1685/2000... Training loss: 0.4270\n",
      "Epoch: 1685/2000... Training loss: 0.3481\n",
      "Epoch: 1686/2000... Training loss: 0.4088\n",
      "Epoch: 1686/2000... Training loss: 0.3669\n",
      "Epoch: 1686/2000... Training loss: 0.4036\n",
      "Epoch: 1686/2000... Training loss: 0.3188\n",
      "Epoch: 1686/2000... Training loss: 0.2856\n",
      "Epoch: 1686/2000... Training loss: 0.4887\n",
      "Epoch: 1686/2000... Training loss: 0.3206\n",
      "Epoch: 1686/2000... Training loss: 0.3847\n",
      "Epoch: 1686/2000... Training loss: 0.3714\n",
      "Epoch: 1686/2000... Training loss: 0.3368\n",
      "Epoch: 1686/2000... Training loss: 0.3814\n",
      "Epoch: 1686/2000... Training loss: 0.4920\n",
      "Epoch: 1686/2000... Training loss: 0.4172\n",
      "Epoch: 1686/2000... Training loss: 0.3762\n",
      "Epoch: 1686/2000... Training loss: 0.4562\n",
      "Epoch: 1686/2000... Training loss: 0.2552\n",
      "Epoch: 1686/2000... Training loss: 0.2732\n",
      "Epoch: 1686/2000... Training loss: 0.3819\n",
      "Epoch: 1686/2000... Training loss: 0.2975\n",
      "Epoch: 1686/2000... Training loss: 0.2704\n",
      "Epoch: 1686/2000... Training loss: 0.3345\n",
      "Epoch: 1686/2000... Training loss: 0.4131\n",
      "Epoch: 1686/2000... Training loss: 0.5263\n",
      "Epoch: 1686/2000... Training loss: 0.4867\n",
      "Epoch: 1686/2000... Training loss: 0.4494\n",
      "Epoch: 1686/2000... Training loss: 0.4640\n",
      "Epoch: 1686/2000... Training loss: 0.3186\n",
      "Epoch: 1686/2000... Training loss: 0.2180\n",
      "Epoch: 1686/2000... Training loss: 0.2493\n",
      "Epoch: 1686/2000... Training loss: 0.3489\n",
      "Epoch: 1686/2000... Training loss: 0.4140\n",
      "Epoch: 1687/2000... Training loss: 0.4509\n",
      "Epoch: 1687/2000... Training loss: 0.4485\n",
      "Epoch: 1687/2000... Training loss: 0.3574\n",
      "Epoch: 1687/2000... Training loss: 0.3788\n",
      "Epoch: 1687/2000... Training loss: 0.3067\n",
      "Epoch: 1687/2000... Training loss: 0.3938\n",
      "Epoch: 1687/2000... Training loss: 0.4947\n",
      "Epoch: 1687/2000... Training loss: 0.2645\n",
      "Epoch: 1687/2000... Training loss: 0.3829\n",
      "Epoch: 1687/2000... Training loss: 0.3596\n",
      "Epoch: 1687/2000... Training loss: 0.2901\n",
      "Epoch: 1687/2000... Training loss: 0.2975\n",
      "Epoch: 1687/2000... Training loss: 0.2776\n",
      "Epoch: 1687/2000... Training loss: 0.2292\n",
      "Epoch: 1687/2000... Training loss: 0.3191\n",
      "Epoch: 1687/2000... Training loss: 0.3787\n",
      "Epoch: 1687/2000... Training loss: 0.3898\n",
      "Epoch: 1687/2000... Training loss: 0.4834\n",
      "Epoch: 1687/2000... Training loss: 0.3669\n",
      "Epoch: 1687/2000... Training loss: 0.3125\n",
      "Epoch: 1687/2000... Training loss: 0.2958\n",
      "Epoch: 1687/2000... Training loss: 0.2858\n",
      "Epoch: 1687/2000... Training loss: 0.3152\n",
      "Epoch: 1687/2000... Training loss: 0.2807\n",
      "Epoch: 1687/2000... Training loss: 0.2213\n",
      "Epoch: 1687/2000... Training loss: 0.3358\n",
      "Epoch: 1687/2000... Training loss: 0.3389\n",
      "Epoch: 1687/2000... Training loss: 0.3292\n",
      "Epoch: 1687/2000... Training loss: 0.3981\n",
      "Epoch: 1687/2000... Training loss: 0.3214\n",
      "Epoch: 1687/2000... Training loss: 0.3344\n",
      "Epoch: 1688/2000... Training loss: 0.2353\n",
      "Epoch: 1688/2000... Training loss: 0.3183\n",
      "Epoch: 1688/2000... Training loss: 0.3608\n",
      "Epoch: 1688/2000... Training loss: 0.3888\n",
      "Epoch: 1688/2000... Training loss: 0.4098\n",
      "Epoch: 1688/2000... Training loss: 0.4332\n",
      "Epoch: 1688/2000... Training loss: 0.4956\n",
      "Epoch: 1688/2000... Training loss: 0.2890\n",
      "Epoch: 1688/2000... Training loss: 0.3568\n",
      "Epoch: 1688/2000... Training loss: 0.4588\n",
      "Epoch: 1688/2000... Training loss: 0.3482\n",
      "Epoch: 1688/2000... Training loss: 0.5821\n",
      "Epoch: 1688/2000... Training loss: 0.4391\n",
      "Epoch: 1688/2000... Training loss: 0.4660\n",
      "Epoch: 1688/2000... Training loss: 0.3814\n",
      "Epoch: 1688/2000... Training loss: 0.2837\n",
      "Epoch: 1688/2000... Training loss: 0.3214\n",
      "Epoch: 1688/2000... Training loss: 0.4009\n",
      "Epoch: 1688/2000... Training loss: 0.2955\n",
      "Epoch: 1688/2000... Training loss: 0.3067\n",
      "Epoch: 1688/2000... Training loss: 0.3034\n",
      "Epoch: 1688/2000... Training loss: 0.2763\n",
      "Epoch: 1688/2000... Training loss: 0.3703\n",
      "Epoch: 1688/2000... Training loss: 0.5829\n",
      "Epoch: 1688/2000... Training loss: 0.2866\n",
      "Epoch: 1688/2000... Training loss: 0.3050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1688/2000... Training loss: 0.4619\n",
      "Epoch: 1688/2000... Training loss: 0.5764\n",
      "Epoch: 1688/2000... Training loss: 0.2444\n",
      "Epoch: 1688/2000... Training loss: 0.3732\n",
      "Epoch: 1688/2000... Training loss: 0.4664\n",
      "Epoch: 1689/2000... Training loss: 0.5401\n",
      "Epoch: 1689/2000... Training loss: 0.3077\n",
      "Epoch: 1689/2000... Training loss: 0.4924\n",
      "Epoch: 1689/2000... Training loss: 0.4031\n",
      "Epoch: 1689/2000... Training loss: 0.3895\n",
      "Epoch: 1689/2000... Training loss: 0.3290\n",
      "Epoch: 1689/2000... Training loss: 0.3527\n",
      "Epoch: 1689/2000... Training loss: 0.3215\n",
      "Epoch: 1689/2000... Training loss: 0.4108\n",
      "Epoch: 1689/2000... Training loss: 0.5231\n",
      "Epoch: 1689/2000... Training loss: 0.4194\n",
      "Epoch: 1689/2000... Training loss: 0.1995\n",
      "Epoch: 1689/2000... Training loss: 0.6055\n",
      "Epoch: 1689/2000... Training loss: 0.3863\n",
      "Epoch: 1689/2000... Training loss: 0.3765\n",
      "Epoch: 1689/2000... Training loss: 0.3353\n",
      "Epoch: 1689/2000... Training loss: 0.4351\n",
      "Epoch: 1689/2000... Training loss: 0.3505\n",
      "Epoch: 1689/2000... Training loss: 0.6158\n",
      "Epoch: 1689/2000... Training loss: 0.4516\n",
      "Epoch: 1689/2000... Training loss: 0.3493\n",
      "Epoch: 1689/2000... Training loss: 0.4320\n",
      "Epoch: 1689/2000... Training loss: 0.4782\n",
      "Epoch: 1689/2000... Training loss: 0.2743\n",
      "Epoch: 1689/2000... Training loss: 0.5388\n",
      "Epoch: 1689/2000... Training loss: 0.3649\n",
      "Epoch: 1689/2000... Training loss: 0.3939\n",
      "Epoch: 1689/2000... Training loss: 0.3566\n",
      "Epoch: 1689/2000... Training loss: 0.3047\n",
      "Epoch: 1689/2000... Training loss: 0.4185\n",
      "Epoch: 1689/2000... Training loss: 0.5405\n",
      "Epoch: 1690/2000... Training loss: 0.3077\n",
      "Epoch: 1690/2000... Training loss: 0.3326\n",
      "Epoch: 1690/2000... Training loss: 0.3201\n",
      "Epoch: 1690/2000... Training loss: 0.3515\n",
      "Epoch: 1690/2000... Training loss: 0.3274\n",
      "Epoch: 1690/2000... Training loss: 0.3852\n",
      "Epoch: 1690/2000... Training loss: 0.4009\n",
      "Epoch: 1690/2000... Training loss: 0.4831\n",
      "Epoch: 1690/2000... Training loss: 0.4108\n",
      "Epoch: 1690/2000... Training loss: 0.2750\n",
      "Epoch: 1690/2000... Training loss: 0.2634\n",
      "Epoch: 1690/2000... Training loss: 0.2532\n",
      "Epoch: 1690/2000... Training loss: 0.3161\n",
      "Epoch: 1690/2000... Training loss: 0.2729\n",
      "Epoch: 1690/2000... Training loss: 0.3788\n",
      "Epoch: 1690/2000... Training loss: 0.4648\n",
      "Epoch: 1690/2000... Training loss: 0.4637\n",
      "Epoch: 1690/2000... Training loss: 0.4285\n",
      "Epoch: 1690/2000... Training loss: 0.4163\n",
      "Epoch: 1690/2000... Training loss: 0.4345\n",
      "Epoch: 1690/2000... Training loss: 0.3632\n",
      "Epoch: 1690/2000... Training loss: 0.4416\n",
      "Epoch: 1690/2000... Training loss: 0.3489\n",
      "Epoch: 1690/2000... Training loss: 0.3061\n",
      "Epoch: 1690/2000... Training loss: 0.4158\n",
      "Epoch: 1690/2000... Training loss: 0.4170\n",
      "Epoch: 1690/2000... Training loss: 0.4379\n",
      "Epoch: 1690/2000... Training loss: 0.5228\n",
      "Epoch: 1690/2000... Training loss: 0.2874\n",
      "Epoch: 1690/2000... Training loss: 0.3115\n",
      "Epoch: 1690/2000... Training loss: 0.3264\n",
      "Epoch: 1691/2000... Training loss: 0.4170\n",
      "Epoch: 1691/2000... Training loss: 0.2690\n",
      "Epoch: 1691/2000... Training loss: 0.4584\n",
      "Epoch: 1691/2000... Training loss: 0.3250\n",
      "Epoch: 1691/2000... Training loss: 0.3066\n",
      "Epoch: 1691/2000... Training loss: 0.3241\n",
      "Epoch: 1691/2000... Training loss: 0.5179\n",
      "Epoch: 1691/2000... Training loss: 0.3752\n",
      "Epoch: 1691/2000... Training loss: 0.3352\n",
      "Epoch: 1691/2000... Training loss: 0.4361\n",
      "Epoch: 1691/2000... Training loss: 0.5002\n",
      "Epoch: 1691/2000... Training loss: 0.4594\n",
      "Epoch: 1691/2000... Training loss: 0.3089\n",
      "Epoch: 1691/2000... Training loss: 0.3235\n",
      "Epoch: 1691/2000... Training loss: 0.4285\n",
      "Epoch: 1691/2000... Training loss: 0.5236\n",
      "Epoch: 1691/2000... Training loss: 0.4470\n",
      "Epoch: 1691/2000... Training loss: 0.3165\n",
      "Epoch: 1691/2000... Training loss: 0.3527\n",
      "Epoch: 1691/2000... Training loss: 0.2902\n",
      "Epoch: 1691/2000... Training loss: 0.2338\n",
      "Epoch: 1691/2000... Training loss: 0.4955\n",
      "Epoch: 1691/2000... Training loss: 0.3789\n",
      "Epoch: 1691/2000... Training loss: 0.3257\n",
      "Epoch: 1691/2000... Training loss: 0.3175\n",
      "Epoch: 1691/2000... Training loss: 0.4231\n",
      "Epoch: 1691/2000... Training loss: 0.4170\n",
      "Epoch: 1691/2000... Training loss: 0.3324\n",
      "Epoch: 1691/2000... Training loss: 0.4939\n",
      "Epoch: 1691/2000... Training loss: 0.4174\n",
      "Epoch: 1691/2000... Training loss: 0.3735\n",
      "Epoch: 1692/2000... Training loss: 0.5377\n",
      "Epoch: 1692/2000... Training loss: 0.3322\n",
      "Epoch: 1692/2000... Training loss: 0.4241\n",
      "Epoch: 1692/2000... Training loss: 0.5149\n",
      "Epoch: 1692/2000... Training loss: 0.4102\n",
      "Epoch: 1692/2000... Training loss: 0.5560\n",
      "Epoch: 1692/2000... Training loss: 0.2758\n",
      "Epoch: 1692/2000... Training loss: 0.5246\n",
      "Epoch: 1692/2000... Training loss: 0.3130\n",
      "Epoch: 1692/2000... Training loss: 0.4059\n",
      "Epoch: 1692/2000... Training loss: 0.2628\n",
      "Epoch: 1692/2000... Training loss: 0.3672\n",
      "Epoch: 1692/2000... Training loss: 0.1720\n",
      "Epoch: 1692/2000... Training loss: 0.3576\n",
      "Epoch: 1692/2000... Training loss: 0.3905\n",
      "Epoch: 1692/2000... Training loss: 0.3571\n",
      "Epoch: 1692/2000... Training loss: 0.2903\n",
      "Epoch: 1692/2000... Training loss: 0.4448\n",
      "Epoch: 1692/2000... Training loss: 0.4137\n",
      "Epoch: 1692/2000... Training loss: 0.4297\n",
      "Epoch: 1692/2000... Training loss: 0.2287\n",
      "Epoch: 1692/2000... Training loss: 0.3370\n",
      "Epoch: 1692/2000... Training loss: 0.2873\n",
      "Epoch: 1692/2000... Training loss: 0.5366\n",
      "Epoch: 1692/2000... Training loss: 0.2983\n",
      "Epoch: 1692/2000... Training loss: 0.3285\n",
      "Epoch: 1692/2000... Training loss: 0.3599\n",
      "Epoch: 1692/2000... Training loss: 0.3197\n",
      "Epoch: 1692/2000... Training loss: 0.4220\n",
      "Epoch: 1692/2000... Training loss: 0.3772\n",
      "Epoch: 1692/2000... Training loss: 0.5967\n",
      "Epoch: 1693/2000... Training loss: 0.2697\n",
      "Epoch: 1693/2000... Training loss: 0.4182\n",
      "Epoch: 1693/2000... Training loss: 0.3012\n",
      "Epoch: 1693/2000... Training loss: 0.4993\n",
      "Epoch: 1693/2000... Training loss: 0.4363\n",
      "Epoch: 1693/2000... Training loss: 0.3216\n",
      "Epoch: 1693/2000... Training loss: 0.4345\n",
      "Epoch: 1693/2000... Training loss: 0.3451\n",
      "Epoch: 1693/2000... Training loss: 0.3952\n",
      "Epoch: 1693/2000... Training loss: 0.4233\n",
      "Epoch: 1693/2000... Training loss: 0.3322\n",
      "Epoch: 1693/2000... Training loss: 0.3785\n",
      "Epoch: 1693/2000... Training loss: 0.5071\n",
      "Epoch: 1693/2000... Training loss: 0.5738\n",
      "Epoch: 1693/2000... Training loss: 0.3913\n",
      "Epoch: 1693/2000... Training loss: 0.3284\n",
      "Epoch: 1693/2000... Training loss: 0.5074\n",
      "Epoch: 1693/2000... Training loss: 0.3797\n",
      "Epoch: 1693/2000... Training loss: 0.5518\n",
      "Epoch: 1693/2000... Training loss: 0.3992\n",
      "Epoch: 1693/2000... Training loss: 0.3526\n",
      "Epoch: 1693/2000... Training loss: 0.3439\n",
      "Epoch: 1693/2000... Training loss: 0.4820\n",
      "Epoch: 1693/2000... Training loss: 0.3801\n",
      "Epoch: 1693/2000... Training loss: 0.4513\n",
      "Epoch: 1693/2000... Training loss: 0.4222\n",
      "Epoch: 1693/2000... Training loss: 0.4473\n",
      "Epoch: 1693/2000... Training loss: 0.4786\n",
      "Epoch: 1693/2000... Training loss: 0.2315\n",
      "Epoch: 1693/2000... Training loss: 0.6496\n",
      "Epoch: 1693/2000... Training loss: 0.4875\n",
      "Epoch: 1694/2000... Training loss: 0.3502\n",
      "Epoch: 1694/2000... Training loss: 0.2932\n",
      "Epoch: 1694/2000... Training loss: 0.4561\n",
      "Epoch: 1694/2000... Training loss: 0.5384\n",
      "Epoch: 1694/2000... Training loss: 0.4646\n",
      "Epoch: 1694/2000... Training loss: 0.4508\n",
      "Epoch: 1694/2000... Training loss: 0.3272\n",
      "Epoch: 1694/2000... Training loss: 0.4621\n",
      "Epoch: 1694/2000... Training loss: 0.4218\n",
      "Epoch: 1694/2000... Training loss: 0.3488\n",
      "Epoch: 1694/2000... Training loss: 0.2254\n",
      "Epoch: 1694/2000... Training loss: 0.4152\n",
      "Epoch: 1694/2000... Training loss: 0.3871\n",
      "Epoch: 1694/2000... Training loss: 0.3392\n",
      "Epoch: 1694/2000... Training loss: 0.3087\n",
      "Epoch: 1694/2000... Training loss: 0.3205\n",
      "Epoch: 1694/2000... Training loss: 0.3214\n",
      "Epoch: 1694/2000... Training loss: 0.2983\n",
      "Epoch: 1694/2000... Training loss: 0.3801\n",
      "Epoch: 1694/2000... Training loss: 0.3806\n",
      "Epoch: 1694/2000... Training loss: 0.3506\n",
      "Epoch: 1694/2000... Training loss: 0.4662\n",
      "Epoch: 1694/2000... Training loss: 0.2941\n",
      "Epoch: 1694/2000... Training loss: 0.3746\n",
      "Epoch: 1694/2000... Training loss: 0.3210\n",
      "Epoch: 1694/2000... Training loss: 0.4753\n",
      "Epoch: 1694/2000... Training loss: 0.4426\n",
      "Epoch: 1694/2000... Training loss: 0.3620\n",
      "Epoch: 1694/2000... Training loss: 0.3637\n",
      "Epoch: 1694/2000... Training loss: 0.4824\n",
      "Epoch: 1694/2000... Training loss: 0.3317\n",
      "Epoch: 1695/2000... Training loss: 0.4438\n",
      "Epoch: 1695/2000... Training loss: 0.4403\n",
      "Epoch: 1695/2000... Training loss: 0.3437\n",
      "Epoch: 1695/2000... Training loss: 0.5011\n",
      "Epoch: 1695/2000... Training loss: 0.3583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1695/2000... Training loss: 0.3673\n",
      "Epoch: 1695/2000... Training loss: 0.3832\n",
      "Epoch: 1695/2000... Training loss: 0.4402\n",
      "Epoch: 1695/2000... Training loss: 0.3313\n",
      "Epoch: 1695/2000... Training loss: 0.3801\n",
      "Epoch: 1695/2000... Training loss: 0.4599\n",
      "Epoch: 1695/2000... Training loss: 0.3948\n",
      "Epoch: 1695/2000... Training loss: 0.2949\n",
      "Epoch: 1695/2000... Training loss: 0.2369\n",
      "Epoch: 1695/2000... Training loss: 0.2541\n",
      "Epoch: 1695/2000... Training loss: 0.4916\n",
      "Epoch: 1695/2000... Training loss: 0.3334\n",
      "Epoch: 1695/2000... Training loss: 0.3256\n",
      "Epoch: 1695/2000... Training loss: 0.3703\n",
      "Epoch: 1695/2000... Training loss: 0.1882\n",
      "Epoch: 1695/2000... Training loss: 0.4072\n",
      "Epoch: 1695/2000... Training loss: 0.4737\n",
      "Epoch: 1695/2000... Training loss: 0.4079\n",
      "Epoch: 1695/2000... Training loss: 0.3536\n",
      "Epoch: 1695/2000... Training loss: 0.4267\n",
      "Epoch: 1695/2000... Training loss: 0.5770\n",
      "Epoch: 1695/2000... Training loss: 0.6068\n",
      "Epoch: 1695/2000... Training loss: 0.5646\n",
      "Epoch: 1695/2000... Training loss: 0.4661\n",
      "Epoch: 1695/2000... Training loss: 0.4422\n",
      "Epoch: 1695/2000... Training loss: 0.2927\n",
      "Epoch: 1696/2000... Training loss: 0.3551\n",
      "Epoch: 1696/2000... Training loss: 0.2974\n",
      "Epoch: 1696/2000... Training loss: 0.4688\n",
      "Epoch: 1696/2000... Training loss: 0.5994\n",
      "Epoch: 1696/2000... Training loss: 0.5030\n",
      "Epoch: 1696/2000... Training loss: 0.4641\n",
      "Epoch: 1696/2000... Training loss: 0.3620\n",
      "Epoch: 1696/2000... Training loss: 0.5440\n",
      "Epoch: 1696/2000... Training loss: 0.4020\n",
      "Epoch: 1696/2000... Training loss: 0.3791\n",
      "Epoch: 1696/2000... Training loss: 0.2977\n",
      "Epoch: 1696/2000... Training loss: 0.4145\n",
      "Epoch: 1696/2000... Training loss: 0.2300\n",
      "Epoch: 1696/2000... Training loss: 0.4786\n",
      "Epoch: 1696/2000... Training loss: 0.4521\n",
      "Epoch: 1696/2000... Training loss: 0.3893\n",
      "Epoch: 1696/2000... Training loss: 0.3827\n",
      "Epoch: 1696/2000... Training loss: 0.5663\n",
      "Epoch: 1696/2000... Training loss: 0.2849\n",
      "Epoch: 1696/2000... Training loss: 0.4325\n",
      "Epoch: 1696/2000... Training loss: 0.4197\n",
      "Epoch: 1696/2000... Training loss: 0.4282\n",
      "Epoch: 1696/2000... Training loss: 0.4271\n",
      "Epoch: 1696/2000... Training loss: 0.3858\n",
      "Epoch: 1696/2000... Training loss: 0.3904\n",
      "Epoch: 1696/2000... Training loss: 0.2646\n",
      "Epoch: 1696/2000... Training loss: 0.2811\n",
      "Epoch: 1696/2000... Training loss: 0.4544\n",
      "Epoch: 1696/2000... Training loss: 0.4911\n",
      "Epoch: 1696/2000... Training loss: 0.6057\n",
      "Epoch: 1696/2000... Training loss: 0.4835\n",
      "Epoch: 1697/2000... Training loss: 0.2734\n",
      "Epoch: 1697/2000... Training loss: 0.3436\n",
      "Epoch: 1697/2000... Training loss: 0.4017\n",
      "Epoch: 1697/2000... Training loss: 0.3899\n",
      "Epoch: 1697/2000... Training loss: 0.3033\n",
      "Epoch: 1697/2000... Training loss: 0.3309\n",
      "Epoch: 1697/2000... Training loss: 0.4040\n",
      "Epoch: 1697/2000... Training loss: 0.3556\n",
      "Epoch: 1697/2000... Training loss: 0.2177\n",
      "Epoch: 1697/2000... Training loss: 0.2560\n",
      "Epoch: 1697/2000... Training loss: 0.4887\n",
      "Epoch: 1697/2000... Training loss: 0.4031\n",
      "Epoch: 1697/2000... Training loss: 0.4092\n",
      "Epoch: 1697/2000... Training loss: 0.2749\n",
      "Epoch: 1697/2000... Training loss: 0.3805\n",
      "Epoch: 1697/2000... Training loss: 0.4086\n",
      "Epoch: 1697/2000... Training loss: 0.4772\n",
      "Epoch: 1697/2000... Training loss: 0.2714\n",
      "Epoch: 1697/2000... Training loss: 0.3225\n",
      "Epoch: 1697/2000... Training loss: 0.4064\n",
      "Epoch: 1697/2000... Training loss: 0.4926\n",
      "Epoch: 1697/2000... Training loss: 0.2939\n",
      "Epoch: 1697/2000... Training loss: 0.5435\n",
      "Epoch: 1697/2000... Training loss: 0.4604\n",
      "Epoch: 1697/2000... Training loss: 0.2007\n",
      "Epoch: 1697/2000... Training loss: 0.3346\n",
      "Epoch: 1697/2000... Training loss: 0.3649\n",
      "Epoch: 1697/2000... Training loss: 0.4207\n",
      "Epoch: 1697/2000... Training loss: 0.3761\n",
      "Epoch: 1697/2000... Training loss: 0.3526\n",
      "Epoch: 1697/2000... Training loss: 0.4842\n",
      "Epoch: 1698/2000... Training loss: 0.3712\n",
      "Epoch: 1698/2000... Training loss: 0.2323\n",
      "Epoch: 1698/2000... Training loss: 0.2970\n",
      "Epoch: 1698/2000... Training loss: 0.4105\n",
      "Epoch: 1698/2000... Training loss: 0.2595\n",
      "Epoch: 1698/2000... Training loss: 0.3558\n",
      "Epoch: 1698/2000... Training loss: 0.6107\n",
      "Epoch: 1698/2000... Training loss: 0.3284\n",
      "Epoch: 1698/2000... Training loss: 0.3403\n",
      "Epoch: 1698/2000... Training loss: 0.2670\n",
      "Epoch: 1698/2000... Training loss: 0.2710\n",
      "Epoch: 1698/2000... Training loss: 0.3368\n",
      "Epoch: 1698/2000... Training loss: 0.3830\n",
      "Epoch: 1698/2000... Training loss: 0.4413\n",
      "Epoch: 1698/2000... Training loss: 0.2041\n",
      "Epoch: 1698/2000... Training loss: 0.4071\n",
      "Epoch: 1698/2000... Training loss: 0.3893\n",
      "Epoch: 1698/2000... Training loss: 0.3785\n",
      "Epoch: 1698/2000... Training loss: 0.2851\n",
      "Epoch: 1698/2000... Training loss: 0.3853\n",
      "Epoch: 1698/2000... Training loss: 0.2981\n",
      "Epoch: 1698/2000... Training loss: 0.1538\n",
      "Epoch: 1698/2000... Training loss: 0.3410\n",
      "Epoch: 1698/2000... Training loss: 0.3764\n",
      "Epoch: 1698/2000... Training loss: 0.4545\n",
      "Epoch: 1698/2000... Training loss: 0.5338\n",
      "Epoch: 1698/2000... Training loss: 0.4971\n",
      "Epoch: 1698/2000... Training loss: 0.2872\n",
      "Epoch: 1698/2000... Training loss: 0.3582\n",
      "Epoch: 1698/2000... Training loss: 0.2987\n",
      "Epoch: 1698/2000... Training loss: 0.4781\n",
      "Epoch: 1699/2000... Training loss: 0.3243\n",
      "Epoch: 1699/2000... Training loss: 0.4240\n",
      "Epoch: 1699/2000... Training loss: 0.5275\n",
      "Epoch: 1699/2000... Training loss: 0.4572\n",
      "Epoch: 1699/2000... Training loss: 0.5364\n",
      "Epoch: 1699/2000... Training loss: 0.4129\n",
      "Epoch: 1699/2000... Training loss: 0.5149\n",
      "Epoch: 1699/2000... Training loss: 0.3599\n",
      "Epoch: 1699/2000... Training loss: 0.5646\n",
      "Epoch: 1699/2000... Training loss: 0.5570\n",
      "Epoch: 1699/2000... Training loss: 0.4000\n",
      "Epoch: 1699/2000... Training loss: 0.2908\n",
      "Epoch: 1699/2000... Training loss: 0.4586\n",
      "Epoch: 1699/2000... Training loss: 0.2610\n",
      "Epoch: 1699/2000... Training loss: 0.5664\n",
      "Epoch: 1699/2000... Training loss: 0.2253\n",
      "Epoch: 1699/2000... Training loss: 0.3331\n",
      "Epoch: 1699/2000... Training loss: 0.3068\n",
      "Epoch: 1699/2000... Training loss: 0.4378\n",
      "Epoch: 1699/2000... Training loss: 0.4638\n",
      "Epoch: 1699/2000... Training loss: 0.3239\n",
      "Epoch: 1699/2000... Training loss: 0.5184\n",
      "Epoch: 1699/2000... Training loss: 0.4442\n",
      "Epoch: 1699/2000... Training loss: 0.3649\n",
      "Epoch: 1699/2000... Training loss: 0.4060\n",
      "Epoch: 1699/2000... Training loss: 0.3661\n",
      "Epoch: 1699/2000... Training loss: 0.4337\n",
      "Epoch: 1699/2000... Training loss: 0.2988\n",
      "Epoch: 1699/2000... Training loss: 0.3862\n",
      "Epoch: 1699/2000... Training loss: 0.2511\n",
      "Epoch: 1699/2000... Training loss: 0.2179\n",
      "Epoch: 1700/2000... Training loss: 0.4246\n",
      "Epoch: 1700/2000... Training loss: 0.3187\n",
      "Epoch: 1700/2000... Training loss: 0.3248\n",
      "Epoch: 1700/2000... Training loss: 0.3310\n",
      "Epoch: 1700/2000... Training loss: 0.3268\n",
      "Epoch: 1700/2000... Training loss: 0.4145\n",
      "Epoch: 1700/2000... Training loss: 0.3918\n",
      "Epoch: 1700/2000... Training loss: 0.3538\n",
      "Epoch: 1700/2000... Training loss: 0.3692\n",
      "Epoch: 1700/2000... Training loss: 0.4046\n",
      "Epoch: 1700/2000... Training loss: 0.1688\n",
      "Epoch: 1700/2000... Training loss: 0.3764\n",
      "Epoch: 1700/2000... Training loss: 0.4683\n",
      "Epoch: 1700/2000... Training loss: 0.2352\n",
      "Epoch: 1700/2000... Training loss: 0.4629\n",
      "Epoch: 1700/2000... Training loss: 0.3506\n",
      "Epoch: 1700/2000... Training loss: 0.4691\n",
      "Epoch: 1700/2000... Training loss: 0.3562\n",
      "Epoch: 1700/2000... Training loss: 0.3781\n",
      "Epoch: 1700/2000... Training loss: 0.4367\n",
      "Epoch: 1700/2000... Training loss: 0.2646\n",
      "Epoch: 1700/2000... Training loss: 0.3743\n",
      "Epoch: 1700/2000... Training loss: 0.4223\n",
      "Epoch: 1700/2000... Training loss: 0.3682\n",
      "Epoch: 1700/2000... Training loss: 0.4254\n",
      "Epoch: 1700/2000... Training loss: 0.3735\n",
      "Epoch: 1700/2000... Training loss: 0.2850\n",
      "Epoch: 1700/2000... Training loss: 0.3878\n",
      "Epoch: 1700/2000... Training loss: 0.3326\n",
      "Epoch: 1700/2000... Training loss: 0.3882\n",
      "Epoch: 1700/2000... Training loss: 0.4974\n",
      "Epoch: 1701/2000... Training loss: 0.3965\n",
      "Epoch: 1701/2000... Training loss: 0.5171\n",
      "Epoch: 1701/2000... Training loss: 0.2676\n",
      "Epoch: 1701/2000... Training loss: 0.3063\n",
      "Epoch: 1701/2000... Training loss: 0.3487\n",
      "Epoch: 1701/2000... Training loss: 0.4637\n",
      "Epoch: 1701/2000... Training loss: 0.3513\n",
      "Epoch: 1701/2000... Training loss: 0.4115\n",
      "Epoch: 1701/2000... Training loss: 0.2004\n",
      "Epoch: 1701/2000... Training loss: 0.3155\n",
      "Epoch: 1701/2000... Training loss: 0.3177\n",
      "Epoch: 1701/2000... Training loss: 0.3913\n",
      "Epoch: 1701/2000... Training loss: 0.3460\n",
      "Epoch: 1701/2000... Training loss: 0.3578\n",
      "Epoch: 1701/2000... Training loss: 0.2604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1701/2000... Training loss: 0.3446\n",
      "Epoch: 1701/2000... Training loss: 0.3697\n",
      "Epoch: 1701/2000... Training loss: 0.3367\n",
      "Epoch: 1701/2000... Training loss: 0.4722\n",
      "Epoch: 1701/2000... Training loss: 0.2832\n",
      "Epoch: 1701/2000... Training loss: 0.3163\n",
      "Epoch: 1701/2000... Training loss: 0.2799\n",
      "Epoch: 1701/2000... Training loss: 0.4181\n",
      "Epoch: 1701/2000... Training loss: 0.3631\n",
      "Epoch: 1701/2000... Training loss: 0.4315\n",
      "Epoch: 1701/2000... Training loss: 0.4827\n",
      "Epoch: 1701/2000... Training loss: 0.3096\n",
      "Epoch: 1701/2000... Training loss: 0.4201\n",
      "Epoch: 1701/2000... Training loss: 0.3132\n",
      "Epoch: 1701/2000... Training loss: 0.4689\n",
      "Epoch: 1701/2000... Training loss: 0.4924\n",
      "Epoch: 1702/2000... Training loss: 0.4088\n",
      "Epoch: 1702/2000... Training loss: 0.4281\n",
      "Epoch: 1702/2000... Training loss: 0.3919\n",
      "Epoch: 1702/2000... Training loss: 0.3744\n",
      "Epoch: 1702/2000... Training loss: 0.4107\n",
      "Epoch: 1702/2000... Training loss: 0.5501\n",
      "Epoch: 1702/2000... Training loss: 0.3399\n",
      "Epoch: 1702/2000... Training loss: 0.4031\n",
      "Epoch: 1702/2000... Training loss: 0.3403\n",
      "Epoch: 1702/2000... Training loss: 0.3403\n",
      "Epoch: 1702/2000... Training loss: 0.3596\n",
      "Epoch: 1702/2000... Training loss: 0.3945\n",
      "Epoch: 1702/2000... Training loss: 0.4324\n",
      "Epoch: 1702/2000... Training loss: 0.2404\n",
      "Epoch: 1702/2000... Training loss: 0.5905\n",
      "Epoch: 1702/2000... Training loss: 0.4642\n",
      "Epoch: 1702/2000... Training loss: 0.2526\n",
      "Epoch: 1702/2000... Training loss: 0.3041\n",
      "Epoch: 1702/2000... Training loss: 0.3763\n",
      "Epoch: 1702/2000... Training loss: 0.3351\n",
      "Epoch: 1702/2000... Training loss: 0.3744\n",
      "Epoch: 1702/2000... Training loss: 0.4686\n",
      "Epoch: 1702/2000... Training loss: 0.4007\n",
      "Epoch: 1702/2000... Training loss: 0.5208\n",
      "Epoch: 1702/2000... Training loss: 0.3425\n",
      "Epoch: 1702/2000... Training loss: 0.4130\n",
      "Epoch: 1702/2000... Training loss: 0.4802\n",
      "Epoch: 1702/2000... Training loss: 0.4199\n",
      "Epoch: 1702/2000... Training loss: 0.4570\n",
      "Epoch: 1702/2000... Training loss: 0.3186\n",
      "Epoch: 1702/2000... Training loss: 0.5396\n",
      "Epoch: 1703/2000... Training loss: 0.3212\n",
      "Epoch: 1703/2000... Training loss: 0.3716\n",
      "Epoch: 1703/2000... Training loss: 0.3818\n",
      "Epoch: 1703/2000... Training loss: 0.4248\n",
      "Epoch: 1703/2000... Training loss: 0.4510\n",
      "Epoch: 1703/2000... Training loss: 0.4781\n",
      "Epoch: 1703/2000... Training loss: 0.3315\n",
      "Epoch: 1703/2000... Training loss: 0.3120\n",
      "Epoch: 1703/2000... Training loss: 0.3206\n",
      "Epoch: 1703/2000... Training loss: 0.3179\n",
      "Epoch: 1703/2000... Training loss: 0.4828\n",
      "Epoch: 1703/2000... Training loss: 0.4212\n",
      "Epoch: 1703/2000... Training loss: 0.3586\n",
      "Epoch: 1703/2000... Training loss: 0.2971\n",
      "Epoch: 1703/2000... Training loss: 0.4400\n",
      "Epoch: 1703/2000... Training loss: 0.2456\n",
      "Epoch: 1703/2000... Training loss: 0.3473\n",
      "Epoch: 1703/2000... Training loss: 0.2913\n",
      "Epoch: 1703/2000... Training loss: 0.4989\n",
      "Epoch: 1703/2000... Training loss: 0.4079\n",
      "Epoch: 1703/2000... Training loss: 0.4431\n",
      "Epoch: 1703/2000... Training loss: 0.2329\n",
      "Epoch: 1703/2000... Training loss: 0.2765\n",
      "Epoch: 1703/2000... Training loss: 0.3558\n",
      "Epoch: 1703/2000... Training loss: 0.2787\n",
      "Epoch: 1703/2000... Training loss: 0.2802\n",
      "Epoch: 1703/2000... Training loss: 0.2652\n",
      "Epoch: 1703/2000... Training loss: 0.3078\n",
      "Epoch: 1703/2000... Training loss: 0.4292\n",
      "Epoch: 1703/2000... Training loss: 0.3471\n",
      "Epoch: 1703/2000... Training loss: 0.3555\n",
      "Epoch: 1704/2000... Training loss: 0.1764\n",
      "Epoch: 1704/2000... Training loss: 0.4110\n",
      "Epoch: 1704/2000... Training loss: 0.4255\n",
      "Epoch: 1704/2000... Training loss: 0.3277\n",
      "Epoch: 1704/2000... Training loss: 0.4457\n",
      "Epoch: 1704/2000... Training loss: 0.2797\n",
      "Epoch: 1704/2000... Training loss: 0.3734\n",
      "Epoch: 1704/2000... Training loss: 0.3415\n",
      "Epoch: 1704/2000... Training loss: 0.5364\n",
      "Epoch: 1704/2000... Training loss: 0.3029\n",
      "Epoch: 1704/2000... Training loss: 0.3386\n",
      "Epoch: 1704/2000... Training loss: 0.3539\n",
      "Epoch: 1704/2000... Training loss: 0.3254\n",
      "Epoch: 1704/2000... Training loss: 0.5389\n",
      "Epoch: 1704/2000... Training loss: 0.3069\n",
      "Epoch: 1704/2000... Training loss: 0.5152\n",
      "Epoch: 1704/2000... Training loss: 0.2769\n",
      "Epoch: 1704/2000... Training loss: 0.3956\n",
      "Epoch: 1704/2000... Training loss: 0.3112\n",
      "Epoch: 1704/2000... Training loss: 0.3277\n",
      "Epoch: 1704/2000... Training loss: 0.3572\n",
      "Epoch: 1704/2000... Training loss: 0.3053\n",
      "Epoch: 1704/2000... Training loss: 0.3672\n",
      "Epoch: 1704/2000... Training loss: 0.3179\n",
      "Epoch: 1704/2000... Training loss: 0.4291\n",
      "Epoch: 1704/2000... Training loss: 0.4128\n",
      "Epoch: 1704/2000... Training loss: 0.3352\n",
      "Epoch: 1704/2000... Training loss: 0.4459\n",
      "Epoch: 1704/2000... Training loss: 0.1950\n",
      "Epoch: 1704/2000... Training loss: 0.5401\n",
      "Epoch: 1704/2000... Training loss: 0.3290\n",
      "Epoch: 1705/2000... Training loss: 0.3525\n",
      "Epoch: 1705/2000... Training loss: 0.4088\n",
      "Epoch: 1705/2000... Training loss: 0.3391\n",
      "Epoch: 1705/2000... Training loss: 0.4285\n",
      "Epoch: 1705/2000... Training loss: 0.4116\n",
      "Epoch: 1705/2000... Training loss: 0.5243\n",
      "Epoch: 1705/2000... Training loss: 0.4246\n",
      "Epoch: 1705/2000... Training loss: 0.4776\n",
      "Epoch: 1705/2000... Training loss: 0.2959\n",
      "Epoch: 1705/2000... Training loss: 0.3780\n",
      "Epoch: 1705/2000... Training loss: 0.3286\n",
      "Epoch: 1705/2000... Training loss: 0.3893\n",
      "Epoch: 1705/2000... Training loss: 0.3880\n",
      "Epoch: 1705/2000... Training loss: 0.4774\n",
      "Epoch: 1705/2000... Training loss: 0.3620\n",
      "Epoch: 1705/2000... Training loss: 0.3368\n",
      "Epoch: 1705/2000... Training loss: 0.2502\n",
      "Epoch: 1705/2000... Training loss: 0.3352\n",
      "Epoch: 1705/2000... Training loss: 0.5143\n",
      "Epoch: 1705/2000... Training loss: 0.3750\n",
      "Epoch: 1705/2000... Training loss: 0.2774\n",
      "Epoch: 1705/2000... Training loss: 0.2983\n",
      "Epoch: 1705/2000... Training loss: 0.2427\n",
      "Epoch: 1705/2000... Training loss: 0.3949\n",
      "Epoch: 1705/2000... Training loss: 0.3930\n",
      "Epoch: 1705/2000... Training loss: 0.3924\n",
      "Epoch: 1705/2000... Training loss: 0.6336\n",
      "Epoch: 1705/2000... Training loss: 0.2822\n",
      "Epoch: 1705/2000... Training loss: 0.2840\n",
      "Epoch: 1705/2000... Training loss: 0.3654\n",
      "Epoch: 1705/2000... Training loss: 0.3192\n",
      "Epoch: 1706/2000... Training loss: 0.4307\n",
      "Epoch: 1706/2000... Training loss: 0.3962\n",
      "Epoch: 1706/2000... Training loss: 0.3812\n",
      "Epoch: 1706/2000... Training loss: 0.4166\n",
      "Epoch: 1706/2000... Training loss: 0.3862\n",
      "Epoch: 1706/2000... Training loss: 0.3959\n",
      "Epoch: 1706/2000... Training loss: 0.4807\n",
      "Epoch: 1706/2000... Training loss: 0.3640\n",
      "Epoch: 1706/2000... Training loss: 0.4964\n",
      "Epoch: 1706/2000... Training loss: 0.2919\n",
      "Epoch: 1706/2000... Training loss: 0.3697\n",
      "Epoch: 1706/2000... Training loss: 0.2213\n",
      "Epoch: 1706/2000... Training loss: 0.3731\n",
      "Epoch: 1706/2000... Training loss: 0.3364\n",
      "Epoch: 1706/2000... Training loss: 0.4877\n",
      "Epoch: 1706/2000... Training loss: 0.3422\n",
      "Epoch: 1706/2000... Training loss: 0.5207\n",
      "Epoch: 1706/2000... Training loss: 0.2845\n",
      "Epoch: 1706/2000... Training loss: 0.5323\n",
      "Epoch: 1706/2000... Training loss: 0.4222\n",
      "Epoch: 1706/2000... Training loss: 0.3301\n",
      "Epoch: 1706/2000... Training loss: 0.2484\n",
      "Epoch: 1706/2000... Training loss: 0.4559\n",
      "Epoch: 1706/2000... Training loss: 0.5116\n",
      "Epoch: 1706/2000... Training loss: 0.4157\n",
      "Epoch: 1706/2000... Training loss: 0.3270\n",
      "Epoch: 1706/2000... Training loss: 0.3211\n",
      "Epoch: 1706/2000... Training loss: 0.3351\n",
      "Epoch: 1706/2000... Training loss: 0.5580\n",
      "Epoch: 1706/2000... Training loss: 0.4134\n",
      "Epoch: 1706/2000... Training loss: 0.2974\n",
      "Epoch: 1707/2000... Training loss: 0.5189\n",
      "Epoch: 1707/2000... Training loss: 0.3765\n",
      "Epoch: 1707/2000... Training loss: 0.3013\n",
      "Epoch: 1707/2000... Training loss: 0.4618\n",
      "Epoch: 1707/2000... Training loss: 0.4881\n",
      "Epoch: 1707/2000... Training loss: 0.3414\n",
      "Epoch: 1707/2000... Training loss: 0.4364\n",
      "Epoch: 1707/2000... Training loss: 0.3560\n",
      "Epoch: 1707/2000... Training loss: 0.3788\n",
      "Epoch: 1707/2000... Training loss: 0.3610\n",
      "Epoch: 1707/2000... Training loss: 0.3115\n",
      "Epoch: 1707/2000... Training loss: 0.3878\n",
      "Epoch: 1707/2000... Training loss: 0.5219\n",
      "Epoch: 1707/2000... Training loss: 0.4479\n",
      "Epoch: 1707/2000... Training loss: 0.5007\n",
      "Epoch: 1707/2000... Training loss: 0.4697\n",
      "Epoch: 1707/2000... Training loss: 0.3878\n",
      "Epoch: 1707/2000... Training loss: 0.3306\n",
      "Epoch: 1707/2000... Training loss: 0.3863\n",
      "Epoch: 1707/2000... Training loss: 0.5160\n",
      "Epoch: 1707/2000... Training loss: 0.3561\n",
      "Epoch: 1707/2000... Training loss: 0.4617\n",
      "Epoch: 1707/2000... Training loss: 0.1557\n",
      "Epoch: 1707/2000... Training loss: 0.4357\n",
      "Epoch: 1707/2000... Training loss: 0.5465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1707/2000... Training loss: 0.4198\n",
      "Epoch: 1707/2000... Training loss: 0.2922\n",
      "Epoch: 1707/2000... Training loss: 0.3890\n",
      "Epoch: 1707/2000... Training loss: 0.4723\n",
      "Epoch: 1707/2000... Training loss: 0.3393\n",
      "Epoch: 1707/2000... Training loss: 0.6306\n",
      "Epoch: 1708/2000... Training loss: 0.2544\n",
      "Epoch: 1708/2000... Training loss: 0.4722\n",
      "Epoch: 1708/2000... Training loss: 0.5250\n",
      "Epoch: 1708/2000... Training loss: 0.4814\n",
      "Epoch: 1708/2000... Training loss: 0.4079\n",
      "Epoch: 1708/2000... Training loss: 0.2815\n",
      "Epoch: 1708/2000... Training loss: 0.5227\n",
      "Epoch: 1708/2000... Training loss: 0.3356\n",
      "Epoch: 1708/2000... Training loss: 0.5190\n",
      "Epoch: 1708/2000... Training loss: 0.4765\n",
      "Epoch: 1708/2000... Training loss: 0.3529\n",
      "Epoch: 1708/2000... Training loss: 0.3363\n",
      "Epoch: 1708/2000... Training loss: 0.3118\n",
      "Epoch: 1708/2000... Training loss: 0.5121\n",
      "Epoch: 1708/2000... Training loss: 0.3490\n",
      "Epoch: 1708/2000... Training loss: 0.3290\n",
      "Epoch: 1708/2000... Training loss: 0.3782\n",
      "Epoch: 1708/2000... Training loss: 0.4385\n",
      "Epoch: 1708/2000... Training loss: 0.5707\n",
      "Epoch: 1708/2000... Training loss: 0.5316\n",
      "Epoch: 1708/2000... Training loss: 0.4077\n",
      "Epoch: 1708/2000... Training loss: 0.3784\n",
      "Epoch: 1708/2000... Training loss: 0.3536\n",
      "Epoch: 1708/2000... Training loss: 0.4557\n",
      "Epoch: 1708/2000... Training loss: 0.5212\n",
      "Epoch: 1708/2000... Training loss: 0.4706\n",
      "Epoch: 1708/2000... Training loss: 0.4228\n",
      "Epoch: 1708/2000... Training loss: 0.4134\n",
      "Epoch: 1708/2000... Training loss: 0.6143\n",
      "Epoch: 1708/2000... Training loss: 0.5017\n",
      "Epoch: 1708/2000... Training loss: 0.4486\n",
      "Epoch: 1709/2000... Training loss: 0.4978\n",
      "Epoch: 1709/2000... Training loss: 0.3301\n",
      "Epoch: 1709/2000... Training loss: 0.2595\n",
      "Epoch: 1709/2000... Training loss: 0.3280\n",
      "Epoch: 1709/2000... Training loss: 0.3202\n",
      "Epoch: 1709/2000... Training loss: 0.5044\n",
      "Epoch: 1709/2000... Training loss: 0.5587\n",
      "Epoch: 1709/2000... Training loss: 0.4010\n",
      "Epoch: 1709/2000... Training loss: 0.3039\n",
      "Epoch: 1709/2000... Training loss: 0.3713\n",
      "Epoch: 1709/2000... Training loss: 0.4796\n",
      "Epoch: 1709/2000... Training loss: 0.3920\n",
      "Epoch: 1709/2000... Training loss: 0.3861\n",
      "Epoch: 1709/2000... Training loss: 0.4477\n",
      "Epoch: 1709/2000... Training loss: 0.3527\n",
      "Epoch: 1709/2000... Training loss: 0.3613\n",
      "Epoch: 1709/2000... Training loss: 0.3944\n",
      "Epoch: 1709/2000... Training loss: 0.5375\n",
      "Epoch: 1709/2000... Training loss: 0.3104\n",
      "Epoch: 1709/2000... Training loss: 0.3487\n",
      "Epoch: 1709/2000... Training loss: 0.3813\n",
      "Epoch: 1709/2000... Training loss: 0.4956\n",
      "Epoch: 1709/2000... Training loss: 0.3926\n",
      "Epoch: 1709/2000... Training loss: 0.4954\n",
      "Epoch: 1709/2000... Training loss: 0.6497\n",
      "Epoch: 1709/2000... Training loss: 0.4222\n",
      "Epoch: 1709/2000... Training loss: 0.4034\n",
      "Epoch: 1709/2000... Training loss: 0.5442\n",
      "Epoch: 1709/2000... Training loss: 0.4390\n",
      "Epoch: 1709/2000... Training loss: 0.6378\n",
      "Epoch: 1709/2000... Training loss: 0.3960\n",
      "Epoch: 1710/2000... Training loss: 0.3922\n",
      "Epoch: 1710/2000... Training loss: 0.2376\n",
      "Epoch: 1710/2000... Training loss: 0.4830\n",
      "Epoch: 1710/2000... Training loss: 0.4908\n",
      "Epoch: 1710/2000... Training loss: 0.5488\n",
      "Epoch: 1710/2000... Training loss: 0.5623\n",
      "Epoch: 1710/2000... Training loss: 0.3344\n",
      "Epoch: 1710/2000... Training loss: 0.3547\n",
      "Epoch: 1710/2000... Training loss: 0.4405\n",
      "Epoch: 1710/2000... Training loss: 0.2726\n",
      "Epoch: 1710/2000... Training loss: 0.4582\n",
      "Epoch: 1710/2000... Training loss: 0.3444\n",
      "Epoch: 1710/2000... Training loss: 0.5047\n",
      "Epoch: 1710/2000... Training loss: 0.3525\n",
      "Epoch: 1710/2000... Training loss: 0.4973\n",
      "Epoch: 1710/2000... Training loss: 0.2799\n",
      "Epoch: 1710/2000... Training loss: 0.3484\n",
      "Epoch: 1710/2000... Training loss: 0.4492\n",
      "Epoch: 1710/2000... Training loss: 0.3467\n",
      "Epoch: 1710/2000... Training loss: 0.3486\n",
      "Epoch: 1710/2000... Training loss: 0.5434\n",
      "Epoch: 1710/2000... Training loss: 0.2955\n",
      "Epoch: 1710/2000... Training loss: 0.2749\n",
      "Epoch: 1710/2000... Training loss: 0.4612\n",
      "Epoch: 1710/2000... Training loss: 0.3501\n",
      "Epoch: 1710/2000... Training loss: 0.3814\n",
      "Epoch: 1710/2000... Training loss: 0.3211\n",
      "Epoch: 1710/2000... Training loss: 0.4089\n",
      "Epoch: 1710/2000... Training loss: 0.3552\n",
      "Epoch: 1710/2000... Training loss: 0.4276\n",
      "Epoch: 1710/2000... Training loss: 0.5939\n",
      "Epoch: 1711/2000... Training loss: 0.3197\n",
      "Epoch: 1711/2000... Training loss: 0.3475\n",
      "Epoch: 1711/2000... Training loss: 0.4731\n",
      "Epoch: 1711/2000... Training loss: 0.4040\n",
      "Epoch: 1711/2000... Training loss: 0.4227\n",
      "Epoch: 1711/2000... Training loss: 0.3900\n",
      "Epoch: 1711/2000... Training loss: 0.4227\n",
      "Epoch: 1711/2000... Training loss: 0.5310\n",
      "Epoch: 1711/2000... Training loss: 0.3790\n",
      "Epoch: 1711/2000... Training loss: 0.3451\n",
      "Epoch: 1711/2000... Training loss: 0.3596\n",
      "Epoch: 1711/2000... Training loss: 0.3702\n",
      "Epoch: 1711/2000... Training loss: 0.3091\n",
      "Epoch: 1711/2000... Training loss: 0.6016\n",
      "Epoch: 1711/2000... Training loss: 0.4820\n",
      "Epoch: 1711/2000... Training loss: 0.3371\n",
      "Epoch: 1711/2000... Training loss: 0.5027\n",
      "Epoch: 1711/2000... Training loss: 0.3951\n",
      "Epoch: 1711/2000... Training loss: 0.4040\n",
      "Epoch: 1711/2000... Training loss: 0.4364\n",
      "Epoch: 1711/2000... Training loss: 0.4034\n",
      "Epoch: 1711/2000... Training loss: 0.3293\n",
      "Epoch: 1711/2000... Training loss: 0.6677\n",
      "Epoch: 1711/2000... Training loss: 0.3952\n",
      "Epoch: 1711/2000... Training loss: 0.2970\n",
      "Epoch: 1711/2000... Training loss: 0.3298\n",
      "Epoch: 1711/2000... Training loss: 0.4309\n",
      "Epoch: 1711/2000... Training loss: 0.4457\n",
      "Epoch: 1711/2000... Training loss: 0.4254\n",
      "Epoch: 1711/2000... Training loss: 0.3914\n",
      "Epoch: 1711/2000... Training loss: 0.3490\n",
      "Epoch: 1712/2000... Training loss: 0.6093\n",
      "Epoch: 1712/2000... Training loss: 0.3704\n",
      "Epoch: 1712/2000... Training loss: 0.2271\n",
      "Epoch: 1712/2000... Training loss: 0.7809\n",
      "Epoch: 1712/2000... Training loss: 0.2801\n",
      "Epoch: 1712/2000... Training loss: 0.2458\n",
      "Epoch: 1712/2000... Training loss: 0.3743\n",
      "Epoch: 1712/2000... Training loss: 0.3562\n",
      "Epoch: 1712/2000... Training loss: 0.3303\n",
      "Epoch: 1712/2000... Training loss: 0.4956\n",
      "Epoch: 1712/2000... Training loss: 0.4493\n",
      "Epoch: 1712/2000... Training loss: 0.2728\n",
      "Epoch: 1712/2000... Training loss: 0.3880\n",
      "Epoch: 1712/2000... Training loss: 0.2996\n",
      "Epoch: 1712/2000... Training loss: 0.3439\n",
      "Epoch: 1712/2000... Training loss: 0.5099\n",
      "Epoch: 1712/2000... Training loss: 0.4250\n",
      "Epoch: 1712/2000... Training loss: 0.7209\n",
      "Epoch: 1712/2000... Training loss: 0.4040\n",
      "Epoch: 1712/2000... Training loss: 0.5609\n",
      "Epoch: 1712/2000... Training loss: 0.3323\n",
      "Epoch: 1712/2000... Training loss: 0.3090\n",
      "Epoch: 1712/2000... Training loss: 0.3697\n",
      "Epoch: 1712/2000... Training loss: 0.3144\n",
      "Epoch: 1712/2000... Training loss: 0.2958\n",
      "Epoch: 1712/2000... Training loss: 0.3967\n",
      "Epoch: 1712/2000... Training loss: 0.3201\n",
      "Epoch: 1712/2000... Training loss: 0.4032\n",
      "Epoch: 1712/2000... Training loss: 0.5072\n",
      "Epoch: 1712/2000... Training loss: 0.4776\n",
      "Epoch: 1712/2000... Training loss: 0.2951\n",
      "Epoch: 1713/2000... Training loss: 0.3678\n",
      "Epoch: 1713/2000... Training loss: 0.5638\n",
      "Epoch: 1713/2000... Training loss: 0.3827\n",
      "Epoch: 1713/2000... Training loss: 0.3908\n",
      "Epoch: 1713/2000... Training loss: 0.4162\n",
      "Epoch: 1713/2000... Training loss: 0.5399\n",
      "Epoch: 1713/2000... Training loss: 0.4385\n",
      "Epoch: 1713/2000... Training loss: 0.4968\n",
      "Epoch: 1713/2000... Training loss: 0.4213\n",
      "Epoch: 1713/2000... Training loss: 0.4319\n",
      "Epoch: 1713/2000... Training loss: 0.4733\n",
      "Epoch: 1713/2000... Training loss: 0.2334\n",
      "Epoch: 1713/2000... Training loss: 0.2919\n",
      "Epoch: 1713/2000... Training loss: 0.3175\n",
      "Epoch: 1713/2000... Training loss: 0.4424\n",
      "Epoch: 1713/2000... Training loss: 0.3899\n",
      "Epoch: 1713/2000... Training loss: 0.3065\n",
      "Epoch: 1713/2000... Training loss: 0.5255\n",
      "Epoch: 1713/2000... Training loss: 0.2755\n",
      "Epoch: 1713/2000... Training loss: 0.3863\n",
      "Epoch: 1713/2000... Training loss: 0.3149\n",
      "Epoch: 1713/2000... Training loss: 0.3164\n",
      "Epoch: 1713/2000... Training loss: 0.4865\n",
      "Epoch: 1713/2000... Training loss: 0.4448\n",
      "Epoch: 1713/2000... Training loss: 0.3827\n",
      "Epoch: 1713/2000... Training loss: 0.5201\n",
      "Epoch: 1713/2000... Training loss: 0.2694\n",
      "Epoch: 1713/2000... Training loss: 0.4865\n",
      "Epoch: 1713/2000... Training loss: 0.3822\n",
      "Epoch: 1713/2000... Training loss: 0.5398\n",
      "Epoch: 1713/2000... Training loss: 0.4564\n",
      "Epoch: 1714/2000... Training loss: 0.3884\n",
      "Epoch: 1714/2000... Training loss: 0.4825\n",
      "Epoch: 1714/2000... Training loss: 0.4325\n",
      "Epoch: 1714/2000... Training loss: 0.3509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1714/2000... Training loss: 0.4684\n",
      "Epoch: 1714/2000... Training loss: 0.4425\n",
      "Epoch: 1714/2000... Training loss: 0.4062\n",
      "Epoch: 1714/2000... Training loss: 0.4439\n",
      "Epoch: 1714/2000... Training loss: 0.6536\n",
      "Epoch: 1714/2000... Training loss: 0.4208\n",
      "Epoch: 1714/2000... Training loss: 0.4954\n",
      "Epoch: 1714/2000... Training loss: 0.5294\n",
      "Epoch: 1714/2000... Training loss: 0.3602\n",
      "Epoch: 1714/2000... Training loss: 0.6016\n",
      "Epoch: 1714/2000... Training loss: 0.3385\n",
      "Epoch: 1714/2000... Training loss: 0.5033\n",
      "Epoch: 1714/2000... Training loss: 0.2765\n",
      "Epoch: 1714/2000... Training loss: 0.3714\n",
      "Epoch: 1714/2000... Training loss: 0.6580\n",
      "Epoch: 1714/2000... Training loss: 0.4720\n",
      "Epoch: 1714/2000... Training loss: 0.4197\n",
      "Epoch: 1714/2000... Training loss: 0.3270\n",
      "Epoch: 1714/2000... Training loss: 0.4463\n",
      "Epoch: 1714/2000... Training loss: 0.5309\n",
      "Epoch: 1714/2000... Training loss: 0.3793\n",
      "Epoch: 1714/2000... Training loss: 0.6815\n",
      "Epoch: 1714/2000... Training loss: 0.3855\n",
      "Epoch: 1714/2000... Training loss: 0.3208\n",
      "Epoch: 1714/2000... Training loss: 0.3539\n",
      "Epoch: 1714/2000... Training loss: 0.3116\n",
      "Epoch: 1714/2000... Training loss: 0.4191\n",
      "Epoch: 1715/2000... Training loss: 0.6884\n",
      "Epoch: 1715/2000... Training loss: 0.2888\n",
      "Epoch: 1715/2000... Training loss: 0.4579\n",
      "Epoch: 1715/2000... Training loss: 0.3509\n",
      "Epoch: 1715/2000... Training loss: 0.3588\n",
      "Epoch: 1715/2000... Training loss: 0.4672\n",
      "Epoch: 1715/2000... Training loss: 0.2795\n",
      "Epoch: 1715/2000... Training loss: 0.4802\n",
      "Epoch: 1715/2000... Training loss: 0.3259\n",
      "Epoch: 1715/2000... Training loss: 0.5142\n",
      "Epoch: 1715/2000... Training loss: 0.3184\n",
      "Epoch: 1715/2000... Training loss: 0.4434\n",
      "Epoch: 1715/2000... Training loss: 0.3699\n",
      "Epoch: 1715/2000... Training loss: 0.4045\n",
      "Epoch: 1715/2000... Training loss: 0.4594\n",
      "Epoch: 1715/2000... Training loss: 0.3454\n",
      "Epoch: 1715/2000... Training loss: 0.5230\n",
      "Epoch: 1715/2000... Training loss: 0.4183\n",
      "Epoch: 1715/2000... Training loss: 0.4510\n",
      "Epoch: 1715/2000... Training loss: 0.4452\n",
      "Epoch: 1715/2000... Training loss: 0.3399\n",
      "Epoch: 1715/2000... Training loss: 0.3743\n",
      "Epoch: 1715/2000... Training loss: 0.4086\n",
      "Epoch: 1715/2000... Training loss: 0.5032\n",
      "Epoch: 1715/2000... Training loss: 0.2570\n",
      "Epoch: 1715/2000... Training loss: 0.5298\n",
      "Epoch: 1715/2000... Training loss: 0.4351\n",
      "Epoch: 1715/2000... Training loss: 0.3675\n",
      "Epoch: 1715/2000... Training loss: 0.4569\n",
      "Epoch: 1715/2000... Training loss: 0.5365\n",
      "Epoch: 1715/2000... Training loss: 0.4955\n",
      "Epoch: 1716/2000... Training loss: 0.3070\n",
      "Epoch: 1716/2000... Training loss: 0.5538\n",
      "Epoch: 1716/2000... Training loss: 0.4120\n",
      "Epoch: 1716/2000... Training loss: 0.5225\n",
      "Epoch: 1716/2000... Training loss: 0.3403\n",
      "Epoch: 1716/2000... Training loss: 0.3378\n",
      "Epoch: 1716/2000... Training loss: 0.5812\n",
      "Epoch: 1716/2000... Training loss: 0.4436\n",
      "Epoch: 1716/2000... Training loss: 0.3874\n",
      "Epoch: 1716/2000... Training loss: 0.3164\n",
      "Epoch: 1716/2000... Training loss: 0.3821\n",
      "Epoch: 1716/2000... Training loss: 0.4075\n",
      "Epoch: 1716/2000... Training loss: 0.4479\n",
      "Epoch: 1716/2000... Training loss: 0.3533\n",
      "Epoch: 1716/2000... Training loss: 0.3595\n",
      "Epoch: 1716/2000... Training loss: 0.4058\n",
      "Epoch: 1716/2000... Training loss: 0.2844\n",
      "Epoch: 1716/2000... Training loss: 0.4219\n",
      "Epoch: 1716/2000... Training loss: 0.3608\n",
      "Epoch: 1716/2000... Training loss: 0.5492\n",
      "Epoch: 1716/2000... Training loss: 0.5101\n",
      "Epoch: 1716/2000... Training loss: 0.4670\n",
      "Epoch: 1716/2000... Training loss: 0.4885\n",
      "Epoch: 1716/2000... Training loss: 0.4161\n",
      "Epoch: 1716/2000... Training loss: 0.3074\n",
      "Epoch: 1716/2000... Training loss: 0.3011\n",
      "Epoch: 1716/2000... Training loss: 0.3946\n",
      "Epoch: 1716/2000... Training loss: 0.3703\n",
      "Epoch: 1716/2000... Training loss: 0.4294\n",
      "Epoch: 1716/2000... Training loss: 0.5232\n",
      "Epoch: 1716/2000... Training loss: 0.4073\n",
      "Epoch: 1717/2000... Training loss: 0.3756\n",
      "Epoch: 1717/2000... Training loss: 0.4103\n",
      "Epoch: 1717/2000... Training loss: 0.5154\n",
      "Epoch: 1717/2000... Training loss: 0.4644\n",
      "Epoch: 1717/2000... Training loss: 0.3295\n",
      "Epoch: 1717/2000... Training loss: 0.2550\n",
      "Epoch: 1717/2000... Training loss: 0.2678\n",
      "Epoch: 1717/2000... Training loss: 0.3295\n",
      "Epoch: 1717/2000... Training loss: 0.5700\n",
      "Epoch: 1717/2000... Training loss: 0.2546\n",
      "Epoch: 1717/2000... Training loss: 0.4611\n",
      "Epoch: 1717/2000... Training loss: 0.3112\n",
      "Epoch: 1717/2000... Training loss: 0.4388\n",
      "Epoch: 1717/2000... Training loss: 0.3277\n",
      "Epoch: 1717/2000... Training loss: 0.5126\n",
      "Epoch: 1717/2000... Training loss: 0.2664\n",
      "Epoch: 1717/2000... Training loss: 0.2718\n",
      "Epoch: 1717/2000... Training loss: 0.3141\n",
      "Epoch: 1717/2000... Training loss: 0.3216\n",
      "Epoch: 1717/2000... Training loss: 0.2149\n",
      "Epoch: 1717/2000... Training loss: 0.2971\n",
      "Epoch: 1717/2000... Training loss: 0.2125\n",
      "Epoch: 1717/2000... Training loss: 0.3533\n",
      "Epoch: 1717/2000... Training loss: 0.3969\n",
      "Epoch: 1717/2000... Training loss: 0.3529\n",
      "Epoch: 1717/2000... Training loss: 0.4691\n",
      "Epoch: 1717/2000... Training loss: 0.4583\n",
      "Epoch: 1717/2000... Training loss: 0.3940\n",
      "Epoch: 1717/2000... Training loss: 0.4366\n",
      "Epoch: 1717/2000... Training loss: 0.3349\n",
      "Epoch: 1717/2000... Training loss: 0.3520\n",
      "Epoch: 1718/2000... Training loss: 0.4334\n",
      "Epoch: 1718/2000... Training loss: 0.2682\n",
      "Epoch: 1718/2000... Training loss: 0.4617\n",
      "Epoch: 1718/2000... Training loss: 0.4678\n",
      "Epoch: 1718/2000... Training loss: 0.4884\n",
      "Epoch: 1718/2000... Training loss: 0.3745\n",
      "Epoch: 1718/2000... Training loss: 0.4614\n",
      "Epoch: 1718/2000... Training loss: 0.6071\n",
      "Epoch: 1718/2000... Training loss: 0.2443\n",
      "Epoch: 1718/2000... Training loss: 0.3814\n",
      "Epoch: 1718/2000... Training loss: 0.3193\n",
      "Epoch: 1718/2000... Training loss: 0.2950\n",
      "Epoch: 1718/2000... Training loss: 0.2426\n",
      "Epoch: 1718/2000... Training loss: 0.4079\n",
      "Epoch: 1718/2000... Training loss: 0.4624\n",
      "Epoch: 1718/2000... Training loss: 0.2739\n",
      "Epoch: 1718/2000... Training loss: 0.2396\n",
      "Epoch: 1718/2000... Training loss: 0.4971\n",
      "Epoch: 1718/2000... Training loss: 0.4058\n",
      "Epoch: 1718/2000... Training loss: 0.3386\n",
      "Epoch: 1718/2000... Training loss: 0.3553\n",
      "Epoch: 1718/2000... Training loss: 0.2927\n",
      "Epoch: 1718/2000... Training loss: 0.3974\n",
      "Epoch: 1718/2000... Training loss: 0.2775\n",
      "Epoch: 1718/2000... Training loss: 0.3576\n",
      "Epoch: 1718/2000... Training loss: 0.4725\n",
      "Epoch: 1718/2000... Training loss: 0.2850\n",
      "Epoch: 1718/2000... Training loss: 0.5919\n",
      "Epoch: 1718/2000... Training loss: 0.4002\n",
      "Epoch: 1718/2000... Training loss: 0.2405\n",
      "Epoch: 1718/2000... Training loss: 0.4874\n",
      "Epoch: 1719/2000... Training loss: 0.3610\n",
      "Epoch: 1719/2000... Training loss: 0.4170\n",
      "Epoch: 1719/2000... Training loss: 0.3784\n",
      "Epoch: 1719/2000... Training loss: 0.4174\n",
      "Epoch: 1719/2000... Training loss: 0.5054\n",
      "Epoch: 1719/2000... Training loss: 0.3035\n",
      "Epoch: 1719/2000... Training loss: 0.3250\n",
      "Epoch: 1719/2000... Training loss: 0.3530\n",
      "Epoch: 1719/2000... Training loss: 0.4411\n",
      "Epoch: 1719/2000... Training loss: 0.2914\n",
      "Epoch: 1719/2000... Training loss: 0.4109\n",
      "Epoch: 1719/2000... Training loss: 0.3394\n",
      "Epoch: 1719/2000... Training loss: 0.3738\n",
      "Epoch: 1719/2000... Training loss: 0.2418\n",
      "Epoch: 1719/2000... Training loss: 0.3518\n",
      "Epoch: 1719/2000... Training loss: 0.2864\n",
      "Epoch: 1719/2000... Training loss: 0.5126\n",
      "Epoch: 1719/2000... Training loss: 0.4034\n",
      "Epoch: 1719/2000... Training loss: 0.3096\n",
      "Epoch: 1719/2000... Training loss: 0.3299\n",
      "Epoch: 1719/2000... Training loss: 0.3134\n",
      "Epoch: 1719/2000... Training loss: 0.2931\n",
      "Epoch: 1719/2000... Training loss: 0.3647\n",
      "Epoch: 1719/2000... Training loss: 0.4919\n",
      "Epoch: 1719/2000... Training loss: 0.3152\n",
      "Epoch: 1719/2000... Training loss: 0.3846\n",
      "Epoch: 1719/2000... Training loss: 0.4889\n",
      "Epoch: 1719/2000... Training loss: 0.3851\n",
      "Epoch: 1719/2000... Training loss: 0.3086\n",
      "Epoch: 1719/2000... Training loss: 0.3827\n",
      "Epoch: 1719/2000... Training loss: 0.4016\n",
      "Epoch: 1720/2000... Training loss: 0.1922\n",
      "Epoch: 1720/2000... Training loss: 0.5269\n",
      "Epoch: 1720/2000... Training loss: 0.3705\n",
      "Epoch: 1720/2000... Training loss: 0.4400\n",
      "Epoch: 1720/2000... Training loss: 0.2859\n",
      "Epoch: 1720/2000... Training loss: 0.3846\n",
      "Epoch: 1720/2000... Training loss: 0.5603\n",
      "Epoch: 1720/2000... Training loss: 0.2254\n",
      "Epoch: 1720/2000... Training loss: 0.2739\n",
      "Epoch: 1720/2000... Training loss: 0.3475\n",
      "Epoch: 1720/2000... Training loss: 0.4687\n",
      "Epoch: 1720/2000... Training loss: 0.3352\n",
      "Epoch: 1720/2000... Training loss: 0.3517\n",
      "Epoch: 1720/2000... Training loss: 0.2111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1720/2000... Training loss: 0.5940\n",
      "Epoch: 1720/2000... Training loss: 0.4271\n",
      "Epoch: 1720/2000... Training loss: 0.2935\n",
      "Epoch: 1720/2000... Training loss: 0.5019\n",
      "Epoch: 1720/2000... Training loss: 0.5791\n",
      "Epoch: 1720/2000... Training loss: 0.3296\n",
      "Epoch: 1720/2000... Training loss: 0.4626\n",
      "Epoch: 1720/2000... Training loss: 0.3512\n",
      "Epoch: 1720/2000... Training loss: 0.4448\n",
      "Epoch: 1720/2000... Training loss: 0.3892\n",
      "Epoch: 1720/2000... Training loss: 0.4108\n",
      "Epoch: 1720/2000... Training loss: 0.3054\n",
      "Epoch: 1720/2000... Training loss: 0.3821\n",
      "Epoch: 1720/2000... Training loss: 0.4157\n",
      "Epoch: 1720/2000... Training loss: 0.4096\n",
      "Epoch: 1720/2000... Training loss: 0.5507\n",
      "Epoch: 1720/2000... Training loss: 0.4437\n",
      "Epoch: 1721/2000... Training loss: 0.3024\n",
      "Epoch: 1721/2000... Training loss: 0.3602\n",
      "Epoch: 1721/2000... Training loss: 0.4292\n",
      "Epoch: 1721/2000... Training loss: 0.4732\n",
      "Epoch: 1721/2000... Training loss: 0.3831\n",
      "Epoch: 1721/2000... Training loss: 0.3107\n",
      "Epoch: 1721/2000... Training loss: 0.3790\n",
      "Epoch: 1721/2000... Training loss: 0.5530\n",
      "Epoch: 1721/2000... Training loss: 0.3777\n",
      "Epoch: 1721/2000... Training loss: 0.3824\n",
      "Epoch: 1721/2000... Training loss: 0.3447\n",
      "Epoch: 1721/2000... Training loss: 0.5245\n",
      "Epoch: 1721/2000... Training loss: 0.3701\n",
      "Epoch: 1721/2000... Training loss: 0.3004\n",
      "Epoch: 1721/2000... Training loss: 0.3806\n",
      "Epoch: 1721/2000... Training loss: 0.5052\n",
      "Epoch: 1721/2000... Training loss: 0.4992\n",
      "Epoch: 1721/2000... Training loss: 0.3390\n",
      "Epoch: 1721/2000... Training loss: 0.3883\n",
      "Epoch: 1721/2000... Training loss: 0.4471\n",
      "Epoch: 1721/2000... Training loss: 0.3326\n",
      "Epoch: 1721/2000... Training loss: 0.4834\n",
      "Epoch: 1721/2000... Training loss: 0.4356\n",
      "Epoch: 1721/2000... Training loss: 0.4758\n",
      "Epoch: 1721/2000... Training loss: 0.5199\n",
      "Epoch: 1721/2000... Training loss: 0.5561\n",
      "Epoch: 1721/2000... Training loss: 0.3253\n",
      "Epoch: 1721/2000... Training loss: 0.3342\n",
      "Epoch: 1721/2000... Training loss: 0.3031\n",
      "Epoch: 1721/2000... Training loss: 0.2616\n",
      "Epoch: 1721/2000... Training loss: 0.3980\n",
      "Epoch: 1722/2000... Training loss: 0.3479\n",
      "Epoch: 1722/2000... Training loss: 0.5303\n",
      "Epoch: 1722/2000... Training loss: 0.4646\n",
      "Epoch: 1722/2000... Training loss: 0.3882\n",
      "Epoch: 1722/2000... Training loss: 0.2970\n",
      "Epoch: 1722/2000... Training loss: 0.4889\n",
      "Epoch: 1722/2000... Training loss: 0.4538\n",
      "Epoch: 1722/2000... Training loss: 0.2913\n",
      "Epoch: 1722/2000... Training loss: 0.3617\n",
      "Epoch: 1722/2000... Training loss: 0.3126\n",
      "Epoch: 1722/2000... Training loss: 0.3794\n",
      "Epoch: 1722/2000... Training loss: 0.3431\n",
      "Epoch: 1722/2000... Training loss: 0.2800\n",
      "Epoch: 1722/2000... Training loss: 0.4519\n",
      "Epoch: 1722/2000... Training loss: 0.1973\n",
      "Epoch: 1722/2000... Training loss: 0.3771\n",
      "Epoch: 1722/2000... Training loss: 0.4660\n",
      "Epoch: 1722/2000... Training loss: 0.3721\n",
      "Epoch: 1722/2000... Training loss: 0.4739\n",
      "Epoch: 1722/2000... Training loss: 0.3977\n",
      "Epoch: 1722/2000... Training loss: 0.4329\n",
      "Epoch: 1722/2000... Training loss: 0.4971\n",
      "Epoch: 1722/2000... Training loss: 0.4169\n",
      "Epoch: 1722/2000... Training loss: 0.3773\n",
      "Epoch: 1722/2000... Training loss: 0.3351\n",
      "Epoch: 1722/2000... Training loss: 0.3212\n",
      "Epoch: 1722/2000... Training loss: 0.3193\n",
      "Epoch: 1722/2000... Training loss: 0.2400\n",
      "Epoch: 1722/2000... Training loss: 0.5233\n",
      "Epoch: 1722/2000... Training loss: 0.2759\n",
      "Epoch: 1722/2000... Training loss: 0.3094\n",
      "Epoch: 1723/2000... Training loss: 0.3429\n",
      "Epoch: 1723/2000... Training loss: 0.3943\n",
      "Epoch: 1723/2000... Training loss: 0.4763\n",
      "Epoch: 1723/2000... Training loss: 0.2820\n",
      "Epoch: 1723/2000... Training loss: 0.4343\n",
      "Epoch: 1723/2000... Training loss: 0.2010\n",
      "Epoch: 1723/2000... Training loss: 0.3908\n",
      "Epoch: 1723/2000... Training loss: 0.5047\n",
      "Epoch: 1723/2000... Training loss: 0.4416\n",
      "Epoch: 1723/2000... Training loss: 0.4874\n",
      "Epoch: 1723/2000... Training loss: 0.3107\n",
      "Epoch: 1723/2000... Training loss: 0.3679\n",
      "Epoch: 1723/2000... Training loss: 0.3780\n",
      "Epoch: 1723/2000... Training loss: 0.3175\n",
      "Epoch: 1723/2000... Training loss: 0.3747\n",
      "Epoch: 1723/2000... Training loss: 0.2591\n",
      "Epoch: 1723/2000... Training loss: 0.3713\n",
      "Epoch: 1723/2000... Training loss: 0.4083\n",
      "Epoch: 1723/2000... Training loss: 0.5183\n",
      "Epoch: 1723/2000... Training loss: 0.6423\n",
      "Epoch: 1723/2000... Training loss: 0.2609\n",
      "Epoch: 1723/2000... Training loss: 0.2604\n",
      "Epoch: 1723/2000... Training loss: 0.2755\n",
      "Epoch: 1723/2000... Training loss: 0.1865\n",
      "Epoch: 1723/2000... Training loss: 0.3439\n",
      "Epoch: 1723/2000... Training loss: 0.4859\n",
      "Epoch: 1723/2000... Training loss: 0.3658\n",
      "Epoch: 1723/2000... Training loss: 0.3095\n",
      "Epoch: 1723/2000... Training loss: 0.6695\n",
      "Epoch: 1723/2000... Training loss: 0.3237\n",
      "Epoch: 1723/2000... Training loss: 0.3604\n",
      "Epoch: 1724/2000... Training loss: 0.3907\n",
      "Epoch: 1724/2000... Training loss: 0.3403\n",
      "Epoch: 1724/2000... Training loss: 0.4378\n",
      "Epoch: 1724/2000... Training loss: 0.5729\n",
      "Epoch: 1724/2000... Training loss: 0.2534\n",
      "Epoch: 1724/2000... Training loss: 0.5668\n",
      "Epoch: 1724/2000... Training loss: 0.4794\n",
      "Epoch: 1724/2000... Training loss: 0.4985\n",
      "Epoch: 1724/2000... Training loss: 0.4854\n",
      "Epoch: 1724/2000... Training loss: 0.3185\n",
      "Epoch: 1724/2000... Training loss: 0.3389\n",
      "Epoch: 1724/2000... Training loss: 0.4645\n",
      "Epoch: 1724/2000... Training loss: 0.3474\n",
      "Epoch: 1724/2000... Training loss: 0.3170\n",
      "Epoch: 1724/2000... Training loss: 0.3981\n",
      "Epoch: 1724/2000... Training loss: 0.3459\n",
      "Epoch: 1724/2000... Training loss: 0.3746\n",
      "Epoch: 1724/2000... Training loss: 0.4540\n",
      "Epoch: 1724/2000... Training loss: 0.3805\n",
      "Epoch: 1724/2000... Training loss: 0.4502\n",
      "Epoch: 1724/2000... Training loss: 0.3335\n",
      "Epoch: 1724/2000... Training loss: 0.3264\n",
      "Epoch: 1724/2000... Training loss: 0.4491\n",
      "Epoch: 1724/2000... Training loss: 0.4759\n",
      "Epoch: 1724/2000... Training loss: 0.3019\n",
      "Epoch: 1724/2000... Training loss: 0.3556\n",
      "Epoch: 1724/2000... Training loss: 0.3687\n",
      "Epoch: 1724/2000... Training loss: 0.5030\n",
      "Epoch: 1724/2000... Training loss: 0.3236\n",
      "Epoch: 1724/2000... Training loss: 0.3609\n",
      "Epoch: 1724/2000... Training loss: 0.5225\n",
      "Epoch: 1725/2000... Training loss: 0.3444\n",
      "Epoch: 1725/2000... Training loss: 0.3935\n",
      "Epoch: 1725/2000... Training loss: 0.3822\n",
      "Epoch: 1725/2000... Training loss: 0.3419\n",
      "Epoch: 1725/2000... Training loss: 0.5051\n",
      "Epoch: 1725/2000... Training loss: 0.4232\n",
      "Epoch: 1725/2000... Training loss: 0.2863\n",
      "Epoch: 1725/2000... Training loss: 0.4184\n",
      "Epoch: 1725/2000... Training loss: 0.2770\n",
      "Epoch: 1725/2000... Training loss: 0.3769\n",
      "Epoch: 1725/2000... Training loss: 0.4241\n",
      "Epoch: 1725/2000... Training loss: 0.4586\n",
      "Epoch: 1725/2000... Training loss: 0.2051\n",
      "Epoch: 1725/2000... Training loss: 0.4180\n",
      "Epoch: 1725/2000... Training loss: 0.4471\n",
      "Epoch: 1725/2000... Training loss: 0.3783\n",
      "Epoch: 1725/2000... Training loss: 0.2939\n",
      "Epoch: 1725/2000... Training loss: 0.3646\n",
      "Epoch: 1725/2000... Training loss: 0.5104\n",
      "Epoch: 1725/2000... Training loss: 0.2500\n",
      "Epoch: 1725/2000... Training loss: 0.3639\n",
      "Epoch: 1725/2000... Training loss: 0.7825\n",
      "Epoch: 1725/2000... Training loss: 0.3589\n",
      "Epoch: 1725/2000... Training loss: 0.4331\n",
      "Epoch: 1725/2000... Training loss: 0.4762\n",
      "Epoch: 1725/2000... Training loss: 0.4752\n",
      "Epoch: 1725/2000... Training loss: 0.3817\n",
      "Epoch: 1725/2000... Training loss: 0.3271\n",
      "Epoch: 1725/2000... Training loss: 0.4068\n",
      "Epoch: 1725/2000... Training loss: 0.3022\n",
      "Epoch: 1725/2000... Training loss: 0.5084\n",
      "Epoch: 1726/2000... Training loss: 0.3827\n",
      "Epoch: 1726/2000... Training loss: 0.4217\n",
      "Epoch: 1726/2000... Training loss: 0.3781\n",
      "Epoch: 1726/2000... Training loss: 0.5081\n",
      "Epoch: 1726/2000... Training loss: 0.5598\n",
      "Epoch: 1726/2000... Training loss: 0.3919\n",
      "Epoch: 1726/2000... Training loss: 0.5363\n",
      "Epoch: 1726/2000... Training loss: 0.4420\n",
      "Epoch: 1726/2000... Training loss: 0.4533\n",
      "Epoch: 1726/2000... Training loss: 0.4550\n",
      "Epoch: 1726/2000... Training loss: 0.4248\n",
      "Epoch: 1726/2000... Training loss: 0.3897\n",
      "Epoch: 1726/2000... Training loss: 0.3042\n",
      "Epoch: 1726/2000... Training loss: 0.4436\n",
      "Epoch: 1726/2000... Training loss: 0.4509\n",
      "Epoch: 1726/2000... Training loss: 0.3309\n",
      "Epoch: 1726/2000... Training loss: 0.4347\n",
      "Epoch: 1726/2000... Training loss: 0.4688\n",
      "Epoch: 1726/2000... Training loss: 0.4067\n",
      "Epoch: 1726/2000... Training loss: 0.3768\n",
      "Epoch: 1726/2000... Training loss: 0.2963\n",
      "Epoch: 1726/2000... Training loss: 0.4277\n",
      "Epoch: 1726/2000... Training loss: 0.4109\n",
      "Epoch: 1726/2000... Training loss: 0.4962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1726/2000... Training loss: 0.5336\n",
      "Epoch: 1726/2000... Training loss: 0.2178\n",
      "Epoch: 1726/2000... Training loss: 0.3174\n",
      "Epoch: 1726/2000... Training loss: 0.4552\n",
      "Epoch: 1726/2000... Training loss: 0.5167\n",
      "Epoch: 1726/2000... Training loss: 0.2946\n",
      "Epoch: 1726/2000... Training loss: 0.3283\n",
      "Epoch: 1727/2000... Training loss: 0.4009\n",
      "Epoch: 1727/2000... Training loss: 0.3069\n",
      "Epoch: 1727/2000... Training loss: 0.3870\n",
      "Epoch: 1727/2000... Training loss: 0.5756\n",
      "Epoch: 1727/2000... Training loss: 0.3196\n",
      "Epoch: 1727/2000... Training loss: 0.2938\n",
      "Epoch: 1727/2000... Training loss: 0.5781\n",
      "Epoch: 1727/2000... Training loss: 0.2815\n",
      "Epoch: 1727/2000... Training loss: 0.4524\n",
      "Epoch: 1727/2000... Training loss: 0.5263\n",
      "Epoch: 1727/2000... Training loss: 0.4203\n",
      "Epoch: 1727/2000... Training loss: 0.2745\n",
      "Epoch: 1727/2000... Training loss: 0.4367\n",
      "Epoch: 1727/2000... Training loss: 0.3573\n",
      "Epoch: 1727/2000... Training loss: 0.2468\n",
      "Epoch: 1727/2000... Training loss: 0.3375\n",
      "Epoch: 1727/2000... Training loss: 0.3475\n",
      "Epoch: 1727/2000... Training loss: 0.2902\n",
      "Epoch: 1727/2000... Training loss: 0.4679\n",
      "Epoch: 1727/2000... Training loss: 0.3800\n",
      "Epoch: 1727/2000... Training loss: 0.3156\n",
      "Epoch: 1727/2000... Training loss: 0.5578\n",
      "Epoch: 1727/2000... Training loss: 0.3236\n",
      "Epoch: 1727/2000... Training loss: 0.2248\n",
      "Epoch: 1727/2000... Training loss: 0.3569\n",
      "Epoch: 1727/2000... Training loss: 0.3682\n",
      "Epoch: 1727/2000... Training loss: 0.3021\n",
      "Epoch: 1727/2000... Training loss: 0.4812\n",
      "Epoch: 1727/2000... Training loss: 0.3574\n",
      "Epoch: 1727/2000... Training loss: 0.5220\n",
      "Epoch: 1727/2000... Training loss: 0.4133\n",
      "Epoch: 1728/2000... Training loss: 0.6099\n",
      "Epoch: 1728/2000... Training loss: 0.4631\n",
      "Epoch: 1728/2000... Training loss: 0.4257\n",
      "Epoch: 1728/2000... Training loss: 0.3741\n",
      "Epoch: 1728/2000... Training loss: 0.4534\n",
      "Epoch: 1728/2000... Training loss: 0.3890\n",
      "Epoch: 1728/2000... Training loss: 0.4057\n",
      "Epoch: 1728/2000... Training loss: 0.3250\n",
      "Epoch: 1728/2000... Training loss: 0.4774\n",
      "Epoch: 1728/2000... Training loss: 0.3191\n",
      "Epoch: 1728/2000... Training loss: 0.5275\n",
      "Epoch: 1728/2000... Training loss: 0.2560\n",
      "Epoch: 1728/2000... Training loss: 0.3164\n",
      "Epoch: 1728/2000... Training loss: 0.3639\n",
      "Epoch: 1728/2000... Training loss: 0.3448\n",
      "Epoch: 1728/2000... Training loss: 0.3058\n",
      "Epoch: 1728/2000... Training loss: 0.3999\n",
      "Epoch: 1728/2000... Training loss: 0.4993\n",
      "Epoch: 1728/2000... Training loss: 0.3647\n",
      "Epoch: 1728/2000... Training loss: 0.3879\n",
      "Epoch: 1728/2000... Training loss: 0.3695\n",
      "Epoch: 1728/2000... Training loss: 0.4774\n",
      "Epoch: 1728/2000... Training loss: 0.3346\n",
      "Epoch: 1728/2000... Training loss: 0.3947\n",
      "Epoch: 1728/2000... Training loss: 0.2975\n",
      "Epoch: 1728/2000... Training loss: 0.3948\n",
      "Epoch: 1728/2000... Training loss: 0.4871\n",
      "Epoch: 1728/2000... Training loss: 0.3119\n",
      "Epoch: 1728/2000... Training loss: 0.3776\n",
      "Epoch: 1728/2000... Training loss: 0.4174\n",
      "Epoch: 1728/2000... Training loss: 0.3481\n",
      "Epoch: 1729/2000... Training loss: 0.2644\n",
      "Epoch: 1729/2000... Training loss: 0.4678\n",
      "Epoch: 1729/2000... Training loss: 0.5910\n",
      "Epoch: 1729/2000... Training loss: 0.4233\n",
      "Epoch: 1729/2000... Training loss: 0.3152\n",
      "Epoch: 1729/2000... Training loss: 0.5115\n",
      "Epoch: 1729/2000... Training loss: 0.3928\n",
      "Epoch: 1729/2000... Training loss: 0.2188\n",
      "Epoch: 1729/2000... Training loss: 0.3478\n",
      "Epoch: 1729/2000... Training loss: 0.3794\n",
      "Epoch: 1729/2000... Training loss: 0.3937\n",
      "Epoch: 1729/2000... Training loss: 0.3032\n",
      "Epoch: 1729/2000... Training loss: 0.2598\n",
      "Epoch: 1729/2000... Training loss: 0.5631\n",
      "Epoch: 1729/2000... Training loss: 0.2940\n",
      "Epoch: 1729/2000... Training loss: 0.5184\n",
      "Epoch: 1729/2000... Training loss: 0.3689\n",
      "Epoch: 1729/2000... Training loss: 0.3196\n",
      "Epoch: 1729/2000... Training loss: 0.6355\n",
      "Epoch: 1729/2000... Training loss: 0.4061\n",
      "Epoch: 1729/2000... Training loss: 0.2887\n",
      "Epoch: 1729/2000... Training loss: 0.3967\n",
      "Epoch: 1729/2000... Training loss: 0.3371\n",
      "Epoch: 1729/2000... Training loss: 0.5696\n",
      "Epoch: 1729/2000... Training loss: 0.4344\n",
      "Epoch: 1729/2000... Training loss: 0.3481\n",
      "Epoch: 1729/2000... Training loss: 0.3567\n",
      "Epoch: 1729/2000... Training loss: 0.4686\n",
      "Epoch: 1729/2000... Training loss: 0.3656\n",
      "Epoch: 1729/2000... Training loss: 0.2169\n",
      "Epoch: 1729/2000... Training loss: 0.4774\n",
      "Epoch: 1730/2000... Training loss: 0.3102\n",
      "Epoch: 1730/2000... Training loss: 0.4964\n",
      "Epoch: 1730/2000... Training loss: 0.3023\n",
      "Epoch: 1730/2000... Training loss: 0.3837\n",
      "Epoch: 1730/2000... Training loss: 0.3592\n",
      "Epoch: 1730/2000... Training loss: 0.4157\n",
      "Epoch: 1730/2000... Training loss: 0.3580\n",
      "Epoch: 1730/2000... Training loss: 0.4745\n",
      "Epoch: 1730/2000... Training loss: 0.2758\n",
      "Epoch: 1730/2000... Training loss: 0.3584\n",
      "Epoch: 1730/2000... Training loss: 0.3841\n",
      "Epoch: 1730/2000... Training loss: 0.5139\n",
      "Epoch: 1730/2000... Training loss: 0.4262\n",
      "Epoch: 1730/2000... Training loss: 0.6269\n",
      "Epoch: 1730/2000... Training loss: 0.3368\n",
      "Epoch: 1730/2000... Training loss: 0.3863\n",
      "Epoch: 1730/2000... Training loss: 0.4869\n",
      "Epoch: 1730/2000... Training loss: 0.3077\n",
      "Epoch: 1730/2000... Training loss: 0.2442\n",
      "Epoch: 1730/2000... Training loss: 0.3875\n",
      "Epoch: 1730/2000... Training loss: 0.4034\n",
      "Epoch: 1730/2000... Training loss: 0.3029\n",
      "Epoch: 1730/2000... Training loss: 0.3735\n",
      "Epoch: 1730/2000... Training loss: 0.3463\n",
      "Epoch: 1730/2000... Training loss: 0.2953\n",
      "Epoch: 1730/2000... Training loss: 0.5782\n",
      "Epoch: 1730/2000... Training loss: 0.3992\n",
      "Epoch: 1730/2000... Training loss: 0.3578\n",
      "Epoch: 1730/2000... Training loss: 0.4423\n",
      "Epoch: 1730/2000... Training loss: 0.3725\n",
      "Epoch: 1730/2000... Training loss: 0.4743\n",
      "Epoch: 1731/2000... Training loss: 0.5038\n",
      "Epoch: 1731/2000... Training loss: 0.4798\n",
      "Epoch: 1731/2000... Training loss: 0.4024\n",
      "Epoch: 1731/2000... Training loss: 0.5322\n",
      "Epoch: 1731/2000... Training loss: 0.3327\n",
      "Epoch: 1731/2000... Training loss: 0.3442\n",
      "Epoch: 1731/2000... Training loss: 0.3028\n",
      "Epoch: 1731/2000... Training loss: 0.3895\n",
      "Epoch: 1731/2000... Training loss: 0.5427\n",
      "Epoch: 1731/2000... Training loss: 0.3764\n",
      "Epoch: 1731/2000... Training loss: 0.4794\n",
      "Epoch: 1731/2000... Training loss: 0.3746\n",
      "Epoch: 1731/2000... Training loss: 0.4057\n",
      "Epoch: 1731/2000... Training loss: 0.2222\n",
      "Epoch: 1731/2000... Training loss: 0.4616\n",
      "Epoch: 1731/2000... Training loss: 0.2079\n",
      "Epoch: 1731/2000... Training loss: 0.2389\n",
      "Epoch: 1731/2000... Training loss: 0.4441\n",
      "Epoch: 1731/2000... Training loss: 0.4667\n",
      "Epoch: 1731/2000... Training loss: 0.4272\n",
      "Epoch: 1731/2000... Training loss: 0.5836\n",
      "Epoch: 1731/2000... Training loss: 0.2504\n",
      "Epoch: 1731/2000... Training loss: 0.4390\n",
      "Epoch: 1731/2000... Training loss: 0.4444\n",
      "Epoch: 1731/2000... Training loss: 0.3976\n",
      "Epoch: 1731/2000... Training loss: 0.3912\n",
      "Epoch: 1731/2000... Training loss: 0.3568\n",
      "Epoch: 1731/2000... Training loss: 0.4477\n",
      "Epoch: 1731/2000... Training loss: 0.4795\n",
      "Epoch: 1731/2000... Training loss: 0.3459\n",
      "Epoch: 1731/2000... Training loss: 0.2404\n",
      "Epoch: 1732/2000... Training loss: 0.3324\n",
      "Epoch: 1732/2000... Training loss: 0.2643\n",
      "Epoch: 1732/2000... Training loss: 0.3217\n",
      "Epoch: 1732/2000... Training loss: 0.6063\n",
      "Epoch: 1732/2000... Training loss: 0.4787\n",
      "Epoch: 1732/2000... Training loss: 0.3443\n",
      "Epoch: 1732/2000... Training loss: 0.2986\n",
      "Epoch: 1732/2000... Training loss: 0.4753\n",
      "Epoch: 1732/2000... Training loss: 0.3579\n",
      "Epoch: 1732/2000... Training loss: 0.3996\n",
      "Epoch: 1732/2000... Training loss: 0.4376\n",
      "Epoch: 1732/2000... Training loss: 0.2808\n",
      "Epoch: 1732/2000... Training loss: 0.5106\n",
      "Epoch: 1732/2000... Training loss: 0.4011\n",
      "Epoch: 1732/2000... Training loss: 0.3656\n",
      "Epoch: 1732/2000... Training loss: 0.4088\n",
      "Epoch: 1732/2000... Training loss: 0.1978\n",
      "Epoch: 1732/2000... Training loss: 0.2833\n",
      "Epoch: 1732/2000... Training loss: 0.5350\n",
      "Epoch: 1732/2000... Training loss: 0.5126\n",
      "Epoch: 1732/2000... Training loss: 0.3208\n",
      "Epoch: 1732/2000... Training loss: 0.2775\n",
      "Epoch: 1732/2000... Training loss: 0.4886\n",
      "Epoch: 1732/2000... Training loss: 0.3853\n",
      "Epoch: 1732/2000... Training loss: 0.2825\n",
      "Epoch: 1732/2000... Training loss: 0.4423\n",
      "Epoch: 1732/2000... Training loss: 0.3220\n",
      "Epoch: 1732/2000... Training loss: 0.2843\n",
      "Epoch: 1732/2000... Training loss: 0.4718\n",
      "Epoch: 1732/2000... Training loss: 0.3505\n",
      "Epoch: 1732/2000... Training loss: 0.4098\n",
      "Epoch: 1733/2000... Training loss: 0.5781\n",
      "Epoch: 1733/2000... Training loss: 0.3422\n",
      "Epoch: 1733/2000... Training loss: 0.4316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1733/2000... Training loss: 0.3161\n",
      "Epoch: 1733/2000... Training loss: 0.3354\n",
      "Epoch: 1733/2000... Training loss: 0.3842\n",
      "Epoch: 1733/2000... Training loss: 0.4112\n",
      "Epoch: 1733/2000... Training loss: 0.3574\n",
      "Epoch: 1733/2000... Training loss: 0.3113\n",
      "Epoch: 1733/2000... Training loss: 0.4387\n",
      "Epoch: 1733/2000... Training loss: 0.3158\n",
      "Epoch: 1733/2000... Training loss: 0.2809\n",
      "Epoch: 1733/2000... Training loss: 0.3357\n",
      "Epoch: 1733/2000... Training loss: 0.4152\n",
      "Epoch: 1733/2000... Training loss: 0.4790\n",
      "Epoch: 1733/2000... Training loss: 0.3838\n",
      "Epoch: 1733/2000... Training loss: 0.4641\n",
      "Epoch: 1733/2000... Training loss: 0.3663\n",
      "Epoch: 1733/2000... Training loss: 0.3073\n",
      "Epoch: 1733/2000... Training loss: 0.3900\n",
      "Epoch: 1733/2000... Training loss: 0.3665\n",
      "Epoch: 1733/2000... Training loss: 0.2883\n",
      "Epoch: 1733/2000... Training loss: 0.3153\n",
      "Epoch: 1733/2000... Training loss: 0.3998\n",
      "Epoch: 1733/2000... Training loss: 0.4554\n",
      "Epoch: 1733/2000... Training loss: 0.5041\n",
      "Epoch: 1733/2000... Training loss: 0.3084\n",
      "Epoch: 1733/2000... Training loss: 0.4673\n",
      "Epoch: 1733/2000... Training loss: 0.3194\n",
      "Epoch: 1733/2000... Training loss: 0.4518\n",
      "Epoch: 1733/2000... Training loss: 0.3548\n",
      "Epoch: 1734/2000... Training loss: 0.2948\n",
      "Epoch: 1734/2000... Training loss: 0.4180\n",
      "Epoch: 1734/2000... Training loss: 0.3417\n",
      "Epoch: 1734/2000... Training loss: 0.3933\n",
      "Epoch: 1734/2000... Training loss: 0.3452\n",
      "Epoch: 1734/2000... Training loss: 0.4972\n",
      "Epoch: 1734/2000... Training loss: 0.4749\n",
      "Epoch: 1734/2000... Training loss: 0.3510\n",
      "Epoch: 1734/2000... Training loss: 0.3918\n",
      "Epoch: 1734/2000... Training loss: 0.4377\n",
      "Epoch: 1734/2000... Training loss: 0.4108\n",
      "Epoch: 1734/2000... Training loss: 0.3776\n",
      "Epoch: 1734/2000... Training loss: 0.4447\n",
      "Epoch: 1734/2000... Training loss: 0.3758\n",
      "Epoch: 1734/2000... Training loss: 0.4124\n",
      "Epoch: 1734/2000... Training loss: 0.3940\n",
      "Epoch: 1734/2000... Training loss: 0.3125\n",
      "Epoch: 1734/2000... Training loss: 0.3974\n",
      "Epoch: 1734/2000... Training loss: 0.2715\n",
      "Epoch: 1734/2000... Training loss: 0.4496\n",
      "Epoch: 1734/2000... Training loss: 0.4014\n",
      "Epoch: 1734/2000... Training loss: 0.2223\n",
      "Epoch: 1734/2000... Training loss: 0.3981\n",
      "Epoch: 1734/2000... Training loss: 0.5031\n",
      "Epoch: 1734/2000... Training loss: 0.2483\n",
      "Epoch: 1734/2000... Training loss: 0.5106\n",
      "Epoch: 1734/2000... Training loss: 0.3474\n",
      "Epoch: 1734/2000... Training loss: 0.2569\n",
      "Epoch: 1734/2000... Training loss: 0.3413\n",
      "Epoch: 1734/2000... Training loss: 0.4558\n",
      "Epoch: 1734/2000... Training loss: 0.3804\n",
      "Epoch: 1735/2000... Training loss: 0.5696\n",
      "Epoch: 1735/2000... Training loss: 0.5265\n",
      "Epoch: 1735/2000... Training loss: 0.3920\n",
      "Epoch: 1735/2000... Training loss: 0.5496\n",
      "Epoch: 1735/2000... Training loss: 0.2370\n",
      "Epoch: 1735/2000... Training loss: 0.5399\n",
      "Epoch: 1735/2000... Training loss: 0.5063\n",
      "Epoch: 1735/2000... Training loss: 0.2997\n",
      "Epoch: 1735/2000... Training loss: 0.5424\n",
      "Epoch: 1735/2000... Training loss: 0.2632\n",
      "Epoch: 1735/2000... Training loss: 0.4077\n",
      "Epoch: 1735/2000... Training loss: 0.3836\n",
      "Epoch: 1735/2000... Training loss: 0.4046\n",
      "Epoch: 1735/2000... Training loss: 0.2722\n",
      "Epoch: 1735/2000... Training loss: 0.4297\n",
      "Epoch: 1735/2000... Training loss: 0.3704\n",
      "Epoch: 1735/2000... Training loss: 0.6234\n",
      "Epoch: 1735/2000... Training loss: 0.2404\n",
      "Epoch: 1735/2000... Training loss: 0.4162\n",
      "Epoch: 1735/2000... Training loss: 0.3440\n",
      "Epoch: 1735/2000... Training loss: 0.3935\n",
      "Epoch: 1735/2000... Training loss: 0.3827\n",
      "Epoch: 1735/2000... Training loss: 0.4501\n",
      "Epoch: 1735/2000... Training loss: 0.4659\n",
      "Epoch: 1735/2000... Training loss: 0.3679\n",
      "Epoch: 1735/2000... Training loss: 0.5482\n",
      "Epoch: 1735/2000... Training loss: 0.4263\n",
      "Epoch: 1735/2000... Training loss: 0.4607\n",
      "Epoch: 1735/2000... Training loss: 0.3403\n",
      "Epoch: 1735/2000... Training loss: 0.3209\n",
      "Epoch: 1735/2000... Training loss: 0.2440\n",
      "Epoch: 1736/2000... Training loss: 0.3967\n",
      "Epoch: 1736/2000... Training loss: 0.3984\n",
      "Epoch: 1736/2000... Training loss: 0.3069\n",
      "Epoch: 1736/2000... Training loss: 0.4379\n",
      "Epoch: 1736/2000... Training loss: 0.4922\n",
      "Epoch: 1736/2000... Training loss: 0.3713\n",
      "Epoch: 1736/2000... Training loss: 0.4585\n",
      "Epoch: 1736/2000... Training loss: 0.3636\n",
      "Epoch: 1736/2000... Training loss: 0.4082\n",
      "Epoch: 1736/2000... Training loss: 0.3646\n",
      "Epoch: 1736/2000... Training loss: 0.4518\n",
      "Epoch: 1736/2000... Training loss: 0.3874\n",
      "Epoch: 1736/2000... Training loss: 0.4062\n",
      "Epoch: 1736/2000... Training loss: 0.3533\n",
      "Epoch: 1736/2000... Training loss: 0.3596\n",
      "Epoch: 1736/2000... Training loss: 0.6250\n",
      "Epoch: 1736/2000... Training loss: 0.7112\n",
      "Epoch: 1736/2000... Training loss: 0.3226\n",
      "Epoch: 1736/2000... Training loss: 0.5285\n",
      "Epoch: 1736/2000... Training loss: 0.2395\n",
      "Epoch: 1736/2000... Training loss: 0.4430\n",
      "Epoch: 1736/2000... Training loss: 0.3473\n",
      "Epoch: 1736/2000... Training loss: 0.2957\n",
      "Epoch: 1736/2000... Training loss: 0.5437\n",
      "Epoch: 1736/2000... Training loss: 0.4077\n",
      "Epoch: 1736/2000... Training loss: 0.3156\n",
      "Epoch: 1736/2000... Training loss: 0.4720\n",
      "Epoch: 1736/2000... Training loss: 0.3637\n",
      "Epoch: 1736/2000... Training loss: 0.3514\n",
      "Epoch: 1736/2000... Training loss: 0.4605\n",
      "Epoch: 1736/2000... Training loss: 0.2947\n",
      "Epoch: 1737/2000... Training loss: 0.3185\n",
      "Epoch: 1737/2000... Training loss: 0.2949\n",
      "Epoch: 1737/2000... Training loss: 0.3907\n",
      "Epoch: 1737/2000... Training loss: 0.4563\n",
      "Epoch: 1737/2000... Training loss: 0.3116\n",
      "Epoch: 1737/2000... Training loss: 0.2780\n",
      "Epoch: 1737/2000... Training loss: 0.4667\n",
      "Epoch: 1737/2000... Training loss: 0.3369\n",
      "Epoch: 1737/2000... Training loss: 0.3427\n",
      "Epoch: 1737/2000... Training loss: 0.2471\n",
      "Epoch: 1737/2000... Training loss: 0.4358\n",
      "Epoch: 1737/2000... Training loss: 0.3239\n",
      "Epoch: 1737/2000... Training loss: 0.3093\n",
      "Epoch: 1737/2000... Training loss: 0.3857\n",
      "Epoch: 1737/2000... Training loss: 0.3617\n",
      "Epoch: 1737/2000... Training loss: 0.3586\n",
      "Epoch: 1737/2000... Training loss: 0.3987\n",
      "Epoch: 1737/2000... Training loss: 0.4712\n",
      "Epoch: 1737/2000... Training loss: 0.5166\n",
      "Epoch: 1737/2000... Training loss: 0.3799\n",
      "Epoch: 1737/2000... Training loss: 0.4576\n",
      "Epoch: 1737/2000... Training loss: 0.3387\n",
      "Epoch: 1737/2000... Training loss: 0.4335\n",
      "Epoch: 1737/2000... Training loss: 0.4018\n",
      "Epoch: 1737/2000... Training loss: 0.3472\n",
      "Epoch: 1737/2000... Training loss: 0.2653\n",
      "Epoch: 1737/2000... Training loss: 0.3902\n",
      "Epoch: 1737/2000... Training loss: 0.4262\n",
      "Epoch: 1737/2000... Training loss: 0.4046\n",
      "Epoch: 1737/2000... Training loss: 0.4539\n",
      "Epoch: 1737/2000... Training loss: 0.4110\n",
      "Epoch: 1738/2000... Training loss: 0.3617\n",
      "Epoch: 1738/2000... Training loss: 0.4267\n",
      "Epoch: 1738/2000... Training loss: 0.3603\n",
      "Epoch: 1738/2000... Training loss: 0.4488\n",
      "Epoch: 1738/2000... Training loss: 0.4055\n",
      "Epoch: 1738/2000... Training loss: 0.4345\n",
      "Epoch: 1738/2000... Training loss: 0.3734\n",
      "Epoch: 1738/2000... Training loss: 0.4505\n",
      "Epoch: 1738/2000... Training loss: 0.3227\n",
      "Epoch: 1738/2000... Training loss: 0.3097\n",
      "Epoch: 1738/2000... Training loss: 0.3872\n",
      "Epoch: 1738/2000... Training loss: 0.4667\n",
      "Epoch: 1738/2000... Training loss: 0.2445\n",
      "Epoch: 1738/2000... Training loss: 0.3953\n",
      "Epoch: 1738/2000... Training loss: 0.4531\n",
      "Epoch: 1738/2000... Training loss: 0.4538\n",
      "Epoch: 1738/2000... Training loss: 0.4576\n",
      "Epoch: 1738/2000... Training loss: 0.3183\n",
      "Epoch: 1738/2000... Training loss: 0.5938\n",
      "Epoch: 1738/2000... Training loss: 0.3652\n",
      "Epoch: 1738/2000... Training loss: 0.4377\n",
      "Epoch: 1738/2000... Training loss: 0.3488\n",
      "Epoch: 1738/2000... Training loss: 0.3293\n",
      "Epoch: 1738/2000... Training loss: 0.4805\n",
      "Epoch: 1738/2000... Training loss: 0.3528\n",
      "Epoch: 1738/2000... Training loss: 0.3623\n",
      "Epoch: 1738/2000... Training loss: 0.3269\n",
      "Epoch: 1738/2000... Training loss: 0.2531\n",
      "Epoch: 1738/2000... Training loss: 0.2679\n",
      "Epoch: 1738/2000... Training loss: 0.3132\n",
      "Epoch: 1738/2000... Training loss: 0.3454\n",
      "Epoch: 1739/2000... Training loss: 0.3869\n",
      "Epoch: 1739/2000... Training loss: 0.4173\n",
      "Epoch: 1739/2000... Training loss: 0.3373\n",
      "Epoch: 1739/2000... Training loss: 0.3652\n",
      "Epoch: 1739/2000... Training loss: 0.4043\n",
      "Epoch: 1739/2000... Training loss: 0.2892\n",
      "Epoch: 1739/2000... Training loss: 0.3850\n",
      "Epoch: 1739/2000... Training loss: 0.6381\n",
      "Epoch: 1739/2000... Training loss: 0.4214\n",
      "Epoch: 1739/2000... Training loss: 0.4046\n",
      "Epoch: 1739/2000... Training loss: 0.2701\n",
      "Epoch: 1739/2000... Training loss: 0.4856\n",
      "Epoch: 1739/2000... Training loss: 0.4753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1739/2000... Training loss: 0.2967\n",
      "Epoch: 1739/2000... Training loss: 0.5043\n",
      "Epoch: 1739/2000... Training loss: 0.3198\n",
      "Epoch: 1739/2000... Training loss: 0.4335\n",
      "Epoch: 1739/2000... Training loss: 0.4603\n",
      "Epoch: 1739/2000... Training loss: 0.3589\n",
      "Epoch: 1739/2000... Training loss: 0.4577\n",
      "Epoch: 1739/2000... Training loss: 0.6272\n",
      "Epoch: 1739/2000... Training loss: 0.4348\n",
      "Epoch: 1739/2000... Training loss: 0.2748\n",
      "Epoch: 1739/2000... Training loss: 0.5327\n",
      "Epoch: 1739/2000... Training loss: 0.2767\n",
      "Epoch: 1739/2000... Training loss: 0.4633\n",
      "Epoch: 1739/2000... Training loss: 0.4662\n",
      "Epoch: 1739/2000... Training loss: 0.3009\n",
      "Epoch: 1739/2000... Training loss: 0.5724\n",
      "Epoch: 1739/2000... Training loss: 0.5268\n",
      "Epoch: 1739/2000... Training loss: 0.5258\n",
      "Epoch: 1740/2000... Training loss: 0.5253\n",
      "Epoch: 1740/2000... Training loss: 0.3546\n",
      "Epoch: 1740/2000... Training loss: 0.3858\n",
      "Epoch: 1740/2000... Training loss: 0.4305\n",
      "Epoch: 1740/2000... Training loss: 0.4228\n",
      "Epoch: 1740/2000... Training loss: 0.5134\n",
      "Epoch: 1740/2000... Training loss: 0.2140\n",
      "Epoch: 1740/2000... Training loss: 0.3272\n",
      "Epoch: 1740/2000... Training loss: 0.3409\n",
      "Epoch: 1740/2000... Training loss: 0.2483\n",
      "Epoch: 1740/2000... Training loss: 0.4440\n",
      "Epoch: 1740/2000... Training loss: 0.3009\n",
      "Epoch: 1740/2000... Training loss: 0.5460\n",
      "Epoch: 1740/2000... Training loss: 0.4628\n",
      "Epoch: 1740/2000... Training loss: 0.4045\n",
      "Epoch: 1740/2000... Training loss: 0.3177\n",
      "Epoch: 1740/2000... Training loss: 0.3372\n",
      "Epoch: 1740/2000... Training loss: 0.5064\n",
      "Epoch: 1740/2000... Training loss: 0.6128\n",
      "Epoch: 1740/2000... Training loss: 0.3989\n",
      "Epoch: 1740/2000... Training loss: 0.2732\n",
      "Epoch: 1740/2000... Training loss: 0.3317\n",
      "Epoch: 1740/2000... Training loss: 0.3307\n",
      "Epoch: 1740/2000... Training loss: 0.4337\n",
      "Epoch: 1740/2000... Training loss: 0.4374\n",
      "Epoch: 1740/2000... Training loss: 0.4840\n",
      "Epoch: 1740/2000... Training loss: 0.4122\n",
      "Epoch: 1740/2000... Training loss: 0.3354\n",
      "Epoch: 1740/2000... Training loss: 0.4083\n",
      "Epoch: 1740/2000... Training loss: 0.6293\n",
      "Epoch: 1740/2000... Training loss: 0.2791\n",
      "Epoch: 1741/2000... Training loss: 0.3081\n",
      "Epoch: 1741/2000... Training loss: 0.4947\n",
      "Epoch: 1741/2000... Training loss: 0.3244\n",
      "Epoch: 1741/2000... Training loss: 0.2653\n",
      "Epoch: 1741/2000... Training loss: 0.3111\n",
      "Epoch: 1741/2000... Training loss: 0.4440\n",
      "Epoch: 1741/2000... Training loss: 0.3897\n",
      "Epoch: 1741/2000... Training loss: 0.2240\n",
      "Epoch: 1741/2000... Training loss: 0.6180\n",
      "Epoch: 1741/2000... Training loss: 0.4051\n",
      "Epoch: 1741/2000... Training loss: 0.3326\n",
      "Epoch: 1741/2000... Training loss: 0.3771\n",
      "Epoch: 1741/2000... Training loss: 0.3255\n",
      "Epoch: 1741/2000... Training loss: 0.2328\n",
      "Epoch: 1741/2000... Training loss: 0.3739\n",
      "Epoch: 1741/2000... Training loss: 0.4935\n",
      "Epoch: 1741/2000... Training loss: 0.5598\n",
      "Epoch: 1741/2000... Training loss: 0.5560\n",
      "Epoch: 1741/2000... Training loss: 0.3793\n",
      "Epoch: 1741/2000... Training loss: 0.2812\n",
      "Epoch: 1741/2000... Training loss: 0.4034\n",
      "Epoch: 1741/2000... Training loss: 0.3153\n",
      "Epoch: 1741/2000... Training loss: 0.3148\n",
      "Epoch: 1741/2000... Training loss: 0.4854\n",
      "Epoch: 1741/2000... Training loss: 0.4979\n",
      "Epoch: 1741/2000... Training loss: 0.2985\n",
      "Epoch: 1741/2000... Training loss: 0.3269\n",
      "Epoch: 1741/2000... Training loss: 0.3733\n",
      "Epoch: 1741/2000... Training loss: 0.4829\n",
      "Epoch: 1741/2000... Training loss: 0.4318\n",
      "Epoch: 1741/2000... Training loss: 0.2217\n",
      "Epoch: 1742/2000... Training loss: 0.3480\n",
      "Epoch: 1742/2000... Training loss: 0.4482\n",
      "Epoch: 1742/2000... Training loss: 0.4816\n",
      "Epoch: 1742/2000... Training loss: 0.2215\n",
      "Epoch: 1742/2000... Training loss: 0.5289\n",
      "Epoch: 1742/2000... Training loss: 0.4636\n",
      "Epoch: 1742/2000... Training loss: 0.4009\n",
      "Epoch: 1742/2000... Training loss: 0.3832\n",
      "Epoch: 1742/2000... Training loss: 0.3926\n",
      "Epoch: 1742/2000... Training loss: 0.5236\n",
      "Epoch: 1742/2000... Training loss: 0.5515\n",
      "Epoch: 1742/2000... Training loss: 0.4308\n",
      "Epoch: 1742/2000... Training loss: 0.2563\n",
      "Epoch: 1742/2000... Training loss: 0.3497\n",
      "Epoch: 1742/2000... Training loss: 0.2619\n",
      "Epoch: 1742/2000... Training loss: 0.3203\n",
      "Epoch: 1742/2000... Training loss: 0.4222\n",
      "Epoch: 1742/2000... Training loss: 0.3593\n",
      "Epoch: 1742/2000... Training loss: 0.2690\n",
      "Epoch: 1742/2000... Training loss: 0.3713\n",
      "Epoch: 1742/2000... Training loss: 0.3076\n",
      "Epoch: 1742/2000... Training loss: 0.5776\n",
      "Epoch: 1742/2000... Training loss: 0.3689\n",
      "Epoch: 1742/2000... Training loss: 0.3269\n",
      "Epoch: 1742/2000... Training loss: 0.3902\n",
      "Epoch: 1742/2000... Training loss: 0.4451\n",
      "Epoch: 1742/2000... Training loss: 0.2287\n",
      "Epoch: 1742/2000... Training loss: 0.4078\n",
      "Epoch: 1742/2000... Training loss: 0.2714\n",
      "Epoch: 1742/2000... Training loss: 0.2500\n",
      "Epoch: 1742/2000... Training loss: 0.4804\n",
      "Epoch: 1743/2000... Training loss: 0.3898\n",
      "Epoch: 1743/2000... Training loss: 0.3617\n",
      "Epoch: 1743/2000... Training loss: 0.1666\n",
      "Epoch: 1743/2000... Training loss: 0.3003\n",
      "Epoch: 1743/2000... Training loss: 0.4033\n",
      "Epoch: 1743/2000... Training loss: 0.3776\n",
      "Epoch: 1743/2000... Training loss: 0.4675\n",
      "Epoch: 1743/2000... Training loss: 0.4526\n",
      "Epoch: 1743/2000... Training loss: 0.5170\n",
      "Epoch: 1743/2000... Training loss: 0.2991\n",
      "Epoch: 1743/2000... Training loss: 0.3318\n",
      "Epoch: 1743/2000... Training loss: 0.4571\n",
      "Epoch: 1743/2000... Training loss: 0.3001\n",
      "Epoch: 1743/2000... Training loss: 0.3821\n",
      "Epoch: 1743/2000... Training loss: 0.3866\n",
      "Epoch: 1743/2000... Training loss: 0.4222\n",
      "Epoch: 1743/2000... Training loss: 0.1780\n",
      "Epoch: 1743/2000... Training loss: 0.4310\n",
      "Epoch: 1743/2000... Training loss: 0.4293\n",
      "Epoch: 1743/2000... Training loss: 0.4647\n",
      "Epoch: 1743/2000... Training loss: 0.4087\n",
      "Epoch: 1743/2000... Training loss: 0.5267\n",
      "Epoch: 1743/2000... Training loss: 0.3093\n",
      "Epoch: 1743/2000... Training loss: 0.3011\n",
      "Epoch: 1743/2000... Training loss: 0.5113\n",
      "Epoch: 1743/2000... Training loss: 0.4412\n",
      "Epoch: 1743/2000... Training loss: 0.3058\n",
      "Epoch: 1743/2000... Training loss: 0.4094\n",
      "Epoch: 1743/2000... Training loss: 0.4646\n",
      "Epoch: 1743/2000... Training loss: 0.4249\n",
      "Epoch: 1743/2000... Training loss: 0.4038\n",
      "Epoch: 1744/2000... Training loss: 0.5531\n",
      "Epoch: 1744/2000... Training loss: 0.3504\n",
      "Epoch: 1744/2000... Training loss: 0.2810\n",
      "Epoch: 1744/2000... Training loss: 0.5126\n",
      "Epoch: 1744/2000... Training loss: 0.4403\n",
      "Epoch: 1744/2000... Training loss: 0.3180\n",
      "Epoch: 1744/2000... Training loss: 0.4569\n",
      "Epoch: 1744/2000... Training loss: 0.4172\n",
      "Epoch: 1744/2000... Training loss: 0.3565\n",
      "Epoch: 1744/2000... Training loss: 0.3643\n",
      "Epoch: 1744/2000... Training loss: 0.4302\n",
      "Epoch: 1744/2000... Training loss: 0.2442\n",
      "Epoch: 1744/2000... Training loss: 0.6378\n",
      "Epoch: 1744/2000... Training loss: 0.4127\n",
      "Epoch: 1744/2000... Training loss: 0.4815\n",
      "Epoch: 1744/2000... Training loss: 0.3248\n",
      "Epoch: 1744/2000... Training loss: 0.4236\n",
      "Epoch: 1744/2000... Training loss: 0.3680\n",
      "Epoch: 1744/2000... Training loss: 0.2950\n",
      "Epoch: 1744/2000... Training loss: 0.3559\n",
      "Epoch: 1744/2000... Training loss: 0.3984\n",
      "Epoch: 1744/2000... Training loss: 0.2814\n",
      "Epoch: 1744/2000... Training loss: 0.3187\n",
      "Epoch: 1744/2000... Training loss: 0.4859\n",
      "Epoch: 1744/2000... Training loss: 0.2610\n",
      "Epoch: 1744/2000... Training loss: 0.2789\n",
      "Epoch: 1744/2000... Training loss: 0.3473\n",
      "Epoch: 1744/2000... Training loss: 0.4279\n",
      "Epoch: 1744/2000... Training loss: 0.3903\n",
      "Epoch: 1744/2000... Training loss: 0.4258\n",
      "Epoch: 1744/2000... Training loss: 0.5392\n",
      "Epoch: 1745/2000... Training loss: 0.3056\n",
      "Epoch: 1745/2000... Training loss: 0.2078\n",
      "Epoch: 1745/2000... Training loss: 0.4082\n",
      "Epoch: 1745/2000... Training loss: 0.3765\n",
      "Epoch: 1745/2000... Training loss: 0.4089\n",
      "Epoch: 1745/2000... Training loss: 0.3612\n",
      "Epoch: 1745/2000... Training loss: 0.2833\n",
      "Epoch: 1745/2000... Training loss: 0.3334\n",
      "Epoch: 1745/2000... Training loss: 0.4219\n",
      "Epoch: 1745/2000... Training loss: 0.5670\n",
      "Epoch: 1745/2000... Training loss: 0.3771\n",
      "Epoch: 1745/2000... Training loss: 0.3823\n",
      "Epoch: 1745/2000... Training loss: 0.4575\n",
      "Epoch: 1745/2000... Training loss: 0.4805\n",
      "Epoch: 1745/2000... Training loss: 0.3866\n",
      "Epoch: 1745/2000... Training loss: 0.3532\n",
      "Epoch: 1745/2000... Training loss: 0.2464\n",
      "Epoch: 1745/2000... Training loss: 0.4863\n",
      "Epoch: 1745/2000... Training loss: 0.4865\n",
      "Epoch: 1745/2000... Training loss: 0.4674\n",
      "Epoch: 1745/2000... Training loss: 0.2460\n",
      "Epoch: 1745/2000... Training loss: 0.3868\n",
      "Epoch: 1745/2000... Training loss: 0.3771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1745/2000... Training loss: 0.4517\n",
      "Epoch: 1745/2000... Training loss: 0.3627\n",
      "Epoch: 1745/2000... Training loss: 0.5556\n",
      "Epoch: 1745/2000... Training loss: 0.3397\n",
      "Epoch: 1745/2000... Training loss: 0.3978\n",
      "Epoch: 1745/2000... Training loss: 0.4407\n",
      "Epoch: 1745/2000... Training loss: 0.4290\n",
      "Epoch: 1745/2000... Training loss: 0.4072\n",
      "Epoch: 1746/2000... Training loss: 0.4834\n",
      "Epoch: 1746/2000... Training loss: 0.3092\n",
      "Epoch: 1746/2000... Training loss: 0.3273\n",
      "Epoch: 1746/2000... Training loss: 0.3359\n",
      "Epoch: 1746/2000... Training loss: 0.3785\n",
      "Epoch: 1746/2000... Training loss: 0.2261\n",
      "Epoch: 1746/2000... Training loss: 0.3619\n",
      "Epoch: 1746/2000... Training loss: 0.3645\n",
      "Epoch: 1746/2000... Training loss: 0.3665\n",
      "Epoch: 1746/2000... Training loss: 0.3391\n",
      "Epoch: 1746/2000... Training loss: 0.4817\n",
      "Epoch: 1746/2000... Training loss: 0.4162\n",
      "Epoch: 1746/2000... Training loss: 0.3975\n",
      "Epoch: 1746/2000... Training loss: 0.3905\n",
      "Epoch: 1746/2000... Training loss: 0.4527\n",
      "Epoch: 1746/2000... Training loss: 0.2777\n",
      "Epoch: 1746/2000... Training loss: 0.4091\n",
      "Epoch: 1746/2000... Training loss: 0.2435\n",
      "Epoch: 1746/2000... Training loss: 0.3750\n",
      "Epoch: 1746/2000... Training loss: 0.4372\n",
      "Epoch: 1746/2000... Training loss: 0.1991\n",
      "Epoch: 1746/2000... Training loss: 0.3693\n",
      "Epoch: 1746/2000... Training loss: 0.4921\n",
      "Epoch: 1746/2000... Training loss: 0.4179\n",
      "Epoch: 1746/2000... Training loss: 0.3055\n",
      "Epoch: 1746/2000... Training loss: 0.3179\n",
      "Epoch: 1746/2000... Training loss: 0.5099\n",
      "Epoch: 1746/2000... Training loss: 0.3668\n",
      "Epoch: 1746/2000... Training loss: 0.3596\n",
      "Epoch: 1746/2000... Training loss: 0.3130\n",
      "Epoch: 1746/2000... Training loss: 0.5813\n",
      "Epoch: 1747/2000... Training loss: 0.4168\n",
      "Epoch: 1747/2000... Training loss: 0.3165\n",
      "Epoch: 1747/2000... Training loss: 0.3273\n",
      "Epoch: 1747/2000... Training loss: 0.4715\n",
      "Epoch: 1747/2000... Training loss: 0.4448\n",
      "Epoch: 1747/2000... Training loss: 0.2996\n",
      "Epoch: 1747/2000... Training loss: 0.5528\n",
      "Epoch: 1747/2000... Training loss: 0.3535\n",
      "Epoch: 1747/2000... Training loss: 0.2709\n",
      "Epoch: 1747/2000... Training loss: 0.4336\n",
      "Epoch: 1747/2000... Training loss: 0.4192\n",
      "Epoch: 1747/2000... Training loss: 0.2939\n",
      "Epoch: 1747/2000... Training loss: 0.2909\n",
      "Epoch: 1747/2000... Training loss: 0.3309\n",
      "Epoch: 1747/2000... Training loss: 0.2394\n",
      "Epoch: 1747/2000... Training loss: 0.6615\n",
      "Epoch: 1747/2000... Training loss: 0.4111\n",
      "Epoch: 1747/2000... Training loss: 0.4724\n",
      "Epoch: 1747/2000... Training loss: 0.3662\n",
      "Epoch: 1747/2000... Training loss: 0.3630\n",
      "Epoch: 1747/2000... Training loss: 0.2988\n",
      "Epoch: 1747/2000... Training loss: 0.3496\n",
      "Epoch: 1747/2000... Training loss: 0.2042\n",
      "Epoch: 1747/2000... Training loss: 0.3663\n",
      "Epoch: 1747/2000... Training loss: 0.4391\n",
      "Epoch: 1747/2000... Training loss: 0.2743\n",
      "Epoch: 1747/2000... Training loss: 0.5003\n",
      "Epoch: 1747/2000... Training loss: 0.3408\n",
      "Epoch: 1747/2000... Training loss: 0.6760\n",
      "Epoch: 1747/2000... Training loss: 0.4902\n",
      "Epoch: 1747/2000... Training loss: 0.4377\n",
      "Epoch: 1748/2000... Training loss: 0.2921\n",
      "Epoch: 1748/2000... Training loss: 0.2600\n",
      "Epoch: 1748/2000... Training loss: 0.4723\n",
      "Epoch: 1748/2000... Training loss: 0.3785\n",
      "Epoch: 1748/2000... Training loss: 0.4650\n",
      "Epoch: 1748/2000... Training loss: 0.6521\n",
      "Epoch: 1748/2000... Training loss: 0.2840\n",
      "Epoch: 1748/2000... Training loss: 0.3595\n",
      "Epoch: 1748/2000... Training loss: 0.3359\n",
      "Epoch: 1748/2000... Training loss: 0.3510\n",
      "Epoch: 1748/2000... Training loss: 0.4267\n",
      "Epoch: 1748/2000... Training loss: 0.4890\n",
      "Epoch: 1748/2000... Training loss: 0.3931\n",
      "Epoch: 1748/2000... Training loss: 0.3078\n",
      "Epoch: 1748/2000... Training loss: 0.4937\n",
      "Epoch: 1748/2000... Training loss: 0.3807\n",
      "Epoch: 1748/2000... Training loss: 0.2908\n",
      "Epoch: 1748/2000... Training loss: 0.4950\n",
      "Epoch: 1748/2000... Training loss: 0.3691\n",
      "Epoch: 1748/2000... Training loss: 0.2366\n",
      "Epoch: 1748/2000... Training loss: 0.2196\n",
      "Epoch: 1748/2000... Training loss: 0.3957\n",
      "Epoch: 1748/2000... Training loss: 0.3118\n",
      "Epoch: 1748/2000... Training loss: 0.4840\n",
      "Epoch: 1748/2000... Training loss: 0.4769\n",
      "Epoch: 1748/2000... Training loss: 0.4683\n",
      "Epoch: 1748/2000... Training loss: 0.3009\n",
      "Epoch: 1748/2000... Training loss: 0.3514\n",
      "Epoch: 1748/2000... Training loss: 0.3458\n",
      "Epoch: 1748/2000... Training loss: 0.3066\n",
      "Epoch: 1748/2000... Training loss: 0.2965\n",
      "Epoch: 1749/2000... Training loss: 0.4008\n",
      "Epoch: 1749/2000... Training loss: 0.3071\n",
      "Epoch: 1749/2000... Training loss: 0.1701\n",
      "Epoch: 1749/2000... Training loss: 0.2927\n",
      "Epoch: 1749/2000... Training loss: 0.3328\n",
      "Epoch: 1749/2000... Training loss: 0.4075\n",
      "Epoch: 1749/2000... Training loss: 0.3915\n",
      "Epoch: 1749/2000... Training loss: 0.3554\n",
      "Epoch: 1749/2000... Training loss: 0.3212\n",
      "Epoch: 1749/2000... Training loss: 0.2737\n",
      "Epoch: 1749/2000... Training loss: 0.4153\n",
      "Epoch: 1749/2000... Training loss: 0.2474\n",
      "Epoch: 1749/2000... Training loss: 0.5584\n",
      "Epoch: 1749/2000... Training loss: 0.3036\n",
      "Epoch: 1749/2000... Training loss: 0.5121\n",
      "Epoch: 1749/2000... Training loss: 0.4490\n",
      "Epoch: 1749/2000... Training loss: 0.2466\n",
      "Epoch: 1749/2000... Training loss: 0.3296\n",
      "Epoch: 1749/2000... Training loss: 0.3749\n",
      "Epoch: 1749/2000... Training loss: 0.3079\n",
      "Epoch: 1749/2000... Training loss: 0.2251\n",
      "Epoch: 1749/2000... Training loss: 0.4581\n",
      "Epoch: 1749/2000... Training loss: 0.2053\n",
      "Epoch: 1749/2000... Training loss: 0.3531\n",
      "Epoch: 1749/2000... Training loss: 0.2835\n",
      "Epoch: 1749/2000... Training loss: 0.4793\n",
      "Epoch: 1749/2000... Training loss: 0.2951\n",
      "Epoch: 1749/2000... Training loss: 0.4586\n",
      "Epoch: 1749/2000... Training loss: 0.5002\n",
      "Epoch: 1749/2000... Training loss: 0.4895\n",
      "Epoch: 1749/2000... Training loss: 0.4984\n",
      "Epoch: 1750/2000... Training loss: 0.4146\n",
      "Epoch: 1750/2000... Training loss: 0.2672\n",
      "Epoch: 1750/2000... Training loss: 0.3407\n",
      "Epoch: 1750/2000... Training loss: 0.4181\n",
      "Epoch: 1750/2000... Training loss: 0.3972\n",
      "Epoch: 1750/2000... Training loss: 0.2610\n",
      "Epoch: 1750/2000... Training loss: 0.3248\n",
      "Epoch: 1750/2000... Training loss: 0.2961\n",
      "Epoch: 1750/2000... Training loss: 0.3910\n",
      "Epoch: 1750/2000... Training loss: 0.3093\n",
      "Epoch: 1750/2000... Training loss: 0.4301\n",
      "Epoch: 1750/2000... Training loss: 0.2426\n",
      "Epoch: 1750/2000... Training loss: 0.3068\n",
      "Epoch: 1750/2000... Training loss: 0.3209\n",
      "Epoch: 1750/2000... Training loss: 0.3315\n",
      "Epoch: 1750/2000... Training loss: 0.3607\n",
      "Epoch: 1750/2000... Training loss: 0.4689\n",
      "Epoch: 1750/2000... Training loss: 0.3054\n",
      "Epoch: 1750/2000... Training loss: 0.1939\n",
      "Epoch: 1750/2000... Training loss: 0.2776\n",
      "Epoch: 1750/2000... Training loss: 0.4034\n",
      "Epoch: 1750/2000... Training loss: 0.4342\n",
      "Epoch: 1750/2000... Training loss: 0.3454\n",
      "Epoch: 1750/2000... Training loss: 0.3379\n",
      "Epoch: 1750/2000... Training loss: 0.3476\n",
      "Epoch: 1750/2000... Training loss: 0.3919\n",
      "Epoch: 1750/2000... Training loss: 0.3892\n",
      "Epoch: 1750/2000... Training loss: 0.4335\n",
      "Epoch: 1750/2000... Training loss: 0.3634\n",
      "Epoch: 1750/2000... Training loss: 0.2795\n",
      "Epoch: 1750/2000... Training loss: 0.4464\n",
      "Epoch: 1751/2000... Training loss: 0.5653\n",
      "Epoch: 1751/2000... Training loss: 0.3859\n",
      "Epoch: 1751/2000... Training loss: 0.4700\n",
      "Epoch: 1751/2000... Training loss: 0.4419\n",
      "Epoch: 1751/2000... Training loss: 0.3822\n",
      "Epoch: 1751/2000... Training loss: 0.5025\n",
      "Epoch: 1751/2000... Training loss: 0.4625\n",
      "Epoch: 1751/2000... Training loss: 0.4561\n",
      "Epoch: 1751/2000... Training loss: 0.5976\n",
      "Epoch: 1751/2000... Training loss: 0.3537\n",
      "Epoch: 1751/2000... Training loss: 0.4298\n",
      "Epoch: 1751/2000... Training loss: 0.3244\n",
      "Epoch: 1751/2000... Training loss: 0.4009\n",
      "Epoch: 1751/2000... Training loss: 0.4729\n",
      "Epoch: 1751/2000... Training loss: 0.5350\n",
      "Epoch: 1751/2000... Training loss: 0.3344\n",
      "Epoch: 1751/2000... Training loss: 0.4691\n",
      "Epoch: 1751/2000... Training loss: 0.4740\n",
      "Epoch: 1751/2000... Training loss: 0.5676\n",
      "Epoch: 1751/2000... Training loss: 0.4520\n",
      "Epoch: 1751/2000... Training loss: 0.4926\n",
      "Epoch: 1751/2000... Training loss: 0.4294\n",
      "Epoch: 1751/2000... Training loss: 0.3438\n",
      "Epoch: 1751/2000... Training loss: 0.4729\n",
      "Epoch: 1751/2000... Training loss: 0.3120\n",
      "Epoch: 1751/2000... Training loss: 0.6172\n",
      "Epoch: 1751/2000... Training loss: 0.4138\n",
      "Epoch: 1751/2000... Training loss: 0.2846\n",
      "Epoch: 1751/2000... Training loss: 0.4370\n",
      "Epoch: 1751/2000... Training loss: 0.2827\n",
      "Epoch: 1751/2000... Training loss: 0.4498\n",
      "Epoch: 1752/2000... Training loss: 0.4081\n",
      "Epoch: 1752/2000... Training loss: 0.5474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1752/2000... Training loss: 0.3524\n",
      "Epoch: 1752/2000... Training loss: 0.3314\n",
      "Epoch: 1752/2000... Training loss: 0.4234\n",
      "Epoch: 1752/2000... Training loss: 0.1650\n",
      "Epoch: 1752/2000... Training loss: 0.4209\n",
      "Epoch: 1752/2000... Training loss: 0.4851\n",
      "Epoch: 1752/2000... Training loss: 0.3370\n",
      "Epoch: 1752/2000... Training loss: 0.3520\n",
      "Epoch: 1752/2000... Training loss: 0.3780\n",
      "Epoch: 1752/2000... Training loss: 0.3046\n",
      "Epoch: 1752/2000... Training loss: 0.3698\n",
      "Epoch: 1752/2000... Training loss: 0.3954\n",
      "Epoch: 1752/2000... Training loss: 0.6090\n",
      "Epoch: 1752/2000... Training loss: 0.2780\n",
      "Epoch: 1752/2000... Training loss: 0.4014\n",
      "Epoch: 1752/2000... Training loss: 0.4014\n",
      "Epoch: 1752/2000... Training loss: 0.4049\n",
      "Epoch: 1752/2000... Training loss: 0.3908\n",
      "Epoch: 1752/2000... Training loss: 0.4272\n",
      "Epoch: 1752/2000... Training loss: 0.3807\n",
      "Epoch: 1752/2000... Training loss: 0.5540\n",
      "Epoch: 1752/2000... Training loss: 0.4453\n",
      "Epoch: 1752/2000... Training loss: 0.3145\n",
      "Epoch: 1752/2000... Training loss: 0.3392\n",
      "Epoch: 1752/2000... Training loss: 0.3275\n",
      "Epoch: 1752/2000... Training loss: 0.2793\n",
      "Epoch: 1752/2000... Training loss: 0.4022\n",
      "Epoch: 1752/2000... Training loss: 0.3718\n",
      "Epoch: 1752/2000... Training loss: 0.2363\n",
      "Epoch: 1753/2000... Training loss: 0.4920\n",
      "Epoch: 1753/2000... Training loss: 0.4589\n",
      "Epoch: 1753/2000... Training loss: 0.5054\n",
      "Epoch: 1753/2000... Training loss: 0.2774\n",
      "Epoch: 1753/2000... Training loss: 0.5115\n",
      "Epoch: 1753/2000... Training loss: 0.3723\n",
      "Epoch: 1753/2000... Training loss: 0.7187\n",
      "Epoch: 1753/2000... Training loss: 0.2881\n",
      "Epoch: 1753/2000... Training loss: 0.3057\n",
      "Epoch: 1753/2000... Training loss: 0.4360\n",
      "Epoch: 1753/2000... Training loss: 0.3834\n",
      "Epoch: 1753/2000... Training loss: 0.3492\n",
      "Epoch: 1753/2000... Training loss: 0.4071\n",
      "Epoch: 1753/2000... Training loss: 0.3219\n",
      "Epoch: 1753/2000... Training loss: 0.3662\n",
      "Epoch: 1753/2000... Training loss: 0.3957\n",
      "Epoch: 1753/2000... Training loss: 0.4049\n",
      "Epoch: 1753/2000... Training loss: 0.4636\n",
      "Epoch: 1753/2000... Training loss: 0.3948\n",
      "Epoch: 1753/2000... Training loss: 0.4197\n",
      "Epoch: 1753/2000... Training loss: 0.4492\n",
      "Epoch: 1753/2000... Training loss: 0.4767\n",
      "Epoch: 1753/2000... Training loss: 0.3280\n",
      "Epoch: 1753/2000... Training loss: 0.3638\n",
      "Epoch: 1753/2000... Training loss: 0.4980\n",
      "Epoch: 1753/2000... Training loss: 0.4499\n",
      "Epoch: 1753/2000... Training loss: 0.3591\n",
      "Epoch: 1753/2000... Training loss: 0.4927\n",
      "Epoch: 1753/2000... Training loss: 0.5216\n",
      "Epoch: 1753/2000... Training loss: 0.3152\n",
      "Epoch: 1753/2000... Training loss: 0.4727\n",
      "Epoch: 1754/2000... Training loss: 0.4149\n",
      "Epoch: 1754/2000... Training loss: 0.2673\n",
      "Epoch: 1754/2000... Training loss: 0.4255\n",
      "Epoch: 1754/2000... Training loss: 0.4913\n",
      "Epoch: 1754/2000... Training loss: 0.4051\n",
      "Epoch: 1754/2000... Training loss: 0.3906\n",
      "Epoch: 1754/2000... Training loss: 0.2864\n",
      "Epoch: 1754/2000... Training loss: 0.2089\n",
      "Epoch: 1754/2000... Training loss: 0.4210\n",
      "Epoch: 1754/2000... Training loss: 0.3721\n",
      "Epoch: 1754/2000... Training loss: 0.2576\n",
      "Epoch: 1754/2000... Training loss: 0.3679\n",
      "Epoch: 1754/2000... Training loss: 0.4196\n",
      "Epoch: 1754/2000... Training loss: 0.2803\n",
      "Epoch: 1754/2000... Training loss: 0.3132\n",
      "Epoch: 1754/2000... Training loss: 0.3141\n",
      "Epoch: 1754/2000... Training loss: 0.4769\n",
      "Epoch: 1754/2000... Training loss: 0.3242\n",
      "Epoch: 1754/2000... Training loss: 0.2508\n",
      "Epoch: 1754/2000... Training loss: 0.3846\n",
      "Epoch: 1754/2000... Training loss: 0.3522\n",
      "Epoch: 1754/2000... Training loss: 0.2905\n",
      "Epoch: 1754/2000... Training loss: 0.5774\n",
      "Epoch: 1754/2000... Training loss: 0.4185\n",
      "Epoch: 1754/2000... Training loss: 0.4577\n",
      "Epoch: 1754/2000... Training loss: 0.3108\n",
      "Epoch: 1754/2000... Training loss: 0.3807\n",
      "Epoch: 1754/2000... Training loss: 0.5059\n",
      "Epoch: 1754/2000... Training loss: 0.2991\n",
      "Epoch: 1754/2000... Training loss: 0.3831\n",
      "Epoch: 1754/2000... Training loss: 0.4939\n",
      "Epoch: 1755/2000... Training loss: 0.2739\n",
      "Epoch: 1755/2000... Training loss: 0.4498\n",
      "Epoch: 1755/2000... Training loss: 0.3103\n",
      "Epoch: 1755/2000... Training loss: 0.4345\n",
      "Epoch: 1755/2000... Training loss: 0.4480\n",
      "Epoch: 1755/2000... Training loss: 0.2698\n",
      "Epoch: 1755/2000... Training loss: 0.3990\n",
      "Epoch: 1755/2000... Training loss: 0.3574\n",
      "Epoch: 1755/2000... Training loss: 0.4543\n",
      "Epoch: 1755/2000... Training loss: 0.2878\n",
      "Epoch: 1755/2000... Training loss: 0.3562\n",
      "Epoch: 1755/2000... Training loss: 0.3238\n",
      "Epoch: 1755/2000... Training loss: 0.4080\n",
      "Epoch: 1755/2000... Training loss: 0.4048\n",
      "Epoch: 1755/2000... Training loss: 0.2471\n",
      "Epoch: 1755/2000... Training loss: 0.4330\n",
      "Epoch: 1755/2000... Training loss: 0.5351\n",
      "Epoch: 1755/2000... Training loss: 0.3430\n",
      "Epoch: 1755/2000... Training loss: 0.3906\n",
      "Epoch: 1755/2000... Training loss: 0.4168\n",
      "Epoch: 1755/2000... Training loss: 0.3747\n",
      "Epoch: 1755/2000... Training loss: 0.4189\n",
      "Epoch: 1755/2000... Training loss: 0.4614\n",
      "Epoch: 1755/2000... Training loss: 0.2098\n",
      "Epoch: 1755/2000... Training loss: 0.3925\n",
      "Epoch: 1755/2000... Training loss: 0.5065\n",
      "Epoch: 1755/2000... Training loss: 0.5995\n",
      "Epoch: 1755/2000... Training loss: 0.4349\n",
      "Epoch: 1755/2000... Training loss: 0.2619\n",
      "Epoch: 1755/2000... Training loss: 0.3814\n",
      "Epoch: 1755/2000... Training loss: 0.5356\n",
      "Epoch: 1756/2000... Training loss: 0.3871\n",
      "Epoch: 1756/2000... Training loss: 0.4086\n",
      "Epoch: 1756/2000... Training loss: 0.6138\n",
      "Epoch: 1756/2000... Training loss: 0.2995\n",
      "Epoch: 1756/2000... Training loss: 0.3110\n",
      "Epoch: 1756/2000... Training loss: 0.4660\n",
      "Epoch: 1756/2000... Training loss: 0.4447\n",
      "Epoch: 1756/2000... Training loss: 0.3660\n",
      "Epoch: 1756/2000... Training loss: 0.2679\n",
      "Epoch: 1756/2000... Training loss: 0.4575\n",
      "Epoch: 1756/2000... Training loss: 0.3437\n",
      "Epoch: 1756/2000... Training loss: 0.3457\n",
      "Epoch: 1756/2000... Training loss: 0.4148\n",
      "Epoch: 1756/2000... Training loss: 0.4447\n",
      "Epoch: 1756/2000... Training loss: 0.4107\n",
      "Epoch: 1756/2000... Training loss: 0.2889\n",
      "Epoch: 1756/2000... Training loss: 0.4379\n",
      "Epoch: 1756/2000... Training loss: 0.3303\n",
      "Epoch: 1756/2000... Training loss: 0.4239\n",
      "Epoch: 1756/2000... Training loss: 0.2527\n",
      "Epoch: 1756/2000... Training loss: 0.3358\n",
      "Epoch: 1756/2000... Training loss: 0.2696\n",
      "Epoch: 1756/2000... Training loss: 0.4065\n",
      "Epoch: 1756/2000... Training loss: 0.3862\n",
      "Epoch: 1756/2000... Training loss: 0.4363\n",
      "Epoch: 1756/2000... Training loss: 0.2946\n",
      "Epoch: 1756/2000... Training loss: 0.2141\n",
      "Epoch: 1756/2000... Training loss: 0.3574\n",
      "Epoch: 1756/2000... Training loss: 0.4078\n",
      "Epoch: 1756/2000... Training loss: 0.3430\n",
      "Epoch: 1756/2000... Training loss: 0.3792\n",
      "Epoch: 1757/2000... Training loss: 0.3825\n",
      "Epoch: 1757/2000... Training loss: 0.4855\n",
      "Epoch: 1757/2000... Training loss: 0.4690\n",
      "Epoch: 1757/2000... Training loss: 0.3574\n",
      "Epoch: 1757/2000... Training loss: 0.4526\n",
      "Epoch: 1757/2000... Training loss: 0.2306\n",
      "Epoch: 1757/2000... Training loss: 0.3507\n",
      "Epoch: 1757/2000... Training loss: 0.4892\n",
      "Epoch: 1757/2000... Training loss: 0.3382\n",
      "Epoch: 1757/2000... Training loss: 0.3339\n",
      "Epoch: 1757/2000... Training loss: 0.4666\n",
      "Epoch: 1757/2000... Training loss: 0.4070\n",
      "Epoch: 1757/2000... Training loss: 0.3141\n",
      "Epoch: 1757/2000... Training loss: 0.3424\n",
      "Epoch: 1757/2000... Training loss: 0.3774\n",
      "Epoch: 1757/2000... Training loss: 0.4358\n",
      "Epoch: 1757/2000... Training loss: 0.4464\n",
      "Epoch: 1757/2000... Training loss: 0.3462\n",
      "Epoch: 1757/2000... Training loss: 0.3042\n",
      "Epoch: 1757/2000... Training loss: 0.4565\n",
      "Epoch: 1757/2000... Training loss: 0.2440\n",
      "Epoch: 1757/2000... Training loss: 0.2809\n",
      "Epoch: 1757/2000... Training loss: 0.4868\n",
      "Epoch: 1757/2000... Training loss: 0.4239\n",
      "Epoch: 1757/2000... Training loss: 0.4316\n",
      "Epoch: 1757/2000... Training loss: 0.3456\n",
      "Epoch: 1757/2000... Training loss: 0.1734\n",
      "Epoch: 1757/2000... Training loss: 0.3798\n",
      "Epoch: 1757/2000... Training loss: 0.4658\n",
      "Epoch: 1757/2000... Training loss: 0.3783\n",
      "Epoch: 1757/2000... Training loss: 0.4128\n",
      "Epoch: 1758/2000... Training loss: 0.4103\n",
      "Epoch: 1758/2000... Training loss: 0.2629\n",
      "Epoch: 1758/2000... Training loss: 0.4490\n",
      "Epoch: 1758/2000... Training loss: 0.3964\n",
      "Epoch: 1758/2000... Training loss: 0.3687\n",
      "Epoch: 1758/2000... Training loss: 0.3986\n",
      "Epoch: 1758/2000... Training loss: 0.4444\n",
      "Epoch: 1758/2000... Training loss: 0.2455\n",
      "Epoch: 1758/2000... Training loss: 0.3486\n",
      "Epoch: 1758/2000... Training loss: 0.2758\n",
      "Epoch: 1758/2000... Training loss: 0.3554\n",
      "Epoch: 1758/2000... Training loss: 0.2463\n",
      "Epoch: 1758/2000... Training loss: 0.5938\n",
      "Epoch: 1758/2000... Training loss: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1758/2000... Training loss: 0.3821\n",
      "Epoch: 1758/2000... Training loss: 0.3164\n",
      "Epoch: 1758/2000... Training loss: 0.3430\n",
      "Epoch: 1758/2000... Training loss: 0.4683\n",
      "Epoch: 1758/2000... Training loss: 0.2020\n",
      "Epoch: 1758/2000... Training loss: 0.3535\n",
      "Epoch: 1758/2000... Training loss: 0.3852\n",
      "Epoch: 1758/2000... Training loss: 0.4253\n",
      "Epoch: 1758/2000... Training loss: 0.3317\n",
      "Epoch: 1758/2000... Training loss: 0.5655\n",
      "Epoch: 1758/2000... Training loss: 0.5156\n",
      "Epoch: 1758/2000... Training loss: 0.3312\n",
      "Epoch: 1758/2000... Training loss: 0.3787\n",
      "Epoch: 1758/2000... Training loss: 0.5699\n",
      "Epoch: 1758/2000... Training loss: 0.3918\n",
      "Epoch: 1758/2000... Training loss: 0.3570\n",
      "Epoch: 1758/2000... Training loss: 0.5451\n",
      "Epoch: 1759/2000... Training loss: 0.2554\n",
      "Epoch: 1759/2000... Training loss: 0.3124\n",
      "Epoch: 1759/2000... Training loss: 0.4507\n",
      "Epoch: 1759/2000... Training loss: 0.3930\n",
      "Epoch: 1759/2000... Training loss: 0.7093\n",
      "Epoch: 1759/2000... Training loss: 0.3123\n",
      "Epoch: 1759/2000... Training loss: 0.2790\n",
      "Epoch: 1759/2000... Training loss: 0.3644\n",
      "Epoch: 1759/2000... Training loss: 0.2853\n",
      "Epoch: 1759/2000... Training loss: 0.2140\n",
      "Epoch: 1759/2000... Training loss: 0.4036\n",
      "Epoch: 1759/2000... Training loss: 0.4672\n",
      "Epoch: 1759/2000... Training loss: 0.3458\n",
      "Epoch: 1759/2000... Training loss: 0.4596\n",
      "Epoch: 1759/2000... Training loss: 0.4164\n",
      "Epoch: 1759/2000... Training loss: 0.3915\n",
      "Epoch: 1759/2000... Training loss: 0.4385\n",
      "Epoch: 1759/2000... Training loss: 0.3472\n",
      "Epoch: 1759/2000... Training loss: 0.5401\n",
      "Epoch: 1759/2000... Training loss: 0.3777\n",
      "Epoch: 1759/2000... Training loss: 0.2782\n",
      "Epoch: 1759/2000... Training loss: 0.4423\n",
      "Epoch: 1759/2000... Training loss: 0.3943\n",
      "Epoch: 1759/2000... Training loss: 0.3680\n",
      "Epoch: 1759/2000... Training loss: 0.5492\n",
      "Epoch: 1759/2000... Training loss: 0.4482\n",
      "Epoch: 1759/2000... Training loss: 0.3994\n",
      "Epoch: 1759/2000... Training loss: 0.3274\n",
      "Epoch: 1759/2000... Training loss: 0.2873\n",
      "Epoch: 1759/2000... Training loss: 0.3967\n",
      "Epoch: 1759/2000... Training loss: 0.4259\n",
      "Epoch: 1760/2000... Training loss: 0.3615\n",
      "Epoch: 1760/2000... Training loss: 0.4416\n",
      "Epoch: 1760/2000... Training loss: 0.3638\n",
      "Epoch: 1760/2000... Training loss: 0.5320\n",
      "Epoch: 1760/2000... Training loss: 0.4156\n",
      "Epoch: 1760/2000... Training loss: 0.3570\n",
      "Epoch: 1760/2000... Training loss: 0.2016\n",
      "Epoch: 1760/2000... Training loss: 0.3515\n",
      "Epoch: 1760/2000... Training loss: 0.3935\n",
      "Epoch: 1760/2000... Training loss: 0.3361\n",
      "Epoch: 1760/2000... Training loss: 0.4658\n",
      "Epoch: 1760/2000... Training loss: 0.6606\n",
      "Epoch: 1760/2000... Training loss: 0.6718\n",
      "Epoch: 1760/2000... Training loss: 0.3143\n",
      "Epoch: 1760/2000... Training loss: 0.2824\n",
      "Epoch: 1760/2000... Training loss: 0.2042\n",
      "Epoch: 1760/2000... Training loss: 0.2615\n",
      "Epoch: 1760/2000... Training loss: 0.3649\n",
      "Epoch: 1760/2000... Training loss: 0.4397\n",
      "Epoch: 1760/2000... Training loss: 0.4221\n",
      "Epoch: 1760/2000... Training loss: 0.3398\n",
      "Epoch: 1760/2000... Training loss: 0.4832\n",
      "Epoch: 1760/2000... Training loss: 0.3030\n",
      "Epoch: 1760/2000... Training loss: 0.4739\n",
      "Epoch: 1760/2000... Training loss: 0.4274\n",
      "Epoch: 1760/2000... Training loss: 0.5602\n",
      "Epoch: 1760/2000... Training loss: 0.2530\n",
      "Epoch: 1760/2000... Training loss: 0.3425\n",
      "Epoch: 1760/2000... Training loss: 0.4065\n",
      "Epoch: 1760/2000... Training loss: 0.3600\n",
      "Epoch: 1760/2000... Training loss: 0.3879\n",
      "Epoch: 1761/2000... Training loss: 0.2590\n",
      "Epoch: 1761/2000... Training loss: 0.3838\n",
      "Epoch: 1761/2000... Training loss: 0.3897\n",
      "Epoch: 1761/2000... Training loss: 0.5085\n",
      "Epoch: 1761/2000... Training loss: 0.4490\n",
      "Epoch: 1761/2000... Training loss: 0.4037\n",
      "Epoch: 1761/2000... Training loss: 0.3879\n",
      "Epoch: 1761/2000... Training loss: 0.3870\n",
      "Epoch: 1761/2000... Training loss: 0.3134\n",
      "Epoch: 1761/2000... Training loss: 0.4483\n",
      "Epoch: 1761/2000... Training loss: 0.3051\n",
      "Epoch: 1761/2000... Training loss: 0.3495\n",
      "Epoch: 1761/2000... Training loss: 0.3518\n",
      "Epoch: 1761/2000... Training loss: 0.4787\n",
      "Epoch: 1761/2000... Training loss: 0.5446\n",
      "Epoch: 1761/2000... Training loss: 0.6593\n",
      "Epoch: 1761/2000... Training loss: 0.4448\n",
      "Epoch: 1761/2000... Training loss: 0.3588\n",
      "Epoch: 1761/2000... Training loss: 0.2591\n",
      "Epoch: 1761/2000... Training loss: 0.3508\n",
      "Epoch: 1761/2000... Training loss: 0.3382\n",
      "Epoch: 1761/2000... Training loss: 0.5399\n",
      "Epoch: 1761/2000... Training loss: 0.4291\n",
      "Epoch: 1761/2000... Training loss: 0.5952\n",
      "Epoch: 1761/2000... Training loss: 0.3652\n",
      "Epoch: 1761/2000... Training loss: 0.3592\n",
      "Epoch: 1761/2000... Training loss: 0.2953\n",
      "Epoch: 1761/2000... Training loss: 0.4839\n",
      "Epoch: 1761/2000... Training loss: 0.3314\n",
      "Epoch: 1761/2000... Training loss: 0.3607\n",
      "Epoch: 1761/2000... Training loss: 0.3414\n",
      "Epoch: 1762/2000... Training loss: 0.2431\n",
      "Epoch: 1762/2000... Training loss: 0.3499\n",
      "Epoch: 1762/2000... Training loss: 0.2303\n",
      "Epoch: 1762/2000... Training loss: 0.2931\n",
      "Epoch: 1762/2000... Training loss: 0.3538\n",
      "Epoch: 1762/2000... Training loss: 0.3700\n",
      "Epoch: 1762/2000... Training loss: 0.2731\n",
      "Epoch: 1762/2000... Training loss: 0.3111\n",
      "Epoch: 1762/2000... Training loss: 0.4687\n",
      "Epoch: 1762/2000... Training loss: 0.3755\n",
      "Epoch: 1762/2000... Training loss: 0.4523\n",
      "Epoch: 1762/2000... Training loss: 0.4542\n",
      "Epoch: 1762/2000... Training loss: 0.4075\n",
      "Epoch: 1762/2000... Training loss: 0.3653\n",
      "Epoch: 1762/2000... Training loss: 0.4692\n",
      "Epoch: 1762/2000... Training loss: 0.3515\n",
      "Epoch: 1762/2000... Training loss: 0.4009\n",
      "Epoch: 1762/2000... Training loss: 0.3618\n",
      "Epoch: 1762/2000... Training loss: 0.5248\n",
      "Epoch: 1762/2000... Training loss: 0.3880\n",
      "Epoch: 1762/2000... Training loss: 0.2680\n",
      "Epoch: 1762/2000... Training loss: 0.3825\n",
      "Epoch: 1762/2000... Training loss: 0.3944\n",
      "Epoch: 1762/2000... Training loss: 0.3080\n",
      "Epoch: 1762/2000... Training loss: 0.4880\n",
      "Epoch: 1762/2000... Training loss: 0.4840\n",
      "Epoch: 1762/2000... Training loss: 0.3801\n",
      "Epoch: 1762/2000... Training loss: 0.4456\n",
      "Epoch: 1762/2000... Training loss: 0.4052\n",
      "Epoch: 1762/2000... Training loss: 0.4588\n",
      "Epoch: 1762/2000... Training loss: 0.2735\n",
      "Epoch: 1763/2000... Training loss: 0.1908\n",
      "Epoch: 1763/2000... Training loss: 0.2922\n",
      "Epoch: 1763/2000... Training loss: 0.4106\n",
      "Epoch: 1763/2000... Training loss: 0.5023\n",
      "Epoch: 1763/2000... Training loss: 0.3151\n",
      "Epoch: 1763/2000... Training loss: 0.2614\n",
      "Epoch: 1763/2000... Training loss: 0.4985\n",
      "Epoch: 1763/2000... Training loss: 0.2526\n",
      "Epoch: 1763/2000... Training loss: 0.3958\n",
      "Epoch: 1763/2000... Training loss: 0.2584\n",
      "Epoch: 1763/2000... Training loss: 0.3995\n",
      "Epoch: 1763/2000... Training loss: 0.3698\n",
      "Epoch: 1763/2000... Training loss: 0.2579\n",
      "Epoch: 1763/2000... Training loss: 0.3964\n",
      "Epoch: 1763/2000... Training loss: 0.3427\n",
      "Epoch: 1763/2000... Training loss: 0.4464\n",
      "Epoch: 1763/2000... Training loss: 0.3935\n",
      "Epoch: 1763/2000... Training loss: 0.3597\n",
      "Epoch: 1763/2000... Training loss: 0.4235\n",
      "Epoch: 1763/2000... Training loss: 0.4820\n",
      "Epoch: 1763/2000... Training loss: 0.4809\n",
      "Epoch: 1763/2000... Training loss: 0.4058\n",
      "Epoch: 1763/2000... Training loss: 0.4083\n",
      "Epoch: 1763/2000... Training loss: 0.4885\n",
      "Epoch: 1763/2000... Training loss: 0.4328\n",
      "Epoch: 1763/2000... Training loss: 0.3812\n",
      "Epoch: 1763/2000... Training loss: 0.4695\n",
      "Epoch: 1763/2000... Training loss: 0.2371\n",
      "Epoch: 1763/2000... Training loss: 0.3076\n",
      "Epoch: 1763/2000... Training loss: 0.4543\n",
      "Epoch: 1763/2000... Training loss: 0.3564\n",
      "Epoch: 1764/2000... Training loss: 0.3994\n",
      "Epoch: 1764/2000... Training loss: 0.3781\n",
      "Epoch: 1764/2000... Training loss: 0.3746\n",
      "Epoch: 1764/2000... Training loss: 0.5039\n",
      "Epoch: 1764/2000... Training loss: 0.2937\n",
      "Epoch: 1764/2000... Training loss: 0.4530\n",
      "Epoch: 1764/2000... Training loss: 0.4698\n",
      "Epoch: 1764/2000... Training loss: 0.3995\n",
      "Epoch: 1764/2000... Training loss: 0.4245\n",
      "Epoch: 1764/2000... Training loss: 0.4592\n",
      "Epoch: 1764/2000... Training loss: 0.4446\n",
      "Epoch: 1764/2000... Training loss: 0.4595\n",
      "Epoch: 1764/2000... Training loss: 0.1756\n",
      "Epoch: 1764/2000... Training loss: 0.4083\n",
      "Epoch: 1764/2000... Training loss: 0.3191\n",
      "Epoch: 1764/2000... Training loss: 0.2131\n",
      "Epoch: 1764/2000... Training loss: 0.3626\n",
      "Epoch: 1764/2000... Training loss: 0.5509\n",
      "Epoch: 1764/2000... Training loss: 0.3851\n",
      "Epoch: 1764/2000... Training loss: 0.3479\n",
      "Epoch: 1764/2000... Training loss: 0.4034\n",
      "Epoch: 1764/2000... Training loss: 0.3395\n",
      "Epoch: 1764/2000... Training loss: 0.3440\n",
      "Epoch: 1764/2000... Training loss: 0.3669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1764/2000... Training loss: 0.3771\n",
      "Epoch: 1764/2000... Training loss: 0.2693\n",
      "Epoch: 1764/2000... Training loss: 0.5215\n",
      "Epoch: 1764/2000... Training loss: 0.3589\n",
      "Epoch: 1764/2000... Training loss: 0.3826\n",
      "Epoch: 1764/2000... Training loss: 0.3887\n",
      "Epoch: 1764/2000... Training loss: 0.4496\n",
      "Epoch: 1765/2000... Training loss: 0.3183\n",
      "Epoch: 1765/2000... Training loss: 0.2978\n",
      "Epoch: 1765/2000... Training loss: 0.3886\n",
      "Epoch: 1765/2000... Training loss: 0.2578\n",
      "Epoch: 1765/2000... Training loss: 0.6210\n",
      "Epoch: 1765/2000... Training loss: 0.4657\n",
      "Epoch: 1765/2000... Training loss: 0.2615\n",
      "Epoch: 1765/2000... Training loss: 0.3684\n",
      "Epoch: 1765/2000... Training loss: 0.3585\n",
      "Epoch: 1765/2000... Training loss: 0.3855\n",
      "Epoch: 1765/2000... Training loss: 0.4233\n",
      "Epoch: 1765/2000... Training loss: 0.3632\n",
      "Epoch: 1765/2000... Training loss: 0.3524\n",
      "Epoch: 1765/2000... Training loss: 0.4800\n",
      "Epoch: 1765/2000... Training loss: 0.3499\n",
      "Epoch: 1765/2000... Training loss: 0.3008\n",
      "Epoch: 1765/2000... Training loss: 0.3543\n",
      "Epoch: 1765/2000... Training loss: 0.3904\n",
      "Epoch: 1765/2000... Training loss: 0.3918\n",
      "Epoch: 1765/2000... Training loss: 0.2433\n",
      "Epoch: 1765/2000... Training loss: 0.4853\n",
      "Epoch: 1765/2000... Training loss: 0.3654\n",
      "Epoch: 1765/2000... Training loss: 0.4004\n",
      "Epoch: 1765/2000... Training loss: 0.4109\n",
      "Epoch: 1765/2000... Training loss: 0.2019\n",
      "Epoch: 1765/2000... Training loss: 0.4008\n",
      "Epoch: 1765/2000... Training loss: 0.3431\n",
      "Epoch: 1765/2000... Training loss: 0.3052\n",
      "Epoch: 1765/2000... Training loss: 0.4338\n",
      "Epoch: 1765/2000... Training loss: 0.2252\n",
      "Epoch: 1765/2000... Training loss: 0.4139\n",
      "Epoch: 1766/2000... Training loss: 0.3327\n",
      "Epoch: 1766/2000... Training loss: 0.3121\n",
      "Epoch: 1766/2000... Training loss: 0.4743\n",
      "Epoch: 1766/2000... Training loss: 0.3263\n",
      "Epoch: 1766/2000... Training loss: 0.3316\n",
      "Epoch: 1766/2000... Training loss: 0.2787\n",
      "Epoch: 1766/2000... Training loss: 0.4452\n",
      "Epoch: 1766/2000... Training loss: 0.4952\n",
      "Epoch: 1766/2000... Training loss: 0.5036\n",
      "Epoch: 1766/2000... Training loss: 0.3905\n",
      "Epoch: 1766/2000... Training loss: 0.5086\n",
      "Epoch: 1766/2000... Training loss: 0.2752\n",
      "Epoch: 1766/2000... Training loss: 0.3529\n",
      "Epoch: 1766/2000... Training loss: 0.4353\n",
      "Epoch: 1766/2000... Training loss: 0.5554\n",
      "Epoch: 1766/2000... Training loss: 0.3699\n",
      "Epoch: 1766/2000... Training loss: 0.4211\n",
      "Epoch: 1766/2000... Training loss: 0.5981\n",
      "Epoch: 1766/2000... Training loss: 0.2835\n",
      "Epoch: 1766/2000... Training loss: 0.4280\n",
      "Epoch: 1766/2000... Training loss: 0.2463\n",
      "Epoch: 1766/2000... Training loss: 0.4012\n",
      "Epoch: 1766/2000... Training loss: 0.2723\n",
      "Epoch: 1766/2000... Training loss: 0.4119\n",
      "Epoch: 1766/2000... Training loss: 0.2449\n",
      "Epoch: 1766/2000... Training loss: 0.3859\n",
      "Epoch: 1766/2000... Training loss: 0.4167\n",
      "Epoch: 1766/2000... Training loss: 0.3998\n",
      "Epoch: 1766/2000... Training loss: 0.2706\n",
      "Epoch: 1766/2000... Training loss: 0.3698\n",
      "Epoch: 1766/2000... Training loss: 0.5430\n",
      "Epoch: 1767/2000... Training loss: 0.5371\n",
      "Epoch: 1767/2000... Training loss: 0.3735\n",
      "Epoch: 1767/2000... Training loss: 0.4239\n",
      "Epoch: 1767/2000... Training loss: 0.3573\n",
      "Epoch: 1767/2000... Training loss: 0.3936\n",
      "Epoch: 1767/2000... Training loss: 0.4144\n",
      "Epoch: 1767/2000... Training loss: 0.4004\n",
      "Epoch: 1767/2000... Training loss: 0.3176\n",
      "Epoch: 1767/2000... Training loss: 0.3580\n",
      "Epoch: 1767/2000... Training loss: 0.4281\n",
      "Epoch: 1767/2000... Training loss: 0.4874\n",
      "Epoch: 1767/2000... Training loss: 0.5081\n",
      "Epoch: 1767/2000... Training loss: 0.3573\n",
      "Epoch: 1767/2000... Training loss: 0.3869\n",
      "Epoch: 1767/2000... Training loss: 0.4714\n",
      "Epoch: 1767/2000... Training loss: 0.3266\n",
      "Epoch: 1767/2000... Training loss: 0.3626\n",
      "Epoch: 1767/2000... Training loss: 0.3602\n",
      "Epoch: 1767/2000... Training loss: 0.4238\n",
      "Epoch: 1767/2000... Training loss: 0.4067\n",
      "Epoch: 1767/2000... Training loss: 0.3875\n",
      "Epoch: 1767/2000... Training loss: 0.3352\n",
      "Epoch: 1767/2000... Training loss: 0.2736\n",
      "Epoch: 1767/2000... Training loss: 0.4301\n",
      "Epoch: 1767/2000... Training loss: 0.3941\n",
      "Epoch: 1767/2000... Training loss: 0.4418\n",
      "Epoch: 1767/2000... Training loss: 0.3333\n",
      "Epoch: 1767/2000... Training loss: 0.3279\n",
      "Epoch: 1767/2000... Training loss: 0.2447\n",
      "Epoch: 1767/2000... Training loss: 0.4084\n",
      "Epoch: 1767/2000... Training loss: 0.3913\n",
      "Epoch: 1768/2000... Training loss: 0.4278\n",
      "Epoch: 1768/2000... Training loss: 0.4090\n",
      "Epoch: 1768/2000... Training loss: 0.3244\n",
      "Epoch: 1768/2000... Training loss: 0.4047\n",
      "Epoch: 1768/2000... Training loss: 0.4400\n",
      "Epoch: 1768/2000... Training loss: 0.3717\n",
      "Epoch: 1768/2000... Training loss: 0.4199\n",
      "Epoch: 1768/2000... Training loss: 0.5158\n",
      "Epoch: 1768/2000... Training loss: 0.3074\n",
      "Epoch: 1768/2000... Training loss: 0.4197\n",
      "Epoch: 1768/2000... Training loss: 0.3956\n",
      "Epoch: 1768/2000... Training loss: 0.2948\n",
      "Epoch: 1768/2000... Training loss: 0.2902\n",
      "Epoch: 1768/2000... Training loss: 0.3561\n",
      "Epoch: 1768/2000... Training loss: 0.3088\n",
      "Epoch: 1768/2000... Training loss: 0.5632\n",
      "Epoch: 1768/2000... Training loss: 0.2697\n",
      "Epoch: 1768/2000... Training loss: 0.4287\n",
      "Epoch: 1768/2000... Training loss: 0.3254\n",
      "Epoch: 1768/2000... Training loss: 0.6031\n",
      "Epoch: 1768/2000... Training loss: 0.3458\n",
      "Epoch: 1768/2000... Training loss: 0.3209\n",
      "Epoch: 1768/2000... Training loss: 0.4429\n",
      "Epoch: 1768/2000... Training loss: 0.6407\n",
      "Epoch: 1768/2000... Training loss: 0.3064\n",
      "Epoch: 1768/2000... Training loss: 0.3183\n",
      "Epoch: 1768/2000... Training loss: 0.4123\n",
      "Epoch: 1768/2000... Training loss: 0.2892\n",
      "Epoch: 1768/2000... Training loss: 0.3763\n",
      "Epoch: 1768/2000... Training loss: 0.4373\n",
      "Epoch: 1768/2000... Training loss: 0.4227\n",
      "Epoch: 1769/2000... Training loss: 0.3550\n",
      "Epoch: 1769/2000... Training loss: 0.3539\n",
      "Epoch: 1769/2000... Training loss: 0.3992\n",
      "Epoch: 1769/2000... Training loss: 0.4340\n",
      "Epoch: 1769/2000... Training loss: 0.4449\n",
      "Epoch: 1769/2000... Training loss: 0.3716\n",
      "Epoch: 1769/2000... Training loss: 0.5313\n",
      "Epoch: 1769/2000... Training loss: 0.2778\n",
      "Epoch: 1769/2000... Training loss: 0.5010\n",
      "Epoch: 1769/2000... Training loss: 0.4545\n",
      "Epoch: 1769/2000... Training loss: 0.4919\n",
      "Epoch: 1769/2000... Training loss: 0.3197\n",
      "Epoch: 1769/2000... Training loss: 0.4192\n",
      "Epoch: 1769/2000... Training loss: 0.2856\n",
      "Epoch: 1769/2000... Training loss: 0.4671\n",
      "Epoch: 1769/2000... Training loss: 0.3045\n",
      "Epoch: 1769/2000... Training loss: 0.4572\n",
      "Epoch: 1769/2000... Training loss: 0.4063\n",
      "Epoch: 1769/2000... Training loss: 0.4070\n",
      "Epoch: 1769/2000... Training loss: 0.2878\n",
      "Epoch: 1769/2000... Training loss: 0.3905\n",
      "Epoch: 1769/2000... Training loss: 0.4086\n",
      "Epoch: 1769/2000... Training loss: 0.3959\n",
      "Epoch: 1769/2000... Training loss: 0.3722\n",
      "Epoch: 1769/2000... Training loss: 0.2949\n",
      "Epoch: 1769/2000... Training loss: 0.4745\n",
      "Epoch: 1769/2000... Training loss: 0.3982\n",
      "Epoch: 1769/2000... Training loss: 0.4608\n",
      "Epoch: 1769/2000... Training loss: 0.3033\n",
      "Epoch: 1769/2000... Training loss: 0.4191\n",
      "Epoch: 1769/2000... Training loss: 0.5396\n",
      "Epoch: 1770/2000... Training loss: 0.4198\n",
      "Epoch: 1770/2000... Training loss: 0.4687\n",
      "Epoch: 1770/2000... Training loss: 0.4310\n",
      "Epoch: 1770/2000... Training loss: 0.4757\n",
      "Epoch: 1770/2000... Training loss: 0.4116\n",
      "Epoch: 1770/2000... Training loss: 0.3760\n",
      "Epoch: 1770/2000... Training loss: 0.4594\n",
      "Epoch: 1770/2000... Training loss: 0.5555\n",
      "Epoch: 1770/2000... Training loss: 0.2572\n",
      "Epoch: 1770/2000... Training loss: 0.3063\n",
      "Epoch: 1770/2000... Training loss: 0.3591\n",
      "Epoch: 1770/2000... Training loss: 0.3857\n",
      "Epoch: 1770/2000... Training loss: 0.3174\n",
      "Epoch: 1770/2000... Training loss: 0.2440\n",
      "Epoch: 1770/2000... Training loss: 0.4361\n",
      "Epoch: 1770/2000... Training loss: 0.3658\n",
      "Epoch: 1770/2000... Training loss: 0.3687\n",
      "Epoch: 1770/2000... Training loss: 0.3582\n",
      "Epoch: 1770/2000... Training loss: 0.3458\n",
      "Epoch: 1770/2000... Training loss: 0.3824\n",
      "Epoch: 1770/2000... Training loss: 0.2621\n",
      "Epoch: 1770/2000... Training loss: 0.3336\n",
      "Epoch: 1770/2000... Training loss: 0.3117\n",
      "Epoch: 1770/2000... Training loss: 0.4067\n",
      "Epoch: 1770/2000... Training loss: 0.2865\n",
      "Epoch: 1770/2000... Training loss: 0.4224\n",
      "Epoch: 1770/2000... Training loss: 0.4407\n",
      "Epoch: 1770/2000... Training loss: 0.3699\n",
      "Epoch: 1770/2000... Training loss: 0.3613\n",
      "Epoch: 1770/2000... Training loss: 0.2886\n",
      "Epoch: 1770/2000... Training loss: 0.5201\n",
      "Epoch: 1771/2000... Training loss: 0.2855\n",
      "Epoch: 1771/2000... Training loss: 0.3133\n",
      "Epoch: 1771/2000... Training loss: 0.3510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1771/2000... Training loss: 0.2694\n",
      "Epoch: 1771/2000... Training loss: 0.4683\n",
      "Epoch: 1771/2000... Training loss: 0.3769\n",
      "Epoch: 1771/2000... Training loss: 0.3357\n",
      "Epoch: 1771/2000... Training loss: 0.4849\n",
      "Epoch: 1771/2000... Training loss: 0.3325\n",
      "Epoch: 1771/2000... Training loss: 0.3612\n",
      "Epoch: 1771/2000... Training loss: 0.5383\n",
      "Epoch: 1771/2000... Training loss: 0.3209\n",
      "Epoch: 1771/2000... Training loss: 0.3496\n",
      "Epoch: 1771/2000... Training loss: 0.3557\n",
      "Epoch: 1771/2000... Training loss: 0.2912\n",
      "Epoch: 1771/2000... Training loss: 0.4010\n",
      "Epoch: 1771/2000... Training loss: 0.3938\n",
      "Epoch: 1771/2000... Training loss: 0.4596\n",
      "Epoch: 1771/2000... Training loss: 0.4018\n",
      "Epoch: 1771/2000... Training loss: 0.3345\n",
      "Epoch: 1771/2000... Training loss: 0.3395\n",
      "Epoch: 1771/2000... Training loss: 0.4829\n",
      "Epoch: 1771/2000... Training loss: 0.3843\n",
      "Epoch: 1771/2000... Training loss: 0.3772\n",
      "Epoch: 1771/2000... Training loss: 0.3331\n",
      "Epoch: 1771/2000... Training loss: 0.3112\n",
      "Epoch: 1771/2000... Training loss: 0.4177\n",
      "Epoch: 1771/2000... Training loss: 0.3897\n",
      "Epoch: 1771/2000... Training loss: 0.5150\n",
      "Epoch: 1771/2000... Training loss: 0.4264\n",
      "Epoch: 1771/2000... Training loss: 0.5193\n",
      "Epoch: 1772/2000... Training loss: 0.4874\n",
      "Epoch: 1772/2000... Training loss: 0.3104\n",
      "Epoch: 1772/2000... Training loss: 0.4647\n",
      "Epoch: 1772/2000... Training loss: 0.4744\n",
      "Epoch: 1772/2000... Training loss: 0.2558\n",
      "Epoch: 1772/2000... Training loss: 0.4045\n",
      "Epoch: 1772/2000... Training loss: 0.4949\n",
      "Epoch: 1772/2000... Training loss: 0.5687\n",
      "Epoch: 1772/2000... Training loss: 0.3976\n",
      "Epoch: 1772/2000... Training loss: 0.3409\n",
      "Epoch: 1772/2000... Training loss: 0.2763\n",
      "Epoch: 1772/2000... Training loss: 0.4430\n",
      "Epoch: 1772/2000... Training loss: 0.3724\n",
      "Epoch: 1772/2000... Training loss: 0.2216\n",
      "Epoch: 1772/2000... Training loss: 0.2817\n",
      "Epoch: 1772/2000... Training loss: 0.3054\n",
      "Epoch: 1772/2000... Training loss: 0.3143\n",
      "Epoch: 1772/2000... Training loss: 0.2284\n",
      "Epoch: 1772/2000... Training loss: 0.2621\n",
      "Epoch: 1772/2000... Training loss: 0.2650\n",
      "Epoch: 1772/2000... Training loss: 0.2924\n",
      "Epoch: 1772/2000... Training loss: 0.4302\n",
      "Epoch: 1772/2000... Training loss: 0.4193\n",
      "Epoch: 1772/2000... Training loss: 0.6342\n",
      "Epoch: 1772/2000... Training loss: 0.3882\n",
      "Epoch: 1772/2000... Training loss: 0.4896\n",
      "Epoch: 1772/2000... Training loss: 0.3004\n",
      "Epoch: 1772/2000... Training loss: 0.2520\n",
      "Epoch: 1772/2000... Training loss: 0.3814\n",
      "Epoch: 1772/2000... Training loss: 0.3701\n",
      "Epoch: 1772/2000... Training loss: 0.2469\n",
      "Epoch: 1773/2000... Training loss: 0.3739\n",
      "Epoch: 1773/2000... Training loss: 0.3065\n",
      "Epoch: 1773/2000... Training loss: 0.4445\n",
      "Epoch: 1773/2000... Training loss: 0.4433\n",
      "Epoch: 1773/2000... Training loss: 0.3671\n",
      "Epoch: 1773/2000... Training loss: 0.4133\n",
      "Epoch: 1773/2000... Training loss: 0.4508\n",
      "Epoch: 1773/2000... Training loss: 0.2613\n",
      "Epoch: 1773/2000... Training loss: 0.3632\n",
      "Epoch: 1773/2000... Training loss: 0.3336\n",
      "Epoch: 1773/2000... Training loss: 0.3511\n",
      "Epoch: 1773/2000... Training loss: 0.3060\n",
      "Epoch: 1773/2000... Training loss: 0.4741\n",
      "Epoch: 1773/2000... Training loss: 0.5363\n",
      "Epoch: 1773/2000... Training loss: 0.4794\n",
      "Epoch: 1773/2000... Training loss: 0.5141\n",
      "Epoch: 1773/2000... Training loss: 0.2979\n",
      "Epoch: 1773/2000... Training loss: 0.2603\n",
      "Epoch: 1773/2000... Training loss: 0.2264\n",
      "Epoch: 1773/2000... Training loss: 0.4604\n",
      "Epoch: 1773/2000... Training loss: 0.4280\n",
      "Epoch: 1773/2000... Training loss: 0.2892\n",
      "Epoch: 1773/2000... Training loss: 0.5457\n",
      "Epoch: 1773/2000... Training loss: 0.3038\n",
      "Epoch: 1773/2000... Training loss: 0.2623\n",
      "Epoch: 1773/2000... Training loss: 0.4561\n",
      "Epoch: 1773/2000... Training loss: 0.3679\n",
      "Epoch: 1773/2000... Training loss: 0.3816\n",
      "Epoch: 1773/2000... Training loss: 0.3632\n",
      "Epoch: 1773/2000... Training loss: 0.4289\n",
      "Epoch: 1773/2000... Training loss: 0.2354\n",
      "Epoch: 1774/2000... Training loss: 0.2498\n",
      "Epoch: 1774/2000... Training loss: 0.2745\n",
      "Epoch: 1774/2000... Training loss: 0.4490\n",
      "Epoch: 1774/2000... Training loss: 0.6291\n",
      "Epoch: 1774/2000... Training loss: 0.3369\n",
      "Epoch: 1774/2000... Training loss: 0.2550\n",
      "Epoch: 1774/2000... Training loss: 0.7069\n",
      "Epoch: 1774/2000... Training loss: 0.3851\n",
      "Epoch: 1774/2000... Training loss: 0.3585\n",
      "Epoch: 1774/2000... Training loss: 0.4348\n",
      "Epoch: 1774/2000... Training loss: 0.5602\n",
      "Epoch: 1774/2000... Training loss: 0.3528\n",
      "Epoch: 1774/2000... Training loss: 0.3416\n",
      "Epoch: 1774/2000... Training loss: 0.3325\n",
      "Epoch: 1774/2000... Training loss: 0.4532\n",
      "Epoch: 1774/2000... Training loss: 0.4562\n",
      "Epoch: 1774/2000... Training loss: 0.5204\n",
      "Epoch: 1774/2000... Training loss: 0.3122\n",
      "Epoch: 1774/2000... Training loss: 0.3350\n",
      "Epoch: 1774/2000... Training loss: 0.3344\n",
      "Epoch: 1774/2000... Training loss: 0.4457\n",
      "Epoch: 1774/2000... Training loss: 0.4992\n",
      "Epoch: 1774/2000... Training loss: 0.4470\n",
      "Epoch: 1774/2000... Training loss: 0.2766\n",
      "Epoch: 1774/2000... Training loss: 0.5064\n",
      "Epoch: 1774/2000... Training loss: 0.5198\n",
      "Epoch: 1774/2000... Training loss: 0.4284\n",
      "Epoch: 1774/2000... Training loss: 0.4215\n",
      "Epoch: 1774/2000... Training loss: 0.4433\n",
      "Epoch: 1774/2000... Training loss: 0.4605\n",
      "Epoch: 1774/2000... Training loss: 0.4056\n",
      "Epoch: 1775/2000... Training loss: 0.3663\n",
      "Epoch: 1775/2000... Training loss: 0.3710\n",
      "Epoch: 1775/2000... Training loss: 0.3866\n",
      "Epoch: 1775/2000... Training loss: 0.4115\n",
      "Epoch: 1775/2000... Training loss: 0.4270\n",
      "Epoch: 1775/2000... Training loss: 0.3559\n",
      "Epoch: 1775/2000... Training loss: 0.4280\n",
      "Epoch: 1775/2000... Training loss: 0.3979\n",
      "Epoch: 1775/2000... Training loss: 0.3764\n",
      "Epoch: 1775/2000... Training loss: 0.3847\n",
      "Epoch: 1775/2000... Training loss: 0.4165\n",
      "Epoch: 1775/2000... Training loss: 0.2882\n",
      "Epoch: 1775/2000... Training loss: 0.4055\n",
      "Epoch: 1775/2000... Training loss: 0.2830\n",
      "Epoch: 1775/2000... Training loss: 0.2957\n",
      "Epoch: 1775/2000... Training loss: 0.6761\n",
      "Epoch: 1775/2000... Training loss: 0.4438\n",
      "Epoch: 1775/2000... Training loss: 0.2582\n",
      "Epoch: 1775/2000... Training loss: 0.5012\n",
      "Epoch: 1775/2000... Training loss: 0.4625\n",
      "Epoch: 1775/2000... Training loss: 0.4220\n",
      "Epoch: 1775/2000... Training loss: 0.3296\n",
      "Epoch: 1775/2000... Training loss: 0.3403\n",
      "Epoch: 1775/2000... Training loss: 0.3032\n",
      "Epoch: 1775/2000... Training loss: 0.3355\n",
      "Epoch: 1775/2000... Training loss: 0.2809\n",
      "Epoch: 1775/2000... Training loss: 0.4627\n",
      "Epoch: 1775/2000... Training loss: 0.3163\n",
      "Epoch: 1775/2000... Training loss: 0.3420\n",
      "Epoch: 1775/2000... Training loss: 0.3359\n",
      "Epoch: 1775/2000... Training loss: 0.3504\n",
      "Epoch: 1776/2000... Training loss: 0.3837\n",
      "Epoch: 1776/2000... Training loss: 0.3070\n",
      "Epoch: 1776/2000... Training loss: 0.3577\n",
      "Epoch: 1776/2000... Training loss: 0.4484\n",
      "Epoch: 1776/2000... Training loss: 0.3651\n",
      "Epoch: 1776/2000... Training loss: 0.4167\n",
      "Epoch: 1776/2000... Training loss: 0.3947\n",
      "Epoch: 1776/2000... Training loss: 0.2541\n",
      "Epoch: 1776/2000... Training loss: 0.3287\n",
      "Epoch: 1776/2000... Training loss: 0.2758\n",
      "Epoch: 1776/2000... Training loss: 0.5829\n",
      "Epoch: 1776/2000... Training loss: 0.4562\n",
      "Epoch: 1776/2000... Training loss: 0.4913\n",
      "Epoch: 1776/2000... Training loss: 0.2821\n",
      "Epoch: 1776/2000... Training loss: 0.6351\n",
      "Epoch: 1776/2000... Training loss: 0.4636\n",
      "Epoch: 1776/2000... Training loss: 0.2087\n",
      "Epoch: 1776/2000... Training loss: 0.4340\n",
      "Epoch: 1776/2000... Training loss: 0.4352\n",
      "Epoch: 1776/2000... Training loss: 0.2476\n",
      "Epoch: 1776/2000... Training loss: 0.3402\n",
      "Epoch: 1776/2000... Training loss: 0.4547\n",
      "Epoch: 1776/2000... Training loss: 0.6393\n",
      "Epoch: 1776/2000... Training loss: 0.5008\n",
      "Epoch: 1776/2000... Training loss: 0.3859\n",
      "Epoch: 1776/2000... Training loss: 0.2804\n",
      "Epoch: 1776/2000... Training loss: 0.4760\n",
      "Epoch: 1776/2000... Training loss: 0.3282\n",
      "Epoch: 1776/2000... Training loss: 0.3254\n",
      "Epoch: 1776/2000... Training loss: 0.3486\n",
      "Epoch: 1776/2000... Training loss: 0.3698\n",
      "Epoch: 1777/2000... Training loss: 0.5231\n",
      "Epoch: 1777/2000... Training loss: 0.3084\n",
      "Epoch: 1777/2000... Training loss: 0.2860\n",
      "Epoch: 1777/2000... Training loss: 0.3391\n",
      "Epoch: 1777/2000... Training loss: 0.2908\n",
      "Epoch: 1777/2000... Training loss: 0.3681\n",
      "Epoch: 1777/2000... Training loss: 0.4027\n",
      "Epoch: 1777/2000... Training loss: 0.3201\n",
      "Epoch: 1777/2000... Training loss: 0.2853\n",
      "Epoch: 1777/2000... Training loss: 0.3723\n",
      "Epoch: 1777/2000... Training loss: 0.3100\n",
      "Epoch: 1777/2000... Training loss: 0.2337\n",
      "Epoch: 1777/2000... Training loss: 0.3473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1777/2000... Training loss: 0.4608\n",
      "Epoch: 1777/2000... Training loss: 0.3634\n",
      "Epoch: 1777/2000... Training loss: 0.5682\n",
      "Epoch: 1777/2000... Training loss: 0.5462\n",
      "Epoch: 1777/2000... Training loss: 0.2033\n",
      "Epoch: 1777/2000... Training loss: 0.4034\n",
      "Epoch: 1777/2000... Training loss: 0.4237\n",
      "Epoch: 1777/2000... Training loss: 0.2993\n",
      "Epoch: 1777/2000... Training loss: 0.2732\n",
      "Epoch: 1777/2000... Training loss: 0.4126\n",
      "Epoch: 1777/2000... Training loss: 0.4160\n",
      "Epoch: 1777/2000... Training loss: 0.3382\n",
      "Epoch: 1777/2000... Training loss: 0.2558\n",
      "Epoch: 1777/2000... Training loss: 0.3291\n",
      "Epoch: 1777/2000... Training loss: 0.4081\n",
      "Epoch: 1777/2000... Training loss: 0.3993\n",
      "Epoch: 1777/2000... Training loss: 0.4091\n",
      "Epoch: 1777/2000... Training loss: 0.3544\n",
      "Epoch: 1778/2000... Training loss: 0.4797\n",
      "Epoch: 1778/2000... Training loss: 0.4562\n",
      "Epoch: 1778/2000... Training loss: 0.2963\n",
      "Epoch: 1778/2000... Training loss: 0.5403\n",
      "Epoch: 1778/2000... Training loss: 0.3434\n",
      "Epoch: 1778/2000... Training loss: 0.5799\n",
      "Epoch: 1778/2000... Training loss: 0.3325\n",
      "Epoch: 1778/2000... Training loss: 0.4460\n",
      "Epoch: 1778/2000... Training loss: 0.3062\n",
      "Epoch: 1778/2000... Training loss: 0.2747\n",
      "Epoch: 1778/2000... Training loss: 0.3248\n",
      "Epoch: 1778/2000... Training loss: 0.3696\n",
      "Epoch: 1778/2000... Training loss: 0.3197\n",
      "Epoch: 1778/2000... Training loss: 0.4912\n",
      "Epoch: 1778/2000... Training loss: 0.3525\n",
      "Epoch: 1778/2000... Training loss: 0.2954\n",
      "Epoch: 1778/2000... Training loss: 0.6699\n",
      "Epoch: 1778/2000... Training loss: 0.3904\n",
      "Epoch: 1778/2000... Training loss: 0.4581\n",
      "Epoch: 1778/2000... Training loss: 0.3966\n",
      "Epoch: 1778/2000... Training loss: 0.3780\n",
      "Epoch: 1778/2000... Training loss: 0.5222\n",
      "Epoch: 1778/2000... Training loss: 0.4488\n",
      "Epoch: 1778/2000... Training loss: 0.4393\n",
      "Epoch: 1778/2000... Training loss: 0.3949\n",
      "Epoch: 1778/2000... Training loss: 0.3321\n",
      "Epoch: 1778/2000... Training loss: 0.3343\n",
      "Epoch: 1778/2000... Training loss: 0.3039\n",
      "Epoch: 1778/2000... Training loss: 0.4120\n",
      "Epoch: 1778/2000... Training loss: 0.3173\n",
      "Epoch: 1778/2000... Training loss: 0.3603\n",
      "Epoch: 1779/2000... Training loss: 0.4381\n",
      "Epoch: 1779/2000... Training loss: 0.4923\n",
      "Epoch: 1779/2000... Training loss: 0.2841\n",
      "Epoch: 1779/2000... Training loss: 0.3340\n",
      "Epoch: 1779/2000... Training loss: 0.2321\n",
      "Epoch: 1779/2000... Training loss: 0.4067\n",
      "Epoch: 1779/2000... Training loss: 0.3640\n",
      "Epoch: 1779/2000... Training loss: 0.3098\n",
      "Epoch: 1779/2000... Training loss: 0.3623\n",
      "Epoch: 1779/2000... Training loss: 0.5760\n",
      "Epoch: 1779/2000... Training loss: 0.4021\n",
      "Epoch: 1779/2000... Training loss: 0.4638\n",
      "Epoch: 1779/2000... Training loss: 0.4150\n",
      "Epoch: 1779/2000... Training loss: 0.3891\n",
      "Epoch: 1779/2000... Training loss: 0.4988\n",
      "Epoch: 1779/2000... Training loss: 0.4509\n",
      "Epoch: 1779/2000... Training loss: 0.4684\n",
      "Epoch: 1779/2000... Training loss: 0.2339\n",
      "Epoch: 1779/2000... Training loss: 0.3344\n",
      "Epoch: 1779/2000... Training loss: 0.3221\n",
      "Epoch: 1779/2000... Training loss: 0.4007\n",
      "Epoch: 1779/2000... Training loss: 0.5572\n",
      "Epoch: 1779/2000... Training loss: 0.3854\n",
      "Epoch: 1779/2000... Training loss: 0.3882\n",
      "Epoch: 1779/2000... Training loss: 0.5206\n",
      "Epoch: 1779/2000... Training loss: 0.5208\n",
      "Epoch: 1779/2000... Training loss: 0.4225\n",
      "Epoch: 1779/2000... Training loss: 0.3187\n",
      "Epoch: 1779/2000... Training loss: 0.5709\n",
      "Epoch: 1779/2000... Training loss: 0.5360\n",
      "Epoch: 1779/2000... Training loss: 0.4615\n",
      "Epoch: 1780/2000... Training loss: 0.5579\n",
      "Epoch: 1780/2000... Training loss: 0.5975\n",
      "Epoch: 1780/2000... Training loss: 0.3978\n",
      "Epoch: 1780/2000... Training loss: 0.5446\n",
      "Epoch: 1780/2000... Training loss: 0.3171\n",
      "Epoch: 1780/2000... Training loss: 0.3381\n",
      "Epoch: 1780/2000... Training loss: 0.3110\n",
      "Epoch: 1780/2000... Training loss: 0.3193\n",
      "Epoch: 1780/2000... Training loss: 0.2924\n",
      "Epoch: 1780/2000... Training loss: 0.2717\n",
      "Epoch: 1780/2000... Training loss: 0.5118\n",
      "Epoch: 1780/2000... Training loss: 0.1956\n",
      "Epoch: 1780/2000... Training loss: 0.5448\n",
      "Epoch: 1780/2000... Training loss: 0.3555\n",
      "Epoch: 1780/2000... Training loss: 0.3323\n",
      "Epoch: 1780/2000... Training loss: 0.2441\n",
      "Epoch: 1780/2000... Training loss: 0.3599\n",
      "Epoch: 1780/2000... Training loss: 0.3421\n",
      "Epoch: 1780/2000... Training loss: 0.2434\n",
      "Epoch: 1780/2000... Training loss: 0.2604\n",
      "Epoch: 1780/2000... Training loss: 0.3633\n",
      "Epoch: 1780/2000... Training loss: 0.4596\n",
      "Epoch: 1780/2000... Training loss: 0.5150\n",
      "Epoch: 1780/2000... Training loss: 0.4434\n",
      "Epoch: 1780/2000... Training loss: 0.4189\n",
      "Epoch: 1780/2000... Training loss: 0.2870\n",
      "Epoch: 1780/2000... Training loss: 0.4279\n",
      "Epoch: 1780/2000... Training loss: 0.4870\n",
      "Epoch: 1780/2000... Training loss: 0.2795\n",
      "Epoch: 1780/2000... Training loss: 0.3580\n",
      "Epoch: 1780/2000... Training loss: 0.3444\n",
      "Epoch: 1781/2000... Training loss: 0.3478\n",
      "Epoch: 1781/2000... Training loss: 0.4191\n",
      "Epoch: 1781/2000... Training loss: 0.5756\n",
      "Epoch: 1781/2000... Training loss: 0.4157\n",
      "Epoch: 1781/2000... Training loss: 0.4251\n",
      "Epoch: 1781/2000... Training loss: 0.3791\n",
      "Epoch: 1781/2000... Training loss: 0.3929\n",
      "Epoch: 1781/2000... Training loss: 0.3683\n",
      "Epoch: 1781/2000... Training loss: 0.3127\n",
      "Epoch: 1781/2000... Training loss: 0.3305\n",
      "Epoch: 1781/2000... Training loss: 0.2753\n",
      "Epoch: 1781/2000... Training loss: 0.4118\n",
      "Epoch: 1781/2000... Training loss: 0.3520\n",
      "Epoch: 1781/2000... Training loss: 0.3751\n",
      "Epoch: 1781/2000... Training loss: 0.3918\n",
      "Epoch: 1781/2000... Training loss: 0.4765\n",
      "Epoch: 1781/2000... Training loss: 0.2811\n",
      "Epoch: 1781/2000... Training loss: 0.2277\n",
      "Epoch: 1781/2000... Training loss: 0.4022\n",
      "Epoch: 1781/2000... Training loss: 0.3801\n",
      "Epoch: 1781/2000... Training loss: 0.2313\n",
      "Epoch: 1781/2000... Training loss: 0.2550\n",
      "Epoch: 1781/2000... Training loss: 0.2011\n",
      "Epoch: 1781/2000... Training loss: 0.4328\n",
      "Epoch: 1781/2000... Training loss: 0.2575\n",
      "Epoch: 1781/2000... Training loss: 0.5379\n",
      "Epoch: 1781/2000... Training loss: 0.3359\n",
      "Epoch: 1781/2000... Training loss: 0.4888\n",
      "Epoch: 1781/2000... Training loss: 0.6125\n",
      "Epoch: 1781/2000... Training loss: 0.3447\n",
      "Epoch: 1781/2000... Training loss: 0.4339\n",
      "Epoch: 1782/2000... Training loss: 0.4116\n",
      "Epoch: 1782/2000... Training loss: 0.3331\n",
      "Epoch: 1782/2000... Training loss: 0.4514\n",
      "Epoch: 1782/2000... Training loss: 0.4214\n",
      "Epoch: 1782/2000... Training loss: 0.2962\n",
      "Epoch: 1782/2000... Training loss: 0.2884\n",
      "Epoch: 1782/2000... Training loss: 0.3732\n",
      "Epoch: 1782/2000... Training loss: 0.3110\n",
      "Epoch: 1782/2000... Training loss: 0.3288\n",
      "Epoch: 1782/2000... Training loss: 0.4764\n",
      "Epoch: 1782/2000... Training loss: 0.3909\n",
      "Epoch: 1782/2000... Training loss: 0.2402\n",
      "Epoch: 1782/2000... Training loss: 0.3956\n",
      "Epoch: 1782/2000... Training loss: 0.5674\n",
      "Epoch: 1782/2000... Training loss: 0.2494\n",
      "Epoch: 1782/2000... Training loss: 0.4669\n",
      "Epoch: 1782/2000... Training loss: 0.3406\n",
      "Epoch: 1782/2000... Training loss: 0.4874\n",
      "Epoch: 1782/2000... Training loss: 0.4913\n",
      "Epoch: 1782/2000... Training loss: 0.4390\n",
      "Epoch: 1782/2000... Training loss: 0.3576\n",
      "Epoch: 1782/2000... Training loss: 0.6142\n",
      "Epoch: 1782/2000... Training loss: 0.5163\n",
      "Epoch: 1782/2000... Training loss: 0.4616\n",
      "Epoch: 1782/2000... Training loss: 0.2249\n",
      "Epoch: 1782/2000... Training loss: 0.6297\n",
      "Epoch: 1782/2000... Training loss: 0.3945\n",
      "Epoch: 1782/2000... Training loss: 0.2983\n",
      "Epoch: 1782/2000... Training loss: 0.3964\n",
      "Epoch: 1782/2000... Training loss: 0.3586\n",
      "Epoch: 1782/2000... Training loss: 0.2556\n",
      "Epoch: 1783/2000... Training loss: 0.2595\n",
      "Epoch: 1783/2000... Training loss: 0.2650\n",
      "Epoch: 1783/2000... Training loss: 0.4267\n",
      "Epoch: 1783/2000... Training loss: 0.3963\n",
      "Epoch: 1783/2000... Training loss: 0.4146\n",
      "Epoch: 1783/2000... Training loss: 0.4269\n",
      "Epoch: 1783/2000... Training loss: 0.3728\n",
      "Epoch: 1783/2000... Training loss: 0.4289\n",
      "Epoch: 1783/2000... Training loss: 0.5114\n",
      "Epoch: 1783/2000... Training loss: 0.2877\n",
      "Epoch: 1783/2000... Training loss: 0.4010\n",
      "Epoch: 1783/2000... Training loss: 0.3345\n",
      "Epoch: 1783/2000... Training loss: 0.5411\n",
      "Epoch: 1783/2000... Training loss: 0.3096\n",
      "Epoch: 1783/2000... Training loss: 0.3984\n",
      "Epoch: 1783/2000... Training loss: 0.4283\n",
      "Epoch: 1783/2000... Training loss: 0.2983\n",
      "Epoch: 1783/2000... Training loss: 0.3411\n",
      "Epoch: 1783/2000... Training loss: 0.5125\n",
      "Epoch: 1783/2000... Training loss: 0.3643\n",
      "Epoch: 1783/2000... Training loss: 0.5028\n",
      "Epoch: 1783/2000... Training loss: 0.3774\n",
      "Epoch: 1783/2000... Training loss: 0.4142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1783/2000... Training loss: 0.4401\n",
      "Epoch: 1783/2000... Training loss: 0.2505\n",
      "Epoch: 1783/2000... Training loss: 0.3948\n",
      "Epoch: 1783/2000... Training loss: 0.3364\n",
      "Epoch: 1783/2000... Training loss: 0.2428\n",
      "Epoch: 1783/2000... Training loss: 0.4246\n",
      "Epoch: 1783/2000... Training loss: 0.3754\n",
      "Epoch: 1783/2000... Training loss: 0.4430\n",
      "Epoch: 1784/2000... Training loss: 0.3946\n",
      "Epoch: 1784/2000... Training loss: 0.6503\n",
      "Epoch: 1784/2000... Training loss: 0.2863\n",
      "Epoch: 1784/2000... Training loss: 0.4044\n",
      "Epoch: 1784/2000... Training loss: 0.6089\n",
      "Epoch: 1784/2000... Training loss: 0.5773\n",
      "Epoch: 1784/2000... Training loss: 0.5240\n",
      "Epoch: 1784/2000... Training loss: 0.3968\n",
      "Epoch: 1784/2000... Training loss: 0.4514\n",
      "Epoch: 1784/2000... Training loss: 0.5258\n",
      "Epoch: 1784/2000... Training loss: 0.3793\n",
      "Epoch: 1784/2000... Training loss: 0.4762\n",
      "Epoch: 1784/2000... Training loss: 0.2299\n",
      "Epoch: 1784/2000... Training loss: 0.4525\n",
      "Epoch: 1784/2000... Training loss: 0.2748\n",
      "Epoch: 1784/2000... Training loss: 0.2308\n",
      "Epoch: 1784/2000... Training loss: 0.5052\n",
      "Epoch: 1784/2000... Training loss: 0.2651\n",
      "Epoch: 1784/2000... Training loss: 0.4520\n",
      "Epoch: 1784/2000... Training loss: 0.3371\n",
      "Epoch: 1784/2000... Training loss: 0.3188\n",
      "Epoch: 1784/2000... Training loss: 0.3196\n",
      "Epoch: 1784/2000... Training loss: 0.3748\n",
      "Epoch: 1784/2000... Training loss: 0.3944\n",
      "Epoch: 1784/2000... Training loss: 0.3815\n",
      "Epoch: 1784/2000... Training loss: 0.4416\n",
      "Epoch: 1784/2000... Training loss: 0.5983\n",
      "Epoch: 1784/2000... Training loss: 0.3078\n",
      "Epoch: 1784/2000... Training loss: 0.3453\n",
      "Epoch: 1784/2000... Training loss: 0.4794\n",
      "Epoch: 1784/2000... Training loss: 0.4016\n",
      "Epoch: 1785/2000... Training loss: 0.2707\n",
      "Epoch: 1785/2000... Training loss: 0.3513\n",
      "Epoch: 1785/2000... Training loss: 0.4485\n",
      "Epoch: 1785/2000... Training loss: 0.4620\n",
      "Epoch: 1785/2000... Training loss: 0.3691\n",
      "Epoch: 1785/2000... Training loss: 0.4901\n",
      "Epoch: 1785/2000... Training loss: 0.3345\n",
      "Epoch: 1785/2000... Training loss: 0.4355\n",
      "Epoch: 1785/2000... Training loss: 0.3344\n",
      "Epoch: 1785/2000... Training loss: 0.2370\n",
      "Epoch: 1785/2000... Training loss: 0.4153\n",
      "Epoch: 1785/2000... Training loss: 0.3062\n",
      "Epoch: 1785/2000... Training loss: 0.2949\n",
      "Epoch: 1785/2000... Training loss: 0.3494\n",
      "Epoch: 1785/2000... Training loss: 0.4377\n",
      "Epoch: 1785/2000... Training loss: 0.3479\n",
      "Epoch: 1785/2000... Training loss: 0.4576\n",
      "Epoch: 1785/2000... Training loss: 0.2706\n",
      "Epoch: 1785/2000... Training loss: 0.4116\n",
      "Epoch: 1785/2000... Training loss: 0.2474\n",
      "Epoch: 1785/2000... Training loss: 0.3368\n",
      "Epoch: 1785/2000... Training loss: 0.4009\n",
      "Epoch: 1785/2000... Training loss: 0.3271\n",
      "Epoch: 1785/2000... Training loss: 0.2577\n",
      "Epoch: 1785/2000... Training loss: 0.2818\n",
      "Epoch: 1785/2000... Training loss: 0.3850\n",
      "Epoch: 1785/2000... Training loss: 0.4621\n",
      "Epoch: 1785/2000... Training loss: 0.4398\n",
      "Epoch: 1785/2000... Training loss: 0.3760\n",
      "Epoch: 1785/2000... Training loss: 0.2687\n",
      "Epoch: 1785/2000... Training loss: 0.3089\n",
      "Epoch: 1786/2000... Training loss: 0.2313\n",
      "Epoch: 1786/2000... Training loss: 0.4096\n",
      "Epoch: 1786/2000... Training loss: 0.4641\n",
      "Epoch: 1786/2000... Training loss: 0.3770\n",
      "Epoch: 1786/2000... Training loss: 0.4087\n",
      "Epoch: 1786/2000... Training loss: 0.5306\n",
      "Epoch: 1786/2000... Training loss: 0.5174\n",
      "Epoch: 1786/2000... Training loss: 0.4170\n",
      "Epoch: 1786/2000... Training loss: 0.5025\n",
      "Epoch: 1786/2000... Training loss: 0.3007\n",
      "Epoch: 1786/2000... Training loss: 0.3576\n",
      "Epoch: 1786/2000... Training loss: 0.4201\n",
      "Epoch: 1786/2000... Training loss: 0.2917\n",
      "Epoch: 1786/2000... Training loss: 0.3761\n",
      "Epoch: 1786/2000... Training loss: 0.3036\n",
      "Epoch: 1786/2000... Training loss: 0.3517\n",
      "Epoch: 1786/2000... Training loss: 0.3161\n",
      "Epoch: 1786/2000... Training loss: 0.3490\n",
      "Epoch: 1786/2000... Training loss: 0.3613\n",
      "Epoch: 1786/2000... Training loss: 0.3575\n",
      "Epoch: 1786/2000... Training loss: 0.2987\n",
      "Epoch: 1786/2000... Training loss: 0.4813\n",
      "Epoch: 1786/2000... Training loss: 0.3624\n",
      "Epoch: 1786/2000... Training loss: 0.2989\n",
      "Epoch: 1786/2000... Training loss: 0.3321\n",
      "Epoch: 1786/2000... Training loss: 0.4180\n",
      "Epoch: 1786/2000... Training loss: 0.4311\n",
      "Epoch: 1786/2000... Training loss: 0.4250\n",
      "Epoch: 1786/2000... Training loss: 0.3858\n",
      "Epoch: 1786/2000... Training loss: 0.3756\n",
      "Epoch: 1786/2000... Training loss: 0.4022\n",
      "Epoch: 1787/2000... Training loss: 0.2992\n",
      "Epoch: 1787/2000... Training loss: 0.2409\n",
      "Epoch: 1787/2000... Training loss: 0.3486\n",
      "Epoch: 1787/2000... Training loss: 0.3318\n",
      "Epoch: 1787/2000... Training loss: 0.3403\n",
      "Epoch: 1787/2000... Training loss: 0.4051\n",
      "Epoch: 1787/2000... Training loss: 0.4093\n",
      "Epoch: 1787/2000... Training loss: 0.5258\n",
      "Epoch: 1787/2000... Training loss: 0.4005\n",
      "Epoch: 1787/2000... Training loss: 0.3838\n",
      "Epoch: 1787/2000... Training loss: 0.3457\n",
      "Epoch: 1787/2000... Training loss: 0.3891\n",
      "Epoch: 1787/2000... Training loss: 0.3466\n",
      "Epoch: 1787/2000... Training loss: 0.5940\n",
      "Epoch: 1787/2000... Training loss: 0.5065\n",
      "Epoch: 1787/2000... Training loss: 0.2274\n",
      "Epoch: 1787/2000... Training loss: 0.4720\n",
      "Epoch: 1787/2000... Training loss: 0.3191\n",
      "Epoch: 1787/2000... Training loss: 0.3695\n",
      "Epoch: 1787/2000... Training loss: 0.5046\n",
      "Epoch: 1787/2000... Training loss: 0.4658\n",
      "Epoch: 1787/2000... Training loss: 0.4215\n",
      "Epoch: 1787/2000... Training loss: 0.4485\n",
      "Epoch: 1787/2000... Training loss: 0.3092\n",
      "Epoch: 1787/2000... Training loss: 0.3838\n",
      "Epoch: 1787/2000... Training loss: 0.4125\n",
      "Epoch: 1787/2000... Training loss: 0.4492\n",
      "Epoch: 1787/2000... Training loss: 0.3530\n",
      "Epoch: 1787/2000... Training loss: 0.4145\n",
      "Epoch: 1787/2000... Training loss: 0.3457\n",
      "Epoch: 1787/2000... Training loss: 0.2325\n",
      "Epoch: 1788/2000... Training loss: 0.4445\n",
      "Epoch: 1788/2000... Training loss: 0.3254\n",
      "Epoch: 1788/2000... Training loss: 0.4589\n",
      "Epoch: 1788/2000... Training loss: 0.3629\n",
      "Epoch: 1788/2000... Training loss: 0.4403\n",
      "Epoch: 1788/2000... Training loss: 0.2923\n",
      "Epoch: 1788/2000... Training loss: 0.3817\n",
      "Epoch: 1788/2000... Training loss: 0.3497\n",
      "Epoch: 1788/2000... Training loss: 0.5418\n",
      "Epoch: 1788/2000... Training loss: 0.3900\n",
      "Epoch: 1788/2000... Training loss: 0.4864\n",
      "Epoch: 1788/2000... Training loss: 0.3926\n",
      "Epoch: 1788/2000... Training loss: 0.3913\n",
      "Epoch: 1788/2000... Training loss: 0.3876\n",
      "Epoch: 1788/2000... Training loss: 0.3842\n",
      "Epoch: 1788/2000... Training loss: 0.3076\n",
      "Epoch: 1788/2000... Training loss: 0.4900\n",
      "Epoch: 1788/2000... Training loss: 0.3440\n",
      "Epoch: 1788/2000... Training loss: 0.5128\n",
      "Epoch: 1788/2000... Training loss: 0.2594\n",
      "Epoch: 1788/2000... Training loss: 0.3543\n",
      "Epoch: 1788/2000... Training loss: 0.4079\n",
      "Epoch: 1788/2000... Training loss: 0.4458\n",
      "Epoch: 1788/2000... Training loss: 0.4275\n",
      "Epoch: 1788/2000... Training loss: 0.5983\n",
      "Epoch: 1788/2000... Training loss: 0.4597\n",
      "Epoch: 1788/2000... Training loss: 0.3958\n",
      "Epoch: 1788/2000... Training loss: 0.3656\n",
      "Epoch: 1788/2000... Training loss: 0.2538\n",
      "Epoch: 1788/2000... Training loss: 0.3869\n",
      "Epoch: 1788/2000... Training loss: 0.4864\n",
      "Epoch: 1789/2000... Training loss: 0.4018\n",
      "Epoch: 1789/2000... Training loss: 0.2735\n",
      "Epoch: 1789/2000... Training loss: 0.3674\n",
      "Epoch: 1789/2000... Training loss: 0.4831\n",
      "Epoch: 1789/2000... Training loss: 0.2209\n",
      "Epoch: 1789/2000... Training loss: 0.3171\n",
      "Epoch: 1789/2000... Training loss: 0.3589\n",
      "Epoch: 1789/2000... Training loss: 0.3859\n",
      "Epoch: 1789/2000... Training loss: 0.3401\n",
      "Epoch: 1789/2000... Training loss: 0.3663\n",
      "Epoch: 1789/2000... Training loss: 0.4265\n",
      "Epoch: 1789/2000... Training loss: 0.2725\n",
      "Epoch: 1789/2000... Training loss: 0.4856\n",
      "Epoch: 1789/2000... Training loss: 0.2681\n",
      "Epoch: 1789/2000... Training loss: 0.3117\n",
      "Epoch: 1789/2000... Training loss: 0.4445\n",
      "Epoch: 1789/2000... Training loss: 0.4099\n",
      "Epoch: 1789/2000... Training loss: 0.3856\n",
      "Epoch: 1789/2000... Training loss: 0.2743\n",
      "Epoch: 1789/2000... Training loss: 0.3274\n",
      "Epoch: 1789/2000... Training loss: 0.3554\n",
      "Epoch: 1789/2000... Training loss: 0.3778\n",
      "Epoch: 1789/2000... Training loss: 0.3698\n",
      "Epoch: 1789/2000... Training loss: 0.5069\n",
      "Epoch: 1789/2000... Training loss: 0.2716\n",
      "Epoch: 1789/2000... Training loss: 0.4698\n",
      "Epoch: 1789/2000... Training loss: 0.2765\n",
      "Epoch: 1789/2000... Training loss: 0.4884\n",
      "Epoch: 1789/2000... Training loss: 0.3942\n",
      "Epoch: 1789/2000... Training loss: 0.1954\n",
      "Epoch: 1789/2000... Training loss: 0.3855\n",
      "Epoch: 1790/2000... Training loss: 0.2079\n",
      "Epoch: 1790/2000... Training loss: 0.3630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1790/2000... Training loss: 0.4863\n",
      "Epoch: 1790/2000... Training loss: 0.4613\n",
      "Epoch: 1790/2000... Training loss: 0.3692\n",
      "Epoch: 1790/2000... Training loss: 0.4964\n",
      "Epoch: 1790/2000... Training loss: 0.3841\n",
      "Epoch: 1790/2000... Training loss: 0.3673\n",
      "Epoch: 1790/2000... Training loss: 0.3616\n",
      "Epoch: 1790/2000... Training loss: 0.3632\n",
      "Epoch: 1790/2000... Training loss: 0.2907\n",
      "Epoch: 1790/2000... Training loss: 0.3955\n",
      "Epoch: 1790/2000... Training loss: 0.3913\n",
      "Epoch: 1790/2000... Training loss: 0.4029\n",
      "Epoch: 1790/2000... Training loss: 0.3079\n",
      "Epoch: 1790/2000... Training loss: 0.3174\n",
      "Epoch: 1790/2000... Training loss: 0.4230\n",
      "Epoch: 1790/2000... Training loss: 0.4300\n",
      "Epoch: 1790/2000... Training loss: 0.6217\n",
      "Epoch: 1790/2000... Training loss: 0.4116\n",
      "Epoch: 1790/2000... Training loss: 0.3175\n",
      "Epoch: 1790/2000... Training loss: 0.4335\n",
      "Epoch: 1790/2000... Training loss: 0.3363\n",
      "Epoch: 1790/2000... Training loss: 0.4353\n",
      "Epoch: 1790/2000... Training loss: 0.3231\n",
      "Epoch: 1790/2000... Training loss: 0.2863\n",
      "Epoch: 1790/2000... Training loss: 0.3038\n",
      "Epoch: 1790/2000... Training loss: 0.4332\n",
      "Epoch: 1790/2000... Training loss: 0.2888\n",
      "Epoch: 1790/2000... Training loss: 0.3573\n",
      "Epoch: 1790/2000... Training loss: 0.5208\n",
      "Epoch: 1791/2000... Training loss: 0.3515\n",
      "Epoch: 1791/2000... Training loss: 0.5375\n",
      "Epoch: 1791/2000... Training loss: 0.3039\n",
      "Epoch: 1791/2000... Training loss: 0.3067\n",
      "Epoch: 1791/2000... Training loss: 0.2885\n",
      "Epoch: 1791/2000... Training loss: 0.4885\n",
      "Epoch: 1791/2000... Training loss: 0.3336\n",
      "Epoch: 1791/2000... Training loss: 0.3472\n",
      "Epoch: 1791/2000... Training loss: 0.3141\n",
      "Epoch: 1791/2000... Training loss: 0.2314\n",
      "Epoch: 1791/2000... Training loss: 0.2155\n",
      "Epoch: 1791/2000... Training loss: 0.4109\n",
      "Epoch: 1791/2000... Training loss: 0.3201\n",
      "Epoch: 1791/2000... Training loss: 0.5678\n",
      "Epoch: 1791/2000... Training loss: 0.3381\n",
      "Epoch: 1791/2000... Training loss: 0.5357\n",
      "Epoch: 1791/2000... Training loss: 0.4207\n",
      "Epoch: 1791/2000... Training loss: 0.1769\n",
      "Epoch: 1791/2000... Training loss: 0.5599\n",
      "Epoch: 1791/2000... Training loss: 0.3326\n",
      "Epoch: 1791/2000... Training loss: 0.2813\n",
      "Epoch: 1791/2000... Training loss: 0.2358\n",
      "Epoch: 1791/2000... Training loss: 0.4024\n",
      "Epoch: 1791/2000... Training loss: 0.3919\n",
      "Epoch: 1791/2000... Training loss: 0.5001\n",
      "Epoch: 1791/2000... Training loss: 0.3351\n",
      "Epoch: 1791/2000... Training loss: 0.3530\n",
      "Epoch: 1791/2000... Training loss: 0.3227\n",
      "Epoch: 1791/2000... Training loss: 0.2720\n",
      "Epoch: 1791/2000... Training loss: 0.3272\n",
      "Epoch: 1791/2000... Training loss: 0.3637\n",
      "Epoch: 1792/2000... Training loss: 0.3257\n",
      "Epoch: 1792/2000... Training loss: 0.3608\n",
      "Epoch: 1792/2000... Training loss: 0.3476\n",
      "Epoch: 1792/2000... Training loss: 0.3017\n",
      "Epoch: 1792/2000... Training loss: 0.2566\n",
      "Epoch: 1792/2000... Training loss: 0.3078\n",
      "Epoch: 1792/2000... Training loss: 0.4582\n",
      "Epoch: 1792/2000... Training loss: 0.2490\n",
      "Epoch: 1792/2000... Training loss: 0.2866\n",
      "Epoch: 1792/2000... Training loss: 0.2832\n",
      "Epoch: 1792/2000... Training loss: 0.2715\n",
      "Epoch: 1792/2000... Training loss: 0.3323\n",
      "Epoch: 1792/2000... Training loss: 0.2752\n",
      "Epoch: 1792/2000... Training loss: 0.3769\n",
      "Epoch: 1792/2000... Training loss: 0.4494\n",
      "Epoch: 1792/2000... Training loss: 0.2506\n",
      "Epoch: 1792/2000... Training loss: 0.3691\n",
      "Epoch: 1792/2000... Training loss: 0.6053\n",
      "Epoch: 1792/2000... Training loss: 0.3669\n",
      "Epoch: 1792/2000... Training loss: 0.3484\n",
      "Epoch: 1792/2000... Training loss: 0.4063\n",
      "Epoch: 1792/2000... Training loss: 0.3480\n",
      "Epoch: 1792/2000... Training loss: 0.4927\n",
      "Epoch: 1792/2000... Training loss: 0.3760\n",
      "Epoch: 1792/2000... Training loss: 0.4058\n",
      "Epoch: 1792/2000... Training loss: 0.5384\n",
      "Epoch: 1792/2000... Training loss: 0.2709\n",
      "Epoch: 1792/2000... Training loss: 0.3748\n",
      "Epoch: 1792/2000... Training loss: 0.4917\n",
      "Epoch: 1792/2000... Training loss: 0.2677\n",
      "Epoch: 1792/2000... Training loss: 0.4024\n",
      "Epoch: 1793/2000... Training loss: 0.4302\n",
      "Epoch: 1793/2000... Training loss: 0.3571\n",
      "Epoch: 1793/2000... Training loss: 0.2951\n",
      "Epoch: 1793/2000... Training loss: 0.3735\n",
      "Epoch: 1793/2000... Training loss: 0.3699\n",
      "Epoch: 1793/2000... Training loss: 0.3030\n",
      "Epoch: 1793/2000... Training loss: 0.3522\n",
      "Epoch: 1793/2000... Training loss: 0.2778\n",
      "Epoch: 1793/2000... Training loss: 0.2847\n",
      "Epoch: 1793/2000... Training loss: 0.4189\n",
      "Epoch: 1793/2000... Training loss: 0.2812\n",
      "Epoch: 1793/2000... Training loss: 0.3924\n",
      "Epoch: 1793/2000... Training loss: 0.4600\n",
      "Epoch: 1793/2000... Training loss: 0.4776\n",
      "Epoch: 1793/2000... Training loss: 0.4498\n",
      "Epoch: 1793/2000... Training loss: 0.4073\n",
      "Epoch: 1793/2000... Training loss: 0.2797\n",
      "Epoch: 1793/2000... Training loss: 0.2978\n",
      "Epoch: 1793/2000... Training loss: 0.4391\n",
      "Epoch: 1793/2000... Training loss: 0.4950\n",
      "Epoch: 1793/2000... Training loss: 0.4571\n",
      "Epoch: 1793/2000... Training loss: 0.5158\n",
      "Epoch: 1793/2000... Training loss: 0.3192\n",
      "Epoch: 1793/2000... Training loss: 0.3584\n",
      "Epoch: 1793/2000... Training loss: 0.3052\n",
      "Epoch: 1793/2000... Training loss: 0.4563\n",
      "Epoch: 1793/2000... Training loss: 0.5703\n",
      "Epoch: 1793/2000... Training loss: 0.3723\n",
      "Epoch: 1793/2000... Training loss: 0.2360\n",
      "Epoch: 1793/2000... Training loss: 0.4475\n",
      "Epoch: 1793/2000... Training loss: 0.2295\n",
      "Epoch: 1794/2000... Training loss: 0.3144\n",
      "Epoch: 1794/2000... Training loss: 0.3308\n",
      "Epoch: 1794/2000... Training loss: 0.3339\n",
      "Epoch: 1794/2000... Training loss: 0.7336\n",
      "Epoch: 1794/2000... Training loss: 0.3120\n",
      "Epoch: 1794/2000... Training loss: 0.4977\n",
      "Epoch: 1794/2000... Training loss: 0.4706\n",
      "Epoch: 1794/2000... Training loss: 0.4437\n",
      "Epoch: 1794/2000... Training loss: 0.2977\n",
      "Epoch: 1794/2000... Training loss: 0.4135\n",
      "Epoch: 1794/2000... Training loss: 0.3613\n",
      "Epoch: 1794/2000... Training loss: 0.4021\n",
      "Epoch: 1794/2000... Training loss: 0.2851\n",
      "Epoch: 1794/2000... Training loss: 0.4214\n",
      "Epoch: 1794/2000... Training loss: 0.3228\n",
      "Epoch: 1794/2000... Training loss: 0.3639\n",
      "Epoch: 1794/2000... Training loss: 0.3630\n",
      "Epoch: 1794/2000... Training loss: 0.4526\n",
      "Epoch: 1794/2000... Training loss: 0.3044\n",
      "Epoch: 1794/2000... Training loss: 0.5118\n",
      "Epoch: 1794/2000... Training loss: 0.2861\n",
      "Epoch: 1794/2000... Training loss: 0.2956\n",
      "Epoch: 1794/2000... Training loss: 0.4761\n",
      "Epoch: 1794/2000... Training loss: 0.2839\n",
      "Epoch: 1794/2000... Training loss: 0.3614\n",
      "Epoch: 1794/2000... Training loss: 0.3398\n",
      "Epoch: 1794/2000... Training loss: 0.3751\n",
      "Epoch: 1794/2000... Training loss: 0.3149\n",
      "Epoch: 1794/2000... Training loss: 0.3886\n",
      "Epoch: 1794/2000... Training loss: 0.4082\n",
      "Epoch: 1794/2000... Training loss: 0.3887\n",
      "Epoch: 1795/2000... Training loss: 0.4030\n",
      "Epoch: 1795/2000... Training loss: 0.4552\n",
      "Epoch: 1795/2000... Training loss: 0.4369\n",
      "Epoch: 1795/2000... Training loss: 0.4594\n",
      "Epoch: 1795/2000... Training loss: 0.3177\n",
      "Epoch: 1795/2000... Training loss: 0.4505\n",
      "Epoch: 1795/2000... Training loss: 0.3634\n",
      "Epoch: 1795/2000... Training loss: 0.5036\n",
      "Epoch: 1795/2000... Training loss: 0.3524\n",
      "Epoch: 1795/2000... Training loss: 0.4654\n",
      "Epoch: 1795/2000... Training loss: 0.3312\n",
      "Epoch: 1795/2000... Training loss: 0.3717\n",
      "Epoch: 1795/2000... Training loss: 0.4673\n",
      "Epoch: 1795/2000... Training loss: 0.3585\n",
      "Epoch: 1795/2000... Training loss: 0.4042\n",
      "Epoch: 1795/2000... Training loss: 0.4692\n",
      "Epoch: 1795/2000... Training loss: 0.4868\n",
      "Epoch: 1795/2000... Training loss: 0.3336\n",
      "Epoch: 1795/2000... Training loss: 0.3117\n",
      "Epoch: 1795/2000... Training loss: 0.3600\n",
      "Epoch: 1795/2000... Training loss: 0.3954\n",
      "Epoch: 1795/2000... Training loss: 0.5042\n",
      "Epoch: 1795/2000... Training loss: 0.5898\n",
      "Epoch: 1795/2000... Training loss: 0.6132\n",
      "Epoch: 1795/2000... Training loss: 0.4959\n",
      "Epoch: 1795/2000... Training loss: 0.4566\n",
      "Epoch: 1795/2000... Training loss: 0.5046\n",
      "Epoch: 1795/2000... Training loss: 0.3581\n",
      "Epoch: 1795/2000... Training loss: 0.2590\n",
      "Epoch: 1795/2000... Training loss: 0.3885\n",
      "Epoch: 1795/2000... Training loss: 0.4314\n",
      "Epoch: 1796/2000... Training loss: 0.3423\n",
      "Epoch: 1796/2000... Training loss: 0.4093\n",
      "Epoch: 1796/2000... Training loss: 0.3467\n",
      "Epoch: 1796/2000... Training loss: 0.4617\n",
      "Epoch: 1796/2000... Training loss: 0.2514\n",
      "Epoch: 1796/2000... Training loss: 0.2790\n",
      "Epoch: 1796/2000... Training loss: 0.3240\n",
      "Epoch: 1796/2000... Training loss: 0.4988\n",
      "Epoch: 1796/2000... Training loss: 0.2430\n",
      "Epoch: 1796/2000... Training loss: 0.2768\n",
      "Epoch: 1796/2000... Training loss: 0.3497\n",
      "Epoch: 1796/2000... Training loss: 0.4173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1796/2000... Training loss: 0.4280\n",
      "Epoch: 1796/2000... Training loss: 0.4151\n",
      "Epoch: 1796/2000... Training loss: 0.1611\n",
      "Epoch: 1796/2000... Training loss: 0.4525\n",
      "Epoch: 1796/2000... Training loss: 0.4162\n",
      "Epoch: 1796/2000... Training loss: 0.2916\n",
      "Epoch: 1796/2000... Training loss: 0.4005\n",
      "Epoch: 1796/2000... Training loss: 0.3111\n",
      "Epoch: 1796/2000... Training loss: 0.4040\n",
      "Epoch: 1796/2000... Training loss: 0.2565\n",
      "Epoch: 1796/2000... Training loss: 0.2985\n",
      "Epoch: 1796/2000... Training loss: 0.4286\n",
      "Epoch: 1796/2000... Training loss: 0.3790\n",
      "Epoch: 1796/2000... Training loss: 0.4708\n",
      "Epoch: 1796/2000... Training loss: 0.1995\n",
      "Epoch: 1796/2000... Training loss: 0.3748\n",
      "Epoch: 1796/2000... Training loss: 0.3605\n",
      "Epoch: 1796/2000... Training loss: 0.3416\n",
      "Epoch: 1796/2000... Training loss: 0.5915\n",
      "Epoch: 1797/2000... Training loss: 0.2529\n",
      "Epoch: 1797/2000... Training loss: 0.5080\n",
      "Epoch: 1797/2000... Training loss: 0.3276\n",
      "Epoch: 1797/2000... Training loss: 0.4173\n",
      "Epoch: 1797/2000... Training loss: 0.3977\n",
      "Epoch: 1797/2000... Training loss: 0.4637\n",
      "Epoch: 1797/2000... Training loss: 0.5084\n",
      "Epoch: 1797/2000... Training loss: 0.3759\n",
      "Epoch: 1797/2000... Training loss: 0.3960\n",
      "Epoch: 1797/2000... Training loss: 0.4156\n",
      "Epoch: 1797/2000... Training loss: 0.4399\n",
      "Epoch: 1797/2000... Training loss: 0.2792\n",
      "Epoch: 1797/2000... Training loss: 0.4032\n",
      "Epoch: 1797/2000... Training loss: 0.3913\n",
      "Epoch: 1797/2000... Training loss: 0.4720\n",
      "Epoch: 1797/2000... Training loss: 0.4550\n",
      "Epoch: 1797/2000... Training loss: 0.3792\n",
      "Epoch: 1797/2000... Training loss: 0.3593\n",
      "Epoch: 1797/2000... Training loss: 0.4513\n",
      "Epoch: 1797/2000... Training loss: 0.3929\n",
      "Epoch: 1797/2000... Training loss: 0.3404\n",
      "Epoch: 1797/2000... Training loss: 0.3396\n",
      "Epoch: 1797/2000... Training loss: 0.4529\n",
      "Epoch: 1797/2000... Training loss: 0.3342\n",
      "Epoch: 1797/2000... Training loss: 0.2859\n",
      "Epoch: 1797/2000... Training loss: 0.4048\n",
      "Epoch: 1797/2000... Training loss: 0.3698\n",
      "Epoch: 1797/2000... Training loss: 0.4901\n",
      "Epoch: 1797/2000... Training loss: 0.3494\n",
      "Epoch: 1797/2000... Training loss: 0.3682\n",
      "Epoch: 1797/2000... Training loss: 0.2519\n",
      "Epoch: 1798/2000... Training loss: 0.2918\n",
      "Epoch: 1798/2000... Training loss: 0.6698\n",
      "Epoch: 1798/2000... Training loss: 0.3281\n",
      "Epoch: 1798/2000... Training loss: 0.4689\n",
      "Epoch: 1798/2000... Training loss: 0.3020\n",
      "Epoch: 1798/2000... Training loss: 0.2247\n",
      "Epoch: 1798/2000... Training loss: 0.2419\n",
      "Epoch: 1798/2000... Training loss: 0.3648\n",
      "Epoch: 1798/2000... Training loss: 0.3287\n",
      "Epoch: 1798/2000... Training loss: 0.3758\n",
      "Epoch: 1798/2000... Training loss: 0.4751\n",
      "Epoch: 1798/2000... Training loss: 0.3600\n",
      "Epoch: 1798/2000... Training loss: 0.2811\n",
      "Epoch: 1798/2000... Training loss: 0.4851\n",
      "Epoch: 1798/2000... Training loss: 0.3548\n",
      "Epoch: 1798/2000... Training loss: 0.4139\n",
      "Epoch: 1798/2000... Training loss: 0.2823\n",
      "Epoch: 1798/2000... Training loss: 0.4551\n",
      "Epoch: 1798/2000... Training loss: 0.4172\n",
      "Epoch: 1798/2000... Training loss: 0.4849\n",
      "Epoch: 1798/2000... Training loss: 0.3263\n",
      "Epoch: 1798/2000... Training loss: 0.4704\n",
      "Epoch: 1798/2000... Training loss: 0.3230\n",
      "Epoch: 1798/2000... Training loss: 0.2863\n",
      "Epoch: 1798/2000... Training loss: 0.1701\n",
      "Epoch: 1798/2000... Training loss: 0.3264\n",
      "Epoch: 1798/2000... Training loss: 0.3934\n",
      "Epoch: 1798/2000... Training loss: 0.2664\n",
      "Epoch: 1798/2000... Training loss: 0.3562\n",
      "Epoch: 1798/2000... Training loss: 0.5587\n",
      "Epoch: 1798/2000... Training loss: 0.4348\n",
      "Epoch: 1799/2000... Training loss: 0.4682\n",
      "Epoch: 1799/2000... Training loss: 0.3270\n",
      "Epoch: 1799/2000... Training loss: 0.3512\n",
      "Epoch: 1799/2000... Training loss: 0.4466\n",
      "Epoch: 1799/2000... Training loss: 0.3968\n",
      "Epoch: 1799/2000... Training loss: 0.6322\n",
      "Epoch: 1799/2000... Training loss: 0.3200\n",
      "Epoch: 1799/2000... Training loss: 0.3053\n",
      "Epoch: 1799/2000... Training loss: 0.2726\n",
      "Epoch: 1799/2000... Training loss: 0.3457\n",
      "Epoch: 1799/2000... Training loss: 0.5259\n",
      "Epoch: 1799/2000... Training loss: 0.4588\n",
      "Epoch: 1799/2000... Training loss: 0.4353\n",
      "Epoch: 1799/2000... Training loss: 0.5114\n",
      "Epoch: 1799/2000... Training loss: 0.4527\n",
      "Epoch: 1799/2000... Training loss: 0.3176\n",
      "Epoch: 1799/2000... Training loss: 0.5926\n",
      "Epoch: 1799/2000... Training loss: 0.3718\n",
      "Epoch: 1799/2000... Training loss: 0.4134\n",
      "Epoch: 1799/2000... Training loss: 0.3538\n",
      "Epoch: 1799/2000... Training loss: 0.3862\n",
      "Epoch: 1799/2000... Training loss: 0.3437\n",
      "Epoch: 1799/2000... Training loss: 0.3032\n",
      "Epoch: 1799/2000... Training loss: 0.6132\n",
      "Epoch: 1799/2000... Training loss: 0.3314\n",
      "Epoch: 1799/2000... Training loss: 0.1965\n",
      "Epoch: 1799/2000... Training loss: 0.3640\n",
      "Epoch: 1799/2000... Training loss: 0.3550\n",
      "Epoch: 1799/2000... Training loss: 0.4435\n",
      "Epoch: 1799/2000... Training loss: 0.4658\n",
      "Epoch: 1799/2000... Training loss: 0.3003\n",
      "Epoch: 1800/2000... Training loss: 0.6033\n",
      "Epoch: 1800/2000... Training loss: 0.3589\n",
      "Epoch: 1800/2000... Training loss: 0.3873\n",
      "Epoch: 1800/2000... Training loss: 0.2594\n",
      "Epoch: 1800/2000... Training loss: 0.3283\n",
      "Epoch: 1800/2000... Training loss: 0.4261\n",
      "Epoch: 1800/2000... Training loss: 0.3190\n",
      "Epoch: 1800/2000... Training loss: 0.3503\n",
      "Epoch: 1800/2000... Training loss: 0.3301\n",
      "Epoch: 1800/2000... Training loss: 0.3338\n",
      "Epoch: 1800/2000... Training loss: 0.3810\n",
      "Epoch: 1800/2000... Training loss: 0.3644\n",
      "Epoch: 1800/2000... Training loss: 0.4758\n",
      "Epoch: 1800/2000... Training loss: 0.3516\n",
      "Epoch: 1800/2000... Training loss: 0.3948\n",
      "Epoch: 1800/2000... Training loss: 0.2161\n",
      "Epoch: 1800/2000... Training loss: 0.5132\n",
      "Epoch: 1800/2000... Training loss: 0.3103\n",
      "Epoch: 1800/2000... Training loss: 0.3493\n",
      "Epoch: 1800/2000... Training loss: 0.2564\n",
      "Epoch: 1800/2000... Training loss: 0.2537\n",
      "Epoch: 1800/2000... Training loss: 0.3646\n",
      "Epoch: 1800/2000... Training loss: 0.4828\n",
      "Epoch: 1800/2000... Training loss: 0.2294\n",
      "Epoch: 1800/2000... Training loss: 0.4200\n",
      "Epoch: 1800/2000... Training loss: 0.5888\n",
      "Epoch: 1800/2000... Training loss: 0.6515\n",
      "Epoch: 1800/2000... Training loss: 0.2935\n",
      "Epoch: 1800/2000... Training loss: 0.2924\n",
      "Epoch: 1800/2000... Training loss: 0.3181\n",
      "Epoch: 1800/2000... Training loss: 0.4562\n",
      "Epoch: 1801/2000... Training loss: 0.4217\n",
      "Epoch: 1801/2000... Training loss: 0.6058\n",
      "Epoch: 1801/2000... Training loss: 0.2676\n",
      "Epoch: 1801/2000... Training loss: 0.4984\n",
      "Epoch: 1801/2000... Training loss: 0.3917\n",
      "Epoch: 1801/2000... Training loss: 0.4837\n",
      "Epoch: 1801/2000... Training loss: 0.5391\n",
      "Epoch: 1801/2000... Training loss: 0.2932\n",
      "Epoch: 1801/2000... Training loss: 0.4146\n",
      "Epoch: 1801/2000... Training loss: 0.3338\n",
      "Epoch: 1801/2000... Training loss: 0.2975\n",
      "Epoch: 1801/2000... Training loss: 0.3414\n",
      "Epoch: 1801/2000... Training loss: 0.4027\n",
      "Epoch: 1801/2000... Training loss: 0.3747\n",
      "Epoch: 1801/2000... Training loss: 0.5296\n",
      "Epoch: 1801/2000... Training loss: 0.2968\n",
      "Epoch: 1801/2000... Training loss: 0.4159\n",
      "Epoch: 1801/2000... Training loss: 0.3812\n",
      "Epoch: 1801/2000... Training loss: 0.3401\n",
      "Epoch: 1801/2000... Training loss: 0.4250\n",
      "Epoch: 1801/2000... Training loss: 0.5389\n",
      "Epoch: 1801/2000... Training loss: 0.4163\n",
      "Epoch: 1801/2000... Training loss: 0.3140\n",
      "Epoch: 1801/2000... Training loss: 0.5475\n",
      "Epoch: 1801/2000... Training loss: 0.3372\n",
      "Epoch: 1801/2000... Training loss: 0.2733\n",
      "Epoch: 1801/2000... Training loss: 0.3118\n",
      "Epoch: 1801/2000... Training loss: 0.3779\n",
      "Epoch: 1801/2000... Training loss: 0.4693\n",
      "Epoch: 1801/2000... Training loss: 0.3929\n",
      "Epoch: 1801/2000... Training loss: 0.5009\n",
      "Epoch: 1802/2000... Training loss: 0.4279\n",
      "Epoch: 1802/2000... Training loss: 0.2580\n",
      "Epoch: 1802/2000... Training loss: 0.5001\n",
      "Epoch: 1802/2000... Training loss: 0.5076\n",
      "Epoch: 1802/2000... Training loss: 0.2736\n",
      "Epoch: 1802/2000... Training loss: 0.2344\n",
      "Epoch: 1802/2000... Training loss: 0.3962\n",
      "Epoch: 1802/2000... Training loss: 0.2236\n",
      "Epoch: 1802/2000... Training loss: 0.5065\n",
      "Epoch: 1802/2000... Training loss: 0.5594\n",
      "Epoch: 1802/2000... Training loss: 0.3727\n",
      "Epoch: 1802/2000... Training loss: 0.2950\n",
      "Epoch: 1802/2000... Training loss: 0.4567\n",
      "Epoch: 1802/2000... Training loss: 0.3829\n",
      "Epoch: 1802/2000... Training loss: 0.4515\n",
      "Epoch: 1802/2000... Training loss: 0.4146\n",
      "Epoch: 1802/2000... Training loss: 0.3576\n",
      "Epoch: 1802/2000... Training loss: 0.2911\n",
      "Epoch: 1802/2000... Training loss: 0.4778\n",
      "Epoch: 1802/2000... Training loss: 0.5941\n",
      "Epoch: 1802/2000... Training loss: 0.4243\n",
      "Epoch: 1802/2000... Training loss: 0.3511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1802/2000... Training loss: 0.2800\n",
      "Epoch: 1802/2000... Training loss: 0.3252\n",
      "Epoch: 1802/2000... Training loss: 0.4432\n",
      "Epoch: 1802/2000... Training loss: 0.3863\n",
      "Epoch: 1802/2000... Training loss: 0.4077\n",
      "Epoch: 1802/2000... Training loss: 0.4179\n",
      "Epoch: 1802/2000... Training loss: 0.3803\n",
      "Epoch: 1802/2000... Training loss: 0.3850\n",
      "Epoch: 1802/2000... Training loss: 0.3908\n",
      "Epoch: 1803/2000... Training loss: 0.3306\n",
      "Epoch: 1803/2000... Training loss: 0.3777\n",
      "Epoch: 1803/2000... Training loss: 0.3441\n",
      "Epoch: 1803/2000... Training loss: 0.3456\n",
      "Epoch: 1803/2000... Training loss: 0.4770\n",
      "Epoch: 1803/2000... Training loss: 0.4477\n",
      "Epoch: 1803/2000... Training loss: 0.4801\n",
      "Epoch: 1803/2000... Training loss: 0.4245\n",
      "Epoch: 1803/2000... Training loss: 0.3681\n",
      "Epoch: 1803/2000... Training loss: 0.3260\n",
      "Epoch: 1803/2000... Training loss: 0.4614\n",
      "Epoch: 1803/2000... Training loss: 0.4706\n",
      "Epoch: 1803/2000... Training loss: 0.3347\n",
      "Epoch: 1803/2000... Training loss: 0.4040\n",
      "Epoch: 1803/2000... Training loss: 0.4413\n",
      "Epoch: 1803/2000... Training loss: 0.5221\n",
      "Epoch: 1803/2000... Training loss: 0.2881\n",
      "Epoch: 1803/2000... Training loss: 0.5301\n",
      "Epoch: 1803/2000... Training loss: 0.5517\n",
      "Epoch: 1803/2000... Training loss: 0.4759\n",
      "Epoch: 1803/2000... Training loss: 0.2484\n",
      "Epoch: 1803/2000... Training loss: 0.4227\n",
      "Epoch: 1803/2000... Training loss: 0.4739\n",
      "Epoch: 1803/2000... Training loss: 0.3021\n",
      "Epoch: 1803/2000... Training loss: 0.3233\n",
      "Epoch: 1803/2000... Training loss: 0.4725\n",
      "Epoch: 1803/2000... Training loss: 0.4750\n",
      "Epoch: 1803/2000... Training loss: 0.2926\n",
      "Epoch: 1803/2000... Training loss: 0.3016\n",
      "Epoch: 1803/2000... Training loss: 0.5361\n",
      "Epoch: 1803/2000... Training loss: 0.3642\n",
      "Epoch: 1804/2000... Training loss: 0.3564\n",
      "Epoch: 1804/2000... Training loss: 0.4898\n",
      "Epoch: 1804/2000... Training loss: 0.2996\n",
      "Epoch: 1804/2000... Training loss: 0.4148\n",
      "Epoch: 1804/2000... Training loss: 0.3770\n",
      "Epoch: 1804/2000... Training loss: 0.5042\n",
      "Epoch: 1804/2000... Training loss: 0.4277\n",
      "Epoch: 1804/2000... Training loss: 0.2558\n",
      "Epoch: 1804/2000... Training loss: 0.4667\n",
      "Epoch: 1804/2000... Training loss: 0.4114\n",
      "Epoch: 1804/2000... Training loss: 0.3488\n",
      "Epoch: 1804/2000... Training loss: 0.4599\n",
      "Epoch: 1804/2000... Training loss: 0.3701\n",
      "Epoch: 1804/2000... Training loss: 0.2685\n",
      "Epoch: 1804/2000... Training loss: 0.2626\n",
      "Epoch: 1804/2000... Training loss: 0.2821\n",
      "Epoch: 1804/2000... Training loss: 0.4409\n",
      "Epoch: 1804/2000... Training loss: 0.4376\n",
      "Epoch: 1804/2000... Training loss: 0.4151\n",
      "Epoch: 1804/2000... Training loss: 0.4176\n",
      "Epoch: 1804/2000... Training loss: 0.3275\n",
      "Epoch: 1804/2000... Training loss: 0.4202\n",
      "Epoch: 1804/2000... Training loss: 0.4447\n",
      "Epoch: 1804/2000... Training loss: 0.3200\n",
      "Epoch: 1804/2000... Training loss: 0.2902\n",
      "Epoch: 1804/2000... Training loss: 0.3498\n",
      "Epoch: 1804/2000... Training loss: 0.5099\n",
      "Epoch: 1804/2000... Training loss: 0.5836\n",
      "Epoch: 1804/2000... Training loss: 0.5301\n",
      "Epoch: 1804/2000... Training loss: 0.4879\n",
      "Epoch: 1804/2000... Training loss: 0.3526\n",
      "Epoch: 1805/2000... Training loss: 0.2716\n",
      "Epoch: 1805/2000... Training loss: 0.3719\n",
      "Epoch: 1805/2000... Training loss: 0.4796\n",
      "Epoch: 1805/2000... Training loss: 0.4047\n",
      "Epoch: 1805/2000... Training loss: 0.4747\n",
      "Epoch: 1805/2000... Training loss: 0.3610\n",
      "Epoch: 1805/2000... Training loss: 0.5110\n",
      "Epoch: 1805/2000... Training loss: 0.5362\n",
      "Epoch: 1805/2000... Training loss: 0.2510\n",
      "Epoch: 1805/2000... Training loss: 0.3003\n",
      "Epoch: 1805/2000... Training loss: 0.3272\n",
      "Epoch: 1805/2000... Training loss: 0.4294\n",
      "Epoch: 1805/2000... Training loss: 0.2546\n",
      "Epoch: 1805/2000... Training loss: 0.4127\n",
      "Epoch: 1805/2000... Training loss: 0.3450\n",
      "Epoch: 1805/2000... Training loss: 0.3038\n",
      "Epoch: 1805/2000... Training loss: 0.4946\n",
      "Epoch: 1805/2000... Training loss: 0.2801\n",
      "Epoch: 1805/2000... Training loss: 0.5380\n",
      "Epoch: 1805/2000... Training loss: 0.3054\n",
      "Epoch: 1805/2000... Training loss: 0.4367\n",
      "Epoch: 1805/2000... Training loss: 0.6297\n",
      "Epoch: 1805/2000... Training loss: 0.4015\n",
      "Epoch: 1805/2000... Training loss: 0.5133\n",
      "Epoch: 1805/2000... Training loss: 0.2471\n",
      "Epoch: 1805/2000... Training loss: 0.4367\n",
      "Epoch: 1805/2000... Training loss: 0.3690\n",
      "Epoch: 1805/2000... Training loss: 0.3364\n",
      "Epoch: 1805/2000... Training loss: 0.4491\n",
      "Epoch: 1805/2000... Training loss: 0.2770\n",
      "Epoch: 1805/2000... Training loss: 0.5587\n",
      "Epoch: 1806/2000... Training loss: 0.4238\n",
      "Epoch: 1806/2000... Training loss: 0.3571\n",
      "Epoch: 1806/2000... Training loss: 0.4041\n",
      "Epoch: 1806/2000... Training loss: 0.4211\n",
      "Epoch: 1806/2000... Training loss: 0.3796\n",
      "Epoch: 1806/2000... Training loss: 0.3554\n",
      "Epoch: 1806/2000... Training loss: 0.3753\n",
      "Epoch: 1806/2000... Training loss: 0.3933\n",
      "Epoch: 1806/2000... Training loss: 0.2866\n",
      "Epoch: 1806/2000... Training loss: 0.3398\n",
      "Epoch: 1806/2000... Training loss: 0.5325\n",
      "Epoch: 1806/2000... Training loss: 0.3633\n",
      "Epoch: 1806/2000... Training loss: 0.2822\n",
      "Epoch: 1806/2000... Training loss: 0.3424\n",
      "Epoch: 1806/2000... Training loss: 0.3312\n",
      "Epoch: 1806/2000... Training loss: 0.3323\n",
      "Epoch: 1806/2000... Training loss: 0.5389\n",
      "Epoch: 1806/2000... Training loss: 0.2222\n",
      "Epoch: 1806/2000... Training loss: 0.5036\n",
      "Epoch: 1806/2000... Training loss: 0.4782\n",
      "Epoch: 1806/2000... Training loss: 0.4665\n",
      "Epoch: 1806/2000... Training loss: 0.4267\n",
      "Epoch: 1806/2000... Training loss: 0.4069\n",
      "Epoch: 1806/2000... Training loss: 0.2977\n",
      "Epoch: 1806/2000... Training loss: 0.4142\n",
      "Epoch: 1806/2000... Training loss: 0.4078\n",
      "Epoch: 1806/2000... Training loss: 0.3373\n",
      "Epoch: 1806/2000... Training loss: 0.5128\n",
      "Epoch: 1806/2000... Training loss: 0.2939\n",
      "Epoch: 1806/2000... Training loss: 0.4361\n",
      "Epoch: 1806/2000... Training loss: 0.2226\n",
      "Epoch: 1807/2000... Training loss: 0.3922\n",
      "Epoch: 1807/2000... Training loss: 0.2864\n",
      "Epoch: 1807/2000... Training loss: 0.6254\n",
      "Epoch: 1807/2000... Training loss: 0.3371\n",
      "Epoch: 1807/2000... Training loss: 0.3185\n",
      "Epoch: 1807/2000... Training loss: 0.2994\n",
      "Epoch: 1807/2000... Training loss: 0.4058\n",
      "Epoch: 1807/2000... Training loss: 0.3652\n",
      "Epoch: 1807/2000... Training loss: 0.4056\n",
      "Epoch: 1807/2000... Training loss: 0.3320\n",
      "Epoch: 1807/2000... Training loss: 0.4493\n",
      "Epoch: 1807/2000... Training loss: 0.3162\n",
      "Epoch: 1807/2000... Training loss: 0.4725\n",
      "Epoch: 1807/2000... Training loss: 0.3218\n",
      "Epoch: 1807/2000... Training loss: 0.4944\n",
      "Epoch: 1807/2000... Training loss: 0.3406\n",
      "Epoch: 1807/2000... Training loss: 0.4979\n",
      "Epoch: 1807/2000... Training loss: 0.3236\n",
      "Epoch: 1807/2000... Training loss: 0.2522\n",
      "Epoch: 1807/2000... Training loss: 0.3477\n",
      "Epoch: 1807/2000... Training loss: 0.3756\n",
      "Epoch: 1807/2000... Training loss: 0.3131\n",
      "Epoch: 1807/2000... Training loss: 0.3721\n",
      "Epoch: 1807/2000... Training loss: 0.2900\n",
      "Epoch: 1807/2000... Training loss: 0.3495\n",
      "Epoch: 1807/2000... Training loss: 0.4860\n",
      "Epoch: 1807/2000... Training loss: 0.4471\n",
      "Epoch: 1807/2000... Training loss: 0.2921\n",
      "Epoch: 1807/2000... Training loss: 0.3555\n",
      "Epoch: 1807/2000... Training loss: 0.2900\n",
      "Epoch: 1807/2000... Training loss: 0.4875\n",
      "Epoch: 1808/2000... Training loss: 0.3913\n",
      "Epoch: 1808/2000... Training loss: 0.3452\n",
      "Epoch: 1808/2000... Training loss: 0.4331\n",
      "Epoch: 1808/2000... Training loss: 0.3298\n",
      "Epoch: 1808/2000... Training loss: 0.4429\n",
      "Epoch: 1808/2000... Training loss: 0.3602\n",
      "Epoch: 1808/2000... Training loss: 0.4885\n",
      "Epoch: 1808/2000... Training loss: 0.3443\n",
      "Epoch: 1808/2000... Training loss: 0.2724\n",
      "Epoch: 1808/2000... Training loss: 0.3404\n",
      "Epoch: 1808/2000... Training loss: 0.3861\n",
      "Epoch: 1808/2000... Training loss: 0.2631\n",
      "Epoch: 1808/2000... Training loss: 0.2631\n",
      "Epoch: 1808/2000... Training loss: 0.5914\n",
      "Epoch: 1808/2000... Training loss: 0.3655\n",
      "Epoch: 1808/2000... Training loss: 0.2962\n",
      "Epoch: 1808/2000... Training loss: 0.5957\n",
      "Epoch: 1808/2000... Training loss: 0.2733\n",
      "Epoch: 1808/2000... Training loss: 0.2970\n",
      "Epoch: 1808/2000... Training loss: 0.2706\n",
      "Epoch: 1808/2000... Training loss: 0.2659\n",
      "Epoch: 1808/2000... Training loss: 0.3396\n",
      "Epoch: 1808/2000... Training loss: 0.5010\n",
      "Epoch: 1808/2000... Training loss: 0.3688\n",
      "Epoch: 1808/2000... Training loss: 0.3314\n",
      "Epoch: 1808/2000... Training loss: 0.4387\n",
      "Epoch: 1808/2000... Training loss: 0.4003\n",
      "Epoch: 1808/2000... Training loss: 0.5613\n",
      "Epoch: 1808/2000... Training loss: 0.3334\n",
      "Epoch: 1808/2000... Training loss: 0.3644\n",
      "Epoch: 1808/2000... Training loss: 0.4042\n",
      "Epoch: 1809/2000... Training loss: 0.4142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1809/2000... Training loss: 0.2623\n",
      "Epoch: 1809/2000... Training loss: 0.5073\n",
      "Epoch: 1809/2000... Training loss: 0.3105\n",
      "Epoch: 1809/2000... Training loss: 0.2736\n",
      "Epoch: 1809/2000... Training loss: 0.4321\n",
      "Epoch: 1809/2000... Training loss: 0.4207\n",
      "Epoch: 1809/2000... Training loss: 0.4184\n",
      "Epoch: 1809/2000... Training loss: 0.3518\n",
      "Epoch: 1809/2000... Training loss: 0.5521\n",
      "Epoch: 1809/2000... Training loss: 0.3755\n",
      "Epoch: 1809/2000... Training loss: 0.3521\n",
      "Epoch: 1809/2000... Training loss: 0.2397\n",
      "Epoch: 1809/2000... Training loss: 0.3974\n",
      "Epoch: 1809/2000... Training loss: 0.3836\n",
      "Epoch: 1809/2000... Training loss: 0.2256\n",
      "Epoch: 1809/2000... Training loss: 0.3268\n",
      "Epoch: 1809/2000... Training loss: 0.4446\n",
      "Epoch: 1809/2000... Training loss: 0.3604\n",
      "Epoch: 1809/2000... Training loss: 0.3044\n",
      "Epoch: 1809/2000... Training loss: 0.4315\n",
      "Epoch: 1809/2000... Training loss: 0.4258\n",
      "Epoch: 1809/2000... Training loss: 0.3271\n",
      "Epoch: 1809/2000... Training loss: 0.3012\n",
      "Epoch: 1809/2000... Training loss: 0.3881\n",
      "Epoch: 1809/2000... Training loss: 0.3712\n",
      "Epoch: 1809/2000... Training loss: 0.2512\n",
      "Epoch: 1809/2000... Training loss: 0.1945\n",
      "Epoch: 1809/2000... Training loss: 0.2856\n",
      "Epoch: 1809/2000... Training loss: 0.3810\n",
      "Epoch: 1809/2000... Training loss: 0.4011\n",
      "Epoch: 1810/2000... Training loss: 0.4766\n",
      "Epoch: 1810/2000... Training loss: 0.3204\n",
      "Epoch: 1810/2000... Training loss: 0.4026\n",
      "Epoch: 1810/2000... Training loss: 0.5179\n",
      "Epoch: 1810/2000... Training loss: 0.4290\n",
      "Epoch: 1810/2000... Training loss: 0.3929\n",
      "Epoch: 1810/2000... Training loss: 0.3439\n",
      "Epoch: 1810/2000... Training loss: 0.3420\n",
      "Epoch: 1810/2000... Training loss: 0.3616\n",
      "Epoch: 1810/2000... Training loss: 0.3648\n",
      "Epoch: 1810/2000... Training loss: 0.4796\n",
      "Epoch: 1810/2000... Training loss: 0.4516\n",
      "Epoch: 1810/2000... Training loss: 0.3561\n",
      "Epoch: 1810/2000... Training loss: 0.2880\n",
      "Epoch: 1810/2000... Training loss: 0.3913\n",
      "Epoch: 1810/2000... Training loss: 0.4230\n",
      "Epoch: 1810/2000... Training loss: 0.3608\n",
      "Epoch: 1810/2000... Training loss: 0.3627\n",
      "Epoch: 1810/2000... Training loss: 0.4392\n",
      "Epoch: 1810/2000... Training loss: 0.5092\n",
      "Epoch: 1810/2000... Training loss: 0.3827\n",
      "Epoch: 1810/2000... Training loss: 0.2708\n",
      "Epoch: 1810/2000... Training loss: 0.3564\n",
      "Epoch: 1810/2000... Training loss: 0.4886\n",
      "Epoch: 1810/2000... Training loss: 0.3184\n",
      "Epoch: 1810/2000... Training loss: 0.2400\n",
      "Epoch: 1810/2000... Training loss: 0.3017\n",
      "Epoch: 1810/2000... Training loss: 0.3223\n",
      "Epoch: 1810/2000... Training loss: 0.2752\n",
      "Epoch: 1810/2000... Training loss: 0.4731\n",
      "Epoch: 1810/2000... Training loss: 0.2314\n",
      "Epoch: 1811/2000... Training loss: 0.4065\n",
      "Epoch: 1811/2000... Training loss: 0.3947\n",
      "Epoch: 1811/2000... Training loss: 0.3312\n",
      "Epoch: 1811/2000... Training loss: 0.4901\n",
      "Epoch: 1811/2000... Training loss: 0.2481\n",
      "Epoch: 1811/2000... Training loss: 0.3447\n",
      "Epoch: 1811/2000... Training loss: 0.4822\n",
      "Epoch: 1811/2000... Training loss: 0.3286\n",
      "Epoch: 1811/2000... Training loss: 0.4198\n",
      "Epoch: 1811/2000... Training loss: 0.3948\n",
      "Epoch: 1811/2000... Training loss: 0.3530\n",
      "Epoch: 1811/2000... Training loss: 0.4892\n",
      "Epoch: 1811/2000... Training loss: 0.3891\n",
      "Epoch: 1811/2000... Training loss: 0.3185\n",
      "Epoch: 1811/2000... Training loss: 0.2699\n",
      "Epoch: 1811/2000... Training loss: 0.4616\n",
      "Epoch: 1811/2000... Training loss: 0.4608\n",
      "Epoch: 1811/2000... Training loss: 0.3547\n",
      "Epoch: 1811/2000... Training loss: 0.4760\n",
      "Epoch: 1811/2000... Training loss: 0.3065\n",
      "Epoch: 1811/2000... Training loss: 0.2815\n",
      "Epoch: 1811/2000... Training loss: 0.4389\n",
      "Epoch: 1811/2000... Training loss: 0.3920\n",
      "Epoch: 1811/2000... Training loss: 0.5800\n",
      "Epoch: 1811/2000... Training loss: 0.3176\n",
      "Epoch: 1811/2000... Training loss: 0.2997\n",
      "Epoch: 1811/2000... Training loss: 0.3394\n",
      "Epoch: 1811/2000... Training loss: 0.5432\n",
      "Epoch: 1811/2000... Training loss: 0.4596\n",
      "Epoch: 1811/2000... Training loss: 0.3709\n",
      "Epoch: 1811/2000... Training loss: 0.5130\n",
      "Epoch: 1812/2000... Training loss: 0.3834\n",
      "Epoch: 1812/2000... Training loss: 0.4326\n",
      "Epoch: 1812/2000... Training loss: 0.2453\n",
      "Epoch: 1812/2000... Training loss: 0.3019\n",
      "Epoch: 1812/2000... Training loss: 0.3649\n",
      "Epoch: 1812/2000... Training loss: 0.2659\n",
      "Epoch: 1812/2000... Training loss: 0.3918\n",
      "Epoch: 1812/2000... Training loss: 0.2357\n",
      "Epoch: 1812/2000... Training loss: 0.2196\n",
      "Epoch: 1812/2000... Training loss: 0.3894\n",
      "Epoch: 1812/2000... Training loss: 0.4327\n",
      "Epoch: 1812/2000... Training loss: 0.4750\n",
      "Epoch: 1812/2000... Training loss: 0.5153\n",
      "Epoch: 1812/2000... Training loss: 0.3871\n",
      "Epoch: 1812/2000... Training loss: 0.3781\n",
      "Epoch: 1812/2000... Training loss: 0.3597\n",
      "Epoch: 1812/2000... Training loss: 0.4359\n",
      "Epoch: 1812/2000... Training loss: 0.3951\n",
      "Epoch: 1812/2000... Training loss: 0.5036\n",
      "Epoch: 1812/2000... Training loss: 0.5308\n",
      "Epoch: 1812/2000... Training loss: 0.2939\n",
      "Epoch: 1812/2000... Training loss: 0.3795\n",
      "Epoch: 1812/2000... Training loss: 0.4066\n",
      "Epoch: 1812/2000... Training loss: 0.4026\n",
      "Epoch: 1812/2000... Training loss: 0.3843\n",
      "Epoch: 1812/2000... Training loss: 0.2873\n",
      "Epoch: 1812/2000... Training loss: 0.3946\n",
      "Epoch: 1812/2000... Training loss: 0.3779\n",
      "Epoch: 1812/2000... Training loss: 0.2608\n",
      "Epoch: 1812/2000... Training loss: 0.3362\n",
      "Epoch: 1812/2000... Training loss: 0.4384\n",
      "Epoch: 1813/2000... Training loss: 0.3940\n",
      "Epoch: 1813/2000... Training loss: 0.3892\n",
      "Epoch: 1813/2000... Training loss: 0.3425\n",
      "Epoch: 1813/2000... Training loss: 0.3966\n",
      "Epoch: 1813/2000... Training loss: 0.3740\n",
      "Epoch: 1813/2000... Training loss: 0.3936\n",
      "Epoch: 1813/2000... Training loss: 0.3184\n",
      "Epoch: 1813/2000... Training loss: 0.3295\n",
      "Epoch: 1813/2000... Training loss: 0.3338\n",
      "Epoch: 1813/2000... Training loss: 0.2100\n",
      "Epoch: 1813/2000... Training loss: 0.3098\n",
      "Epoch: 1813/2000... Training loss: 0.2514\n",
      "Epoch: 1813/2000... Training loss: 0.3383\n",
      "Epoch: 1813/2000... Training loss: 0.3825\n",
      "Epoch: 1813/2000... Training loss: 0.3418\n",
      "Epoch: 1813/2000... Training loss: 0.5296\n",
      "Epoch: 1813/2000... Training loss: 0.4671\n",
      "Epoch: 1813/2000... Training loss: 0.6135\n",
      "Epoch: 1813/2000... Training loss: 0.3072\n",
      "Epoch: 1813/2000... Training loss: 0.3314\n",
      "Epoch: 1813/2000... Training loss: 0.3752\n",
      "Epoch: 1813/2000... Training loss: 0.2276\n",
      "Epoch: 1813/2000... Training loss: 0.3606\n",
      "Epoch: 1813/2000... Training loss: 0.3973\n",
      "Epoch: 1813/2000... Training loss: 0.3083\n",
      "Epoch: 1813/2000... Training loss: 0.2960\n",
      "Epoch: 1813/2000... Training loss: 0.3240\n",
      "Epoch: 1813/2000... Training loss: 0.2541\n",
      "Epoch: 1813/2000... Training loss: 0.4171\n",
      "Epoch: 1813/2000... Training loss: 0.3134\n",
      "Epoch: 1813/2000... Training loss: 0.4292\n",
      "Epoch: 1814/2000... Training loss: 0.2938\n",
      "Epoch: 1814/2000... Training loss: 0.3326\n",
      "Epoch: 1814/2000... Training loss: 0.5098\n",
      "Epoch: 1814/2000... Training loss: 0.3991\n",
      "Epoch: 1814/2000... Training loss: 0.4260\n",
      "Epoch: 1814/2000... Training loss: 0.2920\n",
      "Epoch: 1814/2000... Training loss: 0.3371\n",
      "Epoch: 1814/2000... Training loss: 0.4238\n",
      "Epoch: 1814/2000... Training loss: 0.4020\n",
      "Epoch: 1814/2000... Training loss: 0.3849\n",
      "Epoch: 1814/2000... Training loss: 0.3497\n",
      "Epoch: 1814/2000... Training loss: 0.3977\n",
      "Epoch: 1814/2000... Training loss: 0.3445\n",
      "Epoch: 1814/2000... Training loss: 0.4077\n",
      "Epoch: 1814/2000... Training loss: 0.3606\n",
      "Epoch: 1814/2000... Training loss: 0.3375\n",
      "Epoch: 1814/2000... Training loss: 0.2855\n",
      "Epoch: 1814/2000... Training loss: 0.4098\n",
      "Epoch: 1814/2000... Training loss: 0.4709\n",
      "Epoch: 1814/2000... Training loss: 0.2232\n",
      "Epoch: 1814/2000... Training loss: 0.3914\n",
      "Epoch: 1814/2000... Training loss: 0.4024\n",
      "Epoch: 1814/2000... Training loss: 0.3984\n",
      "Epoch: 1814/2000... Training loss: 0.4421\n",
      "Epoch: 1814/2000... Training loss: 0.3251\n",
      "Epoch: 1814/2000... Training loss: 0.1987\n",
      "Epoch: 1814/2000... Training loss: 0.4337\n",
      "Epoch: 1814/2000... Training loss: 0.3354\n",
      "Epoch: 1814/2000... Training loss: 0.4012\n",
      "Epoch: 1814/2000... Training loss: 0.3196\n",
      "Epoch: 1814/2000... Training loss: 0.3534\n",
      "Epoch: 1815/2000... Training loss: 0.3585\n",
      "Epoch: 1815/2000... Training loss: 0.4651\n",
      "Epoch: 1815/2000... Training loss: 0.4268\n",
      "Epoch: 1815/2000... Training loss: 0.3058\n",
      "Epoch: 1815/2000... Training loss: 0.3745\n",
      "Epoch: 1815/2000... Training loss: 0.3419\n",
      "Epoch: 1815/2000... Training loss: 0.5270\n",
      "Epoch: 1815/2000... Training loss: 0.4992\n",
      "Epoch: 1815/2000... Training loss: 0.6443\n",
      "Epoch: 1815/2000... Training loss: 0.4798\n",
      "Epoch: 1815/2000... Training loss: 0.5048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1815/2000... Training loss: 0.2759\n",
      "Epoch: 1815/2000... Training loss: 0.3490\n",
      "Epoch: 1815/2000... Training loss: 0.4806\n",
      "Epoch: 1815/2000... Training loss: 0.4407\n",
      "Epoch: 1815/2000... Training loss: 0.4225\n",
      "Epoch: 1815/2000... Training loss: 0.5055\n",
      "Epoch: 1815/2000... Training loss: 0.3631\n",
      "Epoch: 1815/2000... Training loss: 0.3064\n",
      "Epoch: 1815/2000... Training loss: 0.3112\n",
      "Epoch: 1815/2000... Training loss: 0.4017\n",
      "Epoch: 1815/2000... Training loss: 0.3913\n",
      "Epoch: 1815/2000... Training loss: 0.2319\n",
      "Epoch: 1815/2000... Training loss: 0.3696\n",
      "Epoch: 1815/2000... Training loss: 0.4171\n",
      "Epoch: 1815/2000... Training loss: 0.3969\n",
      "Epoch: 1815/2000... Training loss: 0.5124\n",
      "Epoch: 1815/2000... Training loss: 0.3218\n",
      "Epoch: 1815/2000... Training loss: 0.3364\n",
      "Epoch: 1815/2000... Training loss: 0.2974\n",
      "Epoch: 1815/2000... Training loss: 0.3031\n",
      "Epoch: 1816/2000... Training loss: 0.3510\n",
      "Epoch: 1816/2000... Training loss: 0.4770\n",
      "Epoch: 1816/2000... Training loss: 0.3159\n",
      "Epoch: 1816/2000... Training loss: 0.3731\n",
      "Epoch: 1816/2000... Training loss: 0.4049\n",
      "Epoch: 1816/2000... Training loss: 0.2719\n",
      "Epoch: 1816/2000... Training loss: 0.3221\n",
      "Epoch: 1816/2000... Training loss: 0.2040\n",
      "Epoch: 1816/2000... Training loss: 0.3193\n",
      "Epoch: 1816/2000... Training loss: 0.4601\n",
      "Epoch: 1816/2000... Training loss: 0.3647\n",
      "Epoch: 1816/2000... Training loss: 0.3755\n",
      "Epoch: 1816/2000... Training loss: 0.2256\n",
      "Epoch: 1816/2000... Training loss: 0.2079\n",
      "Epoch: 1816/2000... Training loss: 0.4419\n",
      "Epoch: 1816/2000... Training loss: 0.3676\n",
      "Epoch: 1816/2000... Training loss: 0.3573\n",
      "Epoch: 1816/2000... Training loss: 0.3053\n",
      "Epoch: 1816/2000... Training loss: 0.4817\n",
      "Epoch: 1816/2000... Training loss: 0.4762\n",
      "Epoch: 1816/2000... Training loss: 0.2226\n",
      "Epoch: 1816/2000... Training loss: 0.4490\n",
      "Epoch: 1816/2000... Training loss: 0.3669\n",
      "Epoch: 1816/2000... Training loss: 0.4529\n",
      "Epoch: 1816/2000... Training loss: 0.4291\n",
      "Epoch: 1816/2000... Training loss: 0.2888\n",
      "Epoch: 1816/2000... Training loss: 0.2563\n",
      "Epoch: 1816/2000... Training loss: 0.7368\n",
      "Epoch: 1816/2000... Training loss: 0.3256\n",
      "Epoch: 1816/2000... Training loss: 0.3278\n",
      "Epoch: 1816/2000... Training loss: 0.4443\n",
      "Epoch: 1817/2000... Training loss: 0.3279\n",
      "Epoch: 1817/2000... Training loss: 0.3568\n",
      "Epoch: 1817/2000... Training loss: 0.4630\n",
      "Epoch: 1817/2000... Training loss: 0.3606\n",
      "Epoch: 1817/2000... Training loss: 0.3875\n",
      "Epoch: 1817/2000... Training loss: 0.3326\n",
      "Epoch: 1817/2000... Training loss: 0.4308\n",
      "Epoch: 1817/2000... Training loss: 0.3620\n",
      "Epoch: 1817/2000... Training loss: 0.2756\n",
      "Epoch: 1817/2000... Training loss: 0.3248\n",
      "Epoch: 1817/2000... Training loss: 0.3656\n",
      "Epoch: 1817/2000... Training loss: 0.4349\n",
      "Epoch: 1817/2000... Training loss: 0.4625\n",
      "Epoch: 1817/2000... Training loss: 0.4514\n",
      "Epoch: 1817/2000... Training loss: 0.3224\n",
      "Epoch: 1817/2000... Training loss: 0.3203\n",
      "Epoch: 1817/2000... Training loss: 0.2396\n",
      "Epoch: 1817/2000... Training loss: 0.3710\n",
      "Epoch: 1817/2000... Training loss: 0.2815\n",
      "Epoch: 1817/2000... Training loss: 0.3538\n",
      "Epoch: 1817/2000... Training loss: 0.3713\n",
      "Epoch: 1817/2000... Training loss: 0.4070\n",
      "Epoch: 1817/2000... Training loss: 0.2429\n",
      "Epoch: 1817/2000... Training loss: 0.5016\n",
      "Epoch: 1817/2000... Training loss: 0.3961\n",
      "Epoch: 1817/2000... Training loss: 0.3220\n",
      "Epoch: 1817/2000... Training loss: 0.3829\n",
      "Epoch: 1817/2000... Training loss: 0.3091\n",
      "Epoch: 1817/2000... Training loss: 0.3476\n",
      "Epoch: 1817/2000... Training loss: 0.2583\n",
      "Epoch: 1817/2000... Training loss: 0.3837\n",
      "Epoch: 1818/2000... Training loss: 0.4788\n",
      "Epoch: 1818/2000... Training loss: 0.4357\n",
      "Epoch: 1818/2000... Training loss: 0.2842\n",
      "Epoch: 1818/2000... Training loss: 0.4651\n",
      "Epoch: 1818/2000... Training loss: 0.4357\n",
      "Epoch: 1818/2000... Training loss: 0.5711\n",
      "Epoch: 1818/2000... Training loss: 0.3430\n",
      "Epoch: 1818/2000... Training loss: 0.2547\n",
      "Epoch: 1818/2000... Training loss: 0.3415\n",
      "Epoch: 1818/2000... Training loss: 0.3533\n",
      "Epoch: 1818/2000... Training loss: 0.3571\n",
      "Epoch: 1818/2000... Training loss: 0.4301\n",
      "Epoch: 1818/2000... Training loss: 0.4772\n",
      "Epoch: 1818/2000... Training loss: 0.5148\n",
      "Epoch: 1818/2000... Training loss: 0.2434\n",
      "Epoch: 1818/2000... Training loss: 0.4428\n",
      "Epoch: 1818/2000... Training loss: 0.5926\n",
      "Epoch: 1818/2000... Training loss: 0.4896\n",
      "Epoch: 1818/2000... Training loss: 0.3170\n",
      "Epoch: 1818/2000... Training loss: 0.3405\n",
      "Epoch: 1818/2000... Training loss: 0.3464\n",
      "Epoch: 1818/2000... Training loss: 0.2506\n",
      "Epoch: 1818/2000... Training loss: 0.3511\n",
      "Epoch: 1818/2000... Training loss: 0.3449\n",
      "Epoch: 1818/2000... Training loss: 0.3056\n",
      "Epoch: 1818/2000... Training loss: 0.3844\n",
      "Epoch: 1818/2000... Training loss: 0.4188\n",
      "Epoch: 1818/2000... Training loss: 0.4640\n",
      "Epoch: 1818/2000... Training loss: 0.4113\n",
      "Epoch: 1818/2000... Training loss: 0.5224\n",
      "Epoch: 1818/2000... Training loss: 0.3996\n",
      "Epoch: 1819/2000... Training loss: 0.4162\n",
      "Epoch: 1819/2000... Training loss: 0.5691\n",
      "Epoch: 1819/2000... Training loss: 0.3327\n",
      "Epoch: 1819/2000... Training loss: 0.3968\n",
      "Epoch: 1819/2000... Training loss: 0.4418\n",
      "Epoch: 1819/2000... Training loss: 0.3274\n",
      "Epoch: 1819/2000... Training loss: 0.4299\n",
      "Epoch: 1819/2000... Training loss: 0.3110\n",
      "Epoch: 1819/2000... Training loss: 0.2664\n",
      "Epoch: 1819/2000... Training loss: 0.3941\n",
      "Epoch: 1819/2000... Training loss: 0.3311\n",
      "Epoch: 1819/2000... Training loss: 0.2768\n",
      "Epoch: 1819/2000... Training loss: 0.2556\n",
      "Epoch: 1819/2000... Training loss: 0.3556\n",
      "Epoch: 1819/2000... Training loss: 0.4201\n",
      "Epoch: 1819/2000... Training loss: 0.3488\n",
      "Epoch: 1819/2000... Training loss: 0.4477\n",
      "Epoch: 1819/2000... Training loss: 0.5109\n",
      "Epoch: 1819/2000... Training loss: 0.2811\n",
      "Epoch: 1819/2000... Training loss: 0.3930\n",
      "Epoch: 1819/2000... Training loss: 0.3093\n",
      "Epoch: 1819/2000... Training loss: 0.3783\n",
      "Epoch: 1819/2000... Training loss: 0.3891\n",
      "Epoch: 1819/2000... Training loss: 0.5503\n",
      "Epoch: 1819/2000... Training loss: 0.3748\n",
      "Epoch: 1819/2000... Training loss: 0.5065\n",
      "Epoch: 1819/2000... Training loss: 0.4365\n",
      "Epoch: 1819/2000... Training loss: 0.4370\n",
      "Epoch: 1819/2000... Training loss: 0.3341\n",
      "Epoch: 1819/2000... Training loss: 0.2594\n",
      "Epoch: 1819/2000... Training loss: 0.4635\n",
      "Epoch: 1820/2000... Training loss: 0.6232\n",
      "Epoch: 1820/2000... Training loss: 0.3524\n",
      "Epoch: 1820/2000... Training loss: 0.4088\n",
      "Epoch: 1820/2000... Training loss: 0.3225\n",
      "Epoch: 1820/2000... Training loss: 0.3075\n",
      "Epoch: 1820/2000... Training loss: 0.4433\n",
      "Epoch: 1820/2000... Training loss: 0.5502\n",
      "Epoch: 1820/2000... Training loss: 0.3909\n",
      "Epoch: 1820/2000... Training loss: 0.3164\n",
      "Epoch: 1820/2000... Training loss: 0.3749\n",
      "Epoch: 1820/2000... Training loss: 0.3865\n",
      "Epoch: 1820/2000... Training loss: 0.2863\n",
      "Epoch: 1820/2000... Training loss: 0.4561\n",
      "Epoch: 1820/2000... Training loss: 0.3237\n",
      "Epoch: 1820/2000... Training loss: 0.2906\n",
      "Epoch: 1820/2000... Training loss: 0.3096\n",
      "Epoch: 1820/2000... Training loss: 0.2806\n",
      "Epoch: 1820/2000... Training loss: 0.3449\n",
      "Epoch: 1820/2000... Training loss: 0.3655\n",
      "Epoch: 1820/2000... Training loss: 0.2817\n",
      "Epoch: 1820/2000... Training loss: 0.3439\n",
      "Epoch: 1820/2000... Training loss: 0.3720\n",
      "Epoch: 1820/2000... Training loss: 0.2902\n",
      "Epoch: 1820/2000... Training loss: 0.5993\n",
      "Epoch: 1820/2000... Training loss: 0.4618\n",
      "Epoch: 1820/2000... Training loss: 0.4131\n",
      "Epoch: 1820/2000... Training loss: 0.2814\n",
      "Epoch: 1820/2000... Training loss: 0.3658\n",
      "Epoch: 1820/2000... Training loss: 0.3287\n",
      "Epoch: 1820/2000... Training loss: 0.4523\n",
      "Epoch: 1820/2000... Training loss: 0.3037\n",
      "Epoch: 1821/2000... Training loss: 0.3586\n",
      "Epoch: 1821/2000... Training loss: 0.2916\n",
      "Epoch: 1821/2000... Training loss: 0.4944\n",
      "Epoch: 1821/2000... Training loss: 0.3805\n",
      "Epoch: 1821/2000... Training loss: 0.4932\n",
      "Epoch: 1821/2000... Training loss: 0.2360\n",
      "Epoch: 1821/2000... Training loss: 0.3279\n",
      "Epoch: 1821/2000... Training loss: 0.4753\n",
      "Epoch: 1821/2000... Training loss: 0.4652\n",
      "Epoch: 1821/2000... Training loss: 0.4252\n",
      "Epoch: 1821/2000... Training loss: 0.3953\n",
      "Epoch: 1821/2000... Training loss: 0.6634\n",
      "Epoch: 1821/2000... Training loss: 0.4261\n",
      "Epoch: 1821/2000... Training loss: 0.4660\n",
      "Epoch: 1821/2000... Training loss: 0.4490\n",
      "Epoch: 1821/2000... Training loss: 0.4932\n",
      "Epoch: 1821/2000... Training loss: 0.4644\n",
      "Epoch: 1821/2000... Training loss: 0.3238\n",
      "Epoch: 1821/2000... Training loss: 0.2224\n",
      "Epoch: 1821/2000... Training loss: 0.4128\n",
      "Epoch: 1821/2000... Training loss: 0.3085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1821/2000... Training loss: 0.4737\n",
      "Epoch: 1821/2000... Training loss: 0.5426\n",
      "Epoch: 1821/2000... Training loss: 0.3063\n",
      "Epoch: 1821/2000... Training loss: 0.3200\n",
      "Epoch: 1821/2000... Training loss: 0.2580\n",
      "Epoch: 1821/2000... Training loss: 0.4242\n",
      "Epoch: 1821/2000... Training loss: 0.2434\n",
      "Epoch: 1821/2000... Training loss: 0.5079\n",
      "Epoch: 1821/2000... Training loss: 0.2581\n",
      "Epoch: 1821/2000... Training loss: 0.3418\n",
      "Epoch: 1822/2000... Training loss: 0.3711\n",
      "Epoch: 1822/2000... Training loss: 0.5124\n",
      "Epoch: 1822/2000... Training loss: 0.2438\n",
      "Epoch: 1822/2000... Training loss: 0.3850\n",
      "Epoch: 1822/2000... Training loss: 0.2736\n",
      "Epoch: 1822/2000... Training loss: 0.4029\n",
      "Epoch: 1822/2000... Training loss: 0.2544\n",
      "Epoch: 1822/2000... Training loss: 0.3359\n",
      "Epoch: 1822/2000... Training loss: 0.3512\n",
      "Epoch: 1822/2000... Training loss: 0.2344\n",
      "Epoch: 1822/2000... Training loss: 0.3984\n",
      "Epoch: 1822/2000... Training loss: 0.4870\n",
      "Epoch: 1822/2000... Training loss: 0.4636\n",
      "Epoch: 1822/2000... Training loss: 0.4433\n",
      "Epoch: 1822/2000... Training loss: 0.4209\n",
      "Epoch: 1822/2000... Training loss: 0.4324\n",
      "Epoch: 1822/2000... Training loss: 0.3133\n",
      "Epoch: 1822/2000... Training loss: 0.3449\n",
      "Epoch: 1822/2000... Training loss: 0.4179\n",
      "Epoch: 1822/2000... Training loss: 0.4369\n",
      "Epoch: 1822/2000... Training loss: 0.2868\n",
      "Epoch: 1822/2000... Training loss: 0.4693\n",
      "Epoch: 1822/2000... Training loss: 0.2448\n",
      "Epoch: 1822/2000... Training loss: 0.2869\n",
      "Epoch: 1822/2000... Training loss: 0.2062\n",
      "Epoch: 1822/2000... Training loss: 0.4824\n",
      "Epoch: 1822/2000... Training loss: 0.4182\n",
      "Epoch: 1822/2000... Training loss: 0.4469\n",
      "Epoch: 1822/2000... Training loss: 0.1697\n",
      "Epoch: 1822/2000... Training loss: 0.3718\n",
      "Epoch: 1822/2000... Training loss: 0.3875\n",
      "Epoch: 1823/2000... Training loss: 0.5949\n",
      "Epoch: 1823/2000... Training loss: 0.5973\n",
      "Epoch: 1823/2000... Training loss: 0.2682\n",
      "Epoch: 1823/2000... Training loss: 0.3696\n",
      "Epoch: 1823/2000... Training loss: 0.5433\n",
      "Epoch: 1823/2000... Training loss: 0.3923\n",
      "Epoch: 1823/2000... Training loss: 0.3488\n",
      "Epoch: 1823/2000... Training loss: 0.5745\n",
      "Epoch: 1823/2000... Training loss: 0.3894\n",
      "Epoch: 1823/2000... Training loss: 0.4304\n",
      "Epoch: 1823/2000... Training loss: 0.3259\n",
      "Epoch: 1823/2000... Training loss: 0.4332\n",
      "Epoch: 1823/2000... Training loss: 0.3345\n",
      "Epoch: 1823/2000... Training loss: 0.2751\n",
      "Epoch: 1823/2000... Training loss: 0.3604\n",
      "Epoch: 1823/2000... Training loss: 0.3104\n",
      "Epoch: 1823/2000... Training loss: 0.3326\n",
      "Epoch: 1823/2000... Training loss: 0.4677\n",
      "Epoch: 1823/2000... Training loss: 0.4241\n",
      "Epoch: 1823/2000... Training loss: 0.2422\n",
      "Epoch: 1823/2000... Training loss: 0.4597\n",
      "Epoch: 1823/2000... Training loss: 0.4014\n",
      "Epoch: 1823/2000... Training loss: 0.3008\n",
      "Epoch: 1823/2000... Training loss: 0.4828\n",
      "Epoch: 1823/2000... Training loss: 0.3433\n",
      "Epoch: 1823/2000... Training loss: 0.2534\n",
      "Epoch: 1823/2000... Training loss: 0.3811\n",
      "Epoch: 1823/2000... Training loss: 0.5595\n",
      "Epoch: 1823/2000... Training loss: 0.4857\n",
      "Epoch: 1823/2000... Training loss: 0.4385\n",
      "Epoch: 1823/2000... Training loss: 0.3615\n",
      "Epoch: 1824/2000... Training loss: 0.2715\n",
      "Epoch: 1824/2000... Training loss: 0.4067\n",
      "Epoch: 1824/2000... Training loss: 0.3058\n",
      "Epoch: 1824/2000... Training loss: 0.4581\n",
      "Epoch: 1824/2000... Training loss: 0.2920\n",
      "Epoch: 1824/2000... Training loss: 0.3535\n",
      "Epoch: 1824/2000... Training loss: 0.3924\n",
      "Epoch: 1824/2000... Training loss: 0.3544\n",
      "Epoch: 1824/2000... Training loss: 0.6332\n",
      "Epoch: 1824/2000... Training loss: 0.3512\n",
      "Epoch: 1824/2000... Training loss: 0.4523\n",
      "Epoch: 1824/2000... Training loss: 0.3388\n",
      "Epoch: 1824/2000... Training loss: 0.4723\n",
      "Epoch: 1824/2000... Training loss: 0.3895\n",
      "Epoch: 1824/2000... Training loss: 0.2404\n",
      "Epoch: 1824/2000... Training loss: 0.4301\n",
      "Epoch: 1824/2000... Training loss: 0.3491\n",
      "Epoch: 1824/2000... Training loss: 0.3878\n",
      "Epoch: 1824/2000... Training loss: 0.4010\n",
      "Epoch: 1824/2000... Training loss: 0.4762\n",
      "Epoch: 1824/2000... Training loss: 0.4431\n",
      "Epoch: 1824/2000... Training loss: 0.3745\n",
      "Epoch: 1824/2000... Training loss: 0.4551\n",
      "Epoch: 1824/2000... Training loss: 0.2927\n",
      "Epoch: 1824/2000... Training loss: 0.2784\n",
      "Epoch: 1824/2000... Training loss: 0.4664\n",
      "Epoch: 1824/2000... Training loss: 0.4888\n",
      "Epoch: 1824/2000... Training loss: 0.3389\n",
      "Epoch: 1824/2000... Training loss: 0.2416\n",
      "Epoch: 1824/2000... Training loss: 0.3801\n",
      "Epoch: 1824/2000... Training loss: 0.2712\n",
      "Epoch: 1825/2000... Training loss: 0.3468\n",
      "Epoch: 1825/2000... Training loss: 0.4064\n",
      "Epoch: 1825/2000... Training loss: 0.4610\n",
      "Epoch: 1825/2000... Training loss: 0.5714\n",
      "Epoch: 1825/2000... Training loss: 0.3290\n",
      "Epoch: 1825/2000... Training loss: 0.4333\n",
      "Epoch: 1825/2000... Training loss: 0.5334\n",
      "Epoch: 1825/2000... Training loss: 0.3524\n",
      "Epoch: 1825/2000... Training loss: 0.3798\n",
      "Epoch: 1825/2000... Training loss: 0.4452\n",
      "Epoch: 1825/2000... Training loss: 0.3185\n",
      "Epoch: 1825/2000... Training loss: 0.4339\n",
      "Epoch: 1825/2000... Training loss: 0.4396\n",
      "Epoch: 1825/2000... Training loss: 0.2436\n",
      "Epoch: 1825/2000... Training loss: 0.3515\n",
      "Epoch: 1825/2000... Training loss: 0.3227\n",
      "Epoch: 1825/2000... Training loss: 0.4717\n",
      "Epoch: 1825/2000... Training loss: 0.3424\n",
      "Epoch: 1825/2000... Training loss: 0.5327\n",
      "Epoch: 1825/2000... Training loss: 0.2850\n",
      "Epoch: 1825/2000... Training loss: 0.3725\n",
      "Epoch: 1825/2000... Training loss: 0.3886\n",
      "Epoch: 1825/2000... Training loss: 0.3791\n",
      "Epoch: 1825/2000... Training loss: 0.3970\n",
      "Epoch: 1825/2000... Training loss: 0.2861\n",
      "Epoch: 1825/2000... Training loss: 0.3649\n",
      "Epoch: 1825/2000... Training loss: 0.4234\n",
      "Epoch: 1825/2000... Training loss: 0.2844\n",
      "Epoch: 1825/2000... Training loss: 0.2476\n",
      "Epoch: 1825/2000... Training loss: 0.3883\n",
      "Epoch: 1825/2000... Training loss: 0.3690\n",
      "Epoch: 1826/2000... Training loss: 0.4202\n",
      "Epoch: 1826/2000... Training loss: 0.3085\n",
      "Epoch: 1826/2000... Training loss: 0.4249\n",
      "Epoch: 1826/2000... Training loss: 0.3800\n",
      "Epoch: 1826/2000... Training loss: 0.3454\n",
      "Epoch: 1826/2000... Training loss: 0.5624\n",
      "Epoch: 1826/2000... Training loss: 0.3622\n",
      "Epoch: 1826/2000... Training loss: 0.2453\n",
      "Epoch: 1826/2000... Training loss: 0.6168\n",
      "Epoch: 1826/2000... Training loss: 0.3324\n",
      "Epoch: 1826/2000... Training loss: 0.3555\n",
      "Epoch: 1826/2000... Training loss: 0.4885\n",
      "Epoch: 1826/2000... Training loss: 0.4572\n",
      "Epoch: 1826/2000... Training loss: 0.3080\n",
      "Epoch: 1826/2000... Training loss: 0.3115\n",
      "Epoch: 1826/2000... Training loss: 0.5488\n",
      "Epoch: 1826/2000... Training loss: 0.4171\n",
      "Epoch: 1826/2000... Training loss: 0.2984\n",
      "Epoch: 1826/2000... Training loss: 0.3096\n",
      "Epoch: 1826/2000... Training loss: 0.3969\n",
      "Epoch: 1826/2000... Training loss: 0.2419\n",
      "Epoch: 1826/2000... Training loss: 0.4424\n",
      "Epoch: 1826/2000... Training loss: 0.3668\n",
      "Epoch: 1826/2000... Training loss: 0.4092\n",
      "Epoch: 1826/2000... Training loss: 0.4657\n",
      "Epoch: 1826/2000... Training loss: 0.4575\n",
      "Epoch: 1826/2000... Training loss: 0.3784\n",
      "Epoch: 1826/2000... Training loss: 0.4289\n",
      "Epoch: 1826/2000... Training loss: 0.4770\n",
      "Epoch: 1826/2000... Training loss: 0.6557\n",
      "Epoch: 1826/2000... Training loss: 0.2412\n",
      "Epoch: 1827/2000... Training loss: 0.4199\n",
      "Epoch: 1827/2000... Training loss: 0.4342\n",
      "Epoch: 1827/2000... Training loss: 0.3753\n",
      "Epoch: 1827/2000... Training loss: 0.2349\n",
      "Epoch: 1827/2000... Training loss: 0.3986\n",
      "Epoch: 1827/2000... Training loss: 0.4807\n",
      "Epoch: 1827/2000... Training loss: 0.3886\n",
      "Epoch: 1827/2000... Training loss: 0.3163\n",
      "Epoch: 1827/2000... Training loss: 0.4216\n",
      "Epoch: 1827/2000... Training loss: 0.2958\n",
      "Epoch: 1827/2000... Training loss: 0.2193\n",
      "Epoch: 1827/2000... Training loss: 0.3495\n",
      "Epoch: 1827/2000... Training loss: 0.3198\n",
      "Epoch: 1827/2000... Training loss: 0.2522\n",
      "Epoch: 1827/2000... Training loss: 0.5400\n",
      "Epoch: 1827/2000... Training loss: 0.2405\n",
      "Epoch: 1827/2000... Training loss: 0.3727\n",
      "Epoch: 1827/2000... Training loss: 0.4408\n",
      "Epoch: 1827/2000... Training loss: 0.3499\n",
      "Epoch: 1827/2000... Training loss: 0.4108\n",
      "Epoch: 1827/2000... Training loss: 0.3554\n",
      "Epoch: 1827/2000... Training loss: 0.3481\n",
      "Epoch: 1827/2000... Training loss: 0.5385\n",
      "Epoch: 1827/2000... Training loss: 0.2701\n",
      "Epoch: 1827/2000... Training loss: 0.3423\n",
      "Epoch: 1827/2000... Training loss: 0.4905\n",
      "Epoch: 1827/2000... Training loss: 0.3430\n",
      "Epoch: 1827/2000... Training loss: 0.3383\n",
      "Epoch: 1827/2000... Training loss: 0.3024\n",
      "Epoch: 1827/2000... Training loss: 0.4100\n",
      "Epoch: 1827/2000... Training loss: 0.4893\n",
      "Epoch: 1828/2000... Training loss: 0.3734\n",
      "Epoch: 1828/2000... Training loss: 0.2951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1828/2000... Training loss: 0.3908\n",
      "Epoch: 1828/2000... Training loss: 0.3939\n",
      "Epoch: 1828/2000... Training loss: 0.3923\n",
      "Epoch: 1828/2000... Training loss: 0.2640\n",
      "Epoch: 1828/2000... Training loss: 0.2949\n",
      "Epoch: 1828/2000... Training loss: 0.4723\n",
      "Epoch: 1828/2000... Training loss: 0.2759\n",
      "Epoch: 1828/2000... Training loss: 0.3478\n",
      "Epoch: 1828/2000... Training loss: 0.4559\n",
      "Epoch: 1828/2000... Training loss: 0.2837\n",
      "Epoch: 1828/2000... Training loss: 0.3478\n",
      "Epoch: 1828/2000... Training loss: 0.2621\n",
      "Epoch: 1828/2000... Training loss: 0.5377\n",
      "Epoch: 1828/2000... Training loss: 0.3542\n",
      "Epoch: 1828/2000... Training loss: 0.4324\n",
      "Epoch: 1828/2000... Training loss: 0.3826\n",
      "Epoch: 1828/2000... Training loss: 0.3631\n",
      "Epoch: 1828/2000... Training loss: 0.4160\n",
      "Epoch: 1828/2000... Training loss: 0.4113\n",
      "Epoch: 1828/2000... Training loss: 0.4637\n",
      "Epoch: 1828/2000... Training loss: 0.4334\n",
      "Epoch: 1828/2000... Training loss: 0.3553\n",
      "Epoch: 1828/2000... Training loss: 0.2396\n",
      "Epoch: 1828/2000... Training loss: 0.4255\n",
      "Epoch: 1828/2000... Training loss: 0.4947\n",
      "Epoch: 1828/2000... Training loss: 0.5095\n",
      "Epoch: 1828/2000... Training loss: 0.2938\n",
      "Epoch: 1828/2000... Training loss: 0.3088\n",
      "Epoch: 1828/2000... Training loss: 0.4901\n",
      "Epoch: 1829/2000... Training loss: 0.3497\n",
      "Epoch: 1829/2000... Training loss: 0.3747\n",
      "Epoch: 1829/2000... Training loss: 0.3578\n",
      "Epoch: 1829/2000... Training loss: 0.3759\n",
      "Epoch: 1829/2000... Training loss: 0.3628\n",
      "Epoch: 1829/2000... Training loss: 0.3477\n",
      "Epoch: 1829/2000... Training loss: 0.2613\n",
      "Epoch: 1829/2000... Training loss: 0.5185\n",
      "Epoch: 1829/2000... Training loss: 0.2936\n",
      "Epoch: 1829/2000... Training loss: 0.5111\n",
      "Epoch: 1829/2000... Training loss: 0.5047\n",
      "Epoch: 1829/2000... Training loss: 0.3946\n",
      "Epoch: 1829/2000... Training loss: 0.4428\n",
      "Epoch: 1829/2000... Training loss: 0.3934\n",
      "Epoch: 1829/2000... Training loss: 0.3127\n",
      "Epoch: 1829/2000... Training loss: 0.3389\n",
      "Epoch: 1829/2000... Training loss: 0.3278\n",
      "Epoch: 1829/2000... Training loss: 0.2327\n",
      "Epoch: 1829/2000... Training loss: 0.4187\n",
      "Epoch: 1829/2000... Training loss: 0.4452\n",
      "Epoch: 1829/2000... Training loss: 0.3081\n",
      "Epoch: 1829/2000... Training loss: 0.2109\n",
      "Epoch: 1829/2000... Training loss: 0.4707\n",
      "Epoch: 1829/2000... Training loss: 0.2796\n",
      "Epoch: 1829/2000... Training loss: 0.3083\n",
      "Epoch: 1829/2000... Training loss: 0.2642\n",
      "Epoch: 1829/2000... Training loss: 0.2181\n",
      "Epoch: 1829/2000... Training loss: 0.3220\n",
      "Epoch: 1829/2000... Training loss: 0.3787\n",
      "Epoch: 1829/2000... Training loss: 0.3797\n",
      "Epoch: 1829/2000... Training loss: 0.5666\n",
      "Epoch: 1830/2000... Training loss: 0.3493\n",
      "Epoch: 1830/2000... Training loss: 0.4147\n",
      "Epoch: 1830/2000... Training loss: 0.3853\n",
      "Epoch: 1830/2000... Training loss: 0.4522\n",
      "Epoch: 1830/2000... Training loss: 0.5492\n",
      "Epoch: 1830/2000... Training loss: 0.4136\n",
      "Epoch: 1830/2000... Training loss: 0.3207\n",
      "Epoch: 1830/2000... Training loss: 0.3024\n",
      "Epoch: 1830/2000... Training loss: 0.3530\n",
      "Epoch: 1830/2000... Training loss: 0.2156\n",
      "Epoch: 1830/2000... Training loss: 0.2465\n",
      "Epoch: 1830/2000... Training loss: 0.4710\n",
      "Epoch: 1830/2000... Training loss: 0.3297\n",
      "Epoch: 1830/2000... Training loss: 0.3044\n",
      "Epoch: 1830/2000... Training loss: 0.3238\n",
      "Epoch: 1830/2000... Training loss: 0.3668\n",
      "Epoch: 1830/2000... Training loss: 0.3772\n",
      "Epoch: 1830/2000... Training loss: 0.2729\n",
      "Epoch: 1830/2000... Training loss: 0.3667\n",
      "Epoch: 1830/2000... Training loss: 0.3140\n",
      "Epoch: 1830/2000... Training loss: 0.4612\n",
      "Epoch: 1830/2000... Training loss: 0.4689\n",
      "Epoch: 1830/2000... Training loss: 0.2746\n",
      "Epoch: 1830/2000... Training loss: 0.6325\n",
      "Epoch: 1830/2000... Training loss: 0.3904\n",
      "Epoch: 1830/2000... Training loss: 0.3988\n",
      "Epoch: 1830/2000... Training loss: 0.3139\n",
      "Epoch: 1830/2000... Training loss: 0.3167\n",
      "Epoch: 1830/2000... Training loss: 0.2624\n",
      "Epoch: 1830/2000... Training loss: 0.5233\n",
      "Epoch: 1830/2000... Training loss: 0.3544\n",
      "Epoch: 1831/2000... Training loss: 0.3224\n",
      "Epoch: 1831/2000... Training loss: 0.3541\n",
      "Epoch: 1831/2000... Training loss: 0.5594\n",
      "Epoch: 1831/2000... Training loss: 0.2403\n",
      "Epoch: 1831/2000... Training loss: 0.3055\n",
      "Epoch: 1831/2000... Training loss: 0.2942\n",
      "Epoch: 1831/2000... Training loss: 0.3463\n",
      "Epoch: 1831/2000... Training loss: 0.4320\n",
      "Epoch: 1831/2000... Training loss: 0.1436\n",
      "Epoch: 1831/2000... Training loss: 0.3820\n",
      "Epoch: 1831/2000... Training loss: 0.3730\n",
      "Epoch: 1831/2000... Training loss: 0.4033\n",
      "Epoch: 1831/2000... Training loss: 0.3911\n",
      "Epoch: 1831/2000... Training loss: 0.4135\n",
      "Epoch: 1831/2000... Training loss: 0.3852\n",
      "Epoch: 1831/2000... Training loss: 0.3173\n",
      "Epoch: 1831/2000... Training loss: 0.3332\n",
      "Epoch: 1831/2000... Training loss: 0.3802\n",
      "Epoch: 1831/2000... Training loss: 0.2563\n",
      "Epoch: 1831/2000... Training loss: 0.1663\n",
      "Epoch: 1831/2000... Training loss: 0.2611\n",
      "Epoch: 1831/2000... Training loss: 0.5385\n",
      "Epoch: 1831/2000... Training loss: 0.3724\n",
      "Epoch: 1831/2000... Training loss: 0.2457\n",
      "Epoch: 1831/2000... Training loss: 0.2732\n",
      "Epoch: 1831/2000... Training loss: 0.3302\n",
      "Epoch: 1831/2000... Training loss: 0.3119\n",
      "Epoch: 1831/2000... Training loss: 0.4479\n",
      "Epoch: 1831/2000... Training loss: 0.5131\n",
      "Epoch: 1831/2000... Training loss: 0.5303\n",
      "Epoch: 1831/2000... Training loss: 0.2080\n",
      "Epoch: 1832/2000... Training loss: 0.3529\n",
      "Epoch: 1832/2000... Training loss: 0.2860\n",
      "Epoch: 1832/2000... Training loss: 0.4639\n",
      "Epoch: 1832/2000... Training loss: 0.4700\n",
      "Epoch: 1832/2000... Training loss: 0.2638\n",
      "Epoch: 1832/2000... Training loss: 0.3049\n",
      "Epoch: 1832/2000... Training loss: 0.4686\n",
      "Epoch: 1832/2000... Training loss: 0.3652\n",
      "Epoch: 1832/2000... Training loss: 0.4708\n",
      "Epoch: 1832/2000... Training loss: 0.3814\n",
      "Epoch: 1832/2000... Training loss: 0.4874\n",
      "Epoch: 1832/2000... Training loss: 0.3688\n",
      "Epoch: 1832/2000... Training loss: 0.6336\n",
      "Epoch: 1832/2000... Training loss: 0.4023\n",
      "Epoch: 1832/2000... Training loss: 0.4042\n",
      "Epoch: 1832/2000... Training loss: 0.2709\n",
      "Epoch: 1832/2000... Training loss: 0.3650\n",
      "Epoch: 1832/2000... Training loss: 0.3256\n",
      "Epoch: 1832/2000... Training loss: 0.3946\n",
      "Epoch: 1832/2000... Training loss: 0.2572\n",
      "Epoch: 1832/2000... Training loss: 0.2568\n",
      "Epoch: 1832/2000... Training loss: 0.3578\n",
      "Epoch: 1832/2000... Training loss: 0.4414\n",
      "Epoch: 1832/2000... Training loss: 0.3519\n",
      "Epoch: 1832/2000... Training loss: 0.3541\n",
      "Epoch: 1832/2000... Training loss: 0.3164\n",
      "Epoch: 1832/2000... Training loss: 0.3461\n",
      "Epoch: 1832/2000... Training loss: 0.4352\n",
      "Epoch: 1832/2000... Training loss: 0.3523\n",
      "Epoch: 1832/2000... Training loss: 0.3516\n",
      "Epoch: 1832/2000... Training loss: 0.2913\n",
      "Epoch: 1833/2000... Training loss: 0.4735\n",
      "Epoch: 1833/2000... Training loss: 0.3741\n",
      "Epoch: 1833/2000... Training loss: 0.4360\n",
      "Epoch: 1833/2000... Training loss: 0.3693\n",
      "Epoch: 1833/2000... Training loss: 0.3101\n",
      "Epoch: 1833/2000... Training loss: 0.3879\n",
      "Epoch: 1833/2000... Training loss: 0.4126\n",
      "Epoch: 1833/2000... Training loss: 0.3307\n",
      "Epoch: 1833/2000... Training loss: 0.4535\n",
      "Epoch: 1833/2000... Training loss: 0.2758\n",
      "Epoch: 1833/2000... Training loss: 0.3096\n",
      "Epoch: 1833/2000... Training loss: 0.4016\n",
      "Epoch: 1833/2000... Training loss: 0.3612\n",
      "Epoch: 1833/2000... Training loss: 0.2878\n",
      "Epoch: 1833/2000... Training loss: 0.4744\n",
      "Epoch: 1833/2000... Training loss: 0.2825\n",
      "Epoch: 1833/2000... Training loss: 0.6116\n",
      "Epoch: 1833/2000... Training loss: 0.4140\n",
      "Epoch: 1833/2000... Training loss: 0.5261\n",
      "Epoch: 1833/2000... Training loss: 0.3168\n",
      "Epoch: 1833/2000... Training loss: 0.3568\n",
      "Epoch: 1833/2000... Training loss: 0.2651\n",
      "Epoch: 1833/2000... Training loss: 0.4140\n",
      "Epoch: 1833/2000... Training loss: 0.2014\n",
      "Epoch: 1833/2000... Training loss: 0.4092\n",
      "Epoch: 1833/2000... Training loss: 0.2896\n",
      "Epoch: 1833/2000... Training loss: 0.3932\n",
      "Epoch: 1833/2000... Training loss: 0.3191\n",
      "Epoch: 1833/2000... Training loss: 0.3243\n",
      "Epoch: 1833/2000... Training loss: 0.4529\n",
      "Epoch: 1833/2000... Training loss: 0.2929\n",
      "Epoch: 1834/2000... Training loss: 0.3024\n",
      "Epoch: 1834/2000... Training loss: 0.4183\n",
      "Epoch: 1834/2000... Training loss: 0.3515\n",
      "Epoch: 1834/2000... Training loss: 0.3503\n",
      "Epoch: 1834/2000... Training loss: 0.5576\n",
      "Epoch: 1834/2000... Training loss: 0.3989\n",
      "Epoch: 1834/2000... Training loss: 0.3400\n",
      "Epoch: 1834/2000... Training loss: 0.3508\n",
      "Epoch: 1834/2000... Training loss: 0.3760\n",
      "Epoch: 1834/2000... Training loss: 0.4859\n",
      "Epoch: 1834/2000... Training loss: 0.2704\n",
      "Epoch: 1834/2000... Training loss: 0.3324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1834/2000... Training loss: 0.4574\n",
      "Epoch: 1834/2000... Training loss: 0.2287\n",
      "Epoch: 1834/2000... Training loss: 0.4127\n",
      "Epoch: 1834/2000... Training loss: 0.4019\n",
      "Epoch: 1834/2000... Training loss: 0.3373\n",
      "Epoch: 1834/2000... Training loss: 0.4337\n",
      "Epoch: 1834/2000... Training loss: 0.3348\n",
      "Epoch: 1834/2000... Training loss: 0.3798\n",
      "Epoch: 1834/2000... Training loss: 0.3012\n",
      "Epoch: 1834/2000... Training loss: 0.2490\n",
      "Epoch: 1834/2000... Training loss: 0.4501\n",
      "Epoch: 1834/2000... Training loss: 0.3551\n",
      "Epoch: 1834/2000... Training loss: 0.2460\n",
      "Epoch: 1834/2000... Training loss: 0.3490\n",
      "Epoch: 1834/2000... Training loss: 0.4078\n",
      "Epoch: 1834/2000... Training loss: 0.4212\n",
      "Epoch: 1834/2000... Training loss: 0.5274\n",
      "Epoch: 1834/2000... Training loss: 0.5230\n",
      "Epoch: 1834/2000... Training loss: 0.3586\n",
      "Epoch: 1835/2000... Training loss: 0.4316\n",
      "Epoch: 1835/2000... Training loss: 0.3988\n",
      "Epoch: 1835/2000... Training loss: 0.4773\n",
      "Epoch: 1835/2000... Training loss: 0.3246\n",
      "Epoch: 1835/2000... Training loss: 0.4848\n",
      "Epoch: 1835/2000... Training loss: 0.3213\n",
      "Epoch: 1835/2000... Training loss: 0.4184\n",
      "Epoch: 1835/2000... Training loss: 0.3948\n",
      "Epoch: 1835/2000... Training loss: 0.2797\n",
      "Epoch: 1835/2000... Training loss: 0.2504\n",
      "Epoch: 1835/2000... Training loss: 0.4448\n",
      "Epoch: 1835/2000... Training loss: 0.3627\n",
      "Epoch: 1835/2000... Training loss: 0.3261\n",
      "Epoch: 1835/2000... Training loss: 0.3661\n",
      "Epoch: 1835/2000... Training loss: 0.3355\n",
      "Epoch: 1835/2000... Training loss: 0.3817\n",
      "Epoch: 1835/2000... Training loss: 0.3867\n",
      "Epoch: 1835/2000... Training loss: 0.4681\n",
      "Epoch: 1835/2000... Training loss: 0.5645\n",
      "Epoch: 1835/2000... Training loss: 0.3132\n",
      "Epoch: 1835/2000... Training loss: 0.4171\n",
      "Epoch: 1835/2000... Training loss: 0.5307\n",
      "Epoch: 1835/2000... Training loss: 0.3453\n",
      "Epoch: 1835/2000... Training loss: 0.4922\n",
      "Epoch: 1835/2000... Training loss: 0.4235\n",
      "Epoch: 1835/2000... Training loss: 0.4011\n",
      "Epoch: 1835/2000... Training loss: 0.3265\n",
      "Epoch: 1835/2000... Training loss: 0.4891\n",
      "Epoch: 1835/2000... Training loss: 0.3859\n",
      "Epoch: 1835/2000... Training loss: 0.3529\n",
      "Epoch: 1835/2000... Training loss: 0.2872\n",
      "Epoch: 1836/2000... Training loss: 0.3290\n",
      "Epoch: 1836/2000... Training loss: 0.3796\n",
      "Epoch: 1836/2000... Training loss: 0.3735\n",
      "Epoch: 1836/2000... Training loss: 0.3470\n",
      "Epoch: 1836/2000... Training loss: 0.4217\n",
      "Epoch: 1836/2000... Training loss: 0.5795\n",
      "Epoch: 1836/2000... Training loss: 0.5565\n",
      "Epoch: 1836/2000... Training loss: 0.3420\n",
      "Epoch: 1836/2000... Training loss: 0.3796\n",
      "Epoch: 1836/2000... Training loss: 0.4456\n",
      "Epoch: 1836/2000... Training loss: 0.2857\n",
      "Epoch: 1836/2000... Training loss: 0.3097\n",
      "Epoch: 1836/2000... Training loss: 0.4875\n",
      "Epoch: 1836/2000... Training loss: 0.1813\n",
      "Epoch: 1836/2000... Training loss: 0.4574\n",
      "Epoch: 1836/2000... Training loss: 0.3580\n",
      "Epoch: 1836/2000... Training loss: 0.3469\n",
      "Epoch: 1836/2000... Training loss: 0.3729\n",
      "Epoch: 1836/2000... Training loss: 0.4764\n",
      "Epoch: 1836/2000... Training loss: 0.3345\n",
      "Epoch: 1836/2000... Training loss: 0.4099\n",
      "Epoch: 1836/2000... Training loss: 0.3162\n",
      "Epoch: 1836/2000... Training loss: 0.3982\n",
      "Epoch: 1836/2000... Training loss: 0.3050\n",
      "Epoch: 1836/2000... Training loss: 0.3854\n",
      "Epoch: 1836/2000... Training loss: 0.2610\n",
      "Epoch: 1836/2000... Training loss: 0.3832\n",
      "Epoch: 1836/2000... Training loss: 0.2546\n",
      "Epoch: 1836/2000... Training loss: 0.4745\n",
      "Epoch: 1836/2000... Training loss: 0.4490\n",
      "Epoch: 1836/2000... Training loss: 0.5309\n",
      "Epoch: 1837/2000... Training loss: 0.3640\n",
      "Epoch: 1837/2000... Training loss: 0.4818\n",
      "Epoch: 1837/2000... Training loss: 0.2499\n",
      "Epoch: 1837/2000... Training loss: 0.3710\n",
      "Epoch: 1837/2000... Training loss: 0.4023\n",
      "Epoch: 1837/2000... Training loss: 0.4291\n",
      "Epoch: 1837/2000... Training loss: 0.2821\n",
      "Epoch: 1837/2000... Training loss: 0.3771\n",
      "Epoch: 1837/2000... Training loss: 0.4635\n",
      "Epoch: 1837/2000... Training loss: 0.4938\n",
      "Epoch: 1837/2000... Training loss: 0.5327\n",
      "Epoch: 1837/2000... Training loss: 0.5181\n",
      "Epoch: 1837/2000... Training loss: 0.5273\n",
      "Epoch: 1837/2000... Training loss: 0.3297\n",
      "Epoch: 1837/2000... Training loss: 0.4247\n",
      "Epoch: 1837/2000... Training loss: 0.3069\n",
      "Epoch: 1837/2000... Training loss: 0.3765\n",
      "Epoch: 1837/2000... Training loss: 0.2584\n",
      "Epoch: 1837/2000... Training loss: 0.3349\n",
      "Epoch: 1837/2000... Training loss: 0.3351\n",
      "Epoch: 1837/2000... Training loss: 0.3443\n",
      "Epoch: 1837/2000... Training loss: 0.3011\n",
      "Epoch: 1837/2000... Training loss: 0.3878\n",
      "Epoch: 1837/2000... Training loss: 0.5261\n",
      "Epoch: 1837/2000... Training loss: 0.4446\n",
      "Epoch: 1837/2000... Training loss: 0.2865\n",
      "Epoch: 1837/2000... Training loss: 0.4530\n",
      "Epoch: 1837/2000... Training loss: 0.4513\n",
      "Epoch: 1837/2000... Training loss: 0.4689\n",
      "Epoch: 1837/2000... Training loss: 0.3159\n",
      "Epoch: 1837/2000... Training loss: 0.4366\n",
      "Epoch: 1838/2000... Training loss: 0.5053\n",
      "Epoch: 1838/2000... Training loss: 0.2853\n",
      "Epoch: 1838/2000... Training loss: 0.4897\n",
      "Epoch: 1838/2000... Training loss: 0.3482\n",
      "Epoch: 1838/2000... Training loss: 0.3163\n",
      "Epoch: 1838/2000... Training loss: 0.3345\n",
      "Epoch: 1838/2000... Training loss: 0.4889\n",
      "Epoch: 1838/2000... Training loss: 0.2860\n",
      "Epoch: 1838/2000... Training loss: 0.2154\n",
      "Epoch: 1838/2000... Training loss: 0.2874\n",
      "Epoch: 1838/2000... Training loss: 0.5069\n",
      "Epoch: 1838/2000... Training loss: 0.3446\n",
      "Epoch: 1838/2000... Training loss: 0.3549\n",
      "Epoch: 1838/2000... Training loss: 0.3717\n",
      "Epoch: 1838/2000... Training loss: 0.3177\n",
      "Epoch: 1838/2000... Training loss: 0.2758\n",
      "Epoch: 1838/2000... Training loss: 0.3738\n",
      "Epoch: 1838/2000... Training loss: 0.3786\n",
      "Epoch: 1838/2000... Training loss: 0.5648\n",
      "Epoch: 1838/2000... Training loss: 0.4014\n",
      "Epoch: 1838/2000... Training loss: 0.3055\n",
      "Epoch: 1838/2000... Training loss: 0.3058\n",
      "Epoch: 1838/2000... Training loss: 0.3905\n",
      "Epoch: 1838/2000... Training loss: 0.5329\n",
      "Epoch: 1838/2000... Training loss: 0.3751\n",
      "Epoch: 1838/2000... Training loss: 0.3439\n",
      "Epoch: 1838/2000... Training loss: 0.5307\n",
      "Epoch: 1838/2000... Training loss: 0.3944\n",
      "Epoch: 1838/2000... Training loss: 0.3556\n",
      "Epoch: 1838/2000... Training loss: 0.2926\n",
      "Epoch: 1838/2000... Training loss: 0.2711\n",
      "Epoch: 1839/2000... Training loss: 0.3778\n",
      "Epoch: 1839/2000... Training loss: 0.4555\n",
      "Epoch: 1839/2000... Training loss: 0.5582\n",
      "Epoch: 1839/2000... Training loss: 0.5912\n",
      "Epoch: 1839/2000... Training loss: 0.4072\n",
      "Epoch: 1839/2000... Training loss: 0.4575\n",
      "Epoch: 1839/2000... Training loss: 0.3816\n",
      "Epoch: 1839/2000... Training loss: 0.3400\n",
      "Epoch: 1839/2000... Training loss: 0.4834\n",
      "Epoch: 1839/2000... Training loss: 0.3530\n",
      "Epoch: 1839/2000... Training loss: 0.5262\n",
      "Epoch: 1839/2000... Training loss: 0.4067\n",
      "Epoch: 1839/2000... Training loss: 0.4651\n",
      "Epoch: 1839/2000... Training loss: 0.3227\n",
      "Epoch: 1839/2000... Training loss: 0.3534\n",
      "Epoch: 1839/2000... Training loss: 0.2369\n",
      "Epoch: 1839/2000... Training loss: 0.3593\n",
      "Epoch: 1839/2000... Training loss: 0.3822\n",
      "Epoch: 1839/2000... Training loss: 0.2151\n",
      "Epoch: 1839/2000... Training loss: 0.5342\n",
      "Epoch: 1839/2000... Training loss: 0.3046\n",
      "Epoch: 1839/2000... Training loss: 0.3978\n",
      "Epoch: 1839/2000... Training loss: 0.2578\n",
      "Epoch: 1839/2000... Training loss: 0.3032\n",
      "Epoch: 1839/2000... Training loss: 0.4016\n",
      "Epoch: 1839/2000... Training loss: 0.4222\n",
      "Epoch: 1839/2000... Training loss: 0.3189\n",
      "Epoch: 1839/2000... Training loss: 0.3864\n",
      "Epoch: 1839/2000... Training loss: 0.2689\n",
      "Epoch: 1839/2000... Training loss: 0.3896\n",
      "Epoch: 1839/2000... Training loss: 0.3336\n",
      "Epoch: 1840/2000... Training loss: 0.3500\n",
      "Epoch: 1840/2000... Training loss: 0.2708\n",
      "Epoch: 1840/2000... Training loss: 0.4135\n",
      "Epoch: 1840/2000... Training loss: 0.3217\n",
      "Epoch: 1840/2000... Training loss: 0.3129\n",
      "Epoch: 1840/2000... Training loss: 0.2905\n",
      "Epoch: 1840/2000... Training loss: 0.4373\n",
      "Epoch: 1840/2000... Training loss: 0.3505\n",
      "Epoch: 1840/2000... Training loss: 0.3508\n",
      "Epoch: 1840/2000... Training loss: 0.3841\n",
      "Epoch: 1840/2000... Training loss: 0.2805\n",
      "Epoch: 1840/2000... Training loss: 0.3725\n",
      "Epoch: 1840/2000... Training loss: 0.2668\n",
      "Epoch: 1840/2000... Training loss: 0.4855\n",
      "Epoch: 1840/2000... Training loss: 0.4197\n",
      "Epoch: 1840/2000... Training loss: 0.3195\n",
      "Epoch: 1840/2000... Training loss: 0.4783\n",
      "Epoch: 1840/2000... Training loss: 0.3196\n",
      "Epoch: 1840/2000... Training loss: 0.4574\n",
      "Epoch: 1840/2000... Training loss: 0.4116\n",
      "Epoch: 1840/2000... Training loss: 0.2787\n",
      "Epoch: 1840/2000... Training loss: 0.4002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1840/2000... Training loss: 0.4121\n",
      "Epoch: 1840/2000... Training loss: 0.4183\n",
      "Epoch: 1840/2000... Training loss: 0.3857\n",
      "Epoch: 1840/2000... Training loss: 0.5053\n",
      "Epoch: 1840/2000... Training loss: 0.3999\n",
      "Epoch: 1840/2000... Training loss: 0.3224\n",
      "Epoch: 1840/2000... Training loss: 0.5388\n",
      "Epoch: 1840/2000... Training loss: 0.2339\n",
      "Epoch: 1840/2000... Training loss: 0.3529\n",
      "Epoch: 1841/2000... Training loss: 0.3557\n",
      "Epoch: 1841/2000... Training loss: 0.3441\n",
      "Epoch: 1841/2000... Training loss: 0.4160\n",
      "Epoch: 1841/2000... Training loss: 0.3360\n",
      "Epoch: 1841/2000... Training loss: 0.4284\n",
      "Epoch: 1841/2000... Training loss: 0.5176\n",
      "Epoch: 1841/2000... Training loss: 0.4814\n",
      "Epoch: 1841/2000... Training loss: 0.3067\n",
      "Epoch: 1841/2000... Training loss: 0.2407\n",
      "Epoch: 1841/2000... Training loss: 0.3464\n",
      "Epoch: 1841/2000... Training loss: 0.3588\n",
      "Epoch: 1841/2000... Training loss: 0.4188\n",
      "Epoch: 1841/2000... Training loss: 0.3638\n",
      "Epoch: 1841/2000... Training loss: 0.3095\n",
      "Epoch: 1841/2000... Training loss: 0.4712\n",
      "Epoch: 1841/2000... Training loss: 0.4219\n",
      "Epoch: 1841/2000... Training loss: 0.3870\n",
      "Epoch: 1841/2000... Training loss: 0.5245\n",
      "Epoch: 1841/2000... Training loss: 0.2870\n",
      "Epoch: 1841/2000... Training loss: 0.3516\n",
      "Epoch: 1841/2000... Training loss: 0.4354\n",
      "Epoch: 1841/2000... Training loss: 0.2018\n",
      "Epoch: 1841/2000... Training loss: 0.1949\n",
      "Epoch: 1841/2000... Training loss: 0.3492\n",
      "Epoch: 1841/2000... Training loss: 0.2729\n",
      "Epoch: 1841/2000... Training loss: 0.3550\n",
      "Epoch: 1841/2000... Training loss: 0.4172\n",
      "Epoch: 1841/2000... Training loss: 0.2801\n",
      "Epoch: 1841/2000... Training loss: 0.2262\n",
      "Epoch: 1841/2000... Training loss: 0.4093\n",
      "Epoch: 1841/2000... Training loss: 0.5408\n",
      "Epoch: 1842/2000... Training loss: 0.2504\n",
      "Epoch: 1842/2000... Training loss: 0.4057\n",
      "Epoch: 1842/2000... Training loss: 0.4584\n",
      "Epoch: 1842/2000... Training loss: 0.3888\n",
      "Epoch: 1842/2000... Training loss: 0.3689\n",
      "Epoch: 1842/2000... Training loss: 0.3193\n",
      "Epoch: 1842/2000... Training loss: 0.4403\n",
      "Epoch: 1842/2000... Training loss: 0.3162\n",
      "Epoch: 1842/2000... Training loss: 0.2850\n",
      "Epoch: 1842/2000... Training loss: 0.3618\n",
      "Epoch: 1842/2000... Training loss: 0.4056\n",
      "Epoch: 1842/2000... Training loss: 0.4084\n",
      "Epoch: 1842/2000... Training loss: 0.4706\n",
      "Epoch: 1842/2000... Training loss: 0.4986\n",
      "Epoch: 1842/2000... Training loss: 0.3548\n",
      "Epoch: 1842/2000... Training loss: 0.3305\n",
      "Epoch: 1842/2000... Training loss: 0.3407\n",
      "Epoch: 1842/2000... Training loss: 0.2615\n",
      "Epoch: 1842/2000... Training loss: 0.4737\n",
      "Epoch: 1842/2000... Training loss: 0.3007\n",
      "Epoch: 1842/2000... Training loss: 0.2513\n",
      "Epoch: 1842/2000... Training loss: 0.4620\n",
      "Epoch: 1842/2000... Training loss: 0.3400\n",
      "Epoch: 1842/2000... Training loss: 0.2906\n",
      "Epoch: 1842/2000... Training loss: 0.2661\n",
      "Epoch: 1842/2000... Training loss: 0.3174\n",
      "Epoch: 1842/2000... Training loss: 0.2856\n",
      "Epoch: 1842/2000... Training loss: 0.3696\n",
      "Epoch: 1842/2000... Training loss: 0.4896\n",
      "Epoch: 1842/2000... Training loss: 0.3985\n",
      "Epoch: 1842/2000... Training loss: 0.4985\n",
      "Epoch: 1843/2000... Training loss: 0.1773\n",
      "Epoch: 1843/2000... Training loss: 0.2592\n",
      "Epoch: 1843/2000... Training loss: 0.2888\n",
      "Epoch: 1843/2000... Training loss: 0.3108\n",
      "Epoch: 1843/2000... Training loss: 0.4906\n",
      "Epoch: 1843/2000... Training loss: 0.4179\n",
      "Epoch: 1843/2000... Training loss: 0.5754\n",
      "Epoch: 1843/2000... Training loss: 0.2198\n",
      "Epoch: 1843/2000... Training loss: 0.3874\n",
      "Epoch: 1843/2000... Training loss: 0.2147\n",
      "Epoch: 1843/2000... Training loss: 0.4169\n",
      "Epoch: 1843/2000... Training loss: 0.5795\n",
      "Epoch: 1843/2000... Training loss: 0.3766\n",
      "Epoch: 1843/2000... Training loss: 0.3468\n",
      "Epoch: 1843/2000... Training loss: 0.3760\n",
      "Epoch: 1843/2000... Training loss: 0.3064\n",
      "Epoch: 1843/2000... Training loss: 0.2359\n",
      "Epoch: 1843/2000... Training loss: 0.3658\n",
      "Epoch: 1843/2000... Training loss: 0.3075\n",
      "Epoch: 1843/2000... Training loss: 0.3469\n",
      "Epoch: 1843/2000... Training loss: 0.4547\n",
      "Epoch: 1843/2000... Training loss: 0.2604\n",
      "Epoch: 1843/2000... Training loss: 0.2841\n",
      "Epoch: 1843/2000... Training loss: 0.3906\n",
      "Epoch: 1843/2000... Training loss: 0.2619\n",
      "Epoch: 1843/2000... Training loss: 0.2485\n",
      "Epoch: 1843/2000... Training loss: 0.3978\n",
      "Epoch: 1843/2000... Training loss: 0.4310\n",
      "Epoch: 1843/2000... Training loss: 0.3978\n",
      "Epoch: 1843/2000... Training loss: 0.4587\n",
      "Epoch: 1843/2000... Training loss: 0.3155\n",
      "Epoch: 1844/2000... Training loss: 0.3130\n",
      "Epoch: 1844/2000... Training loss: 0.3701\n",
      "Epoch: 1844/2000... Training loss: 0.2298\n",
      "Epoch: 1844/2000... Training loss: 0.4441\n",
      "Epoch: 1844/2000... Training loss: 0.3652\n",
      "Epoch: 1844/2000... Training loss: 0.5536\n",
      "Epoch: 1844/2000... Training loss: 0.4039\n",
      "Epoch: 1844/2000... Training loss: 0.3233\n",
      "Epoch: 1844/2000... Training loss: 0.4527\n",
      "Epoch: 1844/2000... Training loss: 0.4051\n",
      "Epoch: 1844/2000... Training loss: 0.4076\n",
      "Epoch: 1844/2000... Training loss: 0.4066\n",
      "Epoch: 1844/2000... Training loss: 0.2219\n",
      "Epoch: 1844/2000... Training loss: 0.4646\n",
      "Epoch: 1844/2000... Training loss: 0.3682\n",
      "Epoch: 1844/2000... Training loss: 0.4275\n",
      "Epoch: 1844/2000... Training loss: 0.4316\n",
      "Epoch: 1844/2000... Training loss: 0.2876\n",
      "Epoch: 1844/2000... Training loss: 0.4006\n",
      "Epoch: 1844/2000... Training loss: 0.2221\n",
      "Epoch: 1844/2000... Training loss: 0.2363\n",
      "Epoch: 1844/2000... Training loss: 0.3721\n",
      "Epoch: 1844/2000... Training loss: 0.3538\n",
      "Epoch: 1844/2000... Training loss: 0.3641\n",
      "Epoch: 1844/2000... Training loss: 0.3654\n",
      "Epoch: 1844/2000... Training loss: 0.4728\n",
      "Epoch: 1844/2000... Training loss: 0.3775\n",
      "Epoch: 1844/2000... Training loss: 0.4142\n",
      "Epoch: 1844/2000... Training loss: 0.3249\n",
      "Epoch: 1844/2000... Training loss: 0.2417\n",
      "Epoch: 1844/2000... Training loss: 0.3919\n",
      "Epoch: 1845/2000... Training loss: 0.4021\n",
      "Epoch: 1845/2000... Training loss: 0.3273\n",
      "Epoch: 1845/2000... Training loss: 0.2280\n",
      "Epoch: 1845/2000... Training loss: 0.3708\n",
      "Epoch: 1845/2000... Training loss: 0.2482\n",
      "Epoch: 1845/2000... Training loss: 0.4041\n",
      "Epoch: 1845/2000... Training loss: 0.3877\n",
      "Epoch: 1845/2000... Training loss: 0.4122\n",
      "Epoch: 1845/2000... Training loss: 0.2968\n",
      "Epoch: 1845/2000... Training loss: 0.4290\n",
      "Epoch: 1845/2000... Training loss: 0.2405\n",
      "Epoch: 1845/2000... Training loss: 0.3725\n",
      "Epoch: 1845/2000... Training loss: 0.4029\n",
      "Epoch: 1845/2000... Training loss: 0.5027\n",
      "Epoch: 1845/2000... Training loss: 0.5635\n",
      "Epoch: 1845/2000... Training loss: 0.3495\n",
      "Epoch: 1845/2000... Training loss: 0.3067\n",
      "Epoch: 1845/2000... Training loss: 0.2691\n",
      "Epoch: 1845/2000... Training loss: 0.4318\n",
      "Epoch: 1845/2000... Training loss: 0.3401\n",
      "Epoch: 1845/2000... Training loss: 0.3498\n",
      "Epoch: 1845/2000... Training loss: 0.4250\n",
      "Epoch: 1845/2000... Training loss: 0.4503\n",
      "Epoch: 1845/2000... Training loss: 0.3215\n",
      "Epoch: 1845/2000... Training loss: 0.3947\n",
      "Epoch: 1845/2000... Training loss: 0.1988\n",
      "Epoch: 1845/2000... Training loss: 0.3098\n",
      "Epoch: 1845/2000... Training loss: 0.4589\n",
      "Epoch: 1845/2000... Training loss: 0.2592\n",
      "Epoch: 1845/2000... Training loss: 0.4404\n",
      "Epoch: 1845/2000... Training loss: 0.4258\n",
      "Epoch: 1846/2000... Training loss: 0.2746\n",
      "Epoch: 1846/2000... Training loss: 0.3936\n",
      "Epoch: 1846/2000... Training loss: 0.3565\n",
      "Epoch: 1846/2000... Training loss: 0.3443\n",
      "Epoch: 1846/2000... Training loss: 0.4596\n",
      "Epoch: 1846/2000... Training loss: 0.5130\n",
      "Epoch: 1846/2000... Training loss: 0.4766\n",
      "Epoch: 1846/2000... Training loss: 0.2381\n",
      "Epoch: 1846/2000... Training loss: 0.3041\n",
      "Epoch: 1846/2000... Training loss: 0.4243\n",
      "Epoch: 1846/2000... Training loss: 0.6509\n",
      "Epoch: 1846/2000... Training loss: 0.3391\n",
      "Epoch: 1846/2000... Training loss: 0.5010\n",
      "Epoch: 1846/2000... Training loss: 0.2616\n",
      "Epoch: 1846/2000... Training loss: 0.3590\n",
      "Epoch: 1846/2000... Training loss: 0.4800\n",
      "Epoch: 1846/2000... Training loss: 0.4972\n",
      "Epoch: 1846/2000... Training loss: 0.4053\n",
      "Epoch: 1846/2000... Training loss: 0.5251\n",
      "Epoch: 1846/2000... Training loss: 0.4312\n",
      "Epoch: 1846/2000... Training loss: 0.4544\n",
      "Epoch: 1846/2000... Training loss: 0.3662\n",
      "Epoch: 1846/2000... Training loss: 0.4364\n",
      "Epoch: 1846/2000... Training loss: 0.4936\n",
      "Epoch: 1846/2000... Training loss: 0.3902\n",
      "Epoch: 1846/2000... Training loss: 0.3850\n",
      "Epoch: 1846/2000... Training loss: 0.3992\n",
      "Epoch: 1846/2000... Training loss: 0.2799\n",
      "Epoch: 1846/2000... Training loss: 0.5294\n",
      "Epoch: 1846/2000... Training loss: 0.4762\n",
      "Epoch: 1846/2000... Training loss: 0.4429\n",
      "Epoch: 1847/2000... Training loss: 0.4855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1847/2000... Training loss: 0.5184\n",
      "Epoch: 1847/2000... Training loss: 0.3508\n",
      "Epoch: 1847/2000... Training loss: 0.3030\n",
      "Epoch: 1847/2000... Training loss: 0.4204\n",
      "Epoch: 1847/2000... Training loss: 0.2623\n",
      "Epoch: 1847/2000... Training loss: 0.4518\n",
      "Epoch: 1847/2000... Training loss: 0.4249\n",
      "Epoch: 1847/2000... Training loss: 0.1442\n",
      "Epoch: 1847/2000... Training loss: 0.4622\n",
      "Epoch: 1847/2000... Training loss: 0.2856\n",
      "Epoch: 1847/2000... Training loss: 0.4078\n",
      "Epoch: 1847/2000... Training loss: 0.4820\n",
      "Epoch: 1847/2000... Training loss: 0.4070\n",
      "Epoch: 1847/2000... Training loss: 0.3494\n",
      "Epoch: 1847/2000... Training loss: 0.6285\n",
      "Epoch: 1847/2000... Training loss: 0.4636\n",
      "Epoch: 1847/2000... Training loss: 0.5892\n",
      "Epoch: 1847/2000... Training loss: 0.3982\n",
      "Epoch: 1847/2000... Training loss: 0.2800\n",
      "Epoch: 1847/2000... Training loss: 0.2701\n",
      "Epoch: 1847/2000... Training loss: 0.3714\n",
      "Epoch: 1847/2000... Training loss: 0.4774\n",
      "Epoch: 1847/2000... Training loss: 0.4354\n",
      "Epoch: 1847/2000... Training loss: 0.2618\n",
      "Epoch: 1847/2000... Training loss: 0.3088\n",
      "Epoch: 1847/2000... Training loss: 0.3830\n",
      "Epoch: 1847/2000... Training loss: 0.4114\n",
      "Epoch: 1847/2000... Training loss: 0.3503\n",
      "Epoch: 1847/2000... Training loss: 0.2048\n",
      "Epoch: 1847/2000... Training loss: 0.5678\n",
      "Epoch: 1848/2000... Training loss: 0.4040\n",
      "Epoch: 1848/2000... Training loss: 0.3143\n",
      "Epoch: 1848/2000... Training loss: 0.3882\n",
      "Epoch: 1848/2000... Training loss: 0.4788\n",
      "Epoch: 1848/2000... Training loss: 0.4171\n",
      "Epoch: 1848/2000... Training loss: 0.3503\n",
      "Epoch: 1848/2000... Training loss: 0.4563\n",
      "Epoch: 1848/2000... Training loss: 0.3050\n",
      "Epoch: 1848/2000... Training loss: 0.4197\n",
      "Epoch: 1848/2000... Training loss: 0.2941\n",
      "Epoch: 1848/2000... Training loss: 0.4520\n",
      "Epoch: 1848/2000... Training loss: 0.2802\n",
      "Epoch: 1848/2000... Training loss: 0.2500\n",
      "Epoch: 1848/2000... Training loss: 0.3020\n",
      "Epoch: 1848/2000... Training loss: 0.3607\n",
      "Epoch: 1848/2000... Training loss: 0.3334\n",
      "Epoch: 1848/2000... Training loss: 0.3566\n",
      "Epoch: 1848/2000... Training loss: 0.3143\n",
      "Epoch: 1848/2000... Training loss: 0.3986\n",
      "Epoch: 1848/2000... Training loss: 0.2452\n",
      "Epoch: 1848/2000... Training loss: 0.4094\n",
      "Epoch: 1848/2000... Training loss: 0.4125\n",
      "Epoch: 1848/2000... Training loss: 0.2052\n",
      "Epoch: 1848/2000... Training loss: 0.4277\n",
      "Epoch: 1848/2000... Training loss: 0.2645\n",
      "Epoch: 1848/2000... Training loss: 0.5950\n",
      "Epoch: 1848/2000... Training loss: 0.2420\n",
      "Epoch: 1848/2000... Training loss: 0.2628\n",
      "Epoch: 1848/2000... Training loss: 0.2327\n",
      "Epoch: 1848/2000... Training loss: 0.3948\n",
      "Epoch: 1848/2000... Training loss: 0.3739\n",
      "Epoch: 1849/2000... Training loss: 0.3803\n",
      "Epoch: 1849/2000... Training loss: 0.4823\n",
      "Epoch: 1849/2000... Training loss: 0.3688\n",
      "Epoch: 1849/2000... Training loss: 0.3319\n",
      "Epoch: 1849/2000... Training loss: 0.5090\n",
      "Epoch: 1849/2000... Training loss: 0.5495\n",
      "Epoch: 1849/2000... Training loss: 0.4583\n",
      "Epoch: 1849/2000... Training loss: 0.4135\n",
      "Epoch: 1849/2000... Training loss: 0.2905\n",
      "Epoch: 1849/2000... Training loss: 0.2766\n",
      "Epoch: 1849/2000... Training loss: 0.4047\n",
      "Epoch: 1849/2000... Training loss: 0.4206\n",
      "Epoch: 1849/2000... Training loss: 0.3249\n",
      "Epoch: 1849/2000... Training loss: 0.3689\n",
      "Epoch: 1849/2000... Training loss: 0.5334\n",
      "Epoch: 1849/2000... Training loss: 0.2985\n",
      "Epoch: 1849/2000... Training loss: 0.3053\n",
      "Epoch: 1849/2000... Training loss: 0.3135\n",
      "Epoch: 1849/2000... Training loss: 0.3373\n",
      "Epoch: 1849/2000... Training loss: 0.2840\n",
      "Epoch: 1849/2000... Training loss: 0.3178\n",
      "Epoch: 1849/2000... Training loss: 0.3737\n",
      "Epoch: 1849/2000... Training loss: 0.3795\n",
      "Epoch: 1849/2000... Training loss: 0.5138\n",
      "Epoch: 1849/2000... Training loss: 0.4549\n",
      "Epoch: 1849/2000... Training loss: 0.3302\n",
      "Epoch: 1849/2000... Training loss: 0.3996\n",
      "Epoch: 1849/2000... Training loss: 0.2892\n",
      "Epoch: 1849/2000... Training loss: 0.3300\n",
      "Epoch: 1849/2000... Training loss: 0.3658\n",
      "Epoch: 1849/2000... Training loss: 0.1694\n",
      "Epoch: 1850/2000... Training loss: 0.6030\n",
      "Epoch: 1850/2000... Training loss: 0.4926\n",
      "Epoch: 1850/2000... Training loss: 0.3070\n",
      "Epoch: 1850/2000... Training loss: 0.4323\n",
      "Epoch: 1850/2000... Training loss: 0.3059\n",
      "Epoch: 1850/2000... Training loss: 0.4267\n",
      "Epoch: 1850/2000... Training loss: 0.4848\n",
      "Epoch: 1850/2000... Training loss: 0.3204\n",
      "Epoch: 1850/2000... Training loss: 0.3910\n",
      "Epoch: 1850/2000... Training loss: 0.3461\n",
      "Epoch: 1850/2000... Training loss: 0.3749\n",
      "Epoch: 1850/2000... Training loss: 0.3317\n",
      "Epoch: 1850/2000... Training loss: 0.4562\n",
      "Epoch: 1850/2000... Training loss: 0.4351\n",
      "Epoch: 1850/2000... Training loss: 0.3238\n",
      "Epoch: 1850/2000... Training loss: 0.3644\n",
      "Epoch: 1850/2000... Training loss: 0.3749\n",
      "Epoch: 1850/2000... Training loss: 0.2098\n",
      "Epoch: 1850/2000... Training loss: 0.4406\n",
      "Epoch: 1850/2000... Training loss: 0.4448\n",
      "Epoch: 1850/2000... Training loss: 0.3351\n",
      "Epoch: 1850/2000... Training loss: 0.3147\n",
      "Epoch: 1850/2000... Training loss: 0.3903\n",
      "Epoch: 1850/2000... Training loss: 0.4600\n",
      "Epoch: 1850/2000... Training loss: 0.2879\n",
      "Epoch: 1850/2000... Training loss: 0.3078\n",
      "Epoch: 1850/2000... Training loss: 0.3637\n",
      "Epoch: 1850/2000... Training loss: 0.2933\n",
      "Epoch: 1850/2000... Training loss: 0.3496\n",
      "Epoch: 1850/2000... Training loss: 0.3013\n",
      "Epoch: 1850/2000... Training loss: 0.3452\n",
      "Epoch: 1851/2000... Training loss: 0.3884\n",
      "Epoch: 1851/2000... Training loss: 0.3023\n",
      "Epoch: 1851/2000... Training loss: 0.3089\n",
      "Epoch: 1851/2000... Training loss: 0.2911\n",
      "Epoch: 1851/2000... Training loss: 0.3073\n",
      "Epoch: 1851/2000... Training loss: 0.4291\n",
      "Epoch: 1851/2000... Training loss: 0.4056\n",
      "Epoch: 1851/2000... Training loss: 0.3518\n",
      "Epoch: 1851/2000... Training loss: 0.3016\n",
      "Epoch: 1851/2000... Training loss: 0.2734\n",
      "Epoch: 1851/2000... Training loss: 0.3786\n",
      "Epoch: 1851/2000... Training loss: 0.3260\n",
      "Epoch: 1851/2000... Training loss: 0.3456\n",
      "Epoch: 1851/2000... Training loss: 0.3971\n",
      "Epoch: 1851/2000... Training loss: 0.2696\n",
      "Epoch: 1851/2000... Training loss: 0.2947\n",
      "Epoch: 1851/2000... Training loss: 0.3920\n",
      "Epoch: 1851/2000... Training loss: 0.2614\n",
      "Epoch: 1851/2000... Training loss: 0.4011\n",
      "Epoch: 1851/2000... Training loss: 0.3868\n",
      "Epoch: 1851/2000... Training loss: 0.3250\n",
      "Epoch: 1851/2000... Training loss: 0.3976\n",
      "Epoch: 1851/2000... Training loss: 0.4752\n",
      "Epoch: 1851/2000... Training loss: 0.3584\n",
      "Epoch: 1851/2000... Training loss: 0.3429\n",
      "Epoch: 1851/2000... Training loss: 0.3047\n",
      "Epoch: 1851/2000... Training loss: 0.5363\n",
      "Epoch: 1851/2000... Training loss: 0.4305\n",
      "Epoch: 1851/2000... Training loss: 0.3053\n",
      "Epoch: 1851/2000... Training loss: 0.3516\n",
      "Epoch: 1851/2000... Training loss: 0.2684\n",
      "Epoch: 1852/2000... Training loss: 0.2562\n",
      "Epoch: 1852/2000... Training loss: 0.2519\n",
      "Epoch: 1852/2000... Training loss: 0.4207\n",
      "Epoch: 1852/2000... Training loss: 0.2924\n",
      "Epoch: 1852/2000... Training loss: 0.4168\n",
      "Epoch: 1852/2000... Training loss: 0.5369\n",
      "Epoch: 1852/2000... Training loss: 0.4831\n",
      "Epoch: 1852/2000... Training loss: 0.5093\n",
      "Epoch: 1852/2000... Training loss: 0.4170\n",
      "Epoch: 1852/2000... Training loss: 0.3832\n",
      "Epoch: 1852/2000... Training loss: 0.2006\n",
      "Epoch: 1852/2000... Training loss: 0.3907\n",
      "Epoch: 1852/2000... Training loss: 0.2943\n",
      "Epoch: 1852/2000... Training loss: 0.4245\n",
      "Epoch: 1852/2000... Training loss: 0.3088\n",
      "Epoch: 1852/2000... Training loss: 0.4227\n",
      "Epoch: 1852/2000... Training loss: 0.2493\n",
      "Epoch: 1852/2000... Training loss: 0.2706\n",
      "Epoch: 1852/2000... Training loss: 0.1704\n",
      "Epoch: 1852/2000... Training loss: 0.2909\n",
      "Epoch: 1852/2000... Training loss: 0.2667\n",
      "Epoch: 1852/2000... Training loss: 0.3650\n",
      "Epoch: 1852/2000... Training loss: 0.4821\n",
      "Epoch: 1852/2000... Training loss: 0.3651\n",
      "Epoch: 1852/2000... Training loss: 0.3404\n",
      "Epoch: 1852/2000... Training loss: 0.3425\n",
      "Epoch: 1852/2000... Training loss: 0.3824\n",
      "Epoch: 1852/2000... Training loss: 0.3986\n",
      "Epoch: 1852/2000... Training loss: 0.2889\n",
      "Epoch: 1852/2000... Training loss: 0.3142\n",
      "Epoch: 1852/2000... Training loss: 0.4622\n",
      "Epoch: 1853/2000... Training loss: 0.5162\n",
      "Epoch: 1853/2000... Training loss: 0.3108\n",
      "Epoch: 1853/2000... Training loss: 0.3825\n",
      "Epoch: 1853/2000... Training loss: 0.6393\n",
      "Epoch: 1853/2000... Training loss: 0.4593\n",
      "Epoch: 1853/2000... Training loss: 0.3712\n",
      "Epoch: 1853/2000... Training loss: 0.4138\n",
      "Epoch: 1853/2000... Training loss: 0.2878\n",
      "Epoch: 1853/2000... Training loss: 0.2633\n",
      "Epoch: 1853/2000... Training loss: 0.2903\n",
      "Epoch: 1853/2000... Training loss: 0.3380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1853/2000... Training loss: 0.4704\n",
      "Epoch: 1853/2000... Training loss: 0.3230\n",
      "Epoch: 1853/2000... Training loss: 0.3238\n",
      "Epoch: 1853/2000... Training loss: 0.4188\n",
      "Epoch: 1853/2000... Training loss: 0.3619\n",
      "Epoch: 1853/2000... Training loss: 0.4573\n",
      "Epoch: 1853/2000... Training loss: 0.2641\n",
      "Epoch: 1853/2000... Training loss: 0.4818\n",
      "Epoch: 1853/2000... Training loss: 0.3228\n",
      "Epoch: 1853/2000... Training loss: 0.1687\n",
      "Epoch: 1853/2000... Training loss: 0.3885\n",
      "Epoch: 1853/2000... Training loss: 0.3196\n",
      "Epoch: 1853/2000... Training loss: 0.4824\n",
      "Epoch: 1853/2000... Training loss: 0.3367\n",
      "Epoch: 1853/2000... Training loss: 0.3686\n",
      "Epoch: 1853/2000... Training loss: 0.3124\n",
      "Epoch: 1853/2000... Training loss: 0.3174\n",
      "Epoch: 1853/2000... Training loss: 0.4650\n",
      "Epoch: 1853/2000... Training loss: 0.4809\n",
      "Epoch: 1853/2000... Training loss: 0.3366\n",
      "Epoch: 1854/2000... Training loss: 0.3673\n",
      "Epoch: 1854/2000... Training loss: 0.4530\n",
      "Epoch: 1854/2000... Training loss: 0.3357\n",
      "Epoch: 1854/2000... Training loss: 0.4564\n",
      "Epoch: 1854/2000... Training loss: 0.5175\n",
      "Epoch: 1854/2000... Training loss: 0.2564\n",
      "Epoch: 1854/2000... Training loss: 0.3512\n",
      "Epoch: 1854/2000... Training loss: 0.4689\n",
      "Epoch: 1854/2000... Training loss: 0.2719\n",
      "Epoch: 1854/2000... Training loss: 0.2384\n",
      "Epoch: 1854/2000... Training loss: 0.4170\n",
      "Epoch: 1854/2000... Training loss: 0.3233\n",
      "Epoch: 1854/2000... Training loss: 0.4302\n",
      "Epoch: 1854/2000... Training loss: 0.3395\n",
      "Epoch: 1854/2000... Training loss: 0.2570\n",
      "Epoch: 1854/2000... Training loss: 0.3621\n",
      "Epoch: 1854/2000... Training loss: 0.4414\n",
      "Epoch: 1854/2000... Training loss: 0.3777\n",
      "Epoch: 1854/2000... Training loss: 0.4525\n",
      "Epoch: 1854/2000... Training loss: 0.4227\n",
      "Epoch: 1854/2000... Training loss: 0.3711\n",
      "Epoch: 1854/2000... Training loss: 0.6246\n",
      "Epoch: 1854/2000... Training loss: 0.3351\n",
      "Epoch: 1854/2000... Training loss: 0.4628\n",
      "Epoch: 1854/2000... Training loss: 0.4436\n",
      "Epoch: 1854/2000... Training loss: 0.4094\n",
      "Epoch: 1854/2000... Training loss: 0.4205\n",
      "Epoch: 1854/2000... Training loss: 0.4214\n",
      "Epoch: 1854/2000... Training loss: 0.2771\n",
      "Epoch: 1854/2000... Training loss: 0.4228\n",
      "Epoch: 1854/2000... Training loss: 0.4204\n",
      "Epoch: 1855/2000... Training loss: 0.2645\n",
      "Epoch: 1855/2000... Training loss: 0.3350\n",
      "Epoch: 1855/2000... Training loss: 0.4246\n",
      "Epoch: 1855/2000... Training loss: 0.2711\n",
      "Epoch: 1855/2000... Training loss: 0.3461\n",
      "Epoch: 1855/2000... Training loss: 0.3214\n",
      "Epoch: 1855/2000... Training loss: 0.3921\n",
      "Epoch: 1855/2000... Training loss: 0.3637\n",
      "Epoch: 1855/2000... Training loss: 0.3735\n",
      "Epoch: 1855/2000... Training loss: 0.4229\n",
      "Epoch: 1855/2000... Training loss: 0.3512\n",
      "Epoch: 1855/2000... Training loss: 0.2729\n",
      "Epoch: 1855/2000... Training loss: 0.3298\n",
      "Epoch: 1855/2000... Training loss: 0.4479\n",
      "Epoch: 1855/2000... Training loss: 0.6145\n",
      "Epoch: 1855/2000... Training loss: 0.3620\n",
      "Epoch: 1855/2000... Training loss: 0.5075\n",
      "Epoch: 1855/2000... Training loss: 0.3223\n",
      "Epoch: 1855/2000... Training loss: 0.4220\n",
      "Epoch: 1855/2000... Training loss: 0.3955\n",
      "Epoch: 1855/2000... Training loss: 0.2772\n",
      "Epoch: 1855/2000... Training loss: 0.5078\n",
      "Epoch: 1855/2000... Training loss: 0.4397\n",
      "Epoch: 1855/2000... Training loss: 0.2752\n",
      "Epoch: 1855/2000... Training loss: 0.4010\n",
      "Epoch: 1855/2000... Training loss: 0.4673\n",
      "Epoch: 1855/2000... Training loss: 0.4184\n",
      "Epoch: 1855/2000... Training loss: 0.4112\n",
      "Epoch: 1855/2000... Training loss: 0.4369\n",
      "Epoch: 1855/2000... Training loss: 0.4065\n",
      "Epoch: 1855/2000... Training loss: 0.1391\n",
      "Epoch: 1856/2000... Training loss: 0.4136\n",
      "Epoch: 1856/2000... Training loss: 0.4105\n",
      "Epoch: 1856/2000... Training loss: 0.3984\n",
      "Epoch: 1856/2000... Training loss: 0.5042\n",
      "Epoch: 1856/2000... Training loss: 0.4842\n",
      "Epoch: 1856/2000... Training loss: 0.5322\n",
      "Epoch: 1856/2000... Training loss: 0.4148\n",
      "Epoch: 1856/2000... Training loss: 0.3503\n",
      "Epoch: 1856/2000... Training loss: 0.3600\n",
      "Epoch: 1856/2000... Training loss: 0.2856\n",
      "Epoch: 1856/2000... Training loss: 0.3942\n",
      "Epoch: 1856/2000... Training loss: 0.4606\n",
      "Epoch: 1856/2000... Training loss: 0.2977\n",
      "Epoch: 1856/2000... Training loss: 0.3734\n",
      "Epoch: 1856/2000... Training loss: 0.3091\n",
      "Epoch: 1856/2000... Training loss: 0.2173\n",
      "Epoch: 1856/2000... Training loss: 0.4100\n",
      "Epoch: 1856/2000... Training loss: 0.3698\n",
      "Epoch: 1856/2000... Training loss: 0.4625\n",
      "Epoch: 1856/2000... Training loss: 0.4131\n",
      "Epoch: 1856/2000... Training loss: 0.4244\n",
      "Epoch: 1856/2000... Training loss: 0.4026\n",
      "Epoch: 1856/2000... Training loss: 0.3812\n",
      "Epoch: 1856/2000... Training loss: 0.3404\n",
      "Epoch: 1856/2000... Training loss: 0.3497\n",
      "Epoch: 1856/2000... Training loss: 0.2822\n",
      "Epoch: 1856/2000... Training loss: 0.2757\n",
      "Epoch: 1856/2000... Training loss: 0.4267\n",
      "Epoch: 1856/2000... Training loss: 0.4595\n",
      "Epoch: 1856/2000... Training loss: 0.3132\n",
      "Epoch: 1856/2000... Training loss: 0.3153\n",
      "Epoch: 1857/2000... Training loss: 0.3184\n",
      "Epoch: 1857/2000... Training loss: 0.3351\n",
      "Epoch: 1857/2000... Training loss: 0.4285\n",
      "Epoch: 1857/2000... Training loss: 0.4617\n",
      "Epoch: 1857/2000... Training loss: 0.4856\n",
      "Epoch: 1857/2000... Training loss: 0.4644\n",
      "Epoch: 1857/2000... Training loss: 0.3356\n",
      "Epoch: 1857/2000... Training loss: 0.2850\n",
      "Epoch: 1857/2000... Training loss: 0.4168\n",
      "Epoch: 1857/2000... Training loss: 0.4542\n",
      "Epoch: 1857/2000... Training loss: 0.4194\n",
      "Epoch: 1857/2000... Training loss: 0.3907\n",
      "Epoch: 1857/2000... Training loss: 0.2693\n",
      "Epoch: 1857/2000... Training loss: 0.2978\n",
      "Epoch: 1857/2000... Training loss: 0.3517\n",
      "Epoch: 1857/2000... Training loss: 0.2156\n",
      "Epoch: 1857/2000... Training loss: 0.4230\n",
      "Epoch: 1857/2000... Training loss: 0.3693\n",
      "Epoch: 1857/2000... Training loss: 0.3473\n",
      "Epoch: 1857/2000... Training loss: 0.3408\n",
      "Epoch: 1857/2000... Training loss: 0.2782\n",
      "Epoch: 1857/2000... Training loss: 0.2093\n",
      "Epoch: 1857/2000... Training loss: 0.3425\n",
      "Epoch: 1857/2000... Training loss: 0.3494\n",
      "Epoch: 1857/2000... Training loss: 0.2320\n",
      "Epoch: 1857/2000... Training loss: 0.5731\n",
      "Epoch: 1857/2000... Training loss: 0.3836\n",
      "Epoch: 1857/2000... Training loss: 0.6410\n",
      "Epoch: 1857/2000... Training loss: 0.2987\n",
      "Epoch: 1857/2000... Training loss: 0.4908\n",
      "Epoch: 1857/2000... Training loss: 0.4231\n",
      "Epoch: 1858/2000... Training loss: 0.3111\n",
      "Epoch: 1858/2000... Training loss: 0.6063\n",
      "Epoch: 1858/2000... Training loss: 0.2628\n",
      "Epoch: 1858/2000... Training loss: 0.4247\n",
      "Epoch: 1858/2000... Training loss: 0.3413\n",
      "Epoch: 1858/2000... Training loss: 0.3707\n",
      "Epoch: 1858/2000... Training loss: 0.4877\n",
      "Epoch: 1858/2000... Training loss: 0.4207\n",
      "Epoch: 1858/2000... Training loss: 0.2534\n",
      "Epoch: 1858/2000... Training loss: 0.2849\n",
      "Epoch: 1858/2000... Training loss: 0.2512\n",
      "Epoch: 1858/2000... Training loss: 0.4461\n",
      "Epoch: 1858/2000... Training loss: 0.3875\n",
      "Epoch: 1858/2000... Training loss: 0.4437\n",
      "Epoch: 1858/2000... Training loss: 0.4514\n",
      "Epoch: 1858/2000... Training loss: 0.5864\n",
      "Epoch: 1858/2000... Training loss: 0.3623\n",
      "Epoch: 1858/2000... Training loss: 0.4404\n",
      "Epoch: 1858/2000... Training loss: 0.3845\n",
      "Epoch: 1858/2000... Training loss: 0.4304\n",
      "Epoch: 1858/2000... Training loss: 0.3969\n",
      "Epoch: 1858/2000... Training loss: 0.2910\n",
      "Epoch: 1858/2000... Training loss: 0.3598\n",
      "Epoch: 1858/2000... Training loss: 0.3725\n",
      "Epoch: 1858/2000... Training loss: 0.3581\n",
      "Epoch: 1858/2000... Training loss: 0.4734\n",
      "Epoch: 1858/2000... Training loss: 0.7957\n",
      "Epoch: 1858/2000... Training loss: 0.4744\n",
      "Epoch: 1858/2000... Training loss: 0.5160\n",
      "Epoch: 1858/2000... Training loss: 0.2241\n",
      "Epoch: 1858/2000... Training loss: 0.4304\n",
      "Epoch: 1859/2000... Training loss: 0.3564\n",
      "Epoch: 1859/2000... Training loss: 0.4078\n",
      "Epoch: 1859/2000... Training loss: 0.3183\n",
      "Epoch: 1859/2000... Training loss: 0.4938\n",
      "Epoch: 1859/2000... Training loss: 0.3151\n",
      "Epoch: 1859/2000... Training loss: 0.3809\n",
      "Epoch: 1859/2000... Training loss: 0.2173\n",
      "Epoch: 1859/2000... Training loss: 0.4351\n",
      "Epoch: 1859/2000... Training loss: 0.4055\n",
      "Epoch: 1859/2000... Training loss: 0.3398\n",
      "Epoch: 1859/2000... Training loss: 0.2753\n",
      "Epoch: 1859/2000... Training loss: 0.1946\n",
      "Epoch: 1859/2000... Training loss: 0.4065\n",
      "Epoch: 1859/2000... Training loss: 0.3133\n",
      "Epoch: 1859/2000... Training loss: 0.5347\n",
      "Epoch: 1859/2000... Training loss: 0.4700\n",
      "Epoch: 1859/2000... Training loss: 0.4107\n",
      "Epoch: 1859/2000... Training loss: 0.3397\n",
      "Epoch: 1859/2000... Training loss: 0.2783\n",
      "Epoch: 1859/2000... Training loss: 0.4606\n",
      "Epoch: 1859/2000... Training loss: 0.3096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1859/2000... Training loss: 0.2557\n",
      "Epoch: 1859/2000... Training loss: 0.4010\n",
      "Epoch: 1859/2000... Training loss: 0.4365\n",
      "Epoch: 1859/2000... Training loss: 0.3356\n",
      "Epoch: 1859/2000... Training loss: 0.2989\n",
      "Epoch: 1859/2000... Training loss: 0.2467\n",
      "Epoch: 1859/2000... Training loss: 0.3853\n",
      "Epoch: 1859/2000... Training loss: 0.2370\n",
      "Epoch: 1859/2000... Training loss: 0.4275\n",
      "Epoch: 1859/2000... Training loss: 0.4473\n",
      "Epoch: 1860/2000... Training loss: 0.4626\n",
      "Epoch: 1860/2000... Training loss: 0.2995\n",
      "Epoch: 1860/2000... Training loss: 0.4163\n",
      "Epoch: 1860/2000... Training loss: 0.2557\n",
      "Epoch: 1860/2000... Training loss: 0.3669\n",
      "Epoch: 1860/2000... Training loss: 0.4670\n",
      "Epoch: 1860/2000... Training loss: 0.2469\n",
      "Epoch: 1860/2000... Training loss: 0.4077\n",
      "Epoch: 1860/2000... Training loss: 0.5889\n",
      "Epoch: 1860/2000... Training loss: 0.4781\n",
      "Epoch: 1860/2000... Training loss: 0.4380\n",
      "Epoch: 1860/2000... Training loss: 0.1454\n",
      "Epoch: 1860/2000... Training loss: 0.2747\n",
      "Epoch: 1860/2000... Training loss: 0.4665\n",
      "Epoch: 1860/2000... Training loss: 0.3571\n",
      "Epoch: 1860/2000... Training loss: 0.2562\n",
      "Epoch: 1860/2000... Training loss: 0.3736\n",
      "Epoch: 1860/2000... Training loss: 0.3462\n",
      "Epoch: 1860/2000... Training loss: 0.4171\n",
      "Epoch: 1860/2000... Training loss: 0.2951\n",
      "Epoch: 1860/2000... Training loss: 0.4599\n",
      "Epoch: 1860/2000... Training loss: 0.3303\n",
      "Epoch: 1860/2000... Training loss: 0.2455\n",
      "Epoch: 1860/2000... Training loss: 0.1669\n",
      "Epoch: 1860/2000... Training loss: 0.2345\n",
      "Epoch: 1860/2000... Training loss: 0.3617\n",
      "Epoch: 1860/2000... Training loss: 0.3268\n",
      "Epoch: 1860/2000... Training loss: 0.4061\n",
      "Epoch: 1860/2000... Training loss: 0.3642\n",
      "Epoch: 1860/2000... Training loss: 0.4466\n",
      "Epoch: 1860/2000... Training loss: 0.3855\n",
      "Epoch: 1861/2000... Training loss: 0.3501\n",
      "Epoch: 1861/2000... Training loss: 0.3014\n",
      "Epoch: 1861/2000... Training loss: 0.3904\n",
      "Epoch: 1861/2000... Training loss: 0.2944\n",
      "Epoch: 1861/2000... Training loss: 0.3512\n",
      "Epoch: 1861/2000... Training loss: 0.2924\n",
      "Epoch: 1861/2000... Training loss: 0.4671\n",
      "Epoch: 1861/2000... Training loss: 0.3602\n",
      "Epoch: 1861/2000... Training loss: 0.4053\n",
      "Epoch: 1861/2000... Training loss: 0.4554\n",
      "Epoch: 1861/2000... Training loss: 0.4899\n",
      "Epoch: 1861/2000... Training loss: 0.2638\n",
      "Epoch: 1861/2000... Training loss: 0.3958\n",
      "Epoch: 1861/2000... Training loss: 0.4169\n",
      "Epoch: 1861/2000... Training loss: 0.4295\n",
      "Epoch: 1861/2000... Training loss: 0.2584\n",
      "Epoch: 1861/2000... Training loss: 0.3190\n",
      "Epoch: 1861/2000... Training loss: 0.4519\n",
      "Epoch: 1861/2000... Training loss: 0.3904\n",
      "Epoch: 1861/2000... Training loss: 0.4739\n",
      "Epoch: 1861/2000... Training loss: 0.3418\n",
      "Epoch: 1861/2000... Training loss: 0.2601\n",
      "Epoch: 1861/2000... Training loss: 0.3911\n",
      "Epoch: 1861/2000... Training loss: 0.4481\n",
      "Epoch: 1861/2000... Training loss: 0.3779\n",
      "Epoch: 1861/2000... Training loss: 0.3310\n",
      "Epoch: 1861/2000... Training loss: 0.3588\n",
      "Epoch: 1861/2000... Training loss: 0.2464\n",
      "Epoch: 1861/2000... Training loss: 0.5042\n",
      "Epoch: 1861/2000... Training loss: 0.2908\n",
      "Epoch: 1861/2000... Training loss: 0.4627\n",
      "Epoch: 1862/2000... Training loss: 0.2403\n",
      "Epoch: 1862/2000... Training loss: 0.2113\n",
      "Epoch: 1862/2000... Training loss: 0.3353\n",
      "Epoch: 1862/2000... Training loss: 0.3096\n",
      "Epoch: 1862/2000... Training loss: 0.2686\n",
      "Epoch: 1862/2000... Training loss: 0.4299\n",
      "Epoch: 1862/2000... Training loss: 0.5008\n",
      "Epoch: 1862/2000... Training loss: 0.3897\n",
      "Epoch: 1862/2000... Training loss: 0.4524\n",
      "Epoch: 1862/2000... Training loss: 0.2967\n",
      "Epoch: 1862/2000... Training loss: 0.5018\n",
      "Epoch: 1862/2000... Training loss: 0.5165\n",
      "Epoch: 1862/2000... Training loss: 0.4247\n",
      "Epoch: 1862/2000... Training loss: 0.2935\n",
      "Epoch: 1862/2000... Training loss: 0.3939\n",
      "Epoch: 1862/2000... Training loss: 0.3476\n",
      "Epoch: 1862/2000... Training loss: 0.3247\n",
      "Epoch: 1862/2000... Training loss: 0.3211\n",
      "Epoch: 1862/2000... Training loss: 0.3145\n",
      "Epoch: 1862/2000... Training loss: 0.2876\n",
      "Epoch: 1862/2000... Training loss: 0.5355\n",
      "Epoch: 1862/2000... Training loss: 0.4472\n",
      "Epoch: 1862/2000... Training loss: 0.4856\n",
      "Epoch: 1862/2000... Training loss: 0.2426\n",
      "Epoch: 1862/2000... Training loss: 0.3073\n",
      "Epoch: 1862/2000... Training loss: 0.3564\n",
      "Epoch: 1862/2000... Training loss: 0.4137\n",
      "Epoch: 1862/2000... Training loss: 0.4538\n",
      "Epoch: 1862/2000... Training loss: 0.5037\n",
      "Epoch: 1862/2000... Training loss: 0.3653\n",
      "Epoch: 1862/2000... Training loss: 0.5527\n",
      "Epoch: 1863/2000... Training loss: 0.3930\n",
      "Epoch: 1863/2000... Training loss: 0.4593\n",
      "Epoch: 1863/2000... Training loss: 0.3967\n",
      "Epoch: 1863/2000... Training loss: 0.3755\n",
      "Epoch: 1863/2000... Training loss: 0.3555\n",
      "Epoch: 1863/2000... Training loss: 0.3137\n",
      "Epoch: 1863/2000... Training loss: 0.3001\n",
      "Epoch: 1863/2000... Training loss: 0.3698\n",
      "Epoch: 1863/2000... Training loss: 0.2934\n",
      "Epoch: 1863/2000... Training loss: 0.2467\n",
      "Epoch: 1863/2000... Training loss: 0.3818\n",
      "Epoch: 1863/2000... Training loss: 0.3748\n",
      "Epoch: 1863/2000... Training loss: 0.2776\n",
      "Epoch: 1863/2000... Training loss: 0.3390\n",
      "Epoch: 1863/2000... Training loss: 0.3771\n",
      "Epoch: 1863/2000... Training loss: 0.2712\n",
      "Epoch: 1863/2000... Training loss: 0.4314\n",
      "Epoch: 1863/2000... Training loss: 0.4007\n",
      "Epoch: 1863/2000... Training loss: 0.2271\n",
      "Epoch: 1863/2000... Training loss: 0.3690\n",
      "Epoch: 1863/2000... Training loss: 0.4588\n",
      "Epoch: 1863/2000... Training loss: 0.3899\n",
      "Epoch: 1863/2000... Training loss: 0.1968\n",
      "Epoch: 1863/2000... Training loss: 0.4178\n",
      "Epoch: 1863/2000... Training loss: 0.4875\n",
      "Epoch: 1863/2000... Training loss: 0.4281\n",
      "Epoch: 1863/2000... Training loss: 0.3143\n",
      "Epoch: 1863/2000... Training loss: 0.2311\n",
      "Epoch: 1863/2000... Training loss: 0.3405\n",
      "Epoch: 1863/2000... Training loss: 0.3115\n",
      "Epoch: 1863/2000... Training loss: 0.4987\n",
      "Epoch: 1864/2000... Training loss: 0.3642\n",
      "Epoch: 1864/2000... Training loss: 0.3556\n",
      "Epoch: 1864/2000... Training loss: 0.3578\n",
      "Epoch: 1864/2000... Training loss: 0.2973\n",
      "Epoch: 1864/2000... Training loss: 0.2305\n",
      "Epoch: 1864/2000... Training loss: 0.7202\n",
      "Epoch: 1864/2000... Training loss: 0.5043\n",
      "Epoch: 1864/2000... Training loss: 0.3476\n",
      "Epoch: 1864/2000... Training loss: 0.3253\n",
      "Epoch: 1864/2000... Training loss: 0.3093\n",
      "Epoch: 1864/2000... Training loss: 0.4849\n",
      "Epoch: 1864/2000... Training loss: 0.4822\n",
      "Epoch: 1864/2000... Training loss: 0.8132\n",
      "Epoch: 1864/2000... Training loss: 0.2718\n",
      "Epoch: 1864/2000... Training loss: 0.4161\n",
      "Epoch: 1864/2000... Training loss: 0.4474\n",
      "Epoch: 1864/2000... Training loss: 0.4195\n",
      "Epoch: 1864/2000... Training loss: 0.3463\n",
      "Epoch: 1864/2000... Training loss: 0.3834\n",
      "Epoch: 1864/2000... Training loss: 0.3958\n",
      "Epoch: 1864/2000... Training loss: 0.4007\n",
      "Epoch: 1864/2000... Training loss: 0.3470\n",
      "Epoch: 1864/2000... Training loss: 0.3274\n",
      "Epoch: 1864/2000... Training loss: 0.4054\n",
      "Epoch: 1864/2000... Training loss: 0.3306\n",
      "Epoch: 1864/2000... Training loss: 0.4251\n",
      "Epoch: 1864/2000... Training loss: 0.2878\n",
      "Epoch: 1864/2000... Training loss: 0.3734\n",
      "Epoch: 1864/2000... Training loss: 0.3446\n",
      "Epoch: 1864/2000... Training loss: 0.4721\n",
      "Epoch: 1864/2000... Training loss: 0.3820\n",
      "Epoch: 1865/2000... Training loss: 0.2963\n",
      "Epoch: 1865/2000... Training loss: 0.2218\n",
      "Epoch: 1865/2000... Training loss: 0.3820\n",
      "Epoch: 1865/2000... Training loss: 0.2582\n",
      "Epoch: 1865/2000... Training loss: 0.4077\n",
      "Epoch: 1865/2000... Training loss: 0.5616\n",
      "Epoch: 1865/2000... Training loss: 0.3358\n",
      "Epoch: 1865/2000... Training loss: 0.5632\n",
      "Epoch: 1865/2000... Training loss: 0.2441\n",
      "Epoch: 1865/2000... Training loss: 0.4380\n",
      "Epoch: 1865/2000... Training loss: 0.4619\n",
      "Epoch: 1865/2000... Training loss: 0.2634\n",
      "Epoch: 1865/2000... Training loss: 0.3272\n",
      "Epoch: 1865/2000... Training loss: 0.2462\n",
      "Epoch: 1865/2000... Training loss: 0.2966\n",
      "Epoch: 1865/2000... Training loss: 0.4285\n",
      "Epoch: 1865/2000... Training loss: 0.4316\n",
      "Epoch: 1865/2000... Training loss: 0.5382\n",
      "Epoch: 1865/2000... Training loss: 0.2776\n",
      "Epoch: 1865/2000... Training loss: 0.3423\n",
      "Epoch: 1865/2000... Training loss: 0.1725\n",
      "Epoch: 1865/2000... Training loss: 0.2630\n",
      "Epoch: 1865/2000... Training loss: 0.4767\n",
      "Epoch: 1865/2000... Training loss: 0.4304\n",
      "Epoch: 1865/2000... Training loss: 0.3554\n",
      "Epoch: 1865/2000... Training loss: 0.2301\n",
      "Epoch: 1865/2000... Training loss: 0.3499\n",
      "Epoch: 1865/2000... Training loss: 0.3282\n",
      "Epoch: 1865/2000... Training loss: 0.4216\n",
      "Epoch: 1865/2000... Training loss: 0.2997\n",
      "Epoch: 1865/2000... Training loss: 0.3077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1866/2000... Training loss: 0.4176\n",
      "Epoch: 1866/2000... Training loss: 0.3493\n",
      "Epoch: 1866/2000... Training loss: 0.4087\n",
      "Epoch: 1866/2000... Training loss: 0.2946\n",
      "Epoch: 1866/2000... Training loss: 0.4818\n",
      "Epoch: 1866/2000... Training loss: 0.3185\n",
      "Epoch: 1866/2000... Training loss: 0.4601\n",
      "Epoch: 1866/2000... Training loss: 0.4114\n",
      "Epoch: 1866/2000... Training loss: 0.2302\n",
      "Epoch: 1866/2000... Training loss: 0.2478\n",
      "Epoch: 1866/2000... Training loss: 0.2947\n",
      "Epoch: 1866/2000... Training loss: 0.2836\n",
      "Epoch: 1866/2000... Training loss: 0.3841\n",
      "Epoch: 1866/2000... Training loss: 0.2135\n",
      "Epoch: 1866/2000... Training loss: 0.3970\n",
      "Epoch: 1866/2000... Training loss: 0.4632\n",
      "Epoch: 1866/2000... Training loss: 0.1840\n",
      "Epoch: 1866/2000... Training loss: 0.3230\n",
      "Epoch: 1866/2000... Training loss: 0.4247\n",
      "Epoch: 1866/2000... Training loss: 0.3620\n",
      "Epoch: 1866/2000... Training loss: 0.3380\n",
      "Epoch: 1866/2000... Training loss: 0.4149\n",
      "Epoch: 1866/2000... Training loss: 0.5187\n",
      "Epoch: 1866/2000... Training loss: 0.4649\n",
      "Epoch: 1866/2000... Training loss: 0.4649\n",
      "Epoch: 1866/2000... Training loss: 0.3540\n",
      "Epoch: 1866/2000... Training loss: 0.3384\n",
      "Epoch: 1866/2000... Training loss: 0.3101\n",
      "Epoch: 1866/2000... Training loss: 0.2709\n",
      "Epoch: 1866/2000... Training loss: 0.3545\n",
      "Epoch: 1866/2000... Training loss: 0.3245\n",
      "Epoch: 1867/2000... Training loss: 0.2799\n",
      "Epoch: 1867/2000... Training loss: 0.3527\n",
      "Epoch: 1867/2000... Training loss: 0.2489\n",
      "Epoch: 1867/2000... Training loss: 0.4578\n",
      "Epoch: 1867/2000... Training loss: 0.4494\n",
      "Epoch: 1867/2000... Training loss: 0.4421\n",
      "Epoch: 1867/2000... Training loss: 0.3554\n",
      "Epoch: 1867/2000... Training loss: 0.3233\n",
      "Epoch: 1867/2000... Training loss: 0.2849\n",
      "Epoch: 1867/2000... Training loss: 0.2678\n",
      "Epoch: 1867/2000... Training loss: 0.2646\n",
      "Epoch: 1867/2000... Training loss: 0.3357\n",
      "Epoch: 1867/2000... Training loss: 0.4204\n",
      "Epoch: 1867/2000... Training loss: 0.3815\n",
      "Epoch: 1867/2000... Training loss: 0.3087\n",
      "Epoch: 1867/2000... Training loss: 0.5405\n",
      "Epoch: 1867/2000... Training loss: 0.2543\n",
      "Epoch: 1867/2000... Training loss: 0.3084\n",
      "Epoch: 1867/2000... Training loss: 0.4109\n",
      "Epoch: 1867/2000... Training loss: 0.5703\n",
      "Epoch: 1867/2000... Training loss: 0.3800\n",
      "Epoch: 1867/2000... Training loss: 0.4842\n",
      "Epoch: 1867/2000... Training loss: 0.4539\n",
      "Epoch: 1867/2000... Training loss: 0.3981\n",
      "Epoch: 1867/2000... Training loss: 0.3615\n",
      "Epoch: 1867/2000... Training loss: 0.4180\n",
      "Epoch: 1867/2000... Training loss: 0.3968\n",
      "Epoch: 1867/2000... Training loss: 0.2810\n",
      "Epoch: 1867/2000... Training loss: 0.2921\n",
      "Epoch: 1867/2000... Training loss: 0.4097\n",
      "Epoch: 1867/2000... Training loss: 0.3066\n",
      "Epoch: 1868/2000... Training loss: 0.4619\n",
      "Epoch: 1868/2000... Training loss: 0.2702\n",
      "Epoch: 1868/2000... Training loss: 0.3649\n",
      "Epoch: 1868/2000... Training loss: 0.2492\n",
      "Epoch: 1868/2000... Training loss: 0.4198\n",
      "Epoch: 1868/2000... Training loss: 0.4843\n",
      "Epoch: 1868/2000... Training loss: 0.4140\n",
      "Epoch: 1868/2000... Training loss: 0.3063\n",
      "Epoch: 1868/2000... Training loss: 0.4048\n",
      "Epoch: 1868/2000... Training loss: 0.3338\n",
      "Epoch: 1868/2000... Training loss: 0.2066\n",
      "Epoch: 1868/2000... Training loss: 0.4776\n",
      "Epoch: 1868/2000... Training loss: 0.3719\n",
      "Epoch: 1868/2000... Training loss: 0.2598\n",
      "Epoch: 1868/2000... Training loss: 0.4896\n",
      "Epoch: 1868/2000... Training loss: 0.3668\n",
      "Epoch: 1868/2000... Training loss: 0.4795\n",
      "Epoch: 1868/2000... Training loss: 0.5095\n",
      "Epoch: 1868/2000... Training loss: 0.2967\n",
      "Epoch: 1868/2000... Training loss: 0.3109\n",
      "Epoch: 1868/2000... Training loss: 0.3448\n",
      "Epoch: 1868/2000... Training loss: 0.4813\n",
      "Epoch: 1868/2000... Training loss: 0.2104\n",
      "Epoch: 1868/2000... Training loss: 0.4339\n",
      "Epoch: 1868/2000... Training loss: 0.2906\n",
      "Epoch: 1868/2000... Training loss: 0.4393\n",
      "Epoch: 1868/2000... Training loss: 0.4031\n",
      "Epoch: 1868/2000... Training loss: 0.5898\n",
      "Epoch: 1868/2000... Training loss: 0.4045\n",
      "Epoch: 1868/2000... Training loss: 0.1718\n",
      "Epoch: 1868/2000... Training loss: 0.3706\n",
      "Epoch: 1869/2000... Training loss: 0.2832\n",
      "Epoch: 1869/2000... Training loss: 0.4649\n",
      "Epoch: 1869/2000... Training loss: 0.3748\n",
      "Epoch: 1869/2000... Training loss: 0.3293\n",
      "Epoch: 1869/2000... Training loss: 0.3932\n",
      "Epoch: 1869/2000... Training loss: 0.3201\n",
      "Epoch: 1869/2000... Training loss: 0.5510\n",
      "Epoch: 1869/2000... Training loss: 0.3747\n",
      "Epoch: 1869/2000... Training loss: 0.4027\n",
      "Epoch: 1869/2000... Training loss: 0.3552\n",
      "Epoch: 1869/2000... Training loss: 0.5053\n",
      "Epoch: 1869/2000... Training loss: 0.5073\n",
      "Epoch: 1869/2000... Training loss: 0.5661\n",
      "Epoch: 1869/2000... Training loss: 0.4165\n",
      "Epoch: 1869/2000... Training loss: 0.4637\n",
      "Epoch: 1869/2000... Training loss: 0.3077\n",
      "Epoch: 1869/2000... Training loss: 0.4007\n",
      "Epoch: 1869/2000... Training loss: 0.4811\n",
      "Epoch: 1869/2000... Training loss: 0.3287\n",
      "Epoch: 1869/2000... Training loss: 0.3247\n",
      "Epoch: 1869/2000... Training loss: 0.4653\n",
      "Epoch: 1869/2000... Training loss: 0.4348\n",
      "Epoch: 1869/2000... Training loss: 0.4103\n",
      "Epoch: 1869/2000... Training loss: 0.3206\n",
      "Epoch: 1869/2000... Training loss: 0.4639\n",
      "Epoch: 1869/2000... Training loss: 0.3065\n",
      "Epoch: 1869/2000... Training loss: 0.3596\n",
      "Epoch: 1869/2000... Training loss: 0.4921\n",
      "Epoch: 1869/2000... Training loss: 0.6398\n",
      "Epoch: 1869/2000... Training loss: 0.3633\n",
      "Epoch: 1869/2000... Training loss: 0.5002\n",
      "Epoch: 1870/2000... Training loss: 0.3363\n",
      "Epoch: 1870/2000... Training loss: 0.2802\n",
      "Epoch: 1870/2000... Training loss: 0.3407\n",
      "Epoch: 1870/2000... Training loss: 0.2578\n",
      "Epoch: 1870/2000... Training loss: 0.3934\n",
      "Epoch: 1870/2000... Training loss: 0.5999\n",
      "Epoch: 1870/2000... Training loss: 0.2883\n",
      "Epoch: 1870/2000... Training loss: 0.4185\n",
      "Epoch: 1870/2000... Training loss: 0.4453\n",
      "Epoch: 1870/2000... Training loss: 0.3752\n",
      "Epoch: 1870/2000... Training loss: 0.4971\n",
      "Epoch: 1870/2000... Training loss: 0.5431\n",
      "Epoch: 1870/2000... Training loss: 0.2926\n",
      "Epoch: 1870/2000... Training loss: 0.3679\n",
      "Epoch: 1870/2000... Training loss: 0.3399\n",
      "Epoch: 1870/2000... Training loss: 0.3852\n",
      "Epoch: 1870/2000... Training loss: 0.3999\n",
      "Epoch: 1870/2000... Training loss: 0.2916\n",
      "Epoch: 1870/2000... Training loss: 0.5477\n",
      "Epoch: 1870/2000... Training loss: 0.3055\n",
      "Epoch: 1870/2000... Training loss: 0.2975\n",
      "Epoch: 1870/2000... Training loss: 0.3026\n",
      "Epoch: 1870/2000... Training loss: 0.3586\n",
      "Epoch: 1870/2000... Training loss: 0.4302\n",
      "Epoch: 1870/2000... Training loss: 0.3188\n",
      "Epoch: 1870/2000... Training loss: 0.3958\n",
      "Epoch: 1870/2000... Training loss: 0.2940\n",
      "Epoch: 1870/2000... Training loss: 0.4102\n",
      "Epoch: 1870/2000... Training loss: 0.3705\n",
      "Epoch: 1870/2000... Training loss: 0.4689\n",
      "Epoch: 1870/2000... Training loss: 0.5343\n",
      "Epoch: 1871/2000... Training loss: 0.4923\n",
      "Epoch: 1871/2000... Training loss: 0.3467\n",
      "Epoch: 1871/2000... Training loss: 0.3736\n",
      "Epoch: 1871/2000... Training loss: 0.2619\n",
      "Epoch: 1871/2000... Training loss: 0.3718\n",
      "Epoch: 1871/2000... Training loss: 0.3966\n",
      "Epoch: 1871/2000... Training loss: 0.4975\n",
      "Epoch: 1871/2000... Training loss: 0.2704\n",
      "Epoch: 1871/2000... Training loss: 0.2449\n",
      "Epoch: 1871/2000... Training loss: 0.5367\n",
      "Epoch: 1871/2000... Training loss: 0.4031\n",
      "Epoch: 1871/2000... Training loss: 0.4820\n",
      "Epoch: 1871/2000... Training loss: 0.4197\n",
      "Epoch: 1871/2000... Training loss: 0.4864\n",
      "Epoch: 1871/2000... Training loss: 0.5538\n",
      "Epoch: 1871/2000... Training loss: 0.3132\n",
      "Epoch: 1871/2000... Training loss: 0.3035\n",
      "Epoch: 1871/2000... Training loss: 0.2877\n",
      "Epoch: 1871/2000... Training loss: 0.3751\n",
      "Epoch: 1871/2000... Training loss: 0.3101\n",
      "Epoch: 1871/2000... Training loss: 0.1903\n",
      "Epoch: 1871/2000... Training loss: 0.3167\n",
      "Epoch: 1871/2000... Training loss: 0.4506\n",
      "Epoch: 1871/2000... Training loss: 0.3617\n",
      "Epoch: 1871/2000... Training loss: 0.3091\n",
      "Epoch: 1871/2000... Training loss: 0.4097\n",
      "Epoch: 1871/2000... Training loss: 0.3732\n",
      "Epoch: 1871/2000... Training loss: 0.5201\n",
      "Epoch: 1871/2000... Training loss: 0.3293\n",
      "Epoch: 1871/2000... Training loss: 0.3369\n",
      "Epoch: 1871/2000... Training loss: 0.3624\n",
      "Epoch: 1872/2000... Training loss: 0.4574\n",
      "Epoch: 1872/2000... Training loss: 0.2634\n",
      "Epoch: 1872/2000... Training loss: 0.3833\n",
      "Epoch: 1872/2000... Training loss: 0.4041\n",
      "Epoch: 1872/2000... Training loss: 0.3772\n",
      "Epoch: 1872/2000... Training loss: 0.2678\n",
      "Epoch: 1872/2000... Training loss: 0.4117\n",
      "Epoch: 1872/2000... Training loss: 0.3907\n",
      "Epoch: 1872/2000... Training loss: 0.5182\n",
      "Epoch: 1872/2000... Training loss: 0.4681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1872/2000... Training loss: 0.3604\n",
      "Epoch: 1872/2000... Training loss: 0.3971\n",
      "Epoch: 1872/2000... Training loss: 0.5697\n",
      "Epoch: 1872/2000... Training loss: 0.4476\n",
      "Epoch: 1872/2000... Training loss: 0.4818\n",
      "Epoch: 1872/2000... Training loss: 0.4122\n",
      "Epoch: 1872/2000... Training loss: 0.2895\n",
      "Epoch: 1872/2000... Training loss: 0.3032\n",
      "Epoch: 1872/2000... Training loss: 0.3444\n",
      "Epoch: 1872/2000... Training loss: 0.4424\n",
      "Epoch: 1872/2000... Training loss: 0.5013\n",
      "Epoch: 1872/2000... Training loss: 0.3788\n",
      "Epoch: 1872/2000... Training loss: 0.2189\n",
      "Epoch: 1872/2000... Training loss: 0.4058\n",
      "Epoch: 1872/2000... Training loss: 0.2811\n",
      "Epoch: 1872/2000... Training loss: 0.3323\n",
      "Epoch: 1872/2000... Training loss: 0.4540\n",
      "Epoch: 1872/2000... Training loss: 0.2160\n",
      "Epoch: 1872/2000... Training loss: 0.5419\n",
      "Epoch: 1872/2000... Training loss: 0.4192\n",
      "Epoch: 1872/2000... Training loss: 0.4682\n",
      "Epoch: 1873/2000... Training loss: 0.3973\n",
      "Epoch: 1873/2000... Training loss: 0.4979\n",
      "Epoch: 1873/2000... Training loss: 0.4072\n",
      "Epoch: 1873/2000... Training loss: 0.4915\n",
      "Epoch: 1873/2000... Training loss: 0.4215\n",
      "Epoch: 1873/2000... Training loss: 0.3627\n",
      "Epoch: 1873/2000... Training loss: 0.3760\n",
      "Epoch: 1873/2000... Training loss: 0.3208\n",
      "Epoch: 1873/2000... Training loss: 0.3368\n",
      "Epoch: 1873/2000... Training loss: 0.3543\n",
      "Epoch: 1873/2000... Training loss: 0.4233\n",
      "Epoch: 1873/2000... Training loss: 0.5431\n",
      "Epoch: 1873/2000... Training loss: 0.4023\n",
      "Epoch: 1873/2000... Training loss: 0.3944\n",
      "Epoch: 1873/2000... Training loss: 0.4204\n",
      "Epoch: 1873/2000... Training loss: 0.3205\n",
      "Epoch: 1873/2000... Training loss: 0.4691\n",
      "Epoch: 1873/2000... Training loss: 0.2285\n",
      "Epoch: 1873/2000... Training loss: 0.3316\n",
      "Epoch: 1873/2000... Training loss: 0.3071\n",
      "Epoch: 1873/2000... Training loss: 0.3611\n",
      "Epoch: 1873/2000... Training loss: 0.6296\n",
      "Epoch: 1873/2000... Training loss: 0.3226\n",
      "Epoch: 1873/2000... Training loss: 0.4351\n",
      "Epoch: 1873/2000... Training loss: 0.3195\n",
      "Epoch: 1873/2000... Training loss: 0.2304\n",
      "Epoch: 1873/2000... Training loss: 0.4986\n",
      "Epoch: 1873/2000... Training loss: 0.5210\n",
      "Epoch: 1873/2000... Training loss: 0.3062\n",
      "Epoch: 1873/2000... Training loss: 0.5770\n",
      "Epoch: 1873/2000... Training loss: 0.4242\n",
      "Epoch: 1874/2000... Training loss: 0.4236\n",
      "Epoch: 1874/2000... Training loss: 0.3885\n",
      "Epoch: 1874/2000... Training loss: 0.2562\n",
      "Epoch: 1874/2000... Training loss: 0.4713\n",
      "Epoch: 1874/2000... Training loss: 0.4183\n",
      "Epoch: 1874/2000... Training loss: 0.3418\n",
      "Epoch: 1874/2000... Training loss: 0.4618\n",
      "Epoch: 1874/2000... Training loss: 0.2980\n",
      "Epoch: 1874/2000... Training loss: 0.3214\n",
      "Epoch: 1874/2000... Training loss: 0.4148\n",
      "Epoch: 1874/2000... Training loss: 0.4100\n",
      "Epoch: 1874/2000... Training loss: 0.3553\n",
      "Epoch: 1874/2000... Training loss: 0.2842\n",
      "Epoch: 1874/2000... Training loss: 0.2961\n",
      "Epoch: 1874/2000... Training loss: 0.5407\n",
      "Epoch: 1874/2000... Training loss: 0.5878\n",
      "Epoch: 1874/2000... Training loss: 0.2420\n",
      "Epoch: 1874/2000... Training loss: 0.4657\n",
      "Epoch: 1874/2000... Training loss: 0.2236\n",
      "Epoch: 1874/2000... Training loss: 0.3411\n",
      "Epoch: 1874/2000... Training loss: 0.3385\n",
      "Epoch: 1874/2000... Training loss: 0.2670\n",
      "Epoch: 1874/2000... Training loss: 0.4820\n",
      "Epoch: 1874/2000... Training loss: 0.4676\n",
      "Epoch: 1874/2000... Training loss: 0.3057\n",
      "Epoch: 1874/2000... Training loss: 0.3636\n",
      "Epoch: 1874/2000... Training loss: 0.4091\n",
      "Epoch: 1874/2000... Training loss: 0.3139\n",
      "Epoch: 1874/2000... Training loss: 0.3543\n",
      "Epoch: 1874/2000... Training loss: 0.3830\n",
      "Epoch: 1874/2000... Training loss: 0.2539\n",
      "Epoch: 1875/2000... Training loss: 0.3706\n",
      "Epoch: 1875/2000... Training loss: 0.4735\n",
      "Epoch: 1875/2000... Training loss: 0.5300\n",
      "Epoch: 1875/2000... Training loss: 0.2921\n",
      "Epoch: 1875/2000... Training loss: 0.4999\n",
      "Epoch: 1875/2000... Training loss: 0.3726\n",
      "Epoch: 1875/2000... Training loss: 0.4331\n",
      "Epoch: 1875/2000... Training loss: 0.3412\n",
      "Epoch: 1875/2000... Training loss: 0.3487\n",
      "Epoch: 1875/2000... Training loss: 0.3648\n",
      "Epoch: 1875/2000... Training loss: 0.2991\n",
      "Epoch: 1875/2000... Training loss: 0.3473\n",
      "Epoch: 1875/2000... Training loss: 0.3932\n",
      "Epoch: 1875/2000... Training loss: 0.4234\n",
      "Epoch: 1875/2000... Training loss: 0.3189\n",
      "Epoch: 1875/2000... Training loss: 0.4546\n",
      "Epoch: 1875/2000... Training loss: 0.4586\n",
      "Epoch: 1875/2000... Training loss: 0.4638\n",
      "Epoch: 1875/2000... Training loss: 0.4543\n",
      "Epoch: 1875/2000... Training loss: 0.4265\n",
      "Epoch: 1875/2000... Training loss: 0.2433\n",
      "Epoch: 1875/2000... Training loss: 0.4352\n",
      "Epoch: 1875/2000... Training loss: 0.6836\n",
      "Epoch: 1875/2000... Training loss: 0.3504\n",
      "Epoch: 1875/2000... Training loss: 0.3887\n",
      "Epoch: 1875/2000... Training loss: 0.5345\n",
      "Epoch: 1875/2000... Training loss: 0.4828\n",
      "Epoch: 1875/2000... Training loss: 0.4250\n",
      "Epoch: 1875/2000... Training loss: 0.5169\n",
      "Epoch: 1875/2000... Training loss: 0.4028\n",
      "Epoch: 1875/2000... Training loss: 0.2981\n",
      "Epoch: 1876/2000... Training loss: 0.2863\n",
      "Epoch: 1876/2000... Training loss: 0.3644\n",
      "Epoch: 1876/2000... Training loss: 0.3730\n",
      "Epoch: 1876/2000... Training loss: 0.3155\n",
      "Epoch: 1876/2000... Training loss: 0.5812\n",
      "Epoch: 1876/2000... Training loss: 0.3682\n",
      "Epoch: 1876/2000... Training loss: 0.4011\n",
      "Epoch: 1876/2000... Training loss: 0.3736\n",
      "Epoch: 1876/2000... Training loss: 0.2938\n",
      "Epoch: 1876/2000... Training loss: 0.3911\n",
      "Epoch: 1876/2000... Training loss: 0.3002\n",
      "Epoch: 1876/2000... Training loss: 0.2678\n",
      "Epoch: 1876/2000... Training loss: 0.3761\n",
      "Epoch: 1876/2000... Training loss: 0.2562\n",
      "Epoch: 1876/2000... Training loss: 0.3530\n",
      "Epoch: 1876/2000... Training loss: 0.4042\n",
      "Epoch: 1876/2000... Training loss: 0.4438\n",
      "Epoch: 1876/2000... Training loss: 0.5316\n",
      "Epoch: 1876/2000... Training loss: 0.4543\n",
      "Epoch: 1876/2000... Training loss: 0.2694\n",
      "Epoch: 1876/2000... Training loss: 0.1910\n",
      "Epoch: 1876/2000... Training loss: 0.2955\n",
      "Epoch: 1876/2000... Training loss: 0.1911\n",
      "Epoch: 1876/2000... Training loss: 0.3499\n",
      "Epoch: 1876/2000... Training loss: 0.4126\n",
      "Epoch: 1876/2000... Training loss: 0.3118\n",
      "Epoch: 1876/2000... Training loss: 0.3933\n",
      "Epoch: 1876/2000... Training loss: 0.2125\n",
      "Epoch: 1876/2000... Training loss: 0.2338\n",
      "Epoch: 1876/2000... Training loss: 0.4199\n",
      "Epoch: 1876/2000... Training loss: 0.3597\n",
      "Epoch: 1877/2000... Training loss: 0.3074\n",
      "Epoch: 1877/2000... Training loss: 0.3844\n",
      "Epoch: 1877/2000... Training loss: 0.4759\n",
      "Epoch: 1877/2000... Training loss: 0.3934\n",
      "Epoch: 1877/2000... Training loss: 0.3511\n",
      "Epoch: 1877/2000... Training loss: 0.3082\n",
      "Epoch: 1877/2000... Training loss: 0.3645\n",
      "Epoch: 1877/2000... Training loss: 0.4732\n",
      "Epoch: 1877/2000... Training loss: 0.2823\n",
      "Epoch: 1877/2000... Training loss: 0.3800\n",
      "Epoch: 1877/2000... Training loss: 0.3368\n",
      "Epoch: 1877/2000... Training loss: 0.5144\n",
      "Epoch: 1877/2000... Training loss: 0.3836\n",
      "Epoch: 1877/2000... Training loss: 0.3838\n",
      "Epoch: 1877/2000... Training loss: 0.3725\n",
      "Epoch: 1877/2000... Training loss: 0.2756\n",
      "Epoch: 1877/2000... Training loss: 0.3360\n",
      "Epoch: 1877/2000... Training loss: 0.3725\n",
      "Epoch: 1877/2000... Training loss: 0.4041\n",
      "Epoch: 1877/2000... Training loss: 0.3525\n",
      "Epoch: 1877/2000... Training loss: 0.3207\n",
      "Epoch: 1877/2000... Training loss: 0.3462\n",
      "Epoch: 1877/2000... Training loss: 0.4605\n",
      "Epoch: 1877/2000... Training loss: 0.4029\n",
      "Epoch: 1877/2000... Training loss: 0.3490\n",
      "Epoch: 1877/2000... Training loss: 0.3095\n",
      "Epoch: 1877/2000... Training loss: 0.4134\n",
      "Epoch: 1877/2000... Training loss: 0.4324\n",
      "Epoch: 1877/2000... Training loss: 0.4688\n",
      "Epoch: 1877/2000... Training loss: 0.2747\n",
      "Epoch: 1877/2000... Training loss: 0.3755\n",
      "Epoch: 1878/2000... Training loss: 0.3580\n",
      "Epoch: 1878/2000... Training loss: 0.4322\n",
      "Epoch: 1878/2000... Training loss: 0.2184\n",
      "Epoch: 1878/2000... Training loss: 0.3988\n",
      "Epoch: 1878/2000... Training loss: 0.4947\n",
      "Epoch: 1878/2000... Training loss: 0.6888\n",
      "Epoch: 1878/2000... Training loss: 0.4582\n",
      "Epoch: 1878/2000... Training loss: 0.4073\n",
      "Epoch: 1878/2000... Training loss: 0.4135\n",
      "Epoch: 1878/2000... Training loss: 0.3424\n",
      "Epoch: 1878/2000... Training loss: 0.3163\n",
      "Epoch: 1878/2000... Training loss: 0.5321\n",
      "Epoch: 1878/2000... Training loss: 0.5038\n",
      "Epoch: 1878/2000... Training loss: 0.3374\n",
      "Epoch: 1878/2000... Training loss: 0.4914\n",
      "Epoch: 1878/2000... Training loss: 0.3005\n",
      "Epoch: 1878/2000... Training loss: 0.3937\n",
      "Epoch: 1878/2000... Training loss: 0.5188\n",
      "Epoch: 1878/2000... Training loss: 0.5205\n",
      "Epoch: 1878/2000... Training loss: 0.4822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1878/2000... Training loss: 0.2812\n",
      "Epoch: 1878/2000... Training loss: 0.2696\n",
      "Epoch: 1878/2000... Training loss: 0.2485\n",
      "Epoch: 1878/2000... Training loss: 0.3072\n",
      "Epoch: 1878/2000... Training loss: 0.4145\n",
      "Epoch: 1878/2000... Training loss: 0.5344\n",
      "Epoch: 1878/2000... Training loss: 0.2834\n",
      "Epoch: 1878/2000... Training loss: 0.6231\n",
      "Epoch: 1878/2000... Training loss: 0.3650\n",
      "Epoch: 1878/2000... Training loss: 0.3764\n",
      "Epoch: 1878/2000... Training loss: 0.4341\n",
      "Epoch: 1879/2000... Training loss: 0.3658\n",
      "Epoch: 1879/2000... Training loss: 0.4091\n",
      "Epoch: 1879/2000... Training loss: 0.2815\n",
      "Epoch: 1879/2000... Training loss: 0.4115\n",
      "Epoch: 1879/2000... Training loss: 0.5136\n",
      "Epoch: 1879/2000... Training loss: 0.4444\n",
      "Epoch: 1879/2000... Training loss: 0.4050\n",
      "Epoch: 1879/2000... Training loss: 0.3330\n",
      "Epoch: 1879/2000... Training loss: 0.3926\n",
      "Epoch: 1879/2000... Training loss: 0.4455\n",
      "Epoch: 1879/2000... Training loss: 0.5642\n",
      "Epoch: 1879/2000... Training loss: 0.4921\n",
      "Epoch: 1879/2000... Training loss: 0.3640\n",
      "Epoch: 1879/2000... Training loss: 0.3856\n",
      "Epoch: 1879/2000... Training loss: 0.6635\n",
      "Epoch: 1879/2000... Training loss: 0.2021\n",
      "Epoch: 1879/2000... Training loss: 0.2893\n",
      "Epoch: 1879/2000... Training loss: 0.3180\n",
      "Epoch: 1879/2000... Training loss: 0.4173\n",
      "Epoch: 1879/2000... Training loss: 0.2909\n",
      "Epoch: 1879/2000... Training loss: 0.3018\n",
      "Epoch: 1879/2000... Training loss: 0.4114\n",
      "Epoch: 1879/2000... Training loss: 0.4651\n",
      "Epoch: 1879/2000... Training loss: 0.4211\n",
      "Epoch: 1879/2000... Training loss: 0.4209\n",
      "Epoch: 1879/2000... Training loss: 0.5572\n",
      "Epoch: 1879/2000... Training loss: 0.3946\n",
      "Epoch: 1879/2000... Training loss: 0.2865\n",
      "Epoch: 1879/2000... Training loss: 0.5587\n",
      "Epoch: 1879/2000... Training loss: 0.3938\n",
      "Epoch: 1879/2000... Training loss: 0.5205\n",
      "Epoch: 1880/2000... Training loss: 0.2898\n",
      "Epoch: 1880/2000... Training loss: 0.3183\n",
      "Epoch: 1880/2000... Training loss: 0.4541\n",
      "Epoch: 1880/2000... Training loss: 0.3929\n",
      "Epoch: 1880/2000... Training loss: 0.2877\n",
      "Epoch: 1880/2000... Training loss: 0.3683\n",
      "Epoch: 1880/2000... Training loss: 0.3240\n",
      "Epoch: 1880/2000... Training loss: 0.4761\n",
      "Epoch: 1880/2000... Training loss: 0.3602\n",
      "Epoch: 1880/2000... Training loss: 0.2851\n",
      "Epoch: 1880/2000... Training loss: 0.3877\n",
      "Epoch: 1880/2000... Training loss: 0.4108\n",
      "Epoch: 1880/2000... Training loss: 0.4111\n",
      "Epoch: 1880/2000... Training loss: 0.2786\n",
      "Epoch: 1880/2000... Training loss: 0.2383\n",
      "Epoch: 1880/2000... Training loss: 0.3672\n",
      "Epoch: 1880/2000... Training loss: 0.3735\n",
      "Epoch: 1880/2000... Training loss: 0.5079\n",
      "Epoch: 1880/2000... Training loss: 0.2588\n",
      "Epoch: 1880/2000... Training loss: 0.4561\n",
      "Epoch: 1880/2000... Training loss: 0.4708\n",
      "Epoch: 1880/2000... Training loss: 0.4952\n",
      "Epoch: 1880/2000... Training loss: 0.5153\n",
      "Epoch: 1880/2000... Training loss: 0.4144\n",
      "Epoch: 1880/2000... Training loss: 0.4802\n",
      "Epoch: 1880/2000... Training loss: 0.3576\n",
      "Epoch: 1880/2000... Training loss: 0.6200\n",
      "Epoch: 1880/2000... Training loss: 0.4597\n",
      "Epoch: 1880/2000... Training loss: 0.4615\n",
      "Epoch: 1880/2000... Training loss: 0.3363\n",
      "Epoch: 1880/2000... Training loss: 0.3540\n",
      "Epoch: 1881/2000... Training loss: 0.3348\n",
      "Epoch: 1881/2000... Training loss: 0.3192\n",
      "Epoch: 1881/2000... Training loss: 0.5044\n",
      "Epoch: 1881/2000... Training loss: 0.3024\n",
      "Epoch: 1881/2000... Training loss: 0.2079\n",
      "Epoch: 1881/2000... Training loss: 0.4040\n",
      "Epoch: 1881/2000... Training loss: 0.3804\n",
      "Epoch: 1881/2000... Training loss: 0.2368\n",
      "Epoch: 1881/2000... Training loss: 0.4945\n",
      "Epoch: 1881/2000... Training loss: 0.4538\n",
      "Epoch: 1881/2000... Training loss: 0.4901\n",
      "Epoch: 1881/2000... Training loss: 0.5011\n",
      "Epoch: 1881/2000... Training loss: 0.3131\n",
      "Epoch: 1881/2000... Training loss: 0.3753\n",
      "Epoch: 1881/2000... Training loss: 0.4793\n",
      "Epoch: 1881/2000... Training loss: 0.3377\n",
      "Epoch: 1881/2000... Training loss: 0.3600\n",
      "Epoch: 1881/2000... Training loss: 0.5166\n",
      "Epoch: 1881/2000... Training loss: 0.3538\n",
      "Epoch: 1881/2000... Training loss: 0.3220\n",
      "Epoch: 1881/2000... Training loss: 0.3191\n",
      "Epoch: 1881/2000... Training loss: 0.4762\n",
      "Epoch: 1881/2000... Training loss: 0.5039\n",
      "Epoch: 1881/2000... Training loss: 0.3057\n",
      "Epoch: 1881/2000... Training loss: 0.2873\n",
      "Epoch: 1881/2000... Training loss: 0.4241\n",
      "Epoch: 1881/2000... Training loss: 0.3261\n",
      "Epoch: 1881/2000... Training loss: 0.4133\n",
      "Epoch: 1881/2000... Training loss: 0.4411\n",
      "Epoch: 1881/2000... Training loss: 0.3586\n",
      "Epoch: 1881/2000... Training loss: 0.3521\n",
      "Epoch: 1882/2000... Training loss: 0.4112\n",
      "Epoch: 1882/2000... Training loss: 0.4771\n",
      "Epoch: 1882/2000... Training loss: 0.3168\n",
      "Epoch: 1882/2000... Training loss: 0.4582\n",
      "Epoch: 1882/2000... Training loss: 0.3895\n",
      "Epoch: 1882/2000... Training loss: 0.3977\n",
      "Epoch: 1882/2000... Training loss: 0.4974\n",
      "Epoch: 1882/2000... Training loss: 0.3171\n",
      "Epoch: 1882/2000... Training loss: 0.4305\n",
      "Epoch: 1882/2000... Training loss: 0.2858\n",
      "Epoch: 1882/2000... Training loss: 0.5668\n",
      "Epoch: 1882/2000... Training loss: 0.3254\n",
      "Epoch: 1882/2000... Training loss: 0.3613\n",
      "Epoch: 1882/2000... Training loss: 0.4902\n",
      "Epoch: 1882/2000... Training loss: 0.3257\n",
      "Epoch: 1882/2000... Training loss: 0.3371\n",
      "Epoch: 1882/2000... Training loss: 0.3483\n",
      "Epoch: 1882/2000... Training loss: 0.3264\n",
      "Epoch: 1882/2000... Training loss: 0.3832\n",
      "Epoch: 1882/2000... Training loss: 0.3450\n",
      "Epoch: 1882/2000... Training loss: 0.3042\n",
      "Epoch: 1882/2000... Training loss: 0.4132\n",
      "Epoch: 1882/2000... Training loss: 0.3866\n",
      "Epoch: 1882/2000... Training loss: 0.3355\n",
      "Epoch: 1882/2000... Training loss: 0.3098\n",
      "Epoch: 1882/2000... Training loss: 0.4511\n",
      "Epoch: 1882/2000... Training loss: 0.4169\n",
      "Epoch: 1882/2000... Training loss: 0.2988\n",
      "Epoch: 1882/2000... Training loss: 0.2061\n",
      "Epoch: 1882/2000... Training loss: 0.4980\n",
      "Epoch: 1882/2000... Training loss: 0.5793\n",
      "Epoch: 1883/2000... Training loss: 0.3239\n",
      "Epoch: 1883/2000... Training loss: 0.3647\n",
      "Epoch: 1883/2000... Training loss: 0.4326\n",
      "Epoch: 1883/2000... Training loss: 0.4430\n",
      "Epoch: 1883/2000... Training loss: 0.3710\n",
      "Epoch: 1883/2000... Training loss: 0.4280\n",
      "Epoch: 1883/2000... Training loss: 0.2659\n",
      "Epoch: 1883/2000... Training loss: 0.4867\n",
      "Epoch: 1883/2000... Training loss: 0.3065\n",
      "Epoch: 1883/2000... Training loss: 0.3775\n",
      "Epoch: 1883/2000... Training loss: 0.3364\n",
      "Epoch: 1883/2000... Training loss: 0.4199\n",
      "Epoch: 1883/2000... Training loss: 0.1692\n",
      "Epoch: 1883/2000... Training loss: 0.2612\n",
      "Epoch: 1883/2000... Training loss: 0.2680\n",
      "Epoch: 1883/2000... Training loss: 0.4992\n",
      "Epoch: 1883/2000... Training loss: 0.5297\n",
      "Epoch: 1883/2000... Training loss: 0.3911\n",
      "Epoch: 1883/2000... Training loss: 0.3141\n",
      "Epoch: 1883/2000... Training loss: 0.3609\n",
      "Epoch: 1883/2000... Training loss: 0.4996\n",
      "Epoch: 1883/2000... Training loss: 0.3634\n",
      "Epoch: 1883/2000... Training loss: 0.3663\n",
      "Epoch: 1883/2000... Training loss: 0.4046\n",
      "Epoch: 1883/2000... Training loss: 0.3158\n",
      "Epoch: 1883/2000... Training loss: 0.4262\n",
      "Epoch: 1883/2000... Training loss: 0.3642\n",
      "Epoch: 1883/2000... Training loss: 0.4494\n",
      "Epoch: 1883/2000... Training loss: 0.2442\n",
      "Epoch: 1883/2000... Training loss: 0.2770\n",
      "Epoch: 1883/2000... Training loss: 0.3810\n",
      "Epoch: 1884/2000... Training loss: 0.4279\n",
      "Epoch: 1884/2000... Training loss: 0.1950\n",
      "Epoch: 1884/2000... Training loss: 0.3373\n",
      "Epoch: 1884/2000... Training loss: 0.2848\n",
      "Epoch: 1884/2000... Training loss: 0.5438\n",
      "Epoch: 1884/2000... Training loss: 0.3073\n",
      "Epoch: 1884/2000... Training loss: 0.3497\n",
      "Epoch: 1884/2000... Training loss: 0.3602\n",
      "Epoch: 1884/2000... Training loss: 0.6486\n",
      "Epoch: 1884/2000... Training loss: 0.3467\n",
      "Epoch: 1884/2000... Training loss: 0.2304\n",
      "Epoch: 1884/2000... Training loss: 0.3421\n",
      "Epoch: 1884/2000... Training loss: 0.2344\n",
      "Epoch: 1884/2000... Training loss: 0.3019\n",
      "Epoch: 1884/2000... Training loss: 0.4068\n",
      "Epoch: 1884/2000... Training loss: 0.3853\n",
      "Epoch: 1884/2000... Training loss: 0.2600\n",
      "Epoch: 1884/2000... Training loss: 0.4259\n",
      "Epoch: 1884/2000... Training loss: 0.3465\n",
      "Epoch: 1884/2000... Training loss: 0.4358\n",
      "Epoch: 1884/2000... Training loss: 0.2819\n",
      "Epoch: 1884/2000... Training loss: 0.4348\n",
      "Epoch: 1884/2000... Training loss: 0.3547\n",
      "Epoch: 1884/2000... Training loss: 0.4609\n",
      "Epoch: 1884/2000... Training loss: 0.3744\n",
      "Epoch: 1884/2000... Training loss: 0.4011\n",
      "Epoch: 1884/2000... Training loss: 0.4032\n",
      "Epoch: 1884/2000... Training loss: 0.2717\n",
      "Epoch: 1884/2000... Training loss: 0.4071\n",
      "Epoch: 1884/2000... Training loss: 0.2676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1884/2000... Training loss: 0.2997\n",
      "Epoch: 1885/2000... Training loss: 0.4035\n",
      "Epoch: 1885/2000... Training loss: 0.4386\n",
      "Epoch: 1885/2000... Training loss: 0.3645\n",
      "Epoch: 1885/2000... Training loss: 0.3479\n",
      "Epoch: 1885/2000... Training loss: 0.4063\n",
      "Epoch: 1885/2000... Training loss: 0.2911\n",
      "Epoch: 1885/2000... Training loss: 0.4304\n",
      "Epoch: 1885/2000... Training loss: 0.2326\n",
      "Epoch: 1885/2000... Training loss: 0.3339\n",
      "Epoch: 1885/2000... Training loss: 0.3340\n",
      "Epoch: 1885/2000... Training loss: 0.5241\n",
      "Epoch: 1885/2000... Training loss: 0.3760\n",
      "Epoch: 1885/2000... Training loss: 0.2946\n",
      "Epoch: 1885/2000... Training loss: 0.2636\n",
      "Epoch: 1885/2000... Training loss: 0.4239\n",
      "Epoch: 1885/2000... Training loss: 0.4527\n",
      "Epoch: 1885/2000... Training loss: 0.3427\n",
      "Epoch: 1885/2000... Training loss: 0.3909\n",
      "Epoch: 1885/2000... Training loss: 0.3401\n",
      "Epoch: 1885/2000... Training loss: 0.3608\n",
      "Epoch: 1885/2000... Training loss: 0.3477\n",
      "Epoch: 1885/2000... Training loss: 0.3569\n",
      "Epoch: 1885/2000... Training loss: 0.3760\n",
      "Epoch: 1885/2000... Training loss: 0.3950\n",
      "Epoch: 1885/2000... Training loss: 0.3293\n",
      "Epoch: 1885/2000... Training loss: 0.2131\n",
      "Epoch: 1885/2000... Training loss: 0.3284\n",
      "Epoch: 1885/2000... Training loss: 0.3287\n",
      "Epoch: 1885/2000... Training loss: 0.3557\n",
      "Epoch: 1885/2000... Training loss: 0.2414\n",
      "Epoch: 1885/2000... Training loss: 0.3040\n",
      "Epoch: 1886/2000... Training loss: 0.4843\n",
      "Epoch: 1886/2000... Training loss: 0.4204\n",
      "Epoch: 1886/2000... Training loss: 0.5008\n",
      "Epoch: 1886/2000... Training loss: 0.3390\n",
      "Epoch: 1886/2000... Training loss: 0.4588\n",
      "Epoch: 1886/2000... Training loss: 0.3405\n",
      "Epoch: 1886/2000... Training loss: 0.4038\n",
      "Epoch: 1886/2000... Training loss: 0.3832\n",
      "Epoch: 1886/2000... Training loss: 0.4307\n",
      "Epoch: 1886/2000... Training loss: 0.5154\n",
      "Epoch: 1886/2000... Training loss: 0.3607\n",
      "Epoch: 1886/2000... Training loss: 0.3127\n",
      "Epoch: 1886/2000... Training loss: 0.4363\n",
      "Epoch: 1886/2000... Training loss: 0.4007\n",
      "Epoch: 1886/2000... Training loss: 0.3524\n",
      "Epoch: 1886/2000... Training loss: 0.1820\n",
      "Epoch: 1886/2000... Training loss: 0.4620\n",
      "Epoch: 1886/2000... Training loss: 0.3901\n",
      "Epoch: 1886/2000... Training loss: 0.4682\n",
      "Epoch: 1886/2000... Training loss: 0.4303\n",
      "Epoch: 1886/2000... Training loss: 0.2862\n",
      "Epoch: 1886/2000... Training loss: 0.3014\n",
      "Epoch: 1886/2000... Training loss: 0.4022\n",
      "Epoch: 1886/2000... Training loss: 0.4421\n",
      "Epoch: 1886/2000... Training loss: 0.3028\n",
      "Epoch: 1886/2000... Training loss: 0.3464\n",
      "Epoch: 1886/2000... Training loss: 0.4359\n",
      "Epoch: 1886/2000... Training loss: 0.3987\n",
      "Epoch: 1886/2000... Training loss: 0.3531\n",
      "Epoch: 1886/2000... Training loss: 0.4599\n",
      "Epoch: 1886/2000... Training loss: 0.4899\n",
      "Epoch: 1887/2000... Training loss: 0.3253\n",
      "Epoch: 1887/2000... Training loss: 0.4022\n",
      "Epoch: 1887/2000... Training loss: 0.3505\n",
      "Epoch: 1887/2000... Training loss: 0.4013\n",
      "Epoch: 1887/2000... Training loss: 0.3762\n",
      "Epoch: 1887/2000... Training loss: 0.3775\n",
      "Epoch: 1887/2000... Training loss: 0.6278\n",
      "Epoch: 1887/2000... Training loss: 0.3275\n",
      "Epoch: 1887/2000... Training loss: 0.4380\n",
      "Epoch: 1887/2000... Training loss: 0.4546\n",
      "Epoch: 1887/2000... Training loss: 0.2169\n",
      "Epoch: 1887/2000... Training loss: 0.2834\n",
      "Epoch: 1887/2000... Training loss: 0.4977\n",
      "Epoch: 1887/2000... Training loss: 0.3739\n",
      "Epoch: 1887/2000... Training loss: 0.2317\n",
      "Epoch: 1887/2000... Training loss: 0.3340\n",
      "Epoch: 1887/2000... Training loss: 0.3930\n",
      "Epoch: 1887/2000... Training loss: 0.1945\n",
      "Epoch: 1887/2000... Training loss: 0.2706\n",
      "Epoch: 1887/2000... Training loss: 0.4202\n",
      "Epoch: 1887/2000... Training loss: 0.2963\n",
      "Epoch: 1887/2000... Training loss: 0.5132\n",
      "Epoch: 1887/2000... Training loss: 0.2285\n",
      "Epoch: 1887/2000... Training loss: 0.3827\n",
      "Epoch: 1887/2000... Training loss: 0.5910\n",
      "Epoch: 1887/2000... Training loss: 0.2625\n",
      "Epoch: 1887/2000... Training loss: 0.5150\n",
      "Epoch: 1887/2000... Training loss: 0.4906\n",
      "Epoch: 1887/2000... Training loss: 0.3004\n",
      "Epoch: 1887/2000... Training loss: 0.4456\n",
      "Epoch: 1887/2000... Training loss: 0.4278\n",
      "Epoch: 1888/2000... Training loss: 0.4032\n",
      "Epoch: 1888/2000... Training loss: 0.3131\n",
      "Epoch: 1888/2000... Training loss: 0.3728\n",
      "Epoch: 1888/2000... Training loss: 0.4583\n",
      "Epoch: 1888/2000... Training loss: 0.5869\n",
      "Epoch: 1888/2000... Training loss: 0.4195\n",
      "Epoch: 1888/2000... Training loss: 0.3503\n",
      "Epoch: 1888/2000... Training loss: 0.3092\n",
      "Epoch: 1888/2000... Training loss: 0.3308\n",
      "Epoch: 1888/2000... Training loss: 0.3693\n",
      "Epoch: 1888/2000... Training loss: 0.3958\n",
      "Epoch: 1888/2000... Training loss: 0.3011\n",
      "Epoch: 1888/2000... Training loss: 0.3393\n",
      "Epoch: 1888/2000... Training loss: 0.4280\n",
      "Epoch: 1888/2000... Training loss: 0.4280\n",
      "Epoch: 1888/2000... Training loss: 0.3062\n",
      "Epoch: 1888/2000... Training loss: 0.3023\n",
      "Epoch: 1888/2000... Training loss: 0.5178\n",
      "Epoch: 1888/2000... Training loss: 0.4775\n",
      "Epoch: 1888/2000... Training loss: 0.2444\n",
      "Epoch: 1888/2000... Training loss: 0.3343\n",
      "Epoch: 1888/2000... Training loss: 0.2891\n",
      "Epoch: 1888/2000... Training loss: 0.4235\n",
      "Epoch: 1888/2000... Training loss: 0.3979\n",
      "Epoch: 1888/2000... Training loss: 0.3107\n",
      "Epoch: 1888/2000... Training loss: 0.3209\n",
      "Epoch: 1888/2000... Training loss: 0.3125\n",
      "Epoch: 1888/2000... Training loss: 0.2522\n",
      "Epoch: 1888/2000... Training loss: 0.2847\n",
      "Epoch: 1888/2000... Training loss: 0.4508\n",
      "Epoch: 1888/2000... Training loss: 0.3360\n",
      "Epoch: 1889/2000... Training loss: 0.3392\n",
      "Epoch: 1889/2000... Training loss: 0.4133\n",
      "Epoch: 1889/2000... Training loss: 0.4184\n",
      "Epoch: 1889/2000... Training loss: 0.2937\n",
      "Epoch: 1889/2000... Training loss: 0.2718\n",
      "Epoch: 1889/2000... Training loss: 0.3747\n",
      "Epoch: 1889/2000... Training loss: 0.4728\n",
      "Epoch: 1889/2000... Training loss: 0.3063\n",
      "Epoch: 1889/2000... Training loss: 0.4476\n",
      "Epoch: 1889/2000... Training loss: 0.3184\n",
      "Epoch: 1889/2000... Training loss: 0.3662\n",
      "Epoch: 1889/2000... Training loss: 0.3886\n",
      "Epoch: 1889/2000... Training loss: 0.4604\n",
      "Epoch: 1889/2000... Training loss: 0.4324\n",
      "Epoch: 1889/2000... Training loss: 0.4387\n",
      "Epoch: 1889/2000... Training loss: 0.2586\n",
      "Epoch: 1889/2000... Training loss: 0.3626\n",
      "Epoch: 1889/2000... Training loss: 0.5400\n",
      "Epoch: 1889/2000... Training loss: 0.4943\n",
      "Epoch: 1889/2000... Training loss: 0.3090\n",
      "Epoch: 1889/2000... Training loss: 0.4981\n",
      "Epoch: 1889/2000... Training loss: 0.2609\n",
      "Epoch: 1889/2000... Training loss: 0.3402\n",
      "Epoch: 1889/2000... Training loss: 0.4180\n",
      "Epoch: 1889/2000... Training loss: 0.2240\n",
      "Epoch: 1889/2000... Training loss: 0.3037\n",
      "Epoch: 1889/2000... Training loss: 0.3513\n",
      "Epoch: 1889/2000... Training loss: 0.3655\n",
      "Epoch: 1889/2000... Training loss: 0.2534\n",
      "Epoch: 1889/2000... Training loss: 0.2857\n",
      "Epoch: 1889/2000... Training loss: 0.2722\n",
      "Epoch: 1890/2000... Training loss: 0.4936\n",
      "Epoch: 1890/2000... Training loss: 0.2927\n",
      "Epoch: 1890/2000... Training loss: 0.3411\n",
      "Epoch: 1890/2000... Training loss: 0.4500\n",
      "Epoch: 1890/2000... Training loss: 0.3645\n",
      "Epoch: 1890/2000... Training loss: 0.3834\n",
      "Epoch: 1890/2000... Training loss: 0.4719\n",
      "Epoch: 1890/2000... Training loss: 0.5045\n",
      "Epoch: 1890/2000... Training loss: 0.3987\n",
      "Epoch: 1890/2000... Training loss: 0.3515\n",
      "Epoch: 1890/2000... Training loss: 0.3843\n",
      "Epoch: 1890/2000... Training loss: 0.3520\n",
      "Epoch: 1890/2000... Training loss: 0.4027\n",
      "Epoch: 1890/2000... Training loss: 0.3348\n",
      "Epoch: 1890/2000... Training loss: 0.4498\n",
      "Epoch: 1890/2000... Training loss: 0.2907\n",
      "Epoch: 1890/2000... Training loss: 0.3916\n",
      "Epoch: 1890/2000... Training loss: 0.3192\n",
      "Epoch: 1890/2000... Training loss: 0.4352\n",
      "Epoch: 1890/2000... Training loss: 0.5222\n",
      "Epoch: 1890/2000... Training loss: 0.3067\n",
      "Epoch: 1890/2000... Training loss: 0.4134\n",
      "Epoch: 1890/2000... Training loss: 0.3082\n",
      "Epoch: 1890/2000... Training loss: 0.2373\n",
      "Epoch: 1890/2000... Training loss: 0.3909\n",
      "Epoch: 1890/2000... Training loss: 0.4915\n",
      "Epoch: 1890/2000... Training loss: 0.4831\n",
      "Epoch: 1890/2000... Training loss: 0.4394\n",
      "Epoch: 1890/2000... Training loss: 0.4279\n",
      "Epoch: 1890/2000... Training loss: 0.4777\n",
      "Epoch: 1890/2000... Training loss: 0.4570\n",
      "Epoch: 1891/2000... Training loss: 0.4237\n",
      "Epoch: 1891/2000... Training loss: 0.4362\n",
      "Epoch: 1891/2000... Training loss: 0.5399\n",
      "Epoch: 1891/2000... Training loss: 0.2557\n",
      "Epoch: 1891/2000... Training loss: 0.4720\n",
      "Epoch: 1891/2000... Training loss: 0.2322\n",
      "Epoch: 1891/2000... Training loss: 0.2095\n",
      "Epoch: 1891/2000... Training loss: 0.3108\n",
      "Epoch: 1891/2000... Training loss: 0.4038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1891/2000... Training loss: 0.3554\n",
      "Epoch: 1891/2000... Training loss: 0.3520\n",
      "Epoch: 1891/2000... Training loss: 0.3059\n",
      "Epoch: 1891/2000... Training loss: 0.3446\n",
      "Epoch: 1891/2000... Training loss: 0.4765\n",
      "Epoch: 1891/2000... Training loss: 0.2663\n",
      "Epoch: 1891/2000... Training loss: 0.3604\n",
      "Epoch: 1891/2000... Training loss: 0.4089\n",
      "Epoch: 1891/2000... Training loss: 0.3779\n",
      "Epoch: 1891/2000... Training loss: 0.3879\n",
      "Epoch: 1891/2000... Training loss: 0.5117\n",
      "Epoch: 1891/2000... Training loss: 0.3843\n",
      "Epoch: 1891/2000... Training loss: 0.3581\n",
      "Epoch: 1891/2000... Training loss: 0.6114\n",
      "Epoch: 1891/2000... Training loss: 0.3856\n",
      "Epoch: 1891/2000... Training loss: 0.2738\n",
      "Epoch: 1891/2000... Training loss: 0.2932\n",
      "Epoch: 1891/2000... Training loss: 0.3173\n",
      "Epoch: 1891/2000... Training loss: 0.6176\n",
      "Epoch: 1891/2000... Training loss: 0.4800\n",
      "Epoch: 1891/2000... Training loss: 0.1830\n",
      "Epoch: 1891/2000... Training loss: 0.3974\n",
      "Epoch: 1892/2000... Training loss: 0.3232\n",
      "Epoch: 1892/2000... Training loss: 0.3251\n",
      "Epoch: 1892/2000... Training loss: 0.3862\n",
      "Epoch: 1892/2000... Training loss: 0.2359\n",
      "Epoch: 1892/2000... Training loss: 0.2633\n",
      "Epoch: 1892/2000... Training loss: 0.3593\n",
      "Epoch: 1892/2000... Training loss: 0.2781\n",
      "Epoch: 1892/2000... Training loss: 0.3071\n",
      "Epoch: 1892/2000... Training loss: 0.3338\n",
      "Epoch: 1892/2000... Training loss: 0.2939\n",
      "Epoch: 1892/2000... Training loss: 0.2584\n",
      "Epoch: 1892/2000... Training loss: 0.3662\n",
      "Epoch: 1892/2000... Training loss: 0.4764\n",
      "Epoch: 1892/2000... Training loss: 0.3779\n",
      "Epoch: 1892/2000... Training loss: 0.3185\n",
      "Epoch: 1892/2000... Training loss: 0.4867\n",
      "Epoch: 1892/2000... Training loss: 0.3825\n",
      "Epoch: 1892/2000... Training loss: 0.3450\n",
      "Epoch: 1892/2000... Training loss: 0.2631\n",
      "Epoch: 1892/2000... Training loss: 0.3394\n",
      "Epoch: 1892/2000... Training loss: 0.4790\n",
      "Epoch: 1892/2000... Training loss: 0.3915\n",
      "Epoch: 1892/2000... Training loss: 0.2877\n",
      "Epoch: 1892/2000... Training loss: 0.3089\n",
      "Epoch: 1892/2000... Training loss: 0.3455\n",
      "Epoch: 1892/2000... Training loss: 0.4948\n",
      "Epoch: 1892/2000... Training loss: 0.4803\n",
      "Epoch: 1892/2000... Training loss: 0.3315\n",
      "Epoch: 1892/2000... Training loss: 0.4816\n",
      "Epoch: 1892/2000... Training loss: 0.4532\n",
      "Epoch: 1892/2000... Training loss: 0.4397\n",
      "Epoch: 1893/2000... Training loss: 0.4299\n",
      "Epoch: 1893/2000... Training loss: 0.3644\n",
      "Epoch: 1893/2000... Training loss: 0.3385\n",
      "Epoch: 1893/2000... Training loss: 0.4026\n",
      "Epoch: 1893/2000... Training loss: 0.4123\n",
      "Epoch: 1893/2000... Training loss: 0.2824\n",
      "Epoch: 1893/2000... Training loss: 0.4342\n",
      "Epoch: 1893/2000... Training loss: 0.3321\n",
      "Epoch: 1893/2000... Training loss: 0.2883\n",
      "Epoch: 1893/2000... Training loss: 0.4176\n",
      "Epoch: 1893/2000... Training loss: 0.3832\n",
      "Epoch: 1893/2000... Training loss: 0.4333\n",
      "Epoch: 1893/2000... Training loss: 0.4256\n",
      "Epoch: 1893/2000... Training loss: 0.4600\n",
      "Epoch: 1893/2000... Training loss: 0.4583\n",
      "Epoch: 1893/2000... Training loss: 0.3964\n",
      "Epoch: 1893/2000... Training loss: 0.3193\n",
      "Epoch: 1893/2000... Training loss: 0.2033\n",
      "Epoch: 1893/2000... Training loss: 0.4642\n",
      "Epoch: 1893/2000... Training loss: 0.3451\n",
      "Epoch: 1893/2000... Training loss: 0.4248\n",
      "Epoch: 1893/2000... Training loss: 0.1801\n",
      "Epoch: 1893/2000... Training loss: 0.4210\n",
      "Epoch: 1893/2000... Training loss: 0.4207\n",
      "Epoch: 1893/2000... Training loss: 0.3239\n",
      "Epoch: 1893/2000... Training loss: 0.2956\n",
      "Epoch: 1893/2000... Training loss: 0.4029\n",
      "Epoch: 1893/2000... Training loss: 0.3547\n",
      "Epoch: 1893/2000... Training loss: 0.4958\n",
      "Epoch: 1893/2000... Training loss: 0.3040\n",
      "Epoch: 1893/2000... Training loss: 0.4097\n",
      "Epoch: 1894/2000... Training loss: 0.6548\n",
      "Epoch: 1894/2000... Training loss: 0.3408\n",
      "Epoch: 1894/2000... Training loss: 0.5182\n",
      "Epoch: 1894/2000... Training loss: 0.3385\n",
      "Epoch: 1894/2000... Training loss: 0.4784\n",
      "Epoch: 1894/2000... Training loss: 0.3529\n",
      "Epoch: 1894/2000... Training loss: 0.4912\n",
      "Epoch: 1894/2000... Training loss: 0.3102\n",
      "Epoch: 1894/2000... Training loss: 0.4873\n",
      "Epoch: 1894/2000... Training loss: 0.5165\n",
      "Epoch: 1894/2000... Training loss: 0.3834\n",
      "Epoch: 1894/2000... Training loss: 0.3574\n",
      "Epoch: 1894/2000... Training loss: 0.1936\n",
      "Epoch: 1894/2000... Training loss: 0.6013\n",
      "Epoch: 1894/2000... Training loss: 0.4137\n",
      "Epoch: 1894/2000... Training loss: 0.2671\n",
      "Epoch: 1894/2000... Training loss: 0.3902\n",
      "Epoch: 1894/2000... Training loss: 0.4729\n",
      "Epoch: 1894/2000... Training loss: 0.3955\n",
      "Epoch: 1894/2000... Training loss: 0.3445\n",
      "Epoch: 1894/2000... Training loss: 0.3624\n",
      "Epoch: 1894/2000... Training loss: 0.4208\n",
      "Epoch: 1894/2000... Training loss: 0.3285\n",
      "Epoch: 1894/2000... Training loss: 0.3389\n",
      "Epoch: 1894/2000... Training loss: 0.3768\n",
      "Epoch: 1894/2000... Training loss: 0.2700\n",
      "Epoch: 1894/2000... Training loss: 0.5182\n",
      "Epoch: 1894/2000... Training loss: 0.2921\n",
      "Epoch: 1894/2000... Training loss: 0.4528\n",
      "Epoch: 1894/2000... Training loss: 0.5069\n",
      "Epoch: 1894/2000... Training loss: 0.2987\n",
      "Epoch: 1895/2000... Training loss: 0.3863\n",
      "Epoch: 1895/2000... Training loss: 0.3829\n",
      "Epoch: 1895/2000... Training loss: 0.4892\n",
      "Epoch: 1895/2000... Training loss: 0.2009\n",
      "Epoch: 1895/2000... Training loss: 0.2894\n",
      "Epoch: 1895/2000... Training loss: 0.2945\n",
      "Epoch: 1895/2000... Training loss: 0.5265\n",
      "Epoch: 1895/2000... Training loss: 0.3282\n",
      "Epoch: 1895/2000... Training loss: 0.4740\n",
      "Epoch: 1895/2000... Training loss: 0.4849\n",
      "Epoch: 1895/2000... Training loss: 0.3306\n",
      "Epoch: 1895/2000... Training loss: 0.3817\n",
      "Epoch: 1895/2000... Training loss: 0.4352\n",
      "Epoch: 1895/2000... Training loss: 0.4182\n",
      "Epoch: 1895/2000... Training loss: 0.3630\n",
      "Epoch: 1895/2000... Training loss: 0.4918\n",
      "Epoch: 1895/2000... Training loss: 0.2503\n",
      "Epoch: 1895/2000... Training loss: 0.3639\n",
      "Epoch: 1895/2000... Training loss: 0.2784\n",
      "Epoch: 1895/2000... Training loss: 0.3893\n",
      "Epoch: 1895/2000... Training loss: 0.3483\n",
      "Epoch: 1895/2000... Training loss: 0.3859\n",
      "Epoch: 1895/2000... Training loss: 0.5293\n",
      "Epoch: 1895/2000... Training loss: 0.3062\n",
      "Epoch: 1895/2000... Training loss: 0.2781\n",
      "Epoch: 1895/2000... Training loss: 0.2943\n",
      "Epoch: 1895/2000... Training loss: 0.4463\n",
      "Epoch: 1895/2000... Training loss: 0.5907\n",
      "Epoch: 1895/2000... Training loss: 0.4478\n",
      "Epoch: 1895/2000... Training loss: 0.3654\n",
      "Epoch: 1895/2000... Training loss: 0.2973\n",
      "Epoch: 1896/2000... Training loss: 0.4181\n",
      "Epoch: 1896/2000... Training loss: 0.6312\n",
      "Epoch: 1896/2000... Training loss: 0.4062\n",
      "Epoch: 1896/2000... Training loss: 0.3844\n",
      "Epoch: 1896/2000... Training loss: 0.4071\n",
      "Epoch: 1896/2000... Training loss: 0.4124\n",
      "Epoch: 1896/2000... Training loss: 0.3740\n",
      "Epoch: 1896/2000... Training loss: 0.3080\n",
      "Epoch: 1896/2000... Training loss: 0.4023\n",
      "Epoch: 1896/2000... Training loss: 0.2576\n",
      "Epoch: 1896/2000... Training loss: 0.3570\n",
      "Epoch: 1896/2000... Training loss: 0.3881\n",
      "Epoch: 1896/2000... Training loss: 0.3721\n",
      "Epoch: 1896/2000... Training loss: 0.4378\n",
      "Epoch: 1896/2000... Training loss: 0.3790\n",
      "Epoch: 1896/2000... Training loss: 0.2943\n",
      "Epoch: 1896/2000... Training loss: 0.3881\n",
      "Epoch: 1896/2000... Training loss: 0.3448\n",
      "Epoch: 1896/2000... Training loss: 0.3727\n",
      "Epoch: 1896/2000... Training loss: 0.4587\n",
      "Epoch: 1896/2000... Training loss: 0.3434\n",
      "Epoch: 1896/2000... Training loss: 0.2955\n",
      "Epoch: 1896/2000... Training loss: 0.4480\n",
      "Epoch: 1896/2000... Training loss: 0.4738\n",
      "Epoch: 1896/2000... Training loss: 0.2780\n",
      "Epoch: 1896/2000... Training loss: 0.4290\n",
      "Epoch: 1896/2000... Training loss: 0.4693\n",
      "Epoch: 1896/2000... Training loss: 0.4397\n",
      "Epoch: 1896/2000... Training loss: 0.3336\n",
      "Epoch: 1896/2000... Training loss: 0.2766\n",
      "Epoch: 1896/2000... Training loss: 0.3920\n",
      "Epoch: 1897/2000... Training loss: 0.4130\n",
      "Epoch: 1897/2000... Training loss: 0.4733\n",
      "Epoch: 1897/2000... Training loss: 0.4595\n",
      "Epoch: 1897/2000... Training loss: 0.2670\n",
      "Epoch: 1897/2000... Training loss: 0.4126\n",
      "Epoch: 1897/2000... Training loss: 0.3385\n",
      "Epoch: 1897/2000... Training loss: 0.5009\n",
      "Epoch: 1897/2000... Training loss: 0.3173\n",
      "Epoch: 1897/2000... Training loss: 0.4773\n",
      "Epoch: 1897/2000... Training loss: 0.4097\n",
      "Epoch: 1897/2000... Training loss: 0.4269\n",
      "Epoch: 1897/2000... Training loss: 0.4211\n",
      "Epoch: 1897/2000... Training loss: 0.3628\n",
      "Epoch: 1897/2000... Training loss: 0.4684\n",
      "Epoch: 1897/2000... Training loss: 0.4670\n",
      "Epoch: 1897/2000... Training loss: 0.3560\n",
      "Epoch: 1897/2000... Training loss: 0.2785\n",
      "Epoch: 1897/2000... Training loss: 0.4144\n",
      "Epoch: 1897/2000... Training loss: 0.3788\n",
      "Epoch: 1897/2000... Training loss: 0.4445\n",
      "Epoch: 1897/2000... Training loss: 0.5879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1897/2000... Training loss: 0.3273\n",
      "Epoch: 1897/2000... Training loss: 0.3766\n",
      "Epoch: 1897/2000... Training loss: 0.3599\n",
      "Epoch: 1897/2000... Training loss: 0.4910\n",
      "Epoch: 1897/2000... Training loss: 0.3829\n",
      "Epoch: 1897/2000... Training loss: 0.3365\n",
      "Epoch: 1897/2000... Training loss: 0.1444\n",
      "Epoch: 1897/2000... Training loss: 0.2874\n",
      "Epoch: 1897/2000... Training loss: 0.2990\n",
      "Epoch: 1897/2000... Training loss: 0.3803\n",
      "Epoch: 1898/2000... Training loss: 0.5284\n",
      "Epoch: 1898/2000... Training loss: 0.4308\n",
      "Epoch: 1898/2000... Training loss: 0.4036\n",
      "Epoch: 1898/2000... Training loss: 0.2283\n",
      "Epoch: 1898/2000... Training loss: 0.2950\n",
      "Epoch: 1898/2000... Training loss: 0.4271\n",
      "Epoch: 1898/2000... Training loss: 0.3352\n",
      "Epoch: 1898/2000... Training loss: 0.4315\n",
      "Epoch: 1898/2000... Training loss: 0.4498\n",
      "Epoch: 1898/2000... Training loss: 0.4424\n",
      "Epoch: 1898/2000... Training loss: 0.4470\n",
      "Epoch: 1898/2000... Training loss: 0.5455\n",
      "Epoch: 1898/2000... Training loss: 0.5343\n",
      "Epoch: 1898/2000... Training loss: 0.4671\n",
      "Epoch: 1898/2000... Training loss: 0.3661\n",
      "Epoch: 1898/2000... Training loss: 0.2622\n",
      "Epoch: 1898/2000... Training loss: 0.2811\n",
      "Epoch: 1898/2000... Training loss: 0.5651\n",
      "Epoch: 1898/2000... Training loss: 0.3451\n",
      "Epoch: 1898/2000... Training loss: 0.3851\n",
      "Epoch: 1898/2000... Training loss: 0.3315\n",
      "Epoch: 1898/2000... Training loss: 0.5471\n",
      "Epoch: 1898/2000... Training loss: 0.3691\n",
      "Epoch: 1898/2000... Training loss: 0.3386\n",
      "Epoch: 1898/2000... Training loss: 0.3869\n",
      "Epoch: 1898/2000... Training loss: 0.3594\n",
      "Epoch: 1898/2000... Training loss: 0.3007\n",
      "Epoch: 1898/2000... Training loss: 0.3556\n",
      "Epoch: 1898/2000... Training loss: 0.2844\n",
      "Epoch: 1898/2000... Training loss: 0.2551\n",
      "Epoch: 1898/2000... Training loss: 0.4877\n",
      "Epoch: 1899/2000... Training loss: 0.2356\n",
      "Epoch: 1899/2000... Training loss: 0.4064\n",
      "Epoch: 1899/2000... Training loss: 0.2948\n",
      "Epoch: 1899/2000... Training loss: 0.3409\n",
      "Epoch: 1899/2000... Training loss: 0.4091\n",
      "Epoch: 1899/2000... Training loss: 0.3373\n",
      "Epoch: 1899/2000... Training loss: 0.3037\n",
      "Epoch: 1899/2000... Training loss: 0.3110\n",
      "Epoch: 1899/2000... Training loss: 0.3500\n",
      "Epoch: 1899/2000... Training loss: 0.3609\n",
      "Epoch: 1899/2000... Training loss: 0.4083\n",
      "Epoch: 1899/2000... Training loss: 0.4055\n",
      "Epoch: 1899/2000... Training loss: 0.3378\n",
      "Epoch: 1899/2000... Training loss: 0.4260\n",
      "Epoch: 1899/2000... Training loss: 0.3383\n",
      "Epoch: 1899/2000... Training loss: 0.4983\n",
      "Epoch: 1899/2000... Training loss: 0.3836\n",
      "Epoch: 1899/2000... Training loss: 0.4305\n",
      "Epoch: 1899/2000... Training loss: 0.4666\n",
      "Epoch: 1899/2000... Training loss: 0.3663\n",
      "Epoch: 1899/2000... Training loss: 0.4817\n",
      "Epoch: 1899/2000... Training loss: 0.3348\n",
      "Epoch: 1899/2000... Training loss: 0.3124\n",
      "Epoch: 1899/2000... Training loss: 0.2922\n",
      "Epoch: 1899/2000... Training loss: 0.3220\n",
      "Epoch: 1899/2000... Training loss: 0.3302\n",
      "Epoch: 1899/2000... Training loss: 0.5336\n",
      "Epoch: 1899/2000... Training loss: 0.2395\n",
      "Epoch: 1899/2000... Training loss: 0.3442\n",
      "Epoch: 1899/2000... Training loss: 0.4536\n",
      "Epoch: 1899/2000... Training loss: 0.3817\n",
      "Epoch: 1900/2000... Training loss: 0.5213\n",
      "Epoch: 1900/2000... Training loss: 0.2208\n",
      "Epoch: 1900/2000... Training loss: 0.4287\n",
      "Epoch: 1900/2000... Training loss: 0.2971\n",
      "Epoch: 1900/2000... Training loss: 0.4444\n",
      "Epoch: 1900/2000... Training loss: 0.4135\n",
      "Epoch: 1900/2000... Training loss: 0.4238\n",
      "Epoch: 1900/2000... Training loss: 0.3011\n",
      "Epoch: 1900/2000... Training loss: 0.2721\n",
      "Epoch: 1900/2000... Training loss: 0.4078\n",
      "Epoch: 1900/2000... Training loss: 0.4104\n",
      "Epoch: 1900/2000... Training loss: 0.3060\n",
      "Epoch: 1900/2000... Training loss: 0.4128\n",
      "Epoch: 1900/2000... Training loss: 0.2811\n",
      "Epoch: 1900/2000... Training loss: 0.2852\n",
      "Epoch: 1900/2000... Training loss: 0.2845\n",
      "Epoch: 1900/2000... Training loss: 0.3271\n",
      "Epoch: 1900/2000... Training loss: 0.2960\n",
      "Epoch: 1900/2000... Training loss: 0.3986\n",
      "Epoch: 1900/2000... Training loss: 0.4398\n",
      "Epoch: 1900/2000... Training loss: 0.3053\n",
      "Epoch: 1900/2000... Training loss: 0.4969\n",
      "Epoch: 1900/2000... Training loss: 0.3637\n",
      "Epoch: 1900/2000... Training loss: 0.3367\n",
      "Epoch: 1900/2000... Training loss: 0.2777\n",
      "Epoch: 1900/2000... Training loss: 0.6016\n",
      "Epoch: 1900/2000... Training loss: 0.4659\n",
      "Epoch: 1900/2000... Training loss: 0.3317\n",
      "Epoch: 1900/2000... Training loss: 0.5281\n",
      "Epoch: 1900/2000... Training loss: 0.3724\n",
      "Epoch: 1900/2000... Training loss: 0.4045\n",
      "Epoch: 1901/2000... Training loss: 0.4081\n",
      "Epoch: 1901/2000... Training loss: 0.3354\n",
      "Epoch: 1901/2000... Training loss: 0.3731\n",
      "Epoch: 1901/2000... Training loss: 0.3018\n",
      "Epoch: 1901/2000... Training loss: 0.4727\n",
      "Epoch: 1901/2000... Training loss: 0.4044\n",
      "Epoch: 1901/2000... Training loss: 0.5102\n",
      "Epoch: 1901/2000... Training loss: 0.3292\n",
      "Epoch: 1901/2000... Training loss: 0.5111\n",
      "Epoch: 1901/2000... Training loss: 0.2346\n",
      "Epoch: 1901/2000... Training loss: 0.3742\n",
      "Epoch: 1901/2000... Training loss: 0.3960\n",
      "Epoch: 1901/2000... Training loss: 0.3089\n",
      "Epoch: 1901/2000... Training loss: 0.1764\n",
      "Epoch: 1901/2000... Training loss: 0.4431\n",
      "Epoch: 1901/2000... Training loss: 0.3195\n",
      "Epoch: 1901/2000... Training loss: 0.3089\n",
      "Epoch: 1901/2000... Training loss: 0.3856\n",
      "Epoch: 1901/2000... Training loss: 0.4372\n",
      "Epoch: 1901/2000... Training loss: 0.5109\n",
      "Epoch: 1901/2000... Training loss: 0.4246\n",
      "Epoch: 1901/2000... Training loss: 0.3729\n",
      "Epoch: 1901/2000... Training loss: 0.3501\n",
      "Epoch: 1901/2000... Training loss: 0.2579\n",
      "Epoch: 1901/2000... Training loss: 0.4903\n",
      "Epoch: 1901/2000... Training loss: 0.2448\n",
      "Epoch: 1901/2000... Training loss: 0.2569\n",
      "Epoch: 1901/2000... Training loss: 0.3562\n",
      "Epoch: 1901/2000... Training loss: 0.4126\n",
      "Epoch: 1901/2000... Training loss: 0.4429\n",
      "Epoch: 1901/2000... Training loss: 0.3810\n",
      "Epoch: 1902/2000... Training loss: 0.2891\n",
      "Epoch: 1902/2000... Training loss: 0.4324\n",
      "Epoch: 1902/2000... Training loss: 0.2883\n",
      "Epoch: 1902/2000... Training loss: 0.3347\n",
      "Epoch: 1902/2000... Training loss: 0.3092\n",
      "Epoch: 1902/2000... Training loss: 0.4449\n",
      "Epoch: 1902/2000... Training loss: 0.2589\n",
      "Epoch: 1902/2000... Training loss: 0.2718\n",
      "Epoch: 1902/2000... Training loss: 0.4147\n",
      "Epoch: 1902/2000... Training loss: 0.3603\n",
      "Epoch: 1902/2000... Training loss: 0.3902\n",
      "Epoch: 1902/2000... Training loss: 0.3862\n",
      "Epoch: 1902/2000... Training loss: 0.3904\n",
      "Epoch: 1902/2000... Training loss: 0.3869\n",
      "Epoch: 1902/2000... Training loss: 0.4294\n",
      "Epoch: 1902/2000... Training loss: 0.3423\n",
      "Epoch: 1902/2000... Training loss: 0.3557\n",
      "Epoch: 1902/2000... Training loss: 0.3819\n",
      "Epoch: 1902/2000... Training loss: 0.2814\n",
      "Epoch: 1902/2000... Training loss: 0.3689\n",
      "Epoch: 1902/2000... Training loss: 0.2516\n",
      "Epoch: 1902/2000... Training loss: 0.3658\n",
      "Epoch: 1902/2000... Training loss: 0.2028\n",
      "Epoch: 1902/2000... Training loss: 0.3620\n",
      "Epoch: 1902/2000... Training loss: 0.3715\n",
      "Epoch: 1902/2000... Training loss: 0.3054\n",
      "Epoch: 1902/2000... Training loss: 0.3143\n",
      "Epoch: 1902/2000... Training loss: 0.4603\n",
      "Epoch: 1902/2000... Training loss: 0.4527\n",
      "Epoch: 1902/2000... Training loss: 0.4956\n",
      "Epoch: 1902/2000... Training loss: 0.4224\n",
      "Epoch: 1903/2000... Training loss: 0.3568\n",
      "Epoch: 1903/2000... Training loss: 0.2444\n",
      "Epoch: 1903/2000... Training loss: 0.3408\n",
      "Epoch: 1903/2000... Training loss: 0.4619\n",
      "Epoch: 1903/2000... Training loss: 0.3503\n",
      "Epoch: 1903/2000... Training loss: 0.3564\n",
      "Epoch: 1903/2000... Training loss: 0.2916\n",
      "Epoch: 1903/2000... Training loss: 0.4814\n",
      "Epoch: 1903/2000... Training loss: 0.4272\n",
      "Epoch: 1903/2000... Training loss: 0.3224\n",
      "Epoch: 1903/2000... Training loss: 0.2461\n",
      "Epoch: 1903/2000... Training loss: 0.2224\n",
      "Epoch: 1903/2000... Training loss: 0.1770\n",
      "Epoch: 1903/2000... Training loss: 0.2565\n",
      "Epoch: 1903/2000... Training loss: 0.4085\n",
      "Epoch: 1903/2000... Training loss: 0.4233\n",
      "Epoch: 1903/2000... Training loss: 0.4807\n",
      "Epoch: 1903/2000... Training loss: 0.5044\n",
      "Epoch: 1903/2000... Training loss: 0.4120\n",
      "Epoch: 1903/2000... Training loss: 0.3475\n",
      "Epoch: 1903/2000... Training loss: 0.3591\n",
      "Epoch: 1903/2000... Training loss: 0.3359\n",
      "Epoch: 1903/2000... Training loss: 0.3436\n",
      "Epoch: 1903/2000... Training loss: 0.3705\n",
      "Epoch: 1903/2000... Training loss: 0.4107\n",
      "Epoch: 1903/2000... Training loss: 0.3508\n",
      "Epoch: 1903/2000... Training loss: 0.3608\n",
      "Epoch: 1903/2000... Training loss: 0.2557\n",
      "Epoch: 1903/2000... Training loss: 0.4124\n",
      "Epoch: 1903/2000... Training loss: 0.3844\n",
      "Epoch: 1903/2000... Training loss: 0.3767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1904/2000... Training loss: 0.4835\n",
      "Epoch: 1904/2000... Training loss: 0.3618\n",
      "Epoch: 1904/2000... Training loss: 0.3843\n",
      "Epoch: 1904/2000... Training loss: 0.2949\n",
      "Epoch: 1904/2000... Training loss: 0.2882\n",
      "Epoch: 1904/2000... Training loss: 0.3364\n",
      "Epoch: 1904/2000... Training loss: 0.4012\n",
      "Epoch: 1904/2000... Training loss: 0.2569\n",
      "Epoch: 1904/2000... Training loss: 0.3991\n",
      "Epoch: 1904/2000... Training loss: 0.2744\n",
      "Epoch: 1904/2000... Training loss: 0.5814\n",
      "Epoch: 1904/2000... Training loss: 0.2996\n",
      "Epoch: 1904/2000... Training loss: 0.4276\n",
      "Epoch: 1904/2000... Training loss: 0.4618\n",
      "Epoch: 1904/2000... Training loss: 0.3943\n",
      "Epoch: 1904/2000... Training loss: 0.3169\n",
      "Epoch: 1904/2000... Training loss: 0.4483\n",
      "Epoch: 1904/2000... Training loss: 0.2895\n",
      "Epoch: 1904/2000... Training loss: 0.3790\n",
      "Epoch: 1904/2000... Training loss: 0.3325\n",
      "Epoch: 1904/2000... Training loss: 0.4320\n",
      "Epoch: 1904/2000... Training loss: 0.4144\n",
      "Epoch: 1904/2000... Training loss: 0.4774\n",
      "Epoch: 1904/2000... Training loss: 0.3678\n",
      "Epoch: 1904/2000... Training loss: 0.5389\n",
      "Epoch: 1904/2000... Training loss: 0.3453\n",
      "Epoch: 1904/2000... Training loss: 0.3773\n",
      "Epoch: 1904/2000... Training loss: 0.3645\n",
      "Epoch: 1904/2000... Training loss: 0.2922\n",
      "Epoch: 1904/2000... Training loss: 0.2273\n",
      "Epoch: 1904/2000... Training loss: 0.5246\n",
      "Epoch: 1905/2000... Training loss: 0.4927\n",
      "Epoch: 1905/2000... Training loss: 0.4331\n",
      "Epoch: 1905/2000... Training loss: 0.4443\n",
      "Epoch: 1905/2000... Training loss: 0.3434\n",
      "Epoch: 1905/2000... Training loss: 0.2520\n",
      "Epoch: 1905/2000... Training loss: 0.3853\n",
      "Epoch: 1905/2000... Training loss: 0.2930\n",
      "Epoch: 1905/2000... Training loss: 0.4186\n",
      "Epoch: 1905/2000... Training loss: 0.3916\n",
      "Epoch: 1905/2000... Training loss: 0.2629\n",
      "Epoch: 1905/2000... Training loss: 0.2818\n",
      "Epoch: 1905/2000... Training loss: 0.3457\n",
      "Epoch: 1905/2000... Training loss: 0.3186\n",
      "Epoch: 1905/2000... Training loss: 0.2684\n",
      "Epoch: 1905/2000... Training loss: 0.3908\n",
      "Epoch: 1905/2000... Training loss: 0.3861\n",
      "Epoch: 1905/2000... Training loss: 0.3941\n",
      "Epoch: 1905/2000... Training loss: 0.4083\n",
      "Epoch: 1905/2000... Training loss: 0.3459\n",
      "Epoch: 1905/2000... Training loss: 0.3316\n",
      "Epoch: 1905/2000... Training loss: 0.1832\n",
      "Epoch: 1905/2000... Training loss: 0.2673\n",
      "Epoch: 1905/2000... Training loss: 0.2684\n",
      "Epoch: 1905/2000... Training loss: 0.2547\n",
      "Epoch: 1905/2000... Training loss: 0.4027\n",
      "Epoch: 1905/2000... Training loss: 0.2882\n",
      "Epoch: 1905/2000... Training loss: 0.5119\n",
      "Epoch: 1905/2000... Training loss: 0.4733\n",
      "Epoch: 1905/2000... Training loss: 0.3849\n",
      "Epoch: 1905/2000... Training loss: 0.4574\n",
      "Epoch: 1905/2000... Training loss: 0.4481\n",
      "Epoch: 1906/2000... Training loss: 0.2500\n",
      "Epoch: 1906/2000... Training loss: 0.3317\n",
      "Epoch: 1906/2000... Training loss: 0.3067\n",
      "Epoch: 1906/2000... Training loss: 0.3175\n",
      "Epoch: 1906/2000... Training loss: 0.3021\n",
      "Epoch: 1906/2000... Training loss: 0.5147\n",
      "Epoch: 1906/2000... Training loss: 0.4189\n",
      "Epoch: 1906/2000... Training loss: 0.4523\n",
      "Epoch: 1906/2000... Training loss: 0.3793\n",
      "Epoch: 1906/2000... Training loss: 0.3454\n",
      "Epoch: 1906/2000... Training loss: 0.3210\n",
      "Epoch: 1906/2000... Training loss: 0.3809\n",
      "Epoch: 1906/2000... Training loss: 0.2887\n",
      "Epoch: 1906/2000... Training loss: 0.3337\n",
      "Epoch: 1906/2000... Training loss: 0.3604\n",
      "Epoch: 1906/2000... Training loss: 0.2414\n",
      "Epoch: 1906/2000... Training loss: 0.3573\n",
      "Epoch: 1906/2000... Training loss: 0.2572\n",
      "Epoch: 1906/2000... Training loss: 0.4806\n",
      "Epoch: 1906/2000... Training loss: 0.3903\n",
      "Epoch: 1906/2000... Training loss: 0.2996\n",
      "Epoch: 1906/2000... Training loss: 0.3910\n",
      "Epoch: 1906/2000... Training loss: 0.2804\n",
      "Epoch: 1906/2000... Training loss: 0.4771\n",
      "Epoch: 1906/2000... Training loss: 0.4722\n",
      "Epoch: 1906/2000... Training loss: 0.2141\n",
      "Epoch: 1906/2000... Training loss: 0.6508\n",
      "Epoch: 1906/2000... Training loss: 0.3861\n",
      "Epoch: 1906/2000... Training loss: 0.3652\n",
      "Epoch: 1906/2000... Training loss: 0.3666\n",
      "Epoch: 1906/2000... Training loss: 0.2548\n",
      "Epoch: 1907/2000... Training loss: 0.3243\n",
      "Epoch: 1907/2000... Training loss: 0.3962\n",
      "Epoch: 1907/2000... Training loss: 0.3561\n",
      "Epoch: 1907/2000... Training loss: 0.3527\n",
      "Epoch: 1907/2000... Training loss: 0.3957\n",
      "Epoch: 1907/2000... Training loss: 0.3020\n",
      "Epoch: 1907/2000... Training loss: 0.4707\n",
      "Epoch: 1907/2000... Training loss: 0.3123\n",
      "Epoch: 1907/2000... Training loss: 0.2417\n",
      "Epoch: 1907/2000... Training loss: 0.4234\n",
      "Epoch: 1907/2000... Training loss: 0.3223\n",
      "Epoch: 1907/2000... Training loss: 0.3364\n",
      "Epoch: 1907/2000... Training loss: 0.3475\n",
      "Epoch: 1907/2000... Training loss: 0.4085\n",
      "Epoch: 1907/2000... Training loss: 0.4826\n",
      "Epoch: 1907/2000... Training loss: 0.5135\n",
      "Epoch: 1907/2000... Training loss: 0.3214\n",
      "Epoch: 1907/2000... Training loss: 0.3020\n",
      "Epoch: 1907/2000... Training loss: 0.3291\n",
      "Epoch: 1907/2000... Training loss: 0.4267\n",
      "Epoch: 1907/2000... Training loss: 0.2956\n",
      "Epoch: 1907/2000... Training loss: 0.3561\n",
      "Epoch: 1907/2000... Training loss: 0.2876\n",
      "Epoch: 1907/2000... Training loss: 0.4710\n",
      "Epoch: 1907/2000... Training loss: 0.2538\n",
      "Epoch: 1907/2000... Training loss: 0.4069\n",
      "Epoch: 1907/2000... Training loss: 0.3410\n",
      "Epoch: 1907/2000... Training loss: 0.4781\n",
      "Epoch: 1907/2000... Training loss: 0.4089\n",
      "Epoch: 1907/2000... Training loss: 0.3913\n",
      "Epoch: 1907/2000... Training loss: 0.3911\n",
      "Epoch: 1908/2000... Training loss: 0.3558\n",
      "Epoch: 1908/2000... Training loss: 0.3121\n",
      "Epoch: 1908/2000... Training loss: 0.3716\n",
      "Epoch: 1908/2000... Training loss: 0.4193\n",
      "Epoch: 1908/2000... Training loss: 0.4012\n",
      "Epoch: 1908/2000... Training loss: 0.5596\n",
      "Epoch: 1908/2000... Training loss: 0.2895\n",
      "Epoch: 1908/2000... Training loss: 0.4414\n",
      "Epoch: 1908/2000... Training loss: 0.4367\n",
      "Epoch: 1908/2000... Training loss: 0.4291\n",
      "Epoch: 1908/2000... Training loss: 0.4406\n",
      "Epoch: 1908/2000... Training loss: 0.4115\n",
      "Epoch: 1908/2000... Training loss: 0.3736\n",
      "Epoch: 1908/2000... Training loss: 0.3317\n",
      "Epoch: 1908/2000... Training loss: 0.4678\n",
      "Epoch: 1908/2000... Training loss: 0.2390\n",
      "Epoch: 1908/2000... Training loss: 0.3204\n",
      "Epoch: 1908/2000... Training loss: 0.4239\n",
      "Epoch: 1908/2000... Training loss: 0.3401\n",
      "Epoch: 1908/2000... Training loss: 0.3337\n",
      "Epoch: 1908/2000... Training loss: 0.3585\n",
      "Epoch: 1908/2000... Training loss: 0.2959\n",
      "Epoch: 1908/2000... Training loss: 0.3016\n",
      "Epoch: 1908/2000... Training loss: 0.5366\n",
      "Epoch: 1908/2000... Training loss: 0.4092\n",
      "Epoch: 1908/2000... Training loss: 0.2145\n",
      "Epoch: 1908/2000... Training loss: 0.5909\n",
      "Epoch: 1908/2000... Training loss: 0.4633\n",
      "Epoch: 1908/2000... Training loss: 0.4104\n",
      "Epoch: 1908/2000... Training loss: 0.3218\n",
      "Epoch: 1908/2000... Training loss: 0.3494\n",
      "Epoch: 1909/2000... Training loss: 0.4856\n",
      "Epoch: 1909/2000... Training loss: 0.2411\n",
      "Epoch: 1909/2000... Training loss: 0.4101\n",
      "Epoch: 1909/2000... Training loss: 0.4538\n",
      "Epoch: 1909/2000... Training loss: 0.3247\n",
      "Epoch: 1909/2000... Training loss: 0.3493\n",
      "Epoch: 1909/2000... Training loss: 0.4216\n",
      "Epoch: 1909/2000... Training loss: 0.2377\n",
      "Epoch: 1909/2000... Training loss: 0.2965\n",
      "Epoch: 1909/2000... Training loss: 0.4401\n",
      "Epoch: 1909/2000... Training loss: 0.3064\n",
      "Epoch: 1909/2000... Training loss: 0.3027\n",
      "Epoch: 1909/2000... Training loss: 0.2389\n",
      "Epoch: 1909/2000... Training loss: 0.2304\n",
      "Epoch: 1909/2000... Training loss: 0.2608\n",
      "Epoch: 1909/2000... Training loss: 0.3495\n",
      "Epoch: 1909/2000... Training loss: 0.3318\n",
      "Epoch: 1909/2000... Training loss: 0.4757\n",
      "Epoch: 1909/2000... Training loss: 0.2628\n",
      "Epoch: 1909/2000... Training loss: 0.2843\n",
      "Epoch: 1909/2000... Training loss: 0.3437\n",
      "Epoch: 1909/2000... Training loss: 0.4165\n",
      "Epoch: 1909/2000... Training loss: 0.3531\n",
      "Epoch: 1909/2000... Training loss: 0.3049\n",
      "Epoch: 1909/2000... Training loss: 0.4166\n",
      "Epoch: 1909/2000... Training loss: 0.4315\n",
      "Epoch: 1909/2000... Training loss: 0.4785\n",
      "Epoch: 1909/2000... Training loss: 0.2493\n",
      "Epoch: 1909/2000... Training loss: 0.2854\n",
      "Epoch: 1909/2000... Training loss: 0.3413\n",
      "Epoch: 1909/2000... Training loss: 0.3102\n",
      "Epoch: 1910/2000... Training loss: 0.5229\n",
      "Epoch: 1910/2000... Training loss: 0.3780\n",
      "Epoch: 1910/2000... Training loss: 0.2024\n",
      "Epoch: 1910/2000... Training loss: 0.3138\n",
      "Epoch: 1910/2000... Training loss: 0.3589\n",
      "Epoch: 1910/2000... Training loss: 0.2383\n",
      "Epoch: 1910/2000... Training loss: 0.2769\n",
      "Epoch: 1910/2000... Training loss: 0.2841\n",
      "Epoch: 1910/2000... Training loss: 0.2411\n",
      "Epoch: 1910/2000... Training loss: 0.3236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1910/2000... Training loss: 0.3288\n",
      "Epoch: 1910/2000... Training loss: 0.4723\n",
      "Epoch: 1910/2000... Training loss: 0.3719\n",
      "Epoch: 1910/2000... Training loss: 0.3665\n",
      "Epoch: 1910/2000... Training loss: 0.2698\n",
      "Epoch: 1910/2000... Training loss: 0.3154\n",
      "Epoch: 1910/2000... Training loss: 0.3837\n",
      "Epoch: 1910/2000... Training loss: 0.4389\n",
      "Epoch: 1910/2000... Training loss: 0.3774\n",
      "Epoch: 1910/2000... Training loss: 0.4568\n",
      "Epoch: 1910/2000... Training loss: 0.4866\n",
      "Epoch: 1910/2000... Training loss: 0.3400\n",
      "Epoch: 1910/2000... Training loss: 0.2664\n",
      "Epoch: 1910/2000... Training loss: 0.3514\n",
      "Epoch: 1910/2000... Training loss: 0.3423\n",
      "Epoch: 1910/2000... Training loss: 0.2207\n",
      "Epoch: 1910/2000... Training loss: 0.4131\n",
      "Epoch: 1910/2000... Training loss: 0.3889\n",
      "Epoch: 1910/2000... Training loss: 0.3666\n",
      "Epoch: 1910/2000... Training loss: 0.3169\n",
      "Epoch: 1910/2000... Training loss: 0.3171\n",
      "Epoch: 1911/2000... Training loss: 0.3237\n",
      "Epoch: 1911/2000... Training loss: 0.3984\n",
      "Epoch: 1911/2000... Training loss: 0.3406\n",
      "Epoch: 1911/2000... Training loss: 0.3755\n",
      "Epoch: 1911/2000... Training loss: 0.5880\n",
      "Epoch: 1911/2000... Training loss: 0.3796\n",
      "Epoch: 1911/2000... Training loss: 0.4884\n",
      "Epoch: 1911/2000... Training loss: 0.2894\n",
      "Epoch: 1911/2000... Training loss: 0.3854\n",
      "Epoch: 1911/2000... Training loss: 0.3896\n",
      "Epoch: 1911/2000... Training loss: 0.3237\n",
      "Epoch: 1911/2000... Training loss: 0.2977\n",
      "Epoch: 1911/2000... Training loss: 0.3707\n",
      "Epoch: 1911/2000... Training loss: 0.4382\n",
      "Epoch: 1911/2000... Training loss: 0.2641\n",
      "Epoch: 1911/2000... Training loss: 0.3689\n",
      "Epoch: 1911/2000... Training loss: 0.3709\n",
      "Epoch: 1911/2000... Training loss: 0.2337\n",
      "Epoch: 1911/2000... Training loss: 0.4355\n",
      "Epoch: 1911/2000... Training loss: 0.2516\n",
      "Epoch: 1911/2000... Training loss: 0.3305\n",
      "Epoch: 1911/2000... Training loss: 0.2980\n",
      "Epoch: 1911/2000... Training loss: 0.3037\n",
      "Epoch: 1911/2000... Training loss: 0.3234\n",
      "Epoch: 1911/2000... Training loss: 0.2803\n",
      "Epoch: 1911/2000... Training loss: 0.3485\n",
      "Epoch: 1911/2000... Training loss: 0.5334\n",
      "Epoch: 1911/2000... Training loss: 0.3046\n",
      "Epoch: 1911/2000... Training loss: 0.3578\n",
      "Epoch: 1911/2000... Training loss: 0.4140\n",
      "Epoch: 1911/2000... Training loss: 0.3225\n",
      "Epoch: 1912/2000... Training loss: 0.2738\n",
      "Epoch: 1912/2000... Training loss: 0.2904\n",
      "Epoch: 1912/2000... Training loss: 0.3676\n",
      "Epoch: 1912/2000... Training loss: 0.3914\n",
      "Epoch: 1912/2000... Training loss: 0.4070\n",
      "Epoch: 1912/2000... Training loss: 0.3625\n",
      "Epoch: 1912/2000... Training loss: 0.3758\n",
      "Epoch: 1912/2000... Training loss: 0.4155\n",
      "Epoch: 1912/2000... Training loss: 0.4353\n",
      "Epoch: 1912/2000... Training loss: 0.2529\n",
      "Epoch: 1912/2000... Training loss: 0.3164\n",
      "Epoch: 1912/2000... Training loss: 0.2595\n",
      "Epoch: 1912/2000... Training loss: 0.4899\n",
      "Epoch: 1912/2000... Training loss: 0.3127\n",
      "Epoch: 1912/2000... Training loss: 0.5155\n",
      "Epoch: 1912/2000... Training loss: 0.2409\n",
      "Epoch: 1912/2000... Training loss: 0.2827\n",
      "Epoch: 1912/2000... Training loss: 0.4691\n",
      "Epoch: 1912/2000... Training loss: 0.4742\n",
      "Epoch: 1912/2000... Training loss: 0.4066\n",
      "Epoch: 1912/2000... Training loss: 0.3768\n",
      "Epoch: 1912/2000... Training loss: 0.3432\n",
      "Epoch: 1912/2000... Training loss: 0.4520\n",
      "Epoch: 1912/2000... Training loss: 0.3639\n",
      "Epoch: 1912/2000... Training loss: 0.2555\n",
      "Epoch: 1912/2000... Training loss: 0.3072\n",
      "Epoch: 1912/2000... Training loss: 0.4711\n",
      "Epoch: 1912/2000... Training loss: 0.5539\n",
      "Epoch: 1912/2000... Training loss: 0.2726\n",
      "Epoch: 1912/2000... Training loss: 0.4915\n",
      "Epoch: 1912/2000... Training loss: 0.4641\n",
      "Epoch: 1913/2000... Training loss: 0.2678\n",
      "Epoch: 1913/2000... Training loss: 0.2156\n",
      "Epoch: 1913/2000... Training loss: 0.5196\n",
      "Epoch: 1913/2000... Training loss: 0.3182\n",
      "Epoch: 1913/2000... Training loss: 0.3504\n",
      "Epoch: 1913/2000... Training loss: 0.2893\n",
      "Epoch: 1913/2000... Training loss: 0.3130\n",
      "Epoch: 1913/2000... Training loss: 0.2162\n",
      "Epoch: 1913/2000... Training loss: 0.6021\n",
      "Epoch: 1913/2000... Training loss: 0.3476\n",
      "Epoch: 1913/2000... Training loss: 0.5354\n",
      "Epoch: 1913/2000... Training loss: 0.5401\n",
      "Epoch: 1913/2000... Training loss: 0.5142\n",
      "Epoch: 1913/2000... Training loss: 0.2930\n",
      "Epoch: 1913/2000... Training loss: 0.3064\n",
      "Epoch: 1913/2000... Training loss: 0.6229\n",
      "Epoch: 1913/2000... Training loss: 0.3856\n",
      "Epoch: 1913/2000... Training loss: 0.3138\n",
      "Epoch: 1913/2000... Training loss: 0.4585\n",
      "Epoch: 1913/2000... Training loss: 0.4856\n",
      "Epoch: 1913/2000... Training loss: 0.5217\n",
      "Epoch: 1913/2000... Training loss: 0.4186\n",
      "Epoch: 1913/2000... Training loss: 0.4594\n",
      "Epoch: 1913/2000... Training loss: 0.3806\n",
      "Epoch: 1913/2000... Training loss: 0.3780\n",
      "Epoch: 1913/2000... Training loss: 0.4754\n",
      "Epoch: 1913/2000... Training loss: 0.4736\n",
      "Epoch: 1913/2000... Training loss: 0.4122\n",
      "Epoch: 1913/2000... Training loss: 0.2938\n",
      "Epoch: 1913/2000... Training loss: 0.4182\n",
      "Epoch: 1913/2000... Training loss: 0.4203\n",
      "Epoch: 1914/2000... Training loss: 0.5197\n",
      "Epoch: 1914/2000... Training loss: 0.2780\n",
      "Epoch: 1914/2000... Training loss: 0.2611\n",
      "Epoch: 1914/2000... Training loss: 0.3780\n",
      "Epoch: 1914/2000... Training loss: 0.3994\n",
      "Epoch: 1914/2000... Training loss: 0.3301\n",
      "Epoch: 1914/2000... Training loss: 0.4060\n",
      "Epoch: 1914/2000... Training loss: 0.4167\n",
      "Epoch: 1914/2000... Training loss: 0.3364\n",
      "Epoch: 1914/2000... Training loss: 0.2363\n",
      "Epoch: 1914/2000... Training loss: 0.4111\n",
      "Epoch: 1914/2000... Training loss: 0.3797\n",
      "Epoch: 1914/2000... Training loss: 0.2898\n",
      "Epoch: 1914/2000... Training loss: 0.4856\n",
      "Epoch: 1914/2000... Training loss: 0.4341\n",
      "Epoch: 1914/2000... Training loss: 0.2646\n",
      "Epoch: 1914/2000... Training loss: 0.4458\n",
      "Epoch: 1914/2000... Training loss: 0.3175\n",
      "Epoch: 1914/2000... Training loss: 0.4235\n",
      "Epoch: 1914/2000... Training loss: 0.3922\n",
      "Epoch: 1914/2000... Training loss: 0.3875\n",
      "Epoch: 1914/2000... Training loss: 0.4167\n",
      "Epoch: 1914/2000... Training loss: 0.3451\n",
      "Epoch: 1914/2000... Training loss: 0.5289\n",
      "Epoch: 1914/2000... Training loss: 0.2489\n",
      "Epoch: 1914/2000... Training loss: 0.2868\n",
      "Epoch: 1914/2000... Training loss: 0.3714\n",
      "Epoch: 1914/2000... Training loss: 0.3743\n",
      "Epoch: 1914/2000... Training loss: 0.5082\n",
      "Epoch: 1914/2000... Training loss: 0.1905\n",
      "Epoch: 1914/2000... Training loss: 0.3213\n",
      "Epoch: 1915/2000... Training loss: 0.3800\n",
      "Epoch: 1915/2000... Training loss: 0.2881\n",
      "Epoch: 1915/2000... Training loss: 0.4315\n",
      "Epoch: 1915/2000... Training loss: 0.3226\n",
      "Epoch: 1915/2000... Training loss: 0.3702\n",
      "Epoch: 1915/2000... Training loss: 0.3268\n",
      "Epoch: 1915/2000... Training loss: 0.4774\n",
      "Epoch: 1915/2000... Training loss: 0.3236\n",
      "Epoch: 1915/2000... Training loss: 0.3136\n",
      "Epoch: 1915/2000... Training loss: 0.3906\n",
      "Epoch: 1915/2000... Training loss: 0.3388\n",
      "Epoch: 1915/2000... Training loss: 0.4021\n",
      "Epoch: 1915/2000... Training loss: 0.2756\n",
      "Epoch: 1915/2000... Training loss: 0.3040\n",
      "Epoch: 1915/2000... Training loss: 0.6134\n",
      "Epoch: 1915/2000... Training loss: 0.2811\n",
      "Epoch: 1915/2000... Training loss: 0.4593\n",
      "Epoch: 1915/2000... Training loss: 0.4737\n",
      "Epoch: 1915/2000... Training loss: 0.5198\n",
      "Epoch: 1915/2000... Training loss: 0.3763\n",
      "Epoch: 1915/2000... Training loss: 0.3201\n",
      "Epoch: 1915/2000... Training loss: 0.3095\n",
      "Epoch: 1915/2000... Training loss: 0.6310\n",
      "Epoch: 1915/2000... Training loss: 0.4335\n",
      "Epoch: 1915/2000... Training loss: 0.4340\n",
      "Epoch: 1915/2000... Training loss: 0.4263\n",
      "Epoch: 1915/2000... Training loss: 0.2657\n",
      "Epoch: 1915/2000... Training loss: 0.3344\n",
      "Epoch: 1915/2000... Training loss: 0.3248\n",
      "Epoch: 1915/2000... Training loss: 0.4591\n",
      "Epoch: 1915/2000... Training loss: 0.3395\n",
      "Epoch: 1916/2000... Training loss: 0.4261\n",
      "Epoch: 1916/2000... Training loss: 0.4057\n",
      "Epoch: 1916/2000... Training loss: 0.3812\n",
      "Epoch: 1916/2000... Training loss: 0.3194\n",
      "Epoch: 1916/2000... Training loss: 0.3738\n",
      "Epoch: 1916/2000... Training loss: 0.3865\n",
      "Epoch: 1916/2000... Training loss: 0.2928\n",
      "Epoch: 1916/2000... Training loss: 0.3512\n",
      "Epoch: 1916/2000... Training loss: 0.3722\n",
      "Epoch: 1916/2000... Training loss: 0.3315\n",
      "Epoch: 1916/2000... Training loss: 0.3842\n",
      "Epoch: 1916/2000... Training loss: 0.2953\n",
      "Epoch: 1916/2000... Training loss: 0.2935\n",
      "Epoch: 1916/2000... Training loss: 0.1781\n",
      "Epoch: 1916/2000... Training loss: 0.4116\n",
      "Epoch: 1916/2000... Training loss: 0.3047\n",
      "Epoch: 1916/2000... Training loss: 0.4097\n",
      "Epoch: 1916/2000... Training loss: 0.3680\n",
      "Epoch: 1916/2000... Training loss: 0.3652\n",
      "Epoch: 1916/2000... Training loss: 0.2940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1916/2000... Training loss: 0.2907\n",
      "Epoch: 1916/2000... Training loss: 0.3060\n",
      "Epoch: 1916/2000... Training loss: 0.3080\n",
      "Epoch: 1916/2000... Training loss: 0.3280\n",
      "Epoch: 1916/2000... Training loss: 0.4610\n",
      "Epoch: 1916/2000... Training loss: 0.3640\n",
      "Epoch: 1916/2000... Training loss: 0.4025\n",
      "Epoch: 1916/2000... Training loss: 0.2615\n",
      "Epoch: 1916/2000... Training loss: 0.3905\n",
      "Epoch: 1916/2000... Training loss: 0.4574\n",
      "Epoch: 1916/2000... Training loss: 0.2291\n",
      "Epoch: 1917/2000... Training loss: 0.4706\n",
      "Epoch: 1917/2000... Training loss: 0.5728\n",
      "Epoch: 1917/2000... Training loss: 0.4122\n",
      "Epoch: 1917/2000... Training loss: 0.2884\n",
      "Epoch: 1917/2000... Training loss: 0.3935\n",
      "Epoch: 1917/2000... Training loss: 0.5272\n",
      "Epoch: 1917/2000... Training loss: 0.5523\n",
      "Epoch: 1917/2000... Training loss: 0.4644\n",
      "Epoch: 1917/2000... Training loss: 0.4189\n",
      "Epoch: 1917/2000... Training loss: 0.2967\n",
      "Epoch: 1917/2000... Training loss: 0.3068\n",
      "Epoch: 1917/2000... Training loss: 0.2756\n",
      "Epoch: 1917/2000... Training loss: 0.4934\n",
      "Epoch: 1917/2000... Training loss: 0.3642\n",
      "Epoch: 1917/2000... Training loss: 0.4201\n",
      "Epoch: 1917/2000... Training loss: 0.2571\n",
      "Epoch: 1917/2000... Training loss: 0.5547\n",
      "Epoch: 1917/2000... Training loss: 0.4394\n",
      "Epoch: 1917/2000... Training loss: 0.4418\n",
      "Epoch: 1917/2000... Training loss: 0.2885\n",
      "Epoch: 1917/2000... Training loss: 0.4684\n",
      "Epoch: 1917/2000... Training loss: 0.3526\n",
      "Epoch: 1917/2000... Training loss: 0.3986\n",
      "Epoch: 1917/2000... Training loss: 0.3721\n",
      "Epoch: 1917/2000... Training loss: 0.4184\n",
      "Epoch: 1917/2000... Training loss: 0.2683\n",
      "Epoch: 1917/2000... Training loss: 0.4782\n",
      "Epoch: 1917/2000... Training loss: 0.3871\n",
      "Epoch: 1917/2000... Training loss: 0.3646\n",
      "Epoch: 1917/2000... Training loss: 0.3683\n",
      "Epoch: 1917/2000... Training loss: 0.5185\n",
      "Epoch: 1918/2000... Training loss: 0.3784\n",
      "Epoch: 1918/2000... Training loss: 0.3923\n",
      "Epoch: 1918/2000... Training loss: 0.5128\n",
      "Epoch: 1918/2000... Training loss: 0.3449\n",
      "Epoch: 1918/2000... Training loss: 0.4626\n",
      "Epoch: 1918/2000... Training loss: 0.4068\n",
      "Epoch: 1918/2000... Training loss: 0.4966\n",
      "Epoch: 1918/2000... Training loss: 0.3619\n",
      "Epoch: 1918/2000... Training loss: 0.3817\n",
      "Epoch: 1918/2000... Training loss: 0.2567\n",
      "Epoch: 1918/2000... Training loss: 0.2567\n",
      "Epoch: 1918/2000... Training loss: 0.2697\n",
      "Epoch: 1918/2000... Training loss: 0.3184\n",
      "Epoch: 1918/2000... Training loss: 0.2713\n",
      "Epoch: 1918/2000... Training loss: 0.4905\n",
      "Epoch: 1918/2000... Training loss: 0.3331\n",
      "Epoch: 1918/2000... Training loss: 0.3278\n",
      "Epoch: 1918/2000... Training loss: 0.3865\n",
      "Epoch: 1918/2000... Training loss: 0.4475\n",
      "Epoch: 1918/2000... Training loss: 0.4914\n",
      "Epoch: 1918/2000... Training loss: 0.2847\n",
      "Epoch: 1918/2000... Training loss: 0.3139\n",
      "Epoch: 1918/2000... Training loss: 0.3280\n",
      "Epoch: 1918/2000... Training loss: 0.5049\n",
      "Epoch: 1918/2000... Training loss: 0.1883\n",
      "Epoch: 1918/2000... Training loss: 0.2992\n",
      "Epoch: 1918/2000... Training loss: 0.2544\n",
      "Epoch: 1918/2000... Training loss: 0.3627\n",
      "Epoch: 1918/2000... Training loss: 0.2839\n",
      "Epoch: 1918/2000... Training loss: 0.3602\n",
      "Epoch: 1918/2000... Training loss: 0.2938\n",
      "Epoch: 1919/2000... Training loss: 0.3647\n",
      "Epoch: 1919/2000... Training loss: 0.3555\n",
      "Epoch: 1919/2000... Training loss: 0.4748\n",
      "Epoch: 1919/2000... Training loss: 0.4626\n",
      "Epoch: 1919/2000... Training loss: 0.5163\n",
      "Epoch: 1919/2000... Training loss: 0.3838\n",
      "Epoch: 1919/2000... Training loss: 0.3830\n",
      "Epoch: 1919/2000... Training loss: 0.3407\n",
      "Epoch: 1919/2000... Training loss: 0.3414\n",
      "Epoch: 1919/2000... Training loss: 0.5226\n",
      "Epoch: 1919/2000... Training loss: 0.3699\n",
      "Epoch: 1919/2000... Training loss: 0.3893\n",
      "Epoch: 1919/2000... Training loss: 0.3337\n",
      "Epoch: 1919/2000... Training loss: 0.3090\n",
      "Epoch: 1919/2000... Training loss: 0.4087\n",
      "Epoch: 1919/2000... Training loss: 0.4392\n",
      "Epoch: 1919/2000... Training loss: 0.4065\n",
      "Epoch: 1919/2000... Training loss: 0.2865\n",
      "Epoch: 1919/2000... Training loss: 0.3896\n",
      "Epoch: 1919/2000... Training loss: 0.5155\n",
      "Epoch: 1919/2000... Training loss: 0.3721\n",
      "Epoch: 1919/2000... Training loss: 0.3882\n",
      "Epoch: 1919/2000... Training loss: 0.4721\n",
      "Epoch: 1919/2000... Training loss: 0.3710\n",
      "Epoch: 1919/2000... Training loss: 0.2595\n",
      "Epoch: 1919/2000... Training loss: 0.3738\n",
      "Epoch: 1919/2000... Training loss: 0.5783\n",
      "Epoch: 1919/2000... Training loss: 0.4092\n",
      "Epoch: 1919/2000... Training loss: 0.3543\n",
      "Epoch: 1919/2000... Training loss: 0.4662\n",
      "Epoch: 1919/2000... Training loss: 0.3820\n",
      "Epoch: 1920/2000... Training loss: 0.3408\n",
      "Epoch: 1920/2000... Training loss: 0.4210\n",
      "Epoch: 1920/2000... Training loss: 0.3816\n",
      "Epoch: 1920/2000... Training loss: 0.4402\n",
      "Epoch: 1920/2000... Training loss: 0.3132\n",
      "Epoch: 1920/2000... Training loss: 0.4305\n",
      "Epoch: 1920/2000... Training loss: 0.3906\n",
      "Epoch: 1920/2000... Training loss: 0.3648\n",
      "Epoch: 1920/2000... Training loss: 0.3186\n",
      "Epoch: 1920/2000... Training loss: 0.3908\n",
      "Epoch: 1920/2000... Training loss: 0.4707\n",
      "Epoch: 1920/2000... Training loss: 0.3082\n",
      "Epoch: 1920/2000... Training loss: 0.5641\n",
      "Epoch: 1920/2000... Training loss: 0.4062\n",
      "Epoch: 1920/2000... Training loss: 0.2679\n",
      "Epoch: 1920/2000... Training loss: 0.4049\n",
      "Epoch: 1920/2000... Training loss: 0.2882\n",
      "Epoch: 1920/2000... Training loss: 0.4874\n",
      "Epoch: 1920/2000... Training loss: 0.4190\n",
      "Epoch: 1920/2000... Training loss: 0.2588\n",
      "Epoch: 1920/2000... Training loss: 0.2984\n",
      "Epoch: 1920/2000... Training loss: 0.4359\n",
      "Epoch: 1920/2000... Training loss: 0.3377\n",
      "Epoch: 1920/2000... Training loss: 0.2302\n",
      "Epoch: 1920/2000... Training loss: 0.4424\n",
      "Epoch: 1920/2000... Training loss: 0.4310\n",
      "Epoch: 1920/2000... Training loss: 0.4172\n",
      "Epoch: 1920/2000... Training loss: 0.3792\n",
      "Epoch: 1920/2000... Training loss: 0.4225\n",
      "Epoch: 1920/2000... Training loss: 0.2645\n",
      "Epoch: 1920/2000... Training loss: 0.2703\n",
      "Epoch: 1921/2000... Training loss: 0.4676\n",
      "Epoch: 1921/2000... Training loss: 0.5017\n",
      "Epoch: 1921/2000... Training loss: 0.2783\n",
      "Epoch: 1921/2000... Training loss: 0.3143\n",
      "Epoch: 1921/2000... Training loss: 0.4837\n",
      "Epoch: 1921/2000... Training loss: 0.4475\n",
      "Epoch: 1921/2000... Training loss: 0.4379\n",
      "Epoch: 1921/2000... Training loss: 0.3186\n",
      "Epoch: 1921/2000... Training loss: 0.2363\n",
      "Epoch: 1921/2000... Training loss: 0.3555\n",
      "Epoch: 1921/2000... Training loss: 0.4900\n",
      "Epoch: 1921/2000... Training loss: 0.3188\n",
      "Epoch: 1921/2000... Training loss: 0.2563\n",
      "Epoch: 1921/2000... Training loss: 0.4721\n",
      "Epoch: 1921/2000... Training loss: 0.5887\n",
      "Epoch: 1921/2000... Training loss: 0.3223\n",
      "Epoch: 1921/2000... Training loss: 0.3777\n",
      "Epoch: 1921/2000... Training loss: 0.3266\n",
      "Epoch: 1921/2000... Training loss: 0.4787\n",
      "Epoch: 1921/2000... Training loss: 0.3694\n",
      "Epoch: 1921/2000... Training loss: 0.4092\n",
      "Epoch: 1921/2000... Training loss: 0.5626\n",
      "Epoch: 1921/2000... Training loss: 0.2968\n",
      "Epoch: 1921/2000... Training loss: 0.4992\n",
      "Epoch: 1921/2000... Training loss: 0.3302\n",
      "Epoch: 1921/2000... Training loss: 0.2995\n",
      "Epoch: 1921/2000... Training loss: 0.4426\n",
      "Epoch: 1921/2000... Training loss: 0.2596\n",
      "Epoch: 1921/2000... Training loss: 0.2737\n",
      "Epoch: 1921/2000... Training loss: 0.4376\n",
      "Epoch: 1921/2000... Training loss: 0.4620\n",
      "Epoch: 1922/2000... Training loss: 0.2748\n",
      "Epoch: 1922/2000... Training loss: 0.3014\n",
      "Epoch: 1922/2000... Training loss: 0.3964\n",
      "Epoch: 1922/2000... Training loss: 0.5666\n",
      "Epoch: 1922/2000... Training loss: 0.4719\n",
      "Epoch: 1922/2000... Training loss: 0.4604\n",
      "Epoch: 1922/2000... Training loss: 0.3399\n",
      "Epoch: 1922/2000... Training loss: 0.3163\n",
      "Epoch: 1922/2000... Training loss: 0.3036\n",
      "Epoch: 1922/2000... Training loss: 0.3954\n",
      "Epoch: 1922/2000... Training loss: 0.5002\n",
      "Epoch: 1922/2000... Training loss: 0.2500\n",
      "Epoch: 1922/2000... Training loss: 0.4396\n",
      "Epoch: 1922/2000... Training loss: 0.5321\n",
      "Epoch: 1922/2000... Training loss: 0.1864\n",
      "Epoch: 1922/2000... Training loss: 0.4693\n",
      "Epoch: 1922/2000... Training loss: 0.5884\n",
      "Epoch: 1922/2000... Training loss: 0.2993\n",
      "Epoch: 1922/2000... Training loss: 0.4219\n",
      "Epoch: 1922/2000... Training loss: 0.3954\n",
      "Epoch: 1922/2000... Training loss: 0.2453\n",
      "Epoch: 1922/2000... Training loss: 0.5044\n",
      "Epoch: 1922/2000... Training loss: 0.3442\n",
      "Epoch: 1922/2000... Training loss: 0.4669\n",
      "Epoch: 1922/2000... Training loss: 0.4071\n",
      "Epoch: 1922/2000... Training loss: 0.4059\n",
      "Epoch: 1922/2000... Training loss: 0.3053\n",
      "Epoch: 1922/2000... Training loss: 0.4849\n",
      "Epoch: 1922/2000... Training loss: 0.5453\n",
      "Epoch: 1922/2000... Training loss: 0.6132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1922/2000... Training loss: 0.5735\n",
      "Epoch: 1923/2000... Training loss: 0.2718\n",
      "Epoch: 1923/2000... Training loss: 0.3671\n",
      "Epoch: 1923/2000... Training loss: 0.3996\n",
      "Epoch: 1923/2000... Training loss: 0.4508\n",
      "Epoch: 1923/2000... Training loss: 0.4459\n",
      "Epoch: 1923/2000... Training loss: 0.4448\n",
      "Epoch: 1923/2000... Training loss: 0.2923\n",
      "Epoch: 1923/2000... Training loss: 0.4285\n",
      "Epoch: 1923/2000... Training loss: 0.2006\n",
      "Epoch: 1923/2000... Training loss: 0.4008\n",
      "Epoch: 1923/2000... Training loss: 0.3613\n",
      "Epoch: 1923/2000... Training loss: 0.3298\n",
      "Epoch: 1923/2000... Training loss: 0.3936\n",
      "Epoch: 1923/2000... Training loss: 0.4619\n",
      "Epoch: 1923/2000... Training loss: 0.3442\n",
      "Epoch: 1923/2000... Training loss: 0.3286\n",
      "Epoch: 1923/2000... Training loss: 0.5381\n",
      "Epoch: 1923/2000... Training loss: 0.4209\n",
      "Epoch: 1923/2000... Training loss: 0.3414\n",
      "Epoch: 1923/2000... Training loss: 0.2196\n",
      "Epoch: 1923/2000... Training loss: 0.4497\n",
      "Epoch: 1923/2000... Training loss: 0.2524\n",
      "Epoch: 1923/2000... Training loss: 0.3841\n",
      "Epoch: 1923/2000... Training loss: 0.5226\n",
      "Epoch: 1923/2000... Training loss: 0.4935\n",
      "Epoch: 1923/2000... Training loss: 0.3883\n",
      "Epoch: 1923/2000... Training loss: 0.4209\n",
      "Epoch: 1923/2000... Training loss: 0.3161\n",
      "Epoch: 1923/2000... Training loss: 0.5133\n",
      "Epoch: 1923/2000... Training loss: 0.2888\n",
      "Epoch: 1923/2000... Training loss: 0.5428\n",
      "Epoch: 1924/2000... Training loss: 0.2610\n",
      "Epoch: 1924/2000... Training loss: 0.1988\n",
      "Epoch: 1924/2000... Training loss: 0.3811\n",
      "Epoch: 1924/2000... Training loss: 0.3805\n",
      "Epoch: 1924/2000... Training loss: 0.2830\n",
      "Epoch: 1924/2000... Training loss: 0.3915\n",
      "Epoch: 1924/2000... Training loss: 0.3262\n",
      "Epoch: 1924/2000... Training loss: 0.4491\n",
      "Epoch: 1924/2000... Training loss: 0.2638\n",
      "Epoch: 1924/2000... Training loss: 0.3071\n",
      "Epoch: 1924/2000... Training loss: 0.3939\n",
      "Epoch: 1924/2000... Training loss: 0.4180\n",
      "Epoch: 1924/2000... Training loss: 0.2955\n",
      "Epoch: 1924/2000... Training loss: 0.3156\n",
      "Epoch: 1924/2000... Training loss: 0.2005\n",
      "Epoch: 1924/2000... Training loss: 0.3249\n",
      "Epoch: 1924/2000... Training loss: 0.3751\n",
      "Epoch: 1924/2000... Training loss: 0.3316\n",
      "Epoch: 1924/2000... Training loss: 0.4100\n",
      "Epoch: 1924/2000... Training loss: 0.2329\n",
      "Epoch: 1924/2000... Training loss: 0.1457\n",
      "Epoch: 1924/2000... Training loss: 0.3506\n",
      "Epoch: 1924/2000... Training loss: 0.4132\n",
      "Epoch: 1924/2000... Training loss: 0.3861\n",
      "Epoch: 1924/2000... Training loss: 0.3036\n",
      "Epoch: 1924/2000... Training loss: 0.3927\n",
      "Epoch: 1924/2000... Training loss: 0.3277\n",
      "Epoch: 1924/2000... Training loss: 0.4771\n",
      "Epoch: 1924/2000... Training loss: 0.4523\n",
      "Epoch: 1924/2000... Training loss: 0.3207\n",
      "Epoch: 1924/2000... Training loss: 0.2839\n",
      "Epoch: 1925/2000... Training loss: 0.2875\n",
      "Epoch: 1925/2000... Training loss: 0.4974\n",
      "Epoch: 1925/2000... Training loss: 0.4783\n",
      "Epoch: 1925/2000... Training loss: 0.6012\n",
      "Epoch: 1925/2000... Training loss: 0.2558\n",
      "Epoch: 1925/2000... Training loss: 0.2896\n",
      "Epoch: 1925/2000... Training loss: 0.3367\n",
      "Epoch: 1925/2000... Training loss: 0.2730\n",
      "Epoch: 1925/2000... Training loss: 0.4838\n",
      "Epoch: 1925/2000... Training loss: 0.2921\n",
      "Epoch: 1925/2000... Training loss: 0.2765\n",
      "Epoch: 1925/2000... Training loss: 0.4175\n",
      "Epoch: 1925/2000... Training loss: 0.3458\n",
      "Epoch: 1925/2000... Training loss: 0.3164\n",
      "Epoch: 1925/2000... Training loss: 0.4317\n",
      "Epoch: 1925/2000... Training loss: 0.3648\n",
      "Epoch: 1925/2000... Training loss: 0.3537\n",
      "Epoch: 1925/2000... Training loss: 0.2834\n",
      "Epoch: 1925/2000... Training loss: 0.5698\n",
      "Epoch: 1925/2000... Training loss: 0.3606\n",
      "Epoch: 1925/2000... Training loss: 0.3618\n",
      "Epoch: 1925/2000... Training loss: 0.4527\n",
      "Epoch: 1925/2000... Training loss: 0.2804\n",
      "Epoch: 1925/2000... Training loss: 0.1825\n",
      "Epoch: 1925/2000... Training loss: 0.3468\n",
      "Epoch: 1925/2000... Training loss: 0.2529\n",
      "Epoch: 1925/2000... Training loss: 0.4024\n",
      "Epoch: 1925/2000... Training loss: 0.5435\n",
      "Epoch: 1925/2000... Training loss: 0.3136\n",
      "Epoch: 1925/2000... Training loss: 0.2484\n",
      "Epoch: 1925/2000... Training loss: 0.4485\n",
      "Epoch: 1926/2000... Training loss: 0.3567\n",
      "Epoch: 1926/2000... Training loss: 0.4574\n",
      "Epoch: 1926/2000... Training loss: 0.3585\n",
      "Epoch: 1926/2000... Training loss: 0.3392\n",
      "Epoch: 1926/2000... Training loss: 0.4133\n",
      "Epoch: 1926/2000... Training loss: 0.4400\n",
      "Epoch: 1926/2000... Training loss: 0.5144\n",
      "Epoch: 1926/2000... Training loss: 0.4084\n",
      "Epoch: 1926/2000... Training loss: 0.3771\n",
      "Epoch: 1926/2000... Training loss: 0.4004\n",
      "Epoch: 1926/2000... Training loss: 0.2252\n",
      "Epoch: 1926/2000... Training loss: 0.3801\n",
      "Epoch: 1926/2000... Training loss: 0.2672\n",
      "Epoch: 1926/2000... Training loss: 0.4096\n",
      "Epoch: 1926/2000... Training loss: 0.4401\n",
      "Epoch: 1926/2000... Training loss: 0.3513\n",
      "Epoch: 1926/2000... Training loss: 0.3347\n",
      "Epoch: 1926/2000... Training loss: 0.3867\n",
      "Epoch: 1926/2000... Training loss: 0.4995\n",
      "Epoch: 1926/2000... Training loss: 0.4288\n",
      "Epoch: 1926/2000... Training loss: 0.5904\n",
      "Epoch: 1926/2000... Training loss: 0.3544\n",
      "Epoch: 1926/2000... Training loss: 0.2794\n",
      "Epoch: 1926/2000... Training loss: 0.3638\n",
      "Epoch: 1926/2000... Training loss: 0.3049\n",
      "Epoch: 1926/2000... Training loss: 0.2009\n",
      "Epoch: 1926/2000... Training loss: 0.2823\n",
      "Epoch: 1926/2000... Training loss: 0.5275\n",
      "Epoch: 1926/2000... Training loss: 0.6300\n",
      "Epoch: 1926/2000... Training loss: 0.2941\n",
      "Epoch: 1926/2000... Training loss: 0.3875\n",
      "Epoch: 1927/2000... Training loss: 0.2406\n",
      "Epoch: 1927/2000... Training loss: 0.4510\n",
      "Epoch: 1927/2000... Training loss: 0.3843\n",
      "Epoch: 1927/2000... Training loss: 0.4012\n",
      "Epoch: 1927/2000... Training loss: 0.3932\n",
      "Epoch: 1927/2000... Training loss: 0.3380\n",
      "Epoch: 1927/2000... Training loss: 0.3626\n",
      "Epoch: 1927/2000... Training loss: 0.3289\n",
      "Epoch: 1927/2000... Training loss: 0.3499\n",
      "Epoch: 1927/2000... Training loss: 0.3174\n",
      "Epoch: 1927/2000... Training loss: 0.4839\n",
      "Epoch: 1927/2000... Training loss: 0.4343\n",
      "Epoch: 1927/2000... Training loss: 0.4038\n",
      "Epoch: 1927/2000... Training loss: 0.2896\n",
      "Epoch: 1927/2000... Training loss: 0.4088\n",
      "Epoch: 1927/2000... Training loss: 0.2727\n",
      "Epoch: 1927/2000... Training loss: 0.3167\n",
      "Epoch: 1927/2000... Training loss: 0.2495\n",
      "Epoch: 1927/2000... Training loss: 0.3354\n",
      "Epoch: 1927/2000... Training loss: 0.4138\n",
      "Epoch: 1927/2000... Training loss: 0.3465\n",
      "Epoch: 1927/2000... Training loss: 0.4316\n",
      "Epoch: 1927/2000... Training loss: 0.3699\n",
      "Epoch: 1927/2000... Training loss: 0.3626\n",
      "Epoch: 1927/2000... Training loss: 0.2800\n",
      "Epoch: 1927/2000... Training loss: 0.2608\n",
      "Epoch: 1927/2000... Training loss: 0.2406\n",
      "Epoch: 1927/2000... Training loss: 0.5189\n",
      "Epoch: 1927/2000... Training loss: 0.2781\n",
      "Epoch: 1927/2000... Training loss: 0.4234\n",
      "Epoch: 1927/2000... Training loss: 0.6162\n",
      "Epoch: 1928/2000... Training loss: 0.2838\n",
      "Epoch: 1928/2000... Training loss: 0.3966\n",
      "Epoch: 1928/2000... Training loss: 0.3846\n",
      "Epoch: 1928/2000... Training loss: 0.3801\n",
      "Epoch: 1928/2000... Training loss: 0.3356\n",
      "Epoch: 1928/2000... Training loss: 0.3991\n",
      "Epoch: 1928/2000... Training loss: 0.4408\n",
      "Epoch: 1928/2000... Training loss: 0.3714\n",
      "Epoch: 1928/2000... Training loss: 0.5755\n",
      "Epoch: 1928/2000... Training loss: 0.3383\n",
      "Epoch: 1928/2000... Training loss: 0.4247\n",
      "Epoch: 1928/2000... Training loss: 0.2821\n",
      "Epoch: 1928/2000... Training loss: 0.4316\n",
      "Epoch: 1928/2000... Training loss: 0.3813\n",
      "Epoch: 1928/2000... Training loss: 0.2897\n",
      "Epoch: 1928/2000... Training loss: 0.3885\n",
      "Epoch: 1928/2000... Training loss: 0.4279\n",
      "Epoch: 1928/2000... Training loss: 0.3120\n",
      "Epoch: 1928/2000... Training loss: 0.4107\n",
      "Epoch: 1928/2000... Training loss: 0.5478\n",
      "Epoch: 1928/2000... Training loss: 0.3327\n",
      "Epoch: 1928/2000... Training loss: 0.3774\n",
      "Epoch: 1928/2000... Training loss: 0.2887\n",
      "Epoch: 1928/2000... Training loss: 0.4816\n",
      "Epoch: 1928/2000... Training loss: 0.3466\n",
      "Epoch: 1928/2000... Training loss: 0.4809\n",
      "Epoch: 1928/2000... Training loss: 0.4393\n",
      "Epoch: 1928/2000... Training loss: 0.2581\n",
      "Epoch: 1928/2000... Training loss: 0.2676\n",
      "Epoch: 1928/2000... Training loss: 0.4961\n",
      "Epoch: 1928/2000... Training loss: 0.5118\n",
      "Epoch: 1929/2000... Training loss: 0.2122\n",
      "Epoch: 1929/2000... Training loss: 0.3517\n",
      "Epoch: 1929/2000... Training loss: 0.3575\n",
      "Epoch: 1929/2000... Training loss: 0.4671\n",
      "Epoch: 1929/2000... Training loss: 0.3705\n",
      "Epoch: 1929/2000... Training loss: 0.5922\n",
      "Epoch: 1929/2000... Training loss: 0.4364\n",
      "Epoch: 1929/2000... Training loss: 0.4239\n",
      "Epoch: 1929/2000... Training loss: 0.4034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1929/2000... Training loss: 0.3445\n",
      "Epoch: 1929/2000... Training loss: 0.5316\n",
      "Epoch: 1929/2000... Training loss: 0.3217\n",
      "Epoch: 1929/2000... Training loss: 0.3718\n",
      "Epoch: 1929/2000... Training loss: 0.4577\n",
      "Epoch: 1929/2000... Training loss: 0.4570\n",
      "Epoch: 1929/2000... Training loss: 0.3434\n",
      "Epoch: 1929/2000... Training loss: 0.3541\n",
      "Epoch: 1929/2000... Training loss: 0.2836\n",
      "Epoch: 1929/2000... Training loss: 0.3995\n",
      "Epoch: 1929/2000... Training loss: 0.4419\n",
      "Epoch: 1929/2000... Training loss: 0.2657\n",
      "Epoch: 1929/2000... Training loss: 0.4014\n",
      "Epoch: 1929/2000... Training loss: 0.4999\n",
      "Epoch: 1929/2000... Training loss: 0.3888\n",
      "Epoch: 1929/2000... Training loss: 0.3170\n",
      "Epoch: 1929/2000... Training loss: 0.3972\n",
      "Epoch: 1929/2000... Training loss: 0.5201\n",
      "Epoch: 1929/2000... Training loss: 0.5110\n",
      "Epoch: 1929/2000... Training loss: 0.4137\n",
      "Epoch: 1929/2000... Training loss: 0.4062\n",
      "Epoch: 1929/2000... Training loss: 0.4693\n",
      "Epoch: 1930/2000... Training loss: 0.4625\n",
      "Epoch: 1930/2000... Training loss: 0.4149\n",
      "Epoch: 1930/2000... Training loss: 0.5000\n",
      "Epoch: 1930/2000... Training loss: 0.2970\n",
      "Epoch: 1930/2000... Training loss: 0.4245\n",
      "Epoch: 1930/2000... Training loss: 0.4066\n",
      "Epoch: 1930/2000... Training loss: 0.3932\n",
      "Epoch: 1930/2000... Training loss: 0.3504\n",
      "Epoch: 1930/2000... Training loss: 0.4092\n",
      "Epoch: 1930/2000... Training loss: 0.2300\n",
      "Epoch: 1930/2000... Training loss: 0.5269\n",
      "Epoch: 1930/2000... Training loss: 0.4439\n",
      "Epoch: 1930/2000... Training loss: 0.3718\n",
      "Epoch: 1930/2000... Training loss: 0.3414\n",
      "Epoch: 1930/2000... Training loss: 0.4741\n",
      "Epoch: 1930/2000... Training loss: 0.4607\n",
      "Epoch: 1930/2000... Training loss: 0.4080\n",
      "Epoch: 1930/2000... Training loss: 0.4206\n",
      "Epoch: 1930/2000... Training loss: 0.2660\n",
      "Epoch: 1930/2000... Training loss: 0.5393\n",
      "Epoch: 1930/2000... Training loss: 0.4420\n",
      "Epoch: 1930/2000... Training loss: 0.4873\n",
      "Epoch: 1930/2000... Training loss: 0.5220\n",
      "Epoch: 1930/2000... Training loss: 0.3978\n",
      "Epoch: 1930/2000... Training loss: 0.3418\n",
      "Epoch: 1930/2000... Training loss: 0.4603\n",
      "Epoch: 1930/2000... Training loss: 0.4655\n",
      "Epoch: 1930/2000... Training loss: 0.3740\n",
      "Epoch: 1930/2000... Training loss: 0.2010\n",
      "Epoch: 1930/2000... Training loss: 0.4899\n",
      "Epoch: 1930/2000... Training loss: 0.3575\n",
      "Epoch: 1931/2000... Training loss: 0.4017\n",
      "Epoch: 1931/2000... Training loss: 0.3729\n",
      "Epoch: 1931/2000... Training loss: 0.3817\n",
      "Epoch: 1931/2000... Training loss: 0.5146\n",
      "Epoch: 1931/2000... Training loss: 0.2685\n",
      "Epoch: 1931/2000... Training loss: 0.3541\n",
      "Epoch: 1931/2000... Training loss: 0.4884\n",
      "Epoch: 1931/2000... Training loss: 0.4636\n",
      "Epoch: 1931/2000... Training loss: 0.3179\n",
      "Epoch: 1931/2000... Training loss: 0.3271\n",
      "Epoch: 1931/2000... Training loss: 0.3601\n",
      "Epoch: 1931/2000... Training loss: 0.3482\n",
      "Epoch: 1931/2000... Training loss: 0.4516\n",
      "Epoch: 1931/2000... Training loss: 0.3878\n",
      "Epoch: 1931/2000... Training loss: 0.3273\n",
      "Epoch: 1931/2000... Training loss: 0.3341\n",
      "Epoch: 1931/2000... Training loss: 0.2537\n",
      "Epoch: 1931/2000... Training loss: 0.5369\n",
      "Epoch: 1931/2000... Training loss: 0.3131\n",
      "Epoch: 1931/2000... Training loss: 0.4780\n",
      "Epoch: 1931/2000... Training loss: 0.3095\n",
      "Epoch: 1931/2000... Training loss: 0.4655\n",
      "Epoch: 1931/2000... Training loss: 0.4522\n",
      "Epoch: 1931/2000... Training loss: 0.2487\n",
      "Epoch: 1931/2000... Training loss: 0.4099\n",
      "Epoch: 1931/2000... Training loss: 0.5134\n",
      "Epoch: 1931/2000... Training loss: 0.4952\n",
      "Epoch: 1931/2000... Training loss: 0.5614\n",
      "Epoch: 1931/2000... Training loss: 0.3458\n",
      "Epoch: 1931/2000... Training loss: 0.3850\n",
      "Epoch: 1931/2000... Training loss: 0.3489\n",
      "Epoch: 1932/2000... Training loss: 0.3611\n",
      "Epoch: 1932/2000... Training loss: 0.3765\n",
      "Epoch: 1932/2000... Training loss: 0.3706\n",
      "Epoch: 1932/2000... Training loss: 0.3608\n",
      "Epoch: 1932/2000... Training loss: 0.3547\n",
      "Epoch: 1932/2000... Training loss: 0.3165\n",
      "Epoch: 1932/2000... Training loss: 0.3122\n",
      "Epoch: 1932/2000... Training loss: 0.4289\n",
      "Epoch: 1932/2000... Training loss: 0.3299\n",
      "Epoch: 1932/2000... Training loss: 0.3624\n",
      "Epoch: 1932/2000... Training loss: 0.4141\n",
      "Epoch: 1932/2000... Training loss: 0.4288\n",
      "Epoch: 1932/2000... Training loss: 0.4599\n",
      "Epoch: 1932/2000... Training loss: 0.3721\n",
      "Epoch: 1932/2000... Training loss: 0.6828\n",
      "Epoch: 1932/2000... Training loss: 0.3665\n",
      "Epoch: 1932/2000... Training loss: 0.3358\n",
      "Epoch: 1932/2000... Training loss: 0.3442\n",
      "Epoch: 1932/2000... Training loss: 0.4930\n",
      "Epoch: 1932/2000... Training loss: 0.3966\n",
      "Epoch: 1932/2000... Training loss: 0.3635\n",
      "Epoch: 1932/2000... Training loss: 0.4073\n",
      "Epoch: 1932/2000... Training loss: 0.3840\n",
      "Epoch: 1932/2000... Training loss: 0.3967\n",
      "Epoch: 1932/2000... Training loss: 0.2783\n",
      "Epoch: 1932/2000... Training loss: 0.4721\n",
      "Epoch: 1932/2000... Training loss: 0.3198\n",
      "Epoch: 1932/2000... Training loss: 0.3675\n",
      "Epoch: 1932/2000... Training loss: 0.3677\n",
      "Epoch: 1932/2000... Training loss: 0.5059\n",
      "Epoch: 1932/2000... Training loss: 0.5448\n",
      "Epoch: 1933/2000... Training loss: 0.3833\n",
      "Epoch: 1933/2000... Training loss: 0.3392\n",
      "Epoch: 1933/2000... Training loss: 0.5075\n",
      "Epoch: 1933/2000... Training loss: 0.4628\n",
      "Epoch: 1933/2000... Training loss: 0.3467\n",
      "Epoch: 1933/2000... Training loss: 0.2419\n",
      "Epoch: 1933/2000... Training loss: 0.4993\n",
      "Epoch: 1933/2000... Training loss: 0.2868\n",
      "Epoch: 1933/2000... Training loss: 0.2246\n",
      "Epoch: 1933/2000... Training loss: 0.1991\n",
      "Epoch: 1933/2000... Training loss: 0.2995\n",
      "Epoch: 1933/2000... Training loss: 0.3040\n",
      "Epoch: 1933/2000... Training loss: 0.2781\n",
      "Epoch: 1933/2000... Training loss: 0.4386\n",
      "Epoch: 1933/2000... Training loss: 0.3148\n",
      "Epoch: 1933/2000... Training loss: 0.3544\n",
      "Epoch: 1933/2000... Training loss: 0.4482\n",
      "Epoch: 1933/2000... Training loss: 0.3444\n",
      "Epoch: 1933/2000... Training loss: 0.4065\n",
      "Epoch: 1933/2000... Training loss: 0.3262\n",
      "Epoch: 1933/2000... Training loss: 0.2288\n",
      "Epoch: 1933/2000... Training loss: 0.3232\n",
      "Epoch: 1933/2000... Training loss: 0.2535\n",
      "Epoch: 1933/2000... Training loss: 0.3968\n",
      "Epoch: 1933/2000... Training loss: 0.2677\n",
      "Epoch: 1933/2000... Training loss: 0.3497\n",
      "Epoch: 1933/2000... Training loss: 0.4281\n",
      "Epoch: 1933/2000... Training loss: 0.3587\n",
      "Epoch: 1933/2000... Training loss: 0.4389\n",
      "Epoch: 1933/2000... Training loss: 0.4977\n",
      "Epoch: 1933/2000... Training loss: 0.3353\n",
      "Epoch: 1934/2000... Training loss: 0.3089\n",
      "Epoch: 1934/2000... Training loss: 0.2634\n",
      "Epoch: 1934/2000... Training loss: 0.2857\n",
      "Epoch: 1934/2000... Training loss: 0.2139\n",
      "Epoch: 1934/2000... Training loss: 0.3105\n",
      "Epoch: 1934/2000... Training loss: 0.2518\n",
      "Epoch: 1934/2000... Training loss: 0.3902\n",
      "Epoch: 1934/2000... Training loss: 0.4017\n",
      "Epoch: 1934/2000... Training loss: 0.5918\n",
      "Epoch: 1934/2000... Training loss: 0.3857\n",
      "Epoch: 1934/2000... Training loss: 0.4621\n",
      "Epoch: 1934/2000... Training loss: 0.3367\n",
      "Epoch: 1934/2000... Training loss: 0.2381\n",
      "Epoch: 1934/2000... Training loss: 0.3768\n",
      "Epoch: 1934/2000... Training loss: 0.3021\n",
      "Epoch: 1934/2000... Training loss: 0.2715\n",
      "Epoch: 1934/2000... Training loss: 0.4497\n",
      "Epoch: 1934/2000... Training loss: 0.2647\n",
      "Epoch: 1934/2000... Training loss: 0.4645\n",
      "Epoch: 1934/2000... Training loss: 0.5143\n",
      "Epoch: 1934/2000... Training loss: 0.4400\n",
      "Epoch: 1934/2000... Training loss: 0.2254\n",
      "Epoch: 1934/2000... Training loss: 0.3865\n",
      "Epoch: 1934/2000... Training loss: 0.4562\n",
      "Epoch: 1934/2000... Training loss: 0.4142\n",
      "Epoch: 1934/2000... Training loss: 0.3277\n",
      "Epoch: 1934/2000... Training loss: 0.4731\n",
      "Epoch: 1934/2000... Training loss: 0.3435\n",
      "Epoch: 1934/2000... Training loss: 0.3495\n",
      "Epoch: 1934/2000... Training loss: 0.4038\n",
      "Epoch: 1934/2000... Training loss: 0.3306\n",
      "Epoch: 1935/2000... Training loss: 0.3821\n",
      "Epoch: 1935/2000... Training loss: 0.3301\n",
      "Epoch: 1935/2000... Training loss: 0.2220\n",
      "Epoch: 1935/2000... Training loss: 0.4803\n",
      "Epoch: 1935/2000... Training loss: 0.2703\n",
      "Epoch: 1935/2000... Training loss: 0.4412\n",
      "Epoch: 1935/2000... Training loss: 0.6819\n",
      "Epoch: 1935/2000... Training loss: 0.4005\n",
      "Epoch: 1935/2000... Training loss: 0.3674\n",
      "Epoch: 1935/2000... Training loss: 0.3917\n",
      "Epoch: 1935/2000... Training loss: 0.3017\n",
      "Epoch: 1935/2000... Training loss: 0.2978\n",
      "Epoch: 1935/2000... Training loss: 0.4535\n",
      "Epoch: 1935/2000... Training loss: 0.3546\n",
      "Epoch: 1935/2000... Training loss: 0.4191\n",
      "Epoch: 1935/2000... Training loss: 0.2790\n",
      "Epoch: 1935/2000... Training loss: 0.2285\n",
      "Epoch: 1935/2000... Training loss: 0.4812\n",
      "Epoch: 1935/2000... Training loss: 0.2752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1935/2000... Training loss: 0.4647\n",
      "Epoch: 1935/2000... Training loss: 0.3473\n",
      "Epoch: 1935/2000... Training loss: 0.4173\n",
      "Epoch: 1935/2000... Training loss: 0.3871\n",
      "Epoch: 1935/2000... Training loss: 0.2638\n",
      "Epoch: 1935/2000... Training loss: 0.3250\n",
      "Epoch: 1935/2000... Training loss: 0.3246\n",
      "Epoch: 1935/2000... Training loss: 0.3636\n",
      "Epoch: 1935/2000... Training loss: 0.3841\n",
      "Epoch: 1935/2000... Training loss: 0.3476\n",
      "Epoch: 1935/2000... Training loss: 0.2144\n",
      "Epoch: 1935/2000... Training loss: 0.2444\n",
      "Epoch: 1936/2000... Training loss: 0.2963\n",
      "Epoch: 1936/2000... Training loss: 0.4724\n",
      "Epoch: 1936/2000... Training loss: 0.2644\n",
      "Epoch: 1936/2000... Training loss: 0.2553\n",
      "Epoch: 1936/2000... Training loss: 0.2855\n",
      "Epoch: 1936/2000... Training loss: 0.3683\n",
      "Epoch: 1936/2000... Training loss: 0.4239\n",
      "Epoch: 1936/2000... Training loss: 0.4562\n",
      "Epoch: 1936/2000... Training loss: 0.3948\n",
      "Epoch: 1936/2000... Training loss: 0.3126\n",
      "Epoch: 1936/2000... Training loss: 0.5025\n",
      "Epoch: 1936/2000... Training loss: 0.3268\n",
      "Epoch: 1936/2000... Training loss: 0.3045\n",
      "Epoch: 1936/2000... Training loss: 0.3430\n",
      "Epoch: 1936/2000... Training loss: 0.3339\n",
      "Epoch: 1936/2000... Training loss: 0.4706\n",
      "Epoch: 1936/2000... Training loss: 0.4866\n",
      "Epoch: 1936/2000... Training loss: 0.4945\n",
      "Epoch: 1936/2000... Training loss: 0.5797\n",
      "Epoch: 1936/2000... Training loss: 0.3555\n",
      "Epoch: 1936/2000... Training loss: 0.3113\n",
      "Epoch: 1936/2000... Training loss: 0.3321\n",
      "Epoch: 1936/2000... Training loss: 0.3524\n",
      "Epoch: 1936/2000... Training loss: 0.4596\n",
      "Epoch: 1936/2000... Training loss: 0.3459\n",
      "Epoch: 1936/2000... Training loss: 0.2809\n",
      "Epoch: 1936/2000... Training loss: 0.2875\n",
      "Epoch: 1936/2000... Training loss: 0.4739\n",
      "Epoch: 1936/2000... Training loss: 0.3099\n",
      "Epoch: 1936/2000... Training loss: 0.3689\n",
      "Epoch: 1936/2000... Training loss: 0.4022\n",
      "Epoch: 1937/2000... Training loss: 0.3499\n",
      "Epoch: 1937/2000... Training loss: 0.3312\n",
      "Epoch: 1937/2000... Training loss: 0.2581\n",
      "Epoch: 1937/2000... Training loss: 0.2629\n",
      "Epoch: 1937/2000... Training loss: 0.4427\n",
      "Epoch: 1937/2000... Training loss: 0.3718\n",
      "Epoch: 1937/2000... Training loss: 0.3019\n",
      "Epoch: 1937/2000... Training loss: 0.4059\n",
      "Epoch: 1937/2000... Training loss: 0.3437\n",
      "Epoch: 1937/2000... Training loss: 0.5126\n",
      "Epoch: 1937/2000... Training loss: 0.2438\n",
      "Epoch: 1937/2000... Training loss: 0.4224\n",
      "Epoch: 1937/2000... Training loss: 0.3429\n",
      "Epoch: 1937/2000... Training loss: 0.5656\n",
      "Epoch: 1937/2000... Training loss: 0.4423\n",
      "Epoch: 1937/2000... Training loss: 0.3402\n",
      "Epoch: 1937/2000... Training loss: 0.2045\n",
      "Epoch: 1937/2000... Training loss: 0.2541\n",
      "Epoch: 1937/2000... Training loss: 0.2350\n",
      "Epoch: 1937/2000... Training loss: 0.4161\n",
      "Epoch: 1937/2000... Training loss: 0.3571\n",
      "Epoch: 1937/2000... Training loss: 0.4806\n",
      "Epoch: 1937/2000... Training loss: 0.2445\n",
      "Epoch: 1937/2000... Training loss: 0.2897\n",
      "Epoch: 1937/2000... Training loss: 0.1749\n",
      "Epoch: 1937/2000... Training loss: 0.3283\n",
      "Epoch: 1937/2000... Training loss: 0.4693\n",
      "Epoch: 1937/2000... Training loss: 0.3915\n",
      "Epoch: 1937/2000... Training loss: 0.4240\n",
      "Epoch: 1937/2000... Training loss: 0.4026\n",
      "Epoch: 1937/2000... Training loss: 0.3594\n",
      "Epoch: 1938/2000... Training loss: 0.4076\n",
      "Epoch: 1938/2000... Training loss: 0.4427\n",
      "Epoch: 1938/2000... Training loss: 0.3976\n",
      "Epoch: 1938/2000... Training loss: 0.3300\n",
      "Epoch: 1938/2000... Training loss: 0.3286\n",
      "Epoch: 1938/2000... Training loss: 0.2528\n",
      "Epoch: 1938/2000... Training loss: 0.3137\n",
      "Epoch: 1938/2000... Training loss: 0.4423\n",
      "Epoch: 1938/2000... Training loss: 0.2470\n",
      "Epoch: 1938/2000... Training loss: 0.3928\n",
      "Epoch: 1938/2000... Training loss: 0.4849\n",
      "Epoch: 1938/2000... Training loss: 0.5174\n",
      "Epoch: 1938/2000... Training loss: 0.2865\n",
      "Epoch: 1938/2000... Training loss: 0.3993\n",
      "Epoch: 1938/2000... Training loss: 0.4989\n",
      "Epoch: 1938/2000... Training loss: 0.4084\n",
      "Epoch: 1938/2000... Training loss: 0.3889\n",
      "Epoch: 1938/2000... Training loss: 0.4026\n",
      "Epoch: 1938/2000... Training loss: 0.2764\n",
      "Epoch: 1938/2000... Training loss: 0.3362\n",
      "Epoch: 1938/2000... Training loss: 0.2232\n",
      "Epoch: 1938/2000... Training loss: 0.4575\n",
      "Epoch: 1938/2000... Training loss: 0.4213\n",
      "Epoch: 1938/2000... Training loss: 0.3387\n",
      "Epoch: 1938/2000... Training loss: 0.3152\n",
      "Epoch: 1938/2000... Training loss: 0.5063\n",
      "Epoch: 1938/2000... Training loss: 0.2298\n",
      "Epoch: 1938/2000... Training loss: 0.3907\n",
      "Epoch: 1938/2000... Training loss: 0.3709\n",
      "Epoch: 1938/2000... Training loss: 0.4478\n",
      "Epoch: 1938/2000... Training loss: 0.6381\n",
      "Epoch: 1939/2000... Training loss: 0.3957\n",
      "Epoch: 1939/2000... Training loss: 0.4262\n",
      "Epoch: 1939/2000... Training loss: 0.3154\n",
      "Epoch: 1939/2000... Training loss: 0.2940\n",
      "Epoch: 1939/2000... Training loss: 0.3980\n",
      "Epoch: 1939/2000... Training loss: 0.3539\n",
      "Epoch: 1939/2000... Training loss: 0.4799\n",
      "Epoch: 1939/2000... Training loss: 0.3738\n",
      "Epoch: 1939/2000... Training loss: 0.4375\n",
      "Epoch: 1939/2000... Training loss: 0.3290\n",
      "Epoch: 1939/2000... Training loss: 0.4161\n",
      "Epoch: 1939/2000... Training loss: 0.3634\n",
      "Epoch: 1939/2000... Training loss: 0.2989\n",
      "Epoch: 1939/2000... Training loss: 0.2873\n",
      "Epoch: 1939/2000... Training loss: 0.3081\n",
      "Epoch: 1939/2000... Training loss: 0.4473\n",
      "Epoch: 1939/2000... Training loss: 0.3636\n",
      "Epoch: 1939/2000... Training loss: 0.4196\n",
      "Epoch: 1939/2000... Training loss: 0.2809\n",
      "Epoch: 1939/2000... Training loss: 0.3368\n",
      "Epoch: 1939/2000... Training loss: 0.2746\n",
      "Epoch: 1939/2000... Training loss: 0.2546\n",
      "Epoch: 1939/2000... Training loss: 0.3413\n",
      "Epoch: 1939/2000... Training loss: 0.3364\n",
      "Epoch: 1939/2000... Training loss: 0.3774\n",
      "Epoch: 1939/2000... Training loss: 0.4977\n",
      "Epoch: 1939/2000... Training loss: 0.3460\n",
      "Epoch: 1939/2000... Training loss: 0.4263\n",
      "Epoch: 1939/2000... Training loss: 0.5083\n",
      "Epoch: 1939/2000... Training loss: 0.3408\n",
      "Epoch: 1939/2000... Training loss: 0.4562\n",
      "Epoch: 1940/2000... Training loss: 0.2884\n",
      "Epoch: 1940/2000... Training loss: 0.3047\n",
      "Epoch: 1940/2000... Training loss: 0.3339\n",
      "Epoch: 1940/2000... Training loss: 0.4348\n",
      "Epoch: 1940/2000... Training loss: 0.3855\n",
      "Epoch: 1940/2000... Training loss: 0.5362\n",
      "Epoch: 1940/2000... Training loss: 0.3139\n",
      "Epoch: 1940/2000... Training loss: 0.3958\n",
      "Epoch: 1940/2000... Training loss: 0.3930\n",
      "Epoch: 1940/2000... Training loss: 0.3750\n",
      "Epoch: 1940/2000... Training loss: 0.2886\n",
      "Epoch: 1940/2000... Training loss: 0.4071\n",
      "Epoch: 1940/2000... Training loss: 0.2815\n",
      "Epoch: 1940/2000... Training loss: 0.3671\n",
      "Epoch: 1940/2000... Training loss: 0.2645\n",
      "Epoch: 1940/2000... Training loss: 0.3896\n",
      "Epoch: 1940/2000... Training loss: 0.3588\n",
      "Epoch: 1940/2000... Training loss: 0.3214\n",
      "Epoch: 1940/2000... Training loss: 0.3083\n",
      "Epoch: 1940/2000... Training loss: 0.2014\n",
      "Epoch: 1940/2000... Training loss: 0.3004\n",
      "Epoch: 1940/2000... Training loss: 0.2930\n",
      "Epoch: 1940/2000... Training loss: 0.3589\n",
      "Epoch: 1940/2000... Training loss: 0.3657\n",
      "Epoch: 1940/2000... Training loss: 0.3057\n",
      "Epoch: 1940/2000... Training loss: 0.2718\n",
      "Epoch: 1940/2000... Training loss: 0.3837\n",
      "Epoch: 1940/2000... Training loss: 0.2762\n",
      "Epoch: 1940/2000... Training loss: 0.3920\n",
      "Epoch: 1940/2000... Training loss: 0.3113\n",
      "Epoch: 1940/2000... Training loss: 0.2473\n",
      "Epoch: 1941/2000... Training loss: 0.3067\n",
      "Epoch: 1941/2000... Training loss: 0.3704\n",
      "Epoch: 1941/2000... Training loss: 0.6190\n",
      "Epoch: 1941/2000... Training loss: 0.2918\n",
      "Epoch: 1941/2000... Training loss: 0.4097\n",
      "Epoch: 1941/2000... Training loss: 0.2719\n",
      "Epoch: 1941/2000... Training loss: 0.4063\n",
      "Epoch: 1941/2000... Training loss: 0.2741\n",
      "Epoch: 1941/2000... Training loss: 0.5323\n",
      "Epoch: 1941/2000... Training loss: 0.2605\n",
      "Epoch: 1941/2000... Training loss: 0.3371\n",
      "Epoch: 1941/2000... Training loss: 0.3835\n",
      "Epoch: 1941/2000... Training loss: 0.3172\n",
      "Epoch: 1941/2000... Training loss: 0.3178\n",
      "Epoch: 1941/2000... Training loss: 0.2969\n",
      "Epoch: 1941/2000... Training loss: 0.3415\n",
      "Epoch: 1941/2000... Training loss: 0.4125\n",
      "Epoch: 1941/2000... Training loss: 0.3389\n",
      "Epoch: 1941/2000... Training loss: 0.4241\n",
      "Epoch: 1941/2000... Training loss: 0.3038\n",
      "Epoch: 1941/2000... Training loss: 0.5236\n",
      "Epoch: 1941/2000... Training loss: 0.4597\n",
      "Epoch: 1941/2000... Training loss: 0.4118\n",
      "Epoch: 1941/2000... Training loss: 0.4101\n",
      "Epoch: 1941/2000... Training loss: 0.2825\n",
      "Epoch: 1941/2000... Training loss: 0.5452\n",
      "Epoch: 1941/2000... Training loss: 0.3327\n",
      "Epoch: 1941/2000... Training loss: 0.4636\n",
      "Epoch: 1941/2000... Training loss: 0.5304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1941/2000... Training loss: 0.3264\n",
      "Epoch: 1941/2000... Training loss: 0.3829\n",
      "Epoch: 1942/2000... Training loss: 0.5034\n",
      "Epoch: 1942/2000... Training loss: 0.3901\n",
      "Epoch: 1942/2000... Training loss: 0.4777\n",
      "Epoch: 1942/2000... Training loss: 0.3452\n",
      "Epoch: 1942/2000... Training loss: 0.4144\n",
      "Epoch: 1942/2000... Training loss: 0.3553\n",
      "Epoch: 1942/2000... Training loss: 0.3218\n",
      "Epoch: 1942/2000... Training loss: 0.2743\n",
      "Epoch: 1942/2000... Training loss: 0.5187\n",
      "Epoch: 1942/2000... Training loss: 0.4863\n",
      "Epoch: 1942/2000... Training loss: 0.5055\n",
      "Epoch: 1942/2000... Training loss: 0.5646\n",
      "Epoch: 1942/2000... Training loss: 0.3354\n",
      "Epoch: 1942/2000... Training loss: 0.4036\n",
      "Epoch: 1942/2000... Training loss: 0.4830\n",
      "Epoch: 1942/2000... Training loss: 0.4877\n",
      "Epoch: 1942/2000... Training loss: 0.3982\n",
      "Epoch: 1942/2000... Training loss: 0.2598\n",
      "Epoch: 1942/2000... Training loss: 0.2806\n",
      "Epoch: 1942/2000... Training loss: 0.3319\n",
      "Epoch: 1942/2000... Training loss: 0.4354\n",
      "Epoch: 1942/2000... Training loss: 0.4820\n",
      "Epoch: 1942/2000... Training loss: 0.1972\n",
      "Epoch: 1942/2000... Training loss: 0.4907\n",
      "Epoch: 1942/2000... Training loss: 0.2114\n",
      "Epoch: 1942/2000... Training loss: 0.2535\n",
      "Epoch: 1942/2000... Training loss: 0.3638\n",
      "Epoch: 1942/2000... Training loss: 0.3508\n",
      "Epoch: 1942/2000... Training loss: 0.1567\n",
      "Epoch: 1942/2000... Training loss: 0.2988\n",
      "Epoch: 1942/2000... Training loss: 0.3449\n",
      "Epoch: 1943/2000... Training loss: 0.2406\n",
      "Epoch: 1943/2000... Training loss: 0.2603\n",
      "Epoch: 1943/2000... Training loss: 0.4408\n",
      "Epoch: 1943/2000... Training loss: 0.3264\n",
      "Epoch: 1943/2000... Training loss: 0.3102\n",
      "Epoch: 1943/2000... Training loss: 0.3081\n",
      "Epoch: 1943/2000... Training loss: 0.3199\n",
      "Epoch: 1943/2000... Training loss: 0.2878\n",
      "Epoch: 1943/2000... Training loss: 0.2636\n",
      "Epoch: 1943/2000... Training loss: 0.4096\n",
      "Epoch: 1943/2000... Training loss: 0.4559\n",
      "Epoch: 1943/2000... Training loss: 0.3281\n",
      "Epoch: 1943/2000... Training loss: 0.3930\n",
      "Epoch: 1943/2000... Training loss: 0.2225\n",
      "Epoch: 1943/2000... Training loss: 0.5585\n",
      "Epoch: 1943/2000... Training loss: 0.3646\n",
      "Epoch: 1943/2000... Training loss: 0.5567\n",
      "Epoch: 1943/2000... Training loss: 0.3514\n",
      "Epoch: 1943/2000... Training loss: 0.3517\n",
      "Epoch: 1943/2000... Training loss: 0.3629\n",
      "Epoch: 1943/2000... Training loss: 0.4223\n",
      "Epoch: 1943/2000... Training loss: 0.3936\n",
      "Epoch: 1943/2000... Training loss: 0.3339\n",
      "Epoch: 1943/2000... Training loss: 0.2773\n",
      "Epoch: 1943/2000... Training loss: 0.4623\n",
      "Epoch: 1943/2000... Training loss: 0.4712\n",
      "Epoch: 1943/2000... Training loss: 0.4500\n",
      "Epoch: 1943/2000... Training loss: 0.2651\n",
      "Epoch: 1943/2000... Training loss: 0.3521\n",
      "Epoch: 1943/2000... Training loss: 0.4951\n",
      "Epoch: 1943/2000... Training loss: 0.4881\n",
      "Epoch: 1944/2000... Training loss: 0.4821\n",
      "Epoch: 1944/2000... Training loss: 0.2202\n",
      "Epoch: 1944/2000... Training loss: 0.4286\n",
      "Epoch: 1944/2000... Training loss: 0.3859\n",
      "Epoch: 1944/2000... Training loss: 0.3980\n",
      "Epoch: 1944/2000... Training loss: 0.2626\n",
      "Epoch: 1944/2000... Training loss: 0.4122\n",
      "Epoch: 1944/2000... Training loss: 0.5468\n",
      "Epoch: 1944/2000... Training loss: 0.2803\n",
      "Epoch: 1944/2000... Training loss: 0.2579\n",
      "Epoch: 1944/2000... Training loss: 0.4058\n",
      "Epoch: 1944/2000... Training loss: 0.3321\n",
      "Epoch: 1944/2000... Training loss: 0.4093\n",
      "Epoch: 1944/2000... Training loss: 0.3360\n",
      "Epoch: 1944/2000... Training loss: 0.4335\n",
      "Epoch: 1944/2000... Training loss: 0.3960\n",
      "Epoch: 1944/2000... Training loss: 0.3284\n",
      "Epoch: 1944/2000... Training loss: 0.4270\n",
      "Epoch: 1944/2000... Training loss: 0.3922\n",
      "Epoch: 1944/2000... Training loss: 0.3075\n",
      "Epoch: 1944/2000... Training loss: 0.3252\n",
      "Epoch: 1944/2000... Training loss: 0.4745\n",
      "Epoch: 1944/2000... Training loss: 0.3157\n",
      "Epoch: 1944/2000... Training loss: 0.1810\n",
      "Epoch: 1944/2000... Training loss: 0.2804\n",
      "Epoch: 1944/2000... Training loss: 0.3881\n",
      "Epoch: 1944/2000... Training loss: 0.4150\n",
      "Epoch: 1944/2000... Training loss: 0.3761\n",
      "Epoch: 1944/2000... Training loss: 0.3867\n",
      "Epoch: 1944/2000... Training loss: 0.5971\n",
      "Epoch: 1944/2000... Training loss: 0.2916\n",
      "Epoch: 1945/2000... Training loss: 0.4474\n",
      "Epoch: 1945/2000... Training loss: 0.4453\n",
      "Epoch: 1945/2000... Training loss: 0.3980\n",
      "Epoch: 1945/2000... Training loss: 0.5159\n",
      "Epoch: 1945/2000... Training loss: 0.3668\n",
      "Epoch: 1945/2000... Training loss: 0.3090\n",
      "Epoch: 1945/2000... Training loss: 0.3756\n",
      "Epoch: 1945/2000... Training loss: 0.3026\n",
      "Epoch: 1945/2000... Training loss: 0.3312\n",
      "Epoch: 1945/2000... Training loss: 0.2961\n",
      "Epoch: 1945/2000... Training loss: 0.4747\n",
      "Epoch: 1945/2000... Training loss: 0.3756\n",
      "Epoch: 1945/2000... Training loss: 0.3768\n",
      "Epoch: 1945/2000... Training loss: 0.3386\n",
      "Epoch: 1945/2000... Training loss: 0.4483\n",
      "Epoch: 1945/2000... Training loss: 0.6938\n",
      "Epoch: 1945/2000... Training loss: 0.2558\n",
      "Epoch: 1945/2000... Training loss: 0.2622\n",
      "Epoch: 1945/2000... Training loss: 0.3324\n",
      "Epoch: 1945/2000... Training loss: 0.4283\n",
      "Epoch: 1945/2000... Training loss: 0.5121\n",
      "Epoch: 1945/2000... Training loss: 0.3505\n",
      "Epoch: 1945/2000... Training loss: 0.4893\n",
      "Epoch: 1945/2000... Training loss: 0.2787\n",
      "Epoch: 1945/2000... Training loss: 0.2411\n",
      "Epoch: 1945/2000... Training loss: 0.4286\n",
      "Epoch: 1945/2000... Training loss: 0.4026\n",
      "Epoch: 1945/2000... Training loss: 0.4564\n",
      "Epoch: 1945/2000... Training loss: 0.2943\n",
      "Epoch: 1945/2000... Training loss: 0.3430\n",
      "Epoch: 1945/2000... Training loss: 0.3067\n",
      "Epoch: 1946/2000... Training loss: 0.3510\n",
      "Epoch: 1946/2000... Training loss: 0.3391\n",
      "Epoch: 1946/2000... Training loss: 0.3747\n",
      "Epoch: 1946/2000... Training loss: 0.3991\n",
      "Epoch: 1946/2000... Training loss: 0.5941\n",
      "Epoch: 1946/2000... Training loss: 0.3815\n",
      "Epoch: 1946/2000... Training loss: 0.5496\n",
      "Epoch: 1946/2000... Training loss: 0.2938\n",
      "Epoch: 1946/2000... Training loss: 0.4851\n",
      "Epoch: 1946/2000... Training loss: 0.3769\n",
      "Epoch: 1946/2000... Training loss: 0.3604\n",
      "Epoch: 1946/2000... Training loss: 0.3959\n",
      "Epoch: 1946/2000... Training loss: 0.3284\n",
      "Epoch: 1946/2000... Training loss: 0.3201\n",
      "Epoch: 1946/2000... Training loss: 0.3007\n",
      "Epoch: 1946/2000... Training loss: 0.3471\n",
      "Epoch: 1946/2000... Training loss: 0.5130\n",
      "Epoch: 1946/2000... Training loss: 0.3266\n",
      "Epoch: 1946/2000... Training loss: 0.2700\n",
      "Epoch: 1946/2000... Training loss: 0.3182\n",
      "Epoch: 1946/2000... Training loss: 0.3462\n",
      "Epoch: 1946/2000... Training loss: 0.2784\n",
      "Epoch: 1946/2000... Training loss: 0.3571\n",
      "Epoch: 1946/2000... Training loss: 0.4642\n",
      "Epoch: 1946/2000... Training loss: 0.4553\n",
      "Epoch: 1946/2000... Training loss: 0.5707\n",
      "Epoch: 1946/2000... Training loss: 0.4212\n",
      "Epoch: 1946/2000... Training loss: 0.2966\n",
      "Epoch: 1946/2000... Training loss: 0.2770\n",
      "Epoch: 1946/2000... Training loss: 0.3145\n",
      "Epoch: 1946/2000... Training loss: 0.5390\n",
      "Epoch: 1947/2000... Training loss: 0.3728\n",
      "Epoch: 1947/2000... Training loss: 0.2989\n",
      "Epoch: 1947/2000... Training loss: 0.2013\n",
      "Epoch: 1947/2000... Training loss: 0.4127\n",
      "Epoch: 1947/2000... Training loss: 0.3840\n",
      "Epoch: 1947/2000... Training loss: 0.4492\n",
      "Epoch: 1947/2000... Training loss: 0.3736\n",
      "Epoch: 1947/2000... Training loss: 0.4374\n",
      "Epoch: 1947/2000... Training loss: 0.4611\n",
      "Epoch: 1947/2000... Training loss: 0.2799\n",
      "Epoch: 1947/2000... Training loss: 0.2980\n",
      "Epoch: 1947/2000... Training loss: 0.3445\n",
      "Epoch: 1947/2000... Training loss: 0.5092\n",
      "Epoch: 1947/2000... Training loss: 0.5198\n",
      "Epoch: 1947/2000... Training loss: 0.3821\n",
      "Epoch: 1947/2000... Training loss: 0.3361\n",
      "Epoch: 1947/2000... Training loss: 0.3056\n",
      "Epoch: 1947/2000... Training loss: 0.3050\n",
      "Epoch: 1947/2000... Training loss: 0.3866\n",
      "Epoch: 1947/2000... Training loss: 0.5320\n",
      "Epoch: 1947/2000... Training loss: 0.2376\n",
      "Epoch: 1947/2000... Training loss: 0.2896\n",
      "Epoch: 1947/2000... Training loss: 0.3100\n",
      "Epoch: 1947/2000... Training loss: 0.2736\n",
      "Epoch: 1947/2000... Training loss: 0.4710\n",
      "Epoch: 1947/2000... Training loss: 0.4044\n",
      "Epoch: 1947/2000... Training loss: 0.2581\n",
      "Epoch: 1947/2000... Training loss: 0.5160\n",
      "Epoch: 1947/2000... Training loss: 0.3248\n",
      "Epoch: 1947/2000... Training loss: 0.4050\n",
      "Epoch: 1947/2000... Training loss: 0.2936\n",
      "Epoch: 1948/2000... Training loss: 0.4588\n",
      "Epoch: 1948/2000... Training loss: 0.3916\n",
      "Epoch: 1948/2000... Training loss: 0.3513\n",
      "Epoch: 1948/2000... Training loss: 0.2392\n",
      "Epoch: 1948/2000... Training loss: 0.3191\n",
      "Epoch: 1948/2000... Training loss: 0.3179\n",
      "Epoch: 1948/2000... Training loss: 0.6251\n",
      "Epoch: 1948/2000... Training loss: 0.4703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1948/2000... Training loss: 0.3290\n",
      "Epoch: 1948/2000... Training loss: 0.3370\n",
      "Epoch: 1948/2000... Training loss: 0.5754\n",
      "Epoch: 1948/2000... Training loss: 0.3390\n",
      "Epoch: 1948/2000... Training loss: 0.4929\n",
      "Epoch: 1948/2000... Training loss: 0.3108\n",
      "Epoch: 1948/2000... Training loss: 0.2893\n",
      "Epoch: 1948/2000... Training loss: 0.2786\n",
      "Epoch: 1948/2000... Training loss: 0.3854\n",
      "Epoch: 1948/2000... Training loss: 0.3951\n",
      "Epoch: 1948/2000... Training loss: 0.3514\n",
      "Epoch: 1948/2000... Training loss: 0.3652\n",
      "Epoch: 1948/2000... Training loss: 0.4341\n",
      "Epoch: 1948/2000... Training loss: 0.4644\n",
      "Epoch: 1948/2000... Training loss: 0.4228\n",
      "Epoch: 1948/2000... Training loss: 0.3780\n",
      "Epoch: 1948/2000... Training loss: 0.3294\n",
      "Epoch: 1948/2000... Training loss: 0.5808\n",
      "Epoch: 1948/2000... Training loss: 0.4243\n",
      "Epoch: 1948/2000... Training loss: 0.4280\n",
      "Epoch: 1948/2000... Training loss: 0.3024\n",
      "Epoch: 1948/2000... Training loss: 0.5055\n",
      "Epoch: 1948/2000... Training loss: 0.3232\n",
      "Epoch: 1949/2000... Training loss: 0.3117\n",
      "Epoch: 1949/2000... Training loss: 0.4837\n",
      "Epoch: 1949/2000... Training loss: 0.4412\n",
      "Epoch: 1949/2000... Training loss: 0.3472\n",
      "Epoch: 1949/2000... Training loss: 0.3251\n",
      "Epoch: 1949/2000... Training loss: 0.2141\n",
      "Epoch: 1949/2000... Training loss: 0.3143\n",
      "Epoch: 1949/2000... Training loss: 0.2748\n",
      "Epoch: 1949/2000... Training loss: 0.4141\n",
      "Epoch: 1949/2000... Training loss: 0.4252\n",
      "Epoch: 1949/2000... Training loss: 0.2389\n",
      "Epoch: 1949/2000... Training loss: 0.3148\n",
      "Epoch: 1949/2000... Training loss: 0.5309\n",
      "Epoch: 1949/2000... Training loss: 0.3220\n",
      "Epoch: 1949/2000... Training loss: 0.5632\n",
      "Epoch: 1949/2000... Training loss: 0.3702\n",
      "Epoch: 1949/2000... Training loss: 0.3208\n",
      "Epoch: 1949/2000... Training loss: 0.3926\n",
      "Epoch: 1949/2000... Training loss: 0.3584\n",
      "Epoch: 1949/2000... Training loss: 0.3447\n",
      "Epoch: 1949/2000... Training loss: 0.3693\n",
      "Epoch: 1949/2000... Training loss: 0.3450\n",
      "Epoch: 1949/2000... Training loss: 0.2834\n",
      "Epoch: 1949/2000... Training loss: 0.4575\n",
      "Epoch: 1949/2000... Training loss: 0.3713\n",
      "Epoch: 1949/2000... Training loss: 0.3141\n",
      "Epoch: 1949/2000... Training loss: 0.4665\n",
      "Epoch: 1949/2000... Training loss: 0.4607\n",
      "Epoch: 1949/2000... Training loss: 0.4136\n",
      "Epoch: 1949/2000... Training loss: 0.4103\n",
      "Epoch: 1949/2000... Training loss: 0.2461\n",
      "Epoch: 1950/2000... Training loss: 0.3335\n",
      "Epoch: 1950/2000... Training loss: 0.4831\n",
      "Epoch: 1950/2000... Training loss: 0.5263\n",
      "Epoch: 1950/2000... Training loss: 0.2627\n",
      "Epoch: 1950/2000... Training loss: 0.5393\n",
      "Epoch: 1950/2000... Training loss: 0.3698\n",
      "Epoch: 1950/2000... Training loss: 0.3660\n",
      "Epoch: 1950/2000... Training loss: 0.2881\n",
      "Epoch: 1950/2000... Training loss: 0.3890\n",
      "Epoch: 1950/2000... Training loss: 0.2561\n",
      "Epoch: 1950/2000... Training loss: 0.4683\n",
      "Epoch: 1950/2000... Training loss: 0.3115\n",
      "Epoch: 1950/2000... Training loss: 0.2351\n",
      "Epoch: 1950/2000... Training loss: 0.3290\n",
      "Epoch: 1950/2000... Training loss: 0.5380\n",
      "Epoch: 1950/2000... Training loss: 0.3241\n",
      "Epoch: 1950/2000... Training loss: 0.3865\n",
      "Epoch: 1950/2000... Training loss: 0.2966\n",
      "Epoch: 1950/2000... Training loss: 0.3810\n",
      "Epoch: 1950/2000... Training loss: 0.3807\n",
      "Epoch: 1950/2000... Training loss: 0.3269\n",
      "Epoch: 1950/2000... Training loss: 0.2906\n",
      "Epoch: 1950/2000... Training loss: 0.4608\n",
      "Epoch: 1950/2000... Training loss: 0.3716\n",
      "Epoch: 1950/2000... Training loss: 0.3282\n",
      "Epoch: 1950/2000... Training loss: 0.2776\n",
      "Epoch: 1950/2000... Training loss: 0.5814\n",
      "Epoch: 1950/2000... Training loss: 0.6552\n",
      "Epoch: 1950/2000... Training loss: 0.5417\n",
      "Epoch: 1950/2000... Training loss: 0.4715\n",
      "Epoch: 1950/2000... Training loss: 0.4474\n",
      "Epoch: 1951/2000... Training loss: 0.3727\n",
      "Epoch: 1951/2000... Training loss: 0.4342\n",
      "Epoch: 1951/2000... Training loss: 0.4135\n",
      "Epoch: 1951/2000... Training loss: 0.3766\n",
      "Epoch: 1951/2000... Training loss: 0.3469\n",
      "Epoch: 1951/2000... Training loss: 0.4419\n",
      "Epoch: 1951/2000... Training loss: 0.3114\n",
      "Epoch: 1951/2000... Training loss: 0.5449\n",
      "Epoch: 1951/2000... Training loss: 0.2702\n",
      "Epoch: 1951/2000... Training loss: 0.5490\n",
      "Epoch: 1951/2000... Training loss: 0.2925\n",
      "Epoch: 1951/2000... Training loss: 0.3590\n",
      "Epoch: 1951/2000... Training loss: 0.3848\n",
      "Epoch: 1951/2000... Training loss: 0.2569\n",
      "Epoch: 1951/2000... Training loss: 0.6517\n",
      "Epoch: 1951/2000... Training loss: 0.6285\n",
      "Epoch: 1951/2000... Training loss: 0.5037\n",
      "Epoch: 1951/2000... Training loss: 0.4480\n",
      "Epoch: 1951/2000... Training loss: 0.3227\n",
      "Epoch: 1951/2000... Training loss: 0.4541\n",
      "Epoch: 1951/2000... Training loss: 0.2990\n",
      "Epoch: 1951/2000... Training loss: 0.5727\n",
      "Epoch: 1951/2000... Training loss: 0.3899\n",
      "Epoch: 1951/2000... Training loss: 0.3822\n",
      "Epoch: 1951/2000... Training loss: 0.3888\n",
      "Epoch: 1951/2000... Training loss: 0.3836\n",
      "Epoch: 1951/2000... Training loss: 0.2939\n",
      "Epoch: 1951/2000... Training loss: 0.5487\n",
      "Epoch: 1951/2000... Training loss: 0.5301\n",
      "Epoch: 1951/2000... Training loss: 0.3583\n",
      "Epoch: 1951/2000... Training loss: 0.3334\n",
      "Epoch: 1952/2000... Training loss: 0.3182\n",
      "Epoch: 1952/2000... Training loss: 0.5001\n",
      "Epoch: 1952/2000... Training loss: 0.3562\n",
      "Epoch: 1952/2000... Training loss: 0.4273\n",
      "Epoch: 1952/2000... Training loss: 0.2890\n",
      "Epoch: 1952/2000... Training loss: 0.3901\n",
      "Epoch: 1952/2000... Training loss: 0.5580\n",
      "Epoch: 1952/2000... Training loss: 0.3993\n",
      "Epoch: 1952/2000... Training loss: 0.4691\n",
      "Epoch: 1952/2000... Training loss: 0.3113\n",
      "Epoch: 1952/2000... Training loss: 0.5190\n",
      "Epoch: 1952/2000... Training loss: 0.3690\n",
      "Epoch: 1952/2000... Training loss: 0.2816\n",
      "Epoch: 1952/2000... Training loss: 0.3530\n",
      "Epoch: 1952/2000... Training loss: 0.2443\n",
      "Epoch: 1952/2000... Training loss: 0.2962\n",
      "Epoch: 1952/2000... Training loss: 0.2735\n",
      "Epoch: 1952/2000... Training loss: 0.5132\n",
      "Epoch: 1952/2000... Training loss: 0.3024\n",
      "Epoch: 1952/2000... Training loss: 0.3300\n",
      "Epoch: 1952/2000... Training loss: 0.2928\n",
      "Epoch: 1952/2000... Training loss: 0.3442\n",
      "Epoch: 1952/2000... Training loss: 0.3059\n",
      "Epoch: 1952/2000... Training loss: 0.3475\n",
      "Epoch: 1952/2000... Training loss: 0.2290\n",
      "Epoch: 1952/2000... Training loss: 0.2838\n",
      "Epoch: 1952/2000... Training loss: 0.2756\n",
      "Epoch: 1952/2000... Training loss: 0.5427\n",
      "Epoch: 1952/2000... Training loss: 0.2834\n",
      "Epoch: 1952/2000... Training loss: 0.4546\n",
      "Epoch: 1952/2000... Training loss: 0.3437\n",
      "Epoch: 1953/2000... Training loss: 0.3914\n",
      "Epoch: 1953/2000... Training loss: 0.3285\n",
      "Epoch: 1953/2000... Training loss: 0.3889\n",
      "Epoch: 1953/2000... Training loss: 0.3816\n",
      "Epoch: 1953/2000... Training loss: 0.3658\n",
      "Epoch: 1953/2000... Training loss: 0.4359\n",
      "Epoch: 1953/2000... Training loss: 0.3655\n",
      "Epoch: 1953/2000... Training loss: 0.2251\n",
      "Epoch: 1953/2000... Training loss: 0.4035\n",
      "Epoch: 1953/2000... Training loss: 0.5137\n",
      "Epoch: 1953/2000... Training loss: 0.3173\n",
      "Epoch: 1953/2000... Training loss: 0.3039\n",
      "Epoch: 1953/2000... Training loss: 0.4144\n",
      "Epoch: 1953/2000... Training loss: 0.3141\n",
      "Epoch: 1953/2000... Training loss: 0.3479\n",
      "Epoch: 1953/2000... Training loss: 0.2564\n",
      "Epoch: 1953/2000... Training loss: 0.3872\n",
      "Epoch: 1953/2000... Training loss: 0.3953\n",
      "Epoch: 1953/2000... Training loss: 0.3339\n",
      "Epoch: 1953/2000... Training loss: 0.2862\n",
      "Epoch: 1953/2000... Training loss: 0.4534\n",
      "Epoch: 1953/2000... Training loss: 0.3491\n",
      "Epoch: 1953/2000... Training loss: 0.3425\n",
      "Epoch: 1953/2000... Training loss: 0.3870\n",
      "Epoch: 1953/2000... Training loss: 0.3392\n",
      "Epoch: 1953/2000... Training loss: 0.3156\n",
      "Epoch: 1953/2000... Training loss: 0.3839\n",
      "Epoch: 1953/2000... Training loss: 0.4282\n",
      "Epoch: 1953/2000... Training loss: 0.2886\n",
      "Epoch: 1953/2000... Training loss: 0.4164\n",
      "Epoch: 1953/2000... Training loss: 0.2868\n",
      "Epoch: 1954/2000... Training loss: 0.3364\n",
      "Epoch: 1954/2000... Training loss: 0.2829\n",
      "Epoch: 1954/2000... Training loss: 0.3969\n",
      "Epoch: 1954/2000... Training loss: 0.2945\n",
      "Epoch: 1954/2000... Training loss: 0.3020\n",
      "Epoch: 1954/2000... Training loss: 0.3810\n",
      "Epoch: 1954/2000... Training loss: 0.1999\n",
      "Epoch: 1954/2000... Training loss: 0.3974\n",
      "Epoch: 1954/2000... Training loss: 0.3342\n",
      "Epoch: 1954/2000... Training loss: 0.2860\n",
      "Epoch: 1954/2000... Training loss: 0.2868\n",
      "Epoch: 1954/2000... Training loss: 0.4395\n",
      "Epoch: 1954/2000... Training loss: 0.5909\n",
      "Epoch: 1954/2000... Training loss: 0.3953\n",
      "Epoch: 1954/2000... Training loss: 0.2686\n",
      "Epoch: 1954/2000... Training loss: 0.3416\n",
      "Epoch: 1954/2000... Training loss: 0.3448\n",
      "Epoch: 1954/2000... Training loss: 0.3307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1954/2000... Training loss: 0.3044\n",
      "Epoch: 1954/2000... Training loss: 0.2873\n",
      "Epoch: 1954/2000... Training loss: 0.3780\n",
      "Epoch: 1954/2000... Training loss: 0.3852\n",
      "Epoch: 1954/2000... Training loss: 0.4227\n",
      "Epoch: 1954/2000... Training loss: 0.4603\n",
      "Epoch: 1954/2000... Training loss: 0.4746\n",
      "Epoch: 1954/2000... Training loss: 0.3034\n",
      "Epoch: 1954/2000... Training loss: 0.3219\n",
      "Epoch: 1954/2000... Training loss: 0.3783\n",
      "Epoch: 1954/2000... Training loss: 0.4971\n",
      "Epoch: 1954/2000... Training loss: 0.3463\n",
      "Epoch: 1954/2000... Training loss: 0.4962\n",
      "Epoch: 1955/2000... Training loss: 0.5015\n",
      "Epoch: 1955/2000... Training loss: 0.3437\n",
      "Epoch: 1955/2000... Training loss: 0.5199\n",
      "Epoch: 1955/2000... Training loss: 0.2671\n",
      "Epoch: 1955/2000... Training loss: 0.6149\n",
      "Epoch: 1955/2000... Training loss: 0.1706\n",
      "Epoch: 1955/2000... Training loss: 0.4340\n",
      "Epoch: 1955/2000... Training loss: 0.2423\n",
      "Epoch: 1955/2000... Training loss: 0.3584\n",
      "Epoch: 1955/2000... Training loss: 0.4410\n",
      "Epoch: 1955/2000... Training loss: 0.3136\n",
      "Epoch: 1955/2000... Training loss: 0.2080\n",
      "Epoch: 1955/2000... Training loss: 0.3143\n",
      "Epoch: 1955/2000... Training loss: 0.2151\n",
      "Epoch: 1955/2000... Training loss: 0.2756\n",
      "Epoch: 1955/2000... Training loss: 0.3552\n",
      "Epoch: 1955/2000... Training loss: 0.4055\n",
      "Epoch: 1955/2000... Training loss: 0.3402\n",
      "Epoch: 1955/2000... Training loss: 0.3986\n",
      "Epoch: 1955/2000... Training loss: 0.3843\n",
      "Epoch: 1955/2000... Training loss: 0.4658\n",
      "Epoch: 1955/2000... Training loss: 0.4355\n",
      "Epoch: 1955/2000... Training loss: 0.5799\n",
      "Epoch: 1955/2000... Training loss: 0.3650\n",
      "Epoch: 1955/2000... Training loss: 0.2709\n",
      "Epoch: 1955/2000... Training loss: 0.5559\n",
      "Epoch: 1955/2000... Training loss: 0.4622\n",
      "Epoch: 1955/2000... Training loss: 0.4521\n",
      "Epoch: 1955/2000... Training loss: 0.4415\n",
      "Epoch: 1955/2000... Training loss: 0.3419\n",
      "Epoch: 1955/2000... Training loss: 0.4123\n",
      "Epoch: 1956/2000... Training loss: 0.4611\n",
      "Epoch: 1956/2000... Training loss: 0.4382\n",
      "Epoch: 1956/2000... Training loss: 0.4749\n",
      "Epoch: 1956/2000... Training loss: 0.4173\n",
      "Epoch: 1956/2000... Training loss: 0.2707\n",
      "Epoch: 1956/2000... Training loss: 0.3325\n",
      "Epoch: 1956/2000... Training loss: 0.4185\n",
      "Epoch: 1956/2000... Training loss: 0.4249\n",
      "Epoch: 1956/2000... Training loss: 0.3811\n",
      "Epoch: 1956/2000... Training loss: 0.2586\n",
      "Epoch: 1956/2000... Training loss: 0.4397\n",
      "Epoch: 1956/2000... Training loss: 0.2667\n",
      "Epoch: 1956/2000... Training loss: 0.3422\n",
      "Epoch: 1956/2000... Training loss: 0.4094\n",
      "Epoch: 1956/2000... Training loss: 0.2563\n",
      "Epoch: 1956/2000... Training loss: 0.3051\n",
      "Epoch: 1956/2000... Training loss: 0.3455\n",
      "Epoch: 1956/2000... Training loss: 0.2979\n",
      "Epoch: 1956/2000... Training loss: 0.2779\n",
      "Epoch: 1956/2000... Training loss: 0.3525\n",
      "Epoch: 1956/2000... Training loss: 0.3074\n",
      "Epoch: 1956/2000... Training loss: 0.4275\n",
      "Epoch: 1956/2000... Training loss: 0.2985\n",
      "Epoch: 1956/2000... Training loss: 0.3626\n",
      "Epoch: 1956/2000... Training loss: 0.3688\n",
      "Epoch: 1956/2000... Training loss: 0.3105\n",
      "Epoch: 1956/2000... Training loss: 0.4446\n",
      "Epoch: 1956/2000... Training loss: 0.3794\n",
      "Epoch: 1956/2000... Training loss: 0.2912\n",
      "Epoch: 1956/2000... Training loss: 0.6361\n",
      "Epoch: 1956/2000... Training loss: 0.2730\n",
      "Epoch: 1957/2000... Training loss: 0.3145\n",
      "Epoch: 1957/2000... Training loss: 0.3851\n",
      "Epoch: 1957/2000... Training loss: 0.3308\n",
      "Epoch: 1957/2000... Training loss: 0.3831\n",
      "Epoch: 1957/2000... Training loss: 0.3278\n",
      "Epoch: 1957/2000... Training loss: 0.5727\n",
      "Epoch: 1957/2000... Training loss: 0.3246\n",
      "Epoch: 1957/2000... Training loss: 0.3241\n",
      "Epoch: 1957/2000... Training loss: 0.4250\n",
      "Epoch: 1957/2000... Training loss: 0.3271\n",
      "Epoch: 1957/2000... Training loss: 0.3495\n",
      "Epoch: 1957/2000... Training loss: 0.5154\n",
      "Epoch: 1957/2000... Training loss: 0.4235\n",
      "Epoch: 1957/2000... Training loss: 0.3289\n",
      "Epoch: 1957/2000... Training loss: 0.2459\n",
      "Epoch: 1957/2000... Training loss: 0.5162\n",
      "Epoch: 1957/2000... Training loss: 0.2951\n",
      "Epoch: 1957/2000... Training loss: 0.5867\n",
      "Epoch: 1957/2000... Training loss: 0.2221\n",
      "Epoch: 1957/2000... Training loss: 0.2870\n",
      "Epoch: 1957/2000... Training loss: 0.4471\n",
      "Epoch: 1957/2000... Training loss: 0.3084\n",
      "Epoch: 1957/2000... Training loss: 0.3164\n",
      "Epoch: 1957/2000... Training loss: 0.5991\n",
      "Epoch: 1957/2000... Training loss: 0.4328\n",
      "Epoch: 1957/2000... Training loss: 0.5882\n",
      "Epoch: 1957/2000... Training loss: 0.3545\n",
      "Epoch: 1957/2000... Training loss: 0.2595\n",
      "Epoch: 1957/2000... Training loss: 0.3414\n",
      "Epoch: 1957/2000... Training loss: 0.2674\n",
      "Epoch: 1957/2000... Training loss: 0.4025\n",
      "Epoch: 1958/2000... Training loss: 0.4131\n",
      "Epoch: 1958/2000... Training loss: 0.2558\n",
      "Epoch: 1958/2000... Training loss: 0.3600\n",
      "Epoch: 1958/2000... Training loss: 0.2933\n",
      "Epoch: 1958/2000... Training loss: 0.5569\n",
      "Epoch: 1958/2000... Training loss: 0.1833\n",
      "Epoch: 1958/2000... Training loss: 0.2956\n",
      "Epoch: 1958/2000... Training loss: 0.4249\n",
      "Epoch: 1958/2000... Training loss: 0.2937\n",
      "Epoch: 1958/2000... Training loss: 0.3240\n",
      "Epoch: 1958/2000... Training loss: 0.2671\n",
      "Epoch: 1958/2000... Training loss: 0.3533\n",
      "Epoch: 1958/2000... Training loss: 0.3862\n",
      "Epoch: 1958/2000... Training loss: 0.4128\n",
      "Epoch: 1958/2000... Training loss: 0.3444\n",
      "Epoch: 1958/2000... Training loss: 0.2661\n",
      "Epoch: 1958/2000... Training loss: 0.3202\n",
      "Epoch: 1958/2000... Training loss: 0.2712\n",
      "Epoch: 1958/2000... Training loss: 0.2840\n",
      "Epoch: 1958/2000... Training loss: 0.4158\n",
      "Epoch: 1958/2000... Training loss: 0.4399\n",
      "Epoch: 1958/2000... Training loss: 0.2578\n",
      "Epoch: 1958/2000... Training loss: 0.2617\n",
      "Epoch: 1958/2000... Training loss: 0.3789\n",
      "Epoch: 1958/2000... Training loss: 0.3147\n",
      "Epoch: 1958/2000... Training loss: 0.2415\n",
      "Epoch: 1958/2000... Training loss: 0.3212\n",
      "Epoch: 1958/2000... Training loss: 0.3759\n",
      "Epoch: 1958/2000... Training loss: 0.5339\n",
      "Epoch: 1958/2000... Training loss: 0.3817\n",
      "Epoch: 1958/2000... Training loss: 0.2141\n",
      "Epoch: 1959/2000... Training loss: 0.3114\n",
      "Epoch: 1959/2000... Training loss: 0.3383\n",
      "Epoch: 1959/2000... Training loss: 0.3186\n",
      "Epoch: 1959/2000... Training loss: 0.2071\n",
      "Epoch: 1959/2000... Training loss: 0.3599\n",
      "Epoch: 1959/2000... Training loss: 0.1410\n",
      "Epoch: 1959/2000... Training loss: 0.3948\n",
      "Epoch: 1959/2000... Training loss: 0.3811\n",
      "Epoch: 1959/2000... Training loss: 0.5217\n",
      "Epoch: 1959/2000... Training loss: 0.3213\n",
      "Epoch: 1959/2000... Training loss: 0.2495\n",
      "Epoch: 1959/2000... Training loss: 0.5772\n",
      "Epoch: 1959/2000... Training loss: 0.3353\n",
      "Epoch: 1959/2000... Training loss: 0.3754\n",
      "Epoch: 1959/2000... Training loss: 0.5179\n",
      "Epoch: 1959/2000... Training loss: 0.2774\n",
      "Epoch: 1959/2000... Training loss: 0.3668\n",
      "Epoch: 1959/2000... Training loss: 0.2926\n",
      "Epoch: 1959/2000... Training loss: 0.3213\n",
      "Epoch: 1959/2000... Training loss: 0.3436\n",
      "Epoch: 1959/2000... Training loss: 0.5734\n",
      "Epoch: 1959/2000... Training loss: 0.3683\n",
      "Epoch: 1959/2000... Training loss: 0.2367\n",
      "Epoch: 1959/2000... Training loss: 0.4365\n",
      "Epoch: 1959/2000... Training loss: 0.2895\n",
      "Epoch: 1959/2000... Training loss: 0.2703\n",
      "Epoch: 1959/2000... Training loss: 0.3894\n",
      "Epoch: 1959/2000... Training loss: 0.5596\n",
      "Epoch: 1959/2000... Training loss: 0.2683\n",
      "Epoch: 1959/2000... Training loss: 0.4593\n",
      "Epoch: 1959/2000... Training loss: 0.4478\n",
      "Epoch: 1960/2000... Training loss: 0.3557\n",
      "Epoch: 1960/2000... Training loss: 0.4475\n",
      "Epoch: 1960/2000... Training loss: 0.3962\n",
      "Epoch: 1960/2000... Training loss: 0.3179\n",
      "Epoch: 1960/2000... Training loss: 0.4418\n",
      "Epoch: 1960/2000... Training loss: 0.5536\n",
      "Epoch: 1960/2000... Training loss: 0.2952\n",
      "Epoch: 1960/2000... Training loss: 0.3753\n",
      "Epoch: 1960/2000... Training loss: 0.3392\n",
      "Epoch: 1960/2000... Training loss: 0.3992\n",
      "Epoch: 1960/2000... Training loss: 0.5424\n",
      "Epoch: 1960/2000... Training loss: 0.4896\n",
      "Epoch: 1960/2000... Training loss: 0.4980\n",
      "Epoch: 1960/2000... Training loss: 0.4044\n",
      "Epoch: 1960/2000... Training loss: 0.2995\n",
      "Epoch: 1960/2000... Training loss: 0.2745\n",
      "Epoch: 1960/2000... Training loss: 0.3416\n",
      "Epoch: 1960/2000... Training loss: 0.2213\n",
      "Epoch: 1960/2000... Training loss: 0.3637\n",
      "Epoch: 1960/2000... Training loss: 0.2753\n",
      "Epoch: 1960/2000... Training loss: 0.2589\n",
      "Epoch: 1960/2000... Training loss: 0.3908\n",
      "Epoch: 1960/2000... Training loss: 0.3239\n",
      "Epoch: 1960/2000... Training loss: 0.1938\n",
      "Epoch: 1960/2000... Training loss: 0.4068\n",
      "Epoch: 1960/2000... Training loss: 0.5054\n",
      "Epoch: 1960/2000... Training loss: 0.3821\n",
      "Epoch: 1960/2000... Training loss: 0.1944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1960/2000... Training loss: 0.4061\n",
      "Epoch: 1960/2000... Training loss: 0.3542\n",
      "Epoch: 1960/2000... Training loss: 0.3489\n",
      "Epoch: 1961/2000... Training loss: 0.4268\n",
      "Epoch: 1961/2000... Training loss: 0.3687\n",
      "Epoch: 1961/2000... Training loss: 0.5283\n",
      "Epoch: 1961/2000... Training loss: 0.4306\n",
      "Epoch: 1961/2000... Training loss: 0.3753\n",
      "Epoch: 1961/2000... Training loss: 0.4574\n",
      "Epoch: 1961/2000... Training loss: 0.4193\n",
      "Epoch: 1961/2000... Training loss: 0.2957\n",
      "Epoch: 1961/2000... Training loss: 0.3163\n",
      "Epoch: 1961/2000... Training loss: 0.2895\n",
      "Epoch: 1961/2000... Training loss: 0.3285\n",
      "Epoch: 1961/2000... Training loss: 0.2887\n",
      "Epoch: 1961/2000... Training loss: 0.5071\n",
      "Epoch: 1961/2000... Training loss: 0.4059\n",
      "Epoch: 1961/2000... Training loss: 0.4496\n",
      "Epoch: 1961/2000... Training loss: 0.3594\n",
      "Epoch: 1961/2000... Training loss: 0.4838\n",
      "Epoch: 1961/2000... Training loss: 0.3337\n",
      "Epoch: 1961/2000... Training loss: 0.4536\n",
      "Epoch: 1961/2000... Training loss: 0.1940\n",
      "Epoch: 1961/2000... Training loss: 0.4907\n",
      "Epoch: 1961/2000... Training loss: 0.3864\n",
      "Epoch: 1961/2000... Training loss: 0.5006\n",
      "Epoch: 1961/2000... Training loss: 0.3829\n",
      "Epoch: 1961/2000... Training loss: 0.4164\n",
      "Epoch: 1961/2000... Training loss: 0.3379\n",
      "Epoch: 1961/2000... Training loss: 0.4242\n",
      "Epoch: 1961/2000... Training loss: 0.2203\n",
      "Epoch: 1961/2000... Training loss: 0.2222\n",
      "Epoch: 1961/2000... Training loss: 0.3658\n",
      "Epoch: 1961/2000... Training loss: 0.3640\n",
      "Epoch: 1962/2000... Training loss: 0.4986\n",
      "Epoch: 1962/2000... Training loss: 0.3014\n",
      "Epoch: 1962/2000... Training loss: 0.2043\n",
      "Epoch: 1962/2000... Training loss: 0.4365\n",
      "Epoch: 1962/2000... Training loss: 0.2528\n",
      "Epoch: 1962/2000... Training loss: 0.3507\n",
      "Epoch: 1962/2000... Training loss: 0.3784\n",
      "Epoch: 1962/2000... Training loss: 0.5263\n",
      "Epoch: 1962/2000... Training loss: 0.2976\n",
      "Epoch: 1962/2000... Training loss: 0.4468\n",
      "Epoch: 1962/2000... Training loss: 0.3882\n",
      "Epoch: 1962/2000... Training loss: 0.5334\n",
      "Epoch: 1962/2000... Training loss: 0.3005\n",
      "Epoch: 1962/2000... Training loss: 0.3967\n",
      "Epoch: 1962/2000... Training loss: 0.3441\n",
      "Epoch: 1962/2000... Training loss: 0.4193\n",
      "Epoch: 1962/2000... Training loss: 0.3443\n",
      "Epoch: 1962/2000... Training loss: 0.3494\n",
      "Epoch: 1962/2000... Training loss: 0.3588\n",
      "Epoch: 1962/2000... Training loss: 0.3601\n",
      "Epoch: 1962/2000... Training loss: 0.3471\n",
      "Epoch: 1962/2000... Training loss: 0.3675\n",
      "Epoch: 1962/2000... Training loss: 0.5081\n",
      "Epoch: 1962/2000... Training loss: 0.3251\n",
      "Epoch: 1962/2000... Training loss: 0.3917\n",
      "Epoch: 1962/2000... Training loss: 0.4047\n",
      "Epoch: 1962/2000... Training loss: 0.2876\n",
      "Epoch: 1962/2000... Training loss: 0.2559\n",
      "Epoch: 1962/2000... Training loss: 0.3034\n",
      "Epoch: 1962/2000... Training loss: 0.2642\n",
      "Epoch: 1962/2000... Training loss: 0.3222\n",
      "Epoch: 1963/2000... Training loss: 0.3680\n",
      "Epoch: 1963/2000... Training loss: 0.3912\n",
      "Epoch: 1963/2000... Training loss: 0.2967\n",
      "Epoch: 1963/2000... Training loss: 0.3047\n",
      "Epoch: 1963/2000... Training loss: 0.2669\n",
      "Epoch: 1963/2000... Training loss: 0.3012\n",
      "Epoch: 1963/2000... Training loss: 0.4193\n",
      "Epoch: 1963/2000... Training loss: 0.3978\n",
      "Epoch: 1963/2000... Training loss: 0.2985\n",
      "Epoch: 1963/2000... Training loss: 0.4743\n",
      "Epoch: 1963/2000... Training loss: 0.4279\n",
      "Epoch: 1963/2000... Training loss: 0.4268\n",
      "Epoch: 1963/2000... Training loss: 0.3360\n",
      "Epoch: 1963/2000... Training loss: 0.3262\n",
      "Epoch: 1963/2000... Training loss: 0.3337\n",
      "Epoch: 1963/2000... Training loss: 0.2683\n",
      "Epoch: 1963/2000... Training loss: 0.3099\n",
      "Epoch: 1963/2000... Training loss: 0.3808\n",
      "Epoch: 1963/2000... Training loss: 0.3362\n",
      "Epoch: 1963/2000... Training loss: 0.3122\n",
      "Epoch: 1963/2000... Training loss: 0.3895\n",
      "Epoch: 1963/2000... Training loss: 0.3882\n",
      "Epoch: 1963/2000... Training loss: 0.2890\n",
      "Epoch: 1963/2000... Training loss: 0.2873\n",
      "Epoch: 1963/2000... Training loss: 0.3135\n",
      "Epoch: 1963/2000... Training loss: 0.3597\n",
      "Epoch: 1963/2000... Training loss: 0.2874\n",
      "Epoch: 1963/2000... Training loss: 0.3574\n",
      "Epoch: 1963/2000... Training loss: 0.5676\n",
      "Epoch: 1963/2000... Training loss: 0.3154\n",
      "Epoch: 1963/2000... Training loss: 0.2788\n",
      "Epoch: 1964/2000... Training loss: 0.2878\n",
      "Epoch: 1964/2000... Training loss: 0.3147\n",
      "Epoch: 1964/2000... Training loss: 0.3870\n",
      "Epoch: 1964/2000... Training loss: 0.3014\n",
      "Epoch: 1964/2000... Training loss: 0.3904\n",
      "Epoch: 1964/2000... Training loss: 0.3818\n",
      "Epoch: 1964/2000... Training loss: 0.2927\n",
      "Epoch: 1964/2000... Training loss: 0.3596\n",
      "Epoch: 1964/2000... Training loss: 0.2783\n",
      "Epoch: 1964/2000... Training loss: 0.3814\n",
      "Epoch: 1964/2000... Training loss: 0.3598\n",
      "Epoch: 1964/2000... Training loss: 0.3442\n",
      "Epoch: 1964/2000... Training loss: 0.3505\n",
      "Epoch: 1964/2000... Training loss: 0.2088\n",
      "Epoch: 1964/2000... Training loss: 0.3415\n",
      "Epoch: 1964/2000... Training loss: 0.4436\n",
      "Epoch: 1964/2000... Training loss: 0.3014\n",
      "Epoch: 1964/2000... Training loss: 0.2706\n",
      "Epoch: 1964/2000... Training loss: 0.3795\n",
      "Epoch: 1964/2000... Training loss: 0.2916\n",
      "Epoch: 1964/2000... Training loss: 0.3491\n",
      "Epoch: 1964/2000... Training loss: 0.3847\n",
      "Epoch: 1964/2000... Training loss: 0.2594\n",
      "Epoch: 1964/2000... Training loss: 0.4007\n",
      "Epoch: 1964/2000... Training loss: 0.3905\n",
      "Epoch: 1964/2000... Training loss: 0.4296\n",
      "Epoch: 1964/2000... Training loss: 0.3670\n",
      "Epoch: 1964/2000... Training loss: 0.3546\n",
      "Epoch: 1964/2000... Training loss: 0.2580\n",
      "Epoch: 1964/2000... Training loss: 0.2565\n",
      "Epoch: 1964/2000... Training loss: 0.4469\n",
      "Epoch: 1965/2000... Training loss: 0.3907\n",
      "Epoch: 1965/2000... Training loss: 0.4230\n",
      "Epoch: 1965/2000... Training loss: 0.3713\n",
      "Epoch: 1965/2000... Training loss: 0.4328\n",
      "Epoch: 1965/2000... Training loss: 0.4056\n",
      "Epoch: 1965/2000... Training loss: 0.4740\n",
      "Epoch: 1965/2000... Training loss: 0.4775\n",
      "Epoch: 1965/2000... Training loss: 0.4446\n",
      "Epoch: 1965/2000... Training loss: 0.2787\n",
      "Epoch: 1965/2000... Training loss: 0.4959\n",
      "Epoch: 1965/2000... Training loss: 0.3660\n",
      "Epoch: 1965/2000... Training loss: 0.3835\n",
      "Epoch: 1965/2000... Training loss: 0.2770\n",
      "Epoch: 1965/2000... Training loss: 0.3883\n",
      "Epoch: 1965/2000... Training loss: 0.4169\n",
      "Epoch: 1965/2000... Training loss: 0.3926\n",
      "Epoch: 1965/2000... Training loss: 0.2276\n",
      "Epoch: 1965/2000... Training loss: 0.4187\n",
      "Epoch: 1965/2000... Training loss: 0.2191\n",
      "Epoch: 1965/2000... Training loss: 0.2490\n",
      "Epoch: 1965/2000... Training loss: 0.3232\n",
      "Epoch: 1965/2000... Training loss: 0.4254\n",
      "Epoch: 1965/2000... Training loss: 0.4393\n",
      "Epoch: 1965/2000... Training loss: 0.6024\n",
      "Epoch: 1965/2000... Training loss: 0.3845\n",
      "Epoch: 1965/2000... Training loss: 0.3881\n",
      "Epoch: 1965/2000... Training loss: 0.3950\n",
      "Epoch: 1965/2000... Training loss: 0.4510\n",
      "Epoch: 1965/2000... Training loss: 0.3737\n",
      "Epoch: 1965/2000... Training loss: 0.3451\n",
      "Epoch: 1965/2000... Training loss: 0.3472\n",
      "Epoch: 1966/2000... Training loss: 0.4116\n",
      "Epoch: 1966/2000... Training loss: 0.3837\n",
      "Epoch: 1966/2000... Training loss: 0.3184\n",
      "Epoch: 1966/2000... Training loss: 0.5349\n",
      "Epoch: 1966/2000... Training loss: 0.4635\n",
      "Epoch: 1966/2000... Training loss: 0.3890\n",
      "Epoch: 1966/2000... Training loss: 0.4527\n",
      "Epoch: 1966/2000... Training loss: 0.3076\n",
      "Epoch: 1966/2000... Training loss: 0.3723\n",
      "Epoch: 1966/2000... Training loss: 0.4275\n",
      "Epoch: 1966/2000... Training loss: 0.4247\n",
      "Epoch: 1966/2000... Training loss: 0.2606\n",
      "Epoch: 1966/2000... Training loss: 0.3612\n",
      "Epoch: 1966/2000... Training loss: 0.3871\n",
      "Epoch: 1966/2000... Training loss: 0.4088\n",
      "Epoch: 1966/2000... Training loss: 0.2131\n",
      "Epoch: 1966/2000... Training loss: 0.3146\n",
      "Epoch: 1966/2000... Training loss: 0.2906\n",
      "Epoch: 1966/2000... Training loss: 0.3612\n",
      "Epoch: 1966/2000... Training loss: 0.3709\n",
      "Epoch: 1966/2000... Training loss: 0.2423\n",
      "Epoch: 1966/2000... Training loss: 0.3497\n",
      "Epoch: 1966/2000... Training loss: 0.3278\n",
      "Epoch: 1966/2000... Training loss: 0.5861\n",
      "Epoch: 1966/2000... Training loss: 0.2925\n",
      "Epoch: 1966/2000... Training loss: 0.3890\n",
      "Epoch: 1966/2000... Training loss: 0.3331\n",
      "Epoch: 1966/2000... Training loss: 0.3572\n",
      "Epoch: 1966/2000... Training loss: 0.3092\n",
      "Epoch: 1966/2000... Training loss: 0.3356\n",
      "Epoch: 1966/2000... Training loss: 0.4922\n",
      "Epoch: 1967/2000... Training loss: 0.2811\n",
      "Epoch: 1967/2000... Training loss: 0.3663\n",
      "Epoch: 1967/2000... Training loss: 0.3697\n",
      "Epoch: 1967/2000... Training loss: 0.3397\n",
      "Epoch: 1967/2000... Training loss: 0.4560\n",
      "Epoch: 1967/2000... Training loss: 0.4573\n",
      "Epoch: 1967/2000... Training loss: 0.4151\n",
      "Epoch: 1967/2000... Training loss: 0.3201\n",
      "Epoch: 1967/2000... Training loss: 0.2718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1967/2000... Training loss: 0.1939\n",
      "Epoch: 1967/2000... Training loss: 0.3977\n",
      "Epoch: 1967/2000... Training loss: 0.3586\n",
      "Epoch: 1967/2000... Training loss: 0.3606\n",
      "Epoch: 1967/2000... Training loss: 0.4617\n",
      "Epoch: 1967/2000... Training loss: 0.3148\n",
      "Epoch: 1967/2000... Training loss: 0.3638\n",
      "Epoch: 1967/2000... Training loss: 0.5077\n",
      "Epoch: 1967/2000... Training loss: 0.3870\n",
      "Epoch: 1967/2000... Training loss: 0.2925\n",
      "Epoch: 1967/2000... Training loss: 0.4157\n",
      "Epoch: 1967/2000... Training loss: 0.2463\n",
      "Epoch: 1967/2000... Training loss: 0.2865\n",
      "Epoch: 1967/2000... Training loss: 0.3743\n",
      "Epoch: 1967/2000... Training loss: 0.2212\n",
      "Epoch: 1967/2000... Training loss: 0.5133\n",
      "Epoch: 1967/2000... Training loss: 0.2914\n",
      "Epoch: 1967/2000... Training loss: 0.3116\n",
      "Epoch: 1967/2000... Training loss: 0.2432\n",
      "Epoch: 1967/2000... Training loss: 0.3617\n",
      "Epoch: 1967/2000... Training loss: 0.2955\n",
      "Epoch: 1967/2000... Training loss: 0.3558\n",
      "Epoch: 1968/2000... Training loss: 0.3094\n",
      "Epoch: 1968/2000... Training loss: 0.4143\n",
      "Epoch: 1968/2000... Training loss: 0.4636\n",
      "Epoch: 1968/2000... Training loss: 0.3340\n",
      "Epoch: 1968/2000... Training loss: 0.3502\n",
      "Epoch: 1968/2000... Training loss: 0.4692\n",
      "Epoch: 1968/2000... Training loss: 0.4371\n",
      "Epoch: 1968/2000... Training loss: 0.3708\n",
      "Epoch: 1968/2000... Training loss: 0.2927\n",
      "Epoch: 1968/2000... Training loss: 0.3038\n",
      "Epoch: 1968/2000... Training loss: 0.2906\n",
      "Epoch: 1968/2000... Training loss: 0.3335\n",
      "Epoch: 1968/2000... Training loss: 0.2220\n",
      "Epoch: 1968/2000... Training loss: 0.5495\n",
      "Epoch: 1968/2000... Training loss: 0.4065\n",
      "Epoch: 1968/2000... Training loss: 0.4089\n",
      "Epoch: 1968/2000... Training loss: 0.3693\n",
      "Epoch: 1968/2000... Training loss: 0.3408\n",
      "Epoch: 1968/2000... Training loss: 0.2890\n",
      "Epoch: 1968/2000... Training loss: 0.4410\n",
      "Epoch: 1968/2000... Training loss: 0.3991\n",
      "Epoch: 1968/2000... Training loss: 0.6285\n",
      "Epoch: 1968/2000... Training loss: 0.3431\n",
      "Epoch: 1968/2000... Training loss: 0.4547\n",
      "Epoch: 1968/2000... Training loss: 0.2565\n",
      "Epoch: 1968/2000... Training loss: 0.4363\n",
      "Epoch: 1968/2000... Training loss: 0.3755\n",
      "Epoch: 1968/2000... Training loss: 0.4601\n",
      "Epoch: 1968/2000... Training loss: 0.3723\n",
      "Epoch: 1968/2000... Training loss: 0.4211\n",
      "Epoch: 1968/2000... Training loss: 0.5194\n",
      "Epoch: 1969/2000... Training loss: 0.4541\n",
      "Epoch: 1969/2000... Training loss: 0.4261\n",
      "Epoch: 1969/2000... Training loss: 0.2893\n",
      "Epoch: 1969/2000... Training loss: 0.2524\n",
      "Epoch: 1969/2000... Training loss: 0.3366\n",
      "Epoch: 1969/2000... Training loss: 0.2338\n",
      "Epoch: 1969/2000... Training loss: 0.6090\n",
      "Epoch: 1969/2000... Training loss: 0.4260\n",
      "Epoch: 1969/2000... Training loss: 0.4225\n",
      "Epoch: 1969/2000... Training loss: 0.2925\n",
      "Epoch: 1969/2000... Training loss: 0.3916\n",
      "Epoch: 1969/2000... Training loss: 0.3970\n",
      "Epoch: 1969/2000... Training loss: 0.4836\n",
      "Epoch: 1969/2000... Training loss: 0.4310\n",
      "Epoch: 1969/2000... Training loss: 0.3035\n",
      "Epoch: 1969/2000... Training loss: 0.4317\n",
      "Epoch: 1969/2000... Training loss: 0.3707\n",
      "Epoch: 1969/2000... Training loss: 0.4298\n",
      "Epoch: 1969/2000... Training loss: 0.4003\n",
      "Epoch: 1969/2000... Training loss: 0.2662\n",
      "Epoch: 1969/2000... Training loss: 0.4044\n",
      "Epoch: 1969/2000... Training loss: 0.2171\n",
      "Epoch: 1969/2000... Training loss: 0.4301\n",
      "Epoch: 1969/2000... Training loss: 0.2618\n",
      "Epoch: 1969/2000... Training loss: 0.3313\n",
      "Epoch: 1969/2000... Training loss: 0.4440\n",
      "Epoch: 1969/2000... Training loss: 0.4764\n",
      "Epoch: 1969/2000... Training loss: 0.3168\n",
      "Epoch: 1969/2000... Training loss: 0.3469\n",
      "Epoch: 1969/2000... Training loss: 0.3795\n",
      "Epoch: 1969/2000... Training loss: 0.4552\n",
      "Epoch: 1970/2000... Training loss: 0.4286\n",
      "Epoch: 1970/2000... Training loss: 0.4195\n",
      "Epoch: 1970/2000... Training loss: 0.4500\n",
      "Epoch: 1970/2000... Training loss: 0.2581\n",
      "Epoch: 1970/2000... Training loss: 0.3078\n",
      "Epoch: 1970/2000... Training loss: 0.3986\n",
      "Epoch: 1970/2000... Training loss: 0.4860\n",
      "Epoch: 1970/2000... Training loss: 0.5235\n",
      "Epoch: 1970/2000... Training loss: 0.4108\n",
      "Epoch: 1970/2000... Training loss: 0.4925\n",
      "Epoch: 1970/2000... Training loss: 0.3087\n",
      "Epoch: 1970/2000... Training loss: 0.4028\n",
      "Epoch: 1970/2000... Training loss: 0.4187\n",
      "Epoch: 1970/2000... Training loss: 0.2902\n",
      "Epoch: 1970/2000... Training loss: 0.3380\n",
      "Epoch: 1970/2000... Training loss: 0.2741\n",
      "Epoch: 1970/2000... Training loss: 0.2504\n",
      "Epoch: 1970/2000... Training loss: 0.2798\n",
      "Epoch: 1970/2000... Training loss: 0.2384\n",
      "Epoch: 1970/2000... Training loss: 0.3356\n",
      "Epoch: 1970/2000... Training loss: 0.4231\n",
      "Epoch: 1970/2000... Training loss: 0.4407\n",
      "Epoch: 1970/2000... Training loss: 0.4741\n",
      "Epoch: 1970/2000... Training loss: 0.2254\n",
      "Epoch: 1970/2000... Training loss: 0.4562\n",
      "Epoch: 1970/2000... Training loss: 0.3969\n",
      "Epoch: 1970/2000... Training loss: 0.3931\n",
      "Epoch: 1970/2000... Training loss: 0.3949\n",
      "Epoch: 1970/2000... Training loss: 0.2476\n",
      "Epoch: 1970/2000... Training loss: 0.3313\n",
      "Epoch: 1970/2000... Training loss: 0.2315\n",
      "Epoch: 1971/2000... Training loss: 0.4129\n",
      "Epoch: 1971/2000... Training loss: 0.3239\n",
      "Epoch: 1971/2000... Training loss: 0.4889\n",
      "Epoch: 1971/2000... Training loss: 0.4425\n",
      "Epoch: 1971/2000... Training loss: 0.3502\n",
      "Epoch: 1971/2000... Training loss: 0.3285\n",
      "Epoch: 1971/2000... Training loss: 0.3421\n",
      "Epoch: 1971/2000... Training loss: 0.4547\n",
      "Epoch: 1971/2000... Training loss: 0.3823\n",
      "Epoch: 1971/2000... Training loss: 0.3464\n",
      "Epoch: 1971/2000... Training loss: 0.2236\n",
      "Epoch: 1971/2000... Training loss: 0.3728\n",
      "Epoch: 1971/2000... Training loss: 0.3088\n",
      "Epoch: 1971/2000... Training loss: 0.2009\n",
      "Epoch: 1971/2000... Training loss: 0.4023\n",
      "Epoch: 1971/2000... Training loss: 0.4968\n",
      "Epoch: 1971/2000... Training loss: 0.4416\n",
      "Epoch: 1971/2000... Training loss: 0.3546\n",
      "Epoch: 1971/2000... Training loss: 0.2169\n",
      "Epoch: 1971/2000... Training loss: 0.2950\n",
      "Epoch: 1971/2000... Training loss: 0.4814\n",
      "Epoch: 1971/2000... Training loss: 0.4212\n",
      "Epoch: 1971/2000... Training loss: 0.3277\n",
      "Epoch: 1971/2000... Training loss: 0.3942\n",
      "Epoch: 1971/2000... Training loss: 0.5099\n",
      "Epoch: 1971/2000... Training loss: 0.5096\n",
      "Epoch: 1971/2000... Training loss: 0.3536\n",
      "Epoch: 1971/2000... Training loss: 0.3658\n",
      "Epoch: 1971/2000... Training loss: 0.3387\n",
      "Epoch: 1971/2000... Training loss: 0.4045\n",
      "Epoch: 1971/2000... Training loss: 0.3334\n",
      "Epoch: 1972/2000... Training loss: 0.2684\n",
      "Epoch: 1972/2000... Training loss: 0.4314\n",
      "Epoch: 1972/2000... Training loss: 0.2578\n",
      "Epoch: 1972/2000... Training loss: 0.3450\n",
      "Epoch: 1972/2000... Training loss: 0.2956\n",
      "Epoch: 1972/2000... Training loss: 0.3786\n",
      "Epoch: 1972/2000... Training loss: 0.2009\n",
      "Epoch: 1972/2000... Training loss: 0.2215\n",
      "Epoch: 1972/2000... Training loss: 0.2888\n",
      "Epoch: 1972/2000... Training loss: 0.2981\n",
      "Epoch: 1972/2000... Training loss: 0.4303\n",
      "Epoch: 1972/2000... Training loss: 0.1988\n",
      "Epoch: 1972/2000... Training loss: 0.3142\n",
      "Epoch: 1972/2000... Training loss: 0.3771\n",
      "Epoch: 1972/2000... Training loss: 0.3323\n",
      "Epoch: 1972/2000... Training loss: 0.3000\n",
      "Epoch: 1972/2000... Training loss: 0.3344\n",
      "Epoch: 1972/2000... Training loss: 0.3828\n",
      "Epoch: 1972/2000... Training loss: 0.4440\n",
      "Epoch: 1972/2000... Training loss: 0.3412\n",
      "Epoch: 1972/2000... Training loss: 0.4180\n",
      "Epoch: 1972/2000... Training loss: 0.4099\n",
      "Epoch: 1972/2000... Training loss: 0.4550\n",
      "Epoch: 1972/2000... Training loss: 0.3149\n",
      "Epoch: 1972/2000... Training loss: 0.4139\n",
      "Epoch: 1972/2000... Training loss: 0.2954\n",
      "Epoch: 1972/2000... Training loss: 0.3549\n",
      "Epoch: 1972/2000... Training loss: 0.5381\n",
      "Epoch: 1972/2000... Training loss: 0.2652\n",
      "Epoch: 1972/2000... Training loss: 0.3328\n",
      "Epoch: 1972/2000... Training loss: 0.2999\n",
      "Epoch: 1973/2000... Training loss: 0.2887\n",
      "Epoch: 1973/2000... Training loss: 0.3416\n",
      "Epoch: 1973/2000... Training loss: 0.5241\n",
      "Epoch: 1973/2000... Training loss: 0.2264\n",
      "Epoch: 1973/2000... Training loss: 0.3680\n",
      "Epoch: 1973/2000... Training loss: 0.4224\n",
      "Epoch: 1973/2000... Training loss: 0.4361\n",
      "Epoch: 1973/2000... Training loss: 0.2841\n",
      "Epoch: 1973/2000... Training loss: 0.3036\n",
      "Epoch: 1973/2000... Training loss: 0.3666\n",
      "Epoch: 1973/2000... Training loss: 0.4110\n",
      "Epoch: 1973/2000... Training loss: 0.4369\n",
      "Epoch: 1973/2000... Training loss: 0.5176\n",
      "Epoch: 1973/2000... Training loss: 0.3214\n",
      "Epoch: 1973/2000... Training loss: 0.2721\n",
      "Epoch: 1973/2000... Training loss: 0.4214\n",
      "Epoch: 1973/2000... Training loss: 0.2777\n",
      "Epoch: 1973/2000... Training loss: 0.3703\n",
      "Epoch: 1973/2000... Training loss: 0.5559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1973/2000... Training loss: 0.3379\n",
      "Epoch: 1973/2000... Training loss: 0.3866\n",
      "Epoch: 1973/2000... Training loss: 0.2506\n",
      "Epoch: 1973/2000... Training loss: 0.4475\n",
      "Epoch: 1973/2000... Training loss: 0.4011\n",
      "Epoch: 1973/2000... Training loss: 0.3961\n",
      "Epoch: 1973/2000... Training loss: 0.4198\n",
      "Epoch: 1973/2000... Training loss: 0.5194\n",
      "Epoch: 1973/2000... Training loss: 0.4261\n",
      "Epoch: 1973/2000... Training loss: 0.3288\n",
      "Epoch: 1973/2000... Training loss: 0.3954\n",
      "Epoch: 1973/2000... Training loss: 0.4012\n",
      "Epoch: 1974/2000... Training loss: 0.3663\n",
      "Epoch: 1974/2000... Training loss: 0.3980\n",
      "Epoch: 1974/2000... Training loss: 0.3409\n",
      "Epoch: 1974/2000... Training loss: 0.4650\n",
      "Epoch: 1974/2000... Training loss: 0.3662\n",
      "Epoch: 1974/2000... Training loss: 0.4222\n",
      "Epoch: 1974/2000... Training loss: 0.2341\n",
      "Epoch: 1974/2000... Training loss: 0.2485\n",
      "Epoch: 1974/2000... Training loss: 0.3942\n",
      "Epoch: 1974/2000... Training loss: 0.3551\n",
      "Epoch: 1974/2000... Training loss: 0.4236\n",
      "Epoch: 1974/2000... Training loss: 0.2684\n",
      "Epoch: 1974/2000... Training loss: 0.3891\n",
      "Epoch: 1974/2000... Training loss: 0.3514\n",
      "Epoch: 1974/2000... Training loss: 0.3540\n",
      "Epoch: 1974/2000... Training loss: 0.2825\n",
      "Epoch: 1974/2000... Training loss: 0.2679\n",
      "Epoch: 1974/2000... Training loss: 0.4868\n",
      "Epoch: 1974/2000... Training loss: 0.3406\n",
      "Epoch: 1974/2000... Training loss: 0.4163\n",
      "Epoch: 1974/2000... Training loss: 0.3162\n",
      "Epoch: 1974/2000... Training loss: 0.4284\n",
      "Epoch: 1974/2000... Training loss: 0.3768\n",
      "Epoch: 1974/2000... Training loss: 0.3469\n",
      "Epoch: 1974/2000... Training loss: 0.5188\n",
      "Epoch: 1974/2000... Training loss: 0.3630\n",
      "Epoch: 1974/2000... Training loss: 0.3337\n",
      "Epoch: 1974/2000... Training loss: 0.4596\n",
      "Epoch: 1974/2000... Training loss: 0.3060\n",
      "Epoch: 1974/2000... Training loss: 0.4470\n",
      "Epoch: 1974/2000... Training loss: 0.3426\n",
      "Epoch: 1975/2000... Training loss: 0.2034\n",
      "Epoch: 1975/2000... Training loss: 0.3310\n",
      "Epoch: 1975/2000... Training loss: 0.3367\n",
      "Epoch: 1975/2000... Training loss: 0.4197\n",
      "Epoch: 1975/2000... Training loss: 0.3434\n",
      "Epoch: 1975/2000... Training loss: 0.4193\n",
      "Epoch: 1975/2000... Training loss: 0.3900\n",
      "Epoch: 1975/2000... Training loss: 0.2936\n",
      "Epoch: 1975/2000... Training loss: 0.3304\n",
      "Epoch: 1975/2000... Training loss: 0.4102\n",
      "Epoch: 1975/2000... Training loss: 0.4785\n",
      "Epoch: 1975/2000... Training loss: 0.2465\n",
      "Epoch: 1975/2000... Training loss: 0.5433\n",
      "Epoch: 1975/2000... Training loss: 0.3374\n",
      "Epoch: 1975/2000... Training loss: 0.3396\n",
      "Epoch: 1975/2000... Training loss: 0.2812\n",
      "Epoch: 1975/2000... Training loss: 0.2971\n",
      "Epoch: 1975/2000... Training loss: 0.3331\n",
      "Epoch: 1975/2000... Training loss: 0.3848\n",
      "Epoch: 1975/2000... Training loss: 0.4646\n",
      "Epoch: 1975/2000... Training loss: 0.2811\n",
      "Epoch: 1975/2000... Training loss: 0.3025\n",
      "Epoch: 1975/2000... Training loss: 0.3479\n",
      "Epoch: 1975/2000... Training loss: 0.2957\n",
      "Epoch: 1975/2000... Training loss: 0.2083\n",
      "Epoch: 1975/2000... Training loss: 0.2963\n",
      "Epoch: 1975/2000... Training loss: 0.3922\n",
      "Epoch: 1975/2000... Training loss: 0.3468\n",
      "Epoch: 1975/2000... Training loss: 0.4148\n",
      "Epoch: 1975/2000... Training loss: 0.4345\n",
      "Epoch: 1975/2000... Training loss: 0.3611\n",
      "Epoch: 1976/2000... Training loss: 0.2444\n",
      "Epoch: 1976/2000... Training loss: 0.3246\n",
      "Epoch: 1976/2000... Training loss: 0.3122\n",
      "Epoch: 1976/2000... Training loss: 0.3361\n",
      "Epoch: 1976/2000... Training loss: 0.3903\n",
      "Epoch: 1976/2000... Training loss: 0.3836\n",
      "Epoch: 1976/2000... Training loss: 0.5670\n",
      "Epoch: 1976/2000... Training loss: 0.4259\n",
      "Epoch: 1976/2000... Training loss: 0.4113\n",
      "Epoch: 1976/2000... Training loss: 0.3815\n",
      "Epoch: 1976/2000... Training loss: 0.2479\n",
      "Epoch: 1976/2000... Training loss: 0.3542\n",
      "Epoch: 1976/2000... Training loss: 0.3007\n",
      "Epoch: 1976/2000... Training loss: 0.4821\n",
      "Epoch: 1976/2000... Training loss: 0.3662\n",
      "Epoch: 1976/2000... Training loss: 0.2875\n",
      "Epoch: 1976/2000... Training loss: 0.4416\n",
      "Epoch: 1976/2000... Training loss: 0.3428\n",
      "Epoch: 1976/2000... Training loss: 0.3422\n",
      "Epoch: 1976/2000... Training loss: 0.2980\n",
      "Epoch: 1976/2000... Training loss: 0.4106\n",
      "Epoch: 1976/2000... Training loss: 0.2954\n",
      "Epoch: 1976/2000... Training loss: 0.3755\n",
      "Epoch: 1976/2000... Training loss: 0.6222\n",
      "Epoch: 1976/2000... Training loss: 0.3837\n",
      "Epoch: 1976/2000... Training loss: 0.3688\n",
      "Epoch: 1976/2000... Training loss: 0.4468\n",
      "Epoch: 1976/2000... Training loss: 0.3622\n",
      "Epoch: 1976/2000... Training loss: 0.3321\n",
      "Epoch: 1976/2000... Training loss: 0.3226\n",
      "Epoch: 1976/2000... Training loss: 0.5339\n",
      "Epoch: 1977/2000... Training loss: 0.3760\n",
      "Epoch: 1977/2000... Training loss: 0.3571\n",
      "Epoch: 1977/2000... Training loss: 0.2704\n",
      "Epoch: 1977/2000... Training loss: 0.2757\n",
      "Epoch: 1977/2000... Training loss: 0.4940\n",
      "Epoch: 1977/2000... Training loss: 0.4034\n",
      "Epoch: 1977/2000... Training loss: 0.3440\n",
      "Epoch: 1977/2000... Training loss: 0.2370\n",
      "Epoch: 1977/2000... Training loss: 0.2531\n",
      "Epoch: 1977/2000... Training loss: 0.3878\n",
      "Epoch: 1977/2000... Training loss: 0.4457\n",
      "Epoch: 1977/2000... Training loss: 0.3686\n",
      "Epoch: 1977/2000... Training loss: 0.5022\n",
      "Epoch: 1977/2000... Training loss: 0.3400\n",
      "Epoch: 1977/2000... Training loss: 0.3177\n",
      "Epoch: 1977/2000... Training loss: 0.2090\n",
      "Epoch: 1977/2000... Training loss: 0.3351\n",
      "Epoch: 1977/2000... Training loss: 0.3193\n",
      "Epoch: 1977/2000... Training loss: 0.4309\n",
      "Epoch: 1977/2000... Training loss: 0.4294\n",
      "Epoch: 1977/2000... Training loss: 0.2854\n",
      "Epoch: 1977/2000... Training loss: 0.3330\n",
      "Epoch: 1977/2000... Training loss: 0.1986\n",
      "Epoch: 1977/2000... Training loss: 0.4211\n",
      "Epoch: 1977/2000... Training loss: 0.4409\n",
      "Epoch: 1977/2000... Training loss: 0.2683\n",
      "Epoch: 1977/2000... Training loss: 0.3461\n",
      "Epoch: 1977/2000... Training loss: 0.3647\n",
      "Epoch: 1977/2000... Training loss: 0.3473\n",
      "Epoch: 1977/2000... Training loss: 0.4016\n",
      "Epoch: 1977/2000... Training loss: 0.3325\n",
      "Epoch: 1978/2000... Training loss: 0.5800\n",
      "Epoch: 1978/2000... Training loss: 0.7217\n",
      "Epoch: 1978/2000... Training loss: 0.2336\n",
      "Epoch: 1978/2000... Training loss: 0.2974\n",
      "Epoch: 1978/2000... Training loss: 0.4083\n",
      "Epoch: 1978/2000... Training loss: 0.3443\n",
      "Epoch: 1978/2000... Training loss: 0.3864\n",
      "Epoch: 1978/2000... Training loss: 0.4097\n",
      "Epoch: 1978/2000... Training loss: 0.2607\n",
      "Epoch: 1978/2000... Training loss: 0.4117\n",
      "Epoch: 1978/2000... Training loss: 0.3361\n",
      "Epoch: 1978/2000... Training loss: 0.4635\n",
      "Epoch: 1978/2000... Training loss: 0.3160\n",
      "Epoch: 1978/2000... Training loss: 0.4120\n",
      "Epoch: 1978/2000... Training loss: 0.4152\n",
      "Epoch: 1978/2000... Training loss: 0.2784\n",
      "Epoch: 1978/2000... Training loss: 0.2171\n",
      "Epoch: 1978/2000... Training loss: 0.2691\n",
      "Epoch: 1978/2000... Training loss: 0.3129\n",
      "Epoch: 1978/2000... Training loss: 0.3241\n",
      "Epoch: 1978/2000... Training loss: 0.3302\n",
      "Epoch: 1978/2000... Training loss: 0.3762\n",
      "Epoch: 1978/2000... Training loss: 0.3757\n",
      "Epoch: 1978/2000... Training loss: 0.5286\n",
      "Epoch: 1978/2000... Training loss: 0.2622\n",
      "Epoch: 1978/2000... Training loss: 0.4337\n",
      "Epoch: 1978/2000... Training loss: 0.4059\n",
      "Epoch: 1978/2000... Training loss: 0.4404\n",
      "Epoch: 1978/2000... Training loss: 0.3553\n",
      "Epoch: 1978/2000... Training loss: 0.4929\n",
      "Epoch: 1978/2000... Training loss: 0.2940\n",
      "Epoch: 1979/2000... Training loss: 0.4153\n",
      "Epoch: 1979/2000... Training loss: 0.3829\n",
      "Epoch: 1979/2000... Training loss: 0.4091\n",
      "Epoch: 1979/2000... Training loss: 0.3139\n",
      "Epoch: 1979/2000... Training loss: 0.3953\n",
      "Epoch: 1979/2000... Training loss: 0.4453\n",
      "Epoch: 1979/2000... Training loss: 0.3841\n",
      "Epoch: 1979/2000... Training loss: 0.4360\n",
      "Epoch: 1979/2000... Training loss: 0.2371\n",
      "Epoch: 1979/2000... Training loss: 0.2152\n",
      "Epoch: 1979/2000... Training loss: 0.3716\n",
      "Epoch: 1979/2000... Training loss: 0.3605\n",
      "Epoch: 1979/2000... Training loss: 0.3091\n",
      "Epoch: 1979/2000... Training loss: 0.3837\n",
      "Epoch: 1979/2000... Training loss: 0.3310\n",
      "Epoch: 1979/2000... Training loss: 0.4363\n",
      "Epoch: 1979/2000... Training loss: 0.3045\n",
      "Epoch: 1979/2000... Training loss: 0.3779\n",
      "Epoch: 1979/2000... Training loss: 0.4531\n",
      "Epoch: 1979/2000... Training loss: 0.4273\n",
      "Epoch: 1979/2000... Training loss: 0.4130\n",
      "Epoch: 1979/2000... Training loss: 0.4764\n",
      "Epoch: 1979/2000... Training loss: 0.3677\n",
      "Epoch: 1979/2000... Training loss: 0.2538\n",
      "Epoch: 1979/2000... Training loss: 0.3986\n",
      "Epoch: 1979/2000... Training loss: 0.4853\n",
      "Epoch: 1979/2000... Training loss: 0.4061\n",
      "Epoch: 1979/2000... Training loss: 0.3907\n",
      "Epoch: 1979/2000... Training loss: 0.3323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1979/2000... Training loss: 0.3007\n",
      "Epoch: 1979/2000... Training loss: 0.4186\n",
      "Epoch: 1980/2000... Training loss: 0.3316\n",
      "Epoch: 1980/2000... Training loss: 0.3134\n",
      "Epoch: 1980/2000... Training loss: 0.4620\n",
      "Epoch: 1980/2000... Training loss: 0.4444\n",
      "Epoch: 1980/2000... Training loss: 0.2902\n",
      "Epoch: 1980/2000... Training loss: 0.4988\n",
      "Epoch: 1980/2000... Training loss: 0.2835\n",
      "Epoch: 1980/2000... Training loss: 0.3454\n",
      "Epoch: 1980/2000... Training loss: 0.3481\n",
      "Epoch: 1980/2000... Training loss: 0.3212\n",
      "Epoch: 1980/2000... Training loss: 0.5694\n",
      "Epoch: 1980/2000... Training loss: 0.3916\n",
      "Epoch: 1980/2000... Training loss: 0.5440\n",
      "Epoch: 1980/2000... Training loss: 0.2560\n",
      "Epoch: 1980/2000... Training loss: 0.4809\n",
      "Epoch: 1980/2000... Training loss: 0.5998\n",
      "Epoch: 1980/2000... Training loss: 0.1983\n",
      "Epoch: 1980/2000... Training loss: 0.5246\n",
      "Epoch: 1980/2000... Training loss: 0.3395\n",
      "Epoch: 1980/2000... Training loss: 0.2680\n",
      "Epoch: 1980/2000... Training loss: 0.2792\n",
      "Epoch: 1980/2000... Training loss: 0.3673\n",
      "Epoch: 1980/2000... Training loss: 0.4520\n",
      "Epoch: 1980/2000... Training loss: 0.4209\n",
      "Epoch: 1980/2000... Training loss: 0.4429\n",
      "Epoch: 1980/2000... Training loss: 0.3376\n",
      "Epoch: 1980/2000... Training loss: 0.4265\n",
      "Epoch: 1980/2000... Training loss: 0.4355\n",
      "Epoch: 1980/2000... Training loss: 0.3439\n",
      "Epoch: 1980/2000... Training loss: 0.3654\n",
      "Epoch: 1980/2000... Training loss: 0.4554\n",
      "Epoch: 1981/2000... Training loss: 0.2458\n",
      "Epoch: 1981/2000... Training loss: 0.4354\n",
      "Epoch: 1981/2000... Training loss: 0.3339\n",
      "Epoch: 1981/2000... Training loss: 0.4183\n",
      "Epoch: 1981/2000... Training loss: 0.2880\n",
      "Epoch: 1981/2000... Training loss: 0.7438\n",
      "Epoch: 1981/2000... Training loss: 0.2458\n",
      "Epoch: 1981/2000... Training loss: 0.5720\n",
      "Epoch: 1981/2000... Training loss: 0.5144\n",
      "Epoch: 1981/2000... Training loss: 0.3607\n",
      "Epoch: 1981/2000... Training loss: 0.4304\n",
      "Epoch: 1981/2000... Training loss: 0.3785\n",
      "Epoch: 1981/2000... Training loss: 0.4825\n",
      "Epoch: 1981/2000... Training loss: 0.3842\n",
      "Epoch: 1981/2000... Training loss: 0.4231\n",
      "Epoch: 1981/2000... Training loss: 0.4505\n",
      "Epoch: 1981/2000... Training loss: 0.3760\n",
      "Epoch: 1981/2000... Training loss: 0.4059\n",
      "Epoch: 1981/2000... Training loss: 0.3172\n",
      "Epoch: 1981/2000... Training loss: 0.4280\n",
      "Epoch: 1981/2000... Training loss: 0.5475\n",
      "Epoch: 1981/2000... Training loss: 0.3571\n",
      "Epoch: 1981/2000... Training loss: 0.3427\n",
      "Epoch: 1981/2000... Training loss: 0.4598\n",
      "Epoch: 1981/2000... Training loss: 0.3665\n",
      "Epoch: 1981/2000... Training loss: 0.2146\n",
      "Epoch: 1981/2000... Training loss: 0.3217\n",
      "Epoch: 1981/2000... Training loss: 0.5169\n",
      "Epoch: 1981/2000... Training loss: 0.3380\n",
      "Epoch: 1981/2000... Training loss: 0.3716\n",
      "Epoch: 1981/2000... Training loss: 0.4045\n",
      "Epoch: 1982/2000... Training loss: 0.3061\n",
      "Epoch: 1982/2000... Training loss: 0.2707\n",
      "Epoch: 1982/2000... Training loss: 0.3371\n",
      "Epoch: 1982/2000... Training loss: 0.4064\n",
      "Epoch: 1982/2000... Training loss: 0.3608\n",
      "Epoch: 1982/2000... Training loss: 0.3350\n",
      "Epoch: 1982/2000... Training loss: 0.3857\n",
      "Epoch: 1982/2000... Training loss: 0.3510\n",
      "Epoch: 1982/2000... Training loss: 0.3732\n",
      "Epoch: 1982/2000... Training loss: 0.3000\n",
      "Epoch: 1982/2000... Training loss: 0.4165\n",
      "Epoch: 1982/2000... Training loss: 0.3121\n",
      "Epoch: 1982/2000... Training loss: 0.3646\n",
      "Epoch: 1982/2000... Training loss: 0.1967\n",
      "Epoch: 1982/2000... Training loss: 0.3519\n",
      "Epoch: 1982/2000... Training loss: 0.3822\n",
      "Epoch: 1982/2000... Training loss: 0.4170\n",
      "Epoch: 1982/2000... Training loss: 0.2999\n",
      "Epoch: 1982/2000... Training loss: 0.3295\n",
      "Epoch: 1982/2000... Training loss: 0.3733\n",
      "Epoch: 1982/2000... Training loss: 0.3243\n",
      "Epoch: 1982/2000... Training loss: 0.4688\n",
      "Epoch: 1982/2000... Training loss: 0.5309\n",
      "Epoch: 1982/2000... Training loss: 0.3188\n",
      "Epoch: 1982/2000... Training loss: 0.4976\n",
      "Epoch: 1982/2000... Training loss: 0.4122\n",
      "Epoch: 1982/2000... Training loss: 0.3309\n",
      "Epoch: 1982/2000... Training loss: 0.2470\n",
      "Epoch: 1982/2000... Training loss: 0.4062\n",
      "Epoch: 1982/2000... Training loss: 0.5689\n",
      "Epoch: 1982/2000... Training loss: 0.4859\n",
      "Epoch: 1983/2000... Training loss: 0.2444\n",
      "Epoch: 1983/2000... Training loss: 0.3486\n",
      "Epoch: 1983/2000... Training loss: 0.3322\n",
      "Epoch: 1983/2000... Training loss: 0.2805\n",
      "Epoch: 1983/2000... Training loss: 0.2968\n",
      "Epoch: 1983/2000... Training loss: 0.2969\n",
      "Epoch: 1983/2000... Training loss: 0.2838\n",
      "Epoch: 1983/2000... Training loss: 0.4022\n",
      "Epoch: 1983/2000... Training loss: 0.3097\n",
      "Epoch: 1983/2000... Training loss: 0.4005\n",
      "Epoch: 1983/2000... Training loss: 0.2768\n",
      "Epoch: 1983/2000... Training loss: 0.4420\n",
      "Epoch: 1983/2000... Training loss: 0.3226\n",
      "Epoch: 1983/2000... Training loss: 0.3550\n",
      "Epoch: 1983/2000... Training loss: 0.3348\n",
      "Epoch: 1983/2000... Training loss: 0.2449\n",
      "Epoch: 1983/2000... Training loss: 0.3309\n",
      "Epoch: 1983/2000... Training loss: 0.1938\n",
      "Epoch: 1983/2000... Training loss: 0.5015\n",
      "Epoch: 1983/2000... Training loss: 0.4514\n",
      "Epoch: 1983/2000... Training loss: 0.1788\n",
      "Epoch: 1983/2000... Training loss: 0.2121\n",
      "Epoch: 1983/2000... Training loss: 0.4180\n",
      "Epoch: 1983/2000... Training loss: 0.4269\n",
      "Epoch: 1983/2000... Training loss: 0.3480\n",
      "Epoch: 1983/2000... Training loss: 0.2985\n",
      "Epoch: 1983/2000... Training loss: 0.4307\n",
      "Epoch: 1983/2000... Training loss: 0.3706\n",
      "Epoch: 1983/2000... Training loss: 0.4238\n",
      "Epoch: 1983/2000... Training loss: 0.3329\n",
      "Epoch: 1983/2000... Training loss: 0.2977\n",
      "Epoch: 1984/2000... Training loss: 0.4446\n",
      "Epoch: 1984/2000... Training loss: 0.4969\n",
      "Epoch: 1984/2000... Training loss: 0.6886\n",
      "Epoch: 1984/2000... Training loss: 0.2423\n",
      "Epoch: 1984/2000... Training loss: 0.3495\n",
      "Epoch: 1984/2000... Training loss: 0.2929\n",
      "Epoch: 1984/2000... Training loss: 0.5535\n",
      "Epoch: 1984/2000... Training loss: 0.4299\n",
      "Epoch: 1984/2000... Training loss: 0.4008\n",
      "Epoch: 1984/2000... Training loss: 0.3660\n",
      "Epoch: 1984/2000... Training loss: 0.3322\n",
      "Epoch: 1984/2000... Training loss: 0.4356\n",
      "Epoch: 1984/2000... Training loss: 0.3446\n",
      "Epoch: 1984/2000... Training loss: 0.5250\n",
      "Epoch: 1984/2000... Training loss: 0.2799\n",
      "Epoch: 1984/2000... Training loss: 0.3117\n",
      "Epoch: 1984/2000... Training loss: 0.3481\n",
      "Epoch: 1984/2000... Training loss: 0.3137\n",
      "Epoch: 1984/2000... Training loss: 0.6220\n",
      "Epoch: 1984/2000... Training loss: 0.4291\n",
      "Epoch: 1984/2000... Training loss: 0.5266\n",
      "Epoch: 1984/2000... Training loss: 0.3221\n",
      "Epoch: 1984/2000... Training loss: 0.5603\n",
      "Epoch: 1984/2000... Training loss: 0.4713\n",
      "Epoch: 1984/2000... Training loss: 0.6656\n",
      "Epoch: 1984/2000... Training loss: 0.5001\n",
      "Epoch: 1984/2000... Training loss: 0.3775\n",
      "Epoch: 1984/2000... Training loss: 0.3310\n",
      "Epoch: 1984/2000... Training loss: 0.3086\n",
      "Epoch: 1984/2000... Training loss: 0.3573\n",
      "Epoch: 1984/2000... Training loss: 0.4034\n",
      "Epoch: 1985/2000... Training loss: 0.2665\n",
      "Epoch: 1985/2000... Training loss: 0.3032\n",
      "Epoch: 1985/2000... Training loss: 0.4624\n",
      "Epoch: 1985/2000... Training loss: 0.3294\n",
      "Epoch: 1985/2000... Training loss: 0.3306\n",
      "Epoch: 1985/2000... Training loss: 0.2793\n",
      "Epoch: 1985/2000... Training loss: 0.5889\n",
      "Epoch: 1985/2000... Training loss: 0.3504\n",
      "Epoch: 1985/2000... Training loss: 0.2404\n",
      "Epoch: 1985/2000... Training loss: 0.3492\n",
      "Epoch: 1985/2000... Training loss: 0.3639\n",
      "Epoch: 1985/2000... Training loss: 0.5513\n",
      "Epoch: 1985/2000... Training loss: 0.4274\n",
      "Epoch: 1985/2000... Training loss: 0.3931\n",
      "Epoch: 1985/2000... Training loss: 0.5397\n",
      "Epoch: 1985/2000... Training loss: 0.3937\n",
      "Epoch: 1985/2000... Training loss: 0.4253\n",
      "Epoch: 1985/2000... Training loss: 0.3814\n",
      "Epoch: 1985/2000... Training loss: 0.3373\n",
      "Epoch: 1985/2000... Training loss: 0.4551\n",
      "Epoch: 1985/2000... Training loss: 0.4859\n",
      "Epoch: 1985/2000... Training loss: 0.3376\n",
      "Epoch: 1985/2000... Training loss: 0.3979\n",
      "Epoch: 1985/2000... Training loss: 0.4311\n",
      "Epoch: 1985/2000... Training loss: 0.4262\n",
      "Epoch: 1985/2000... Training loss: 0.2680\n",
      "Epoch: 1985/2000... Training loss: 0.3662\n",
      "Epoch: 1985/2000... Training loss: 0.5183\n",
      "Epoch: 1985/2000... Training loss: 0.3529\n",
      "Epoch: 1985/2000... Training loss: 0.4128\n",
      "Epoch: 1985/2000... Training loss: 0.4029\n",
      "Epoch: 1986/2000... Training loss: 0.4017\n",
      "Epoch: 1986/2000... Training loss: 0.2764\n",
      "Epoch: 1986/2000... Training loss: 0.4971\n",
      "Epoch: 1986/2000... Training loss: 0.3740\n",
      "Epoch: 1986/2000... Training loss: 0.3870\n",
      "Epoch: 1986/2000... Training loss: 0.5202\n",
      "Epoch: 1986/2000... Training loss: 0.3799\n",
      "Epoch: 1986/2000... Training loss: 0.3633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1986/2000... Training loss: 0.3599\n",
      "Epoch: 1986/2000... Training loss: 0.2932\n",
      "Epoch: 1986/2000... Training loss: 0.3627\n",
      "Epoch: 1986/2000... Training loss: 0.4642\n",
      "Epoch: 1986/2000... Training loss: 0.3378\n",
      "Epoch: 1986/2000... Training loss: 0.3800\n",
      "Epoch: 1986/2000... Training loss: 0.3104\n",
      "Epoch: 1986/2000... Training loss: 0.2802\n",
      "Epoch: 1986/2000... Training loss: 0.4288\n",
      "Epoch: 1986/2000... Training loss: 0.5661\n",
      "Epoch: 1986/2000... Training loss: 0.3161\n",
      "Epoch: 1986/2000... Training loss: 0.1908\n",
      "Epoch: 1986/2000... Training loss: 0.3786\n",
      "Epoch: 1986/2000... Training loss: 0.3208\n",
      "Epoch: 1986/2000... Training loss: 0.3098\n",
      "Epoch: 1986/2000... Training loss: 0.3885\n",
      "Epoch: 1986/2000... Training loss: 0.5970\n",
      "Epoch: 1986/2000... Training loss: 0.3260\n",
      "Epoch: 1986/2000... Training loss: 0.4788\n",
      "Epoch: 1986/2000... Training loss: 0.3786\n",
      "Epoch: 1986/2000... Training loss: 0.5419\n",
      "Epoch: 1986/2000... Training loss: 0.3762\n",
      "Epoch: 1986/2000... Training loss: 0.2660\n",
      "Epoch: 1987/2000... Training loss: 0.4612\n",
      "Epoch: 1987/2000... Training loss: 0.3566\n",
      "Epoch: 1987/2000... Training loss: 0.5193\n",
      "Epoch: 1987/2000... Training loss: 0.4112\n",
      "Epoch: 1987/2000... Training loss: 0.4085\n",
      "Epoch: 1987/2000... Training loss: 0.3541\n",
      "Epoch: 1987/2000... Training loss: 0.2412\n",
      "Epoch: 1987/2000... Training loss: 0.2964\n",
      "Epoch: 1987/2000... Training loss: 0.2979\n",
      "Epoch: 1987/2000... Training loss: 0.3593\n",
      "Epoch: 1987/2000... Training loss: 0.4072\n",
      "Epoch: 1987/2000... Training loss: 0.3585\n",
      "Epoch: 1987/2000... Training loss: 0.5897\n",
      "Epoch: 1987/2000... Training loss: 0.6325\n",
      "Epoch: 1987/2000... Training loss: 0.4538\n",
      "Epoch: 1987/2000... Training loss: 0.2750\n",
      "Epoch: 1987/2000... Training loss: 0.2458\n",
      "Epoch: 1987/2000... Training loss: 0.2314\n",
      "Epoch: 1987/2000... Training loss: 0.3736\n",
      "Epoch: 1987/2000... Training loss: 0.1968\n",
      "Epoch: 1987/2000... Training loss: 0.4376\n",
      "Epoch: 1987/2000... Training loss: 0.5257\n",
      "Epoch: 1987/2000... Training loss: 0.4024\n",
      "Epoch: 1987/2000... Training loss: 0.2613\n",
      "Epoch: 1987/2000... Training loss: 0.3593\n",
      "Epoch: 1987/2000... Training loss: 0.2969\n",
      "Epoch: 1987/2000... Training loss: 0.2996\n",
      "Epoch: 1987/2000... Training loss: 0.6007\n",
      "Epoch: 1987/2000... Training loss: 0.4460\n",
      "Epoch: 1987/2000... Training loss: 0.3904\n",
      "Epoch: 1987/2000... Training loss: 0.3744\n",
      "Epoch: 1988/2000... Training loss: 0.3290\n",
      "Epoch: 1988/2000... Training loss: 0.2862\n",
      "Epoch: 1988/2000... Training loss: 0.4083\n",
      "Epoch: 1988/2000... Training loss: 0.3676\n",
      "Epoch: 1988/2000... Training loss: 0.3311\n",
      "Epoch: 1988/2000... Training loss: 0.4315\n",
      "Epoch: 1988/2000... Training loss: 0.3902\n",
      "Epoch: 1988/2000... Training loss: 0.3454\n",
      "Epoch: 1988/2000... Training loss: 0.4778\n",
      "Epoch: 1988/2000... Training loss: 0.3998\n",
      "Epoch: 1988/2000... Training loss: 0.4158\n",
      "Epoch: 1988/2000... Training loss: 0.3461\n",
      "Epoch: 1988/2000... Training loss: 0.2638\n",
      "Epoch: 1988/2000... Training loss: 0.2489\n",
      "Epoch: 1988/2000... Training loss: 0.3253\n",
      "Epoch: 1988/2000... Training loss: 0.3115\n",
      "Epoch: 1988/2000... Training loss: 0.4051\n",
      "Epoch: 1988/2000... Training loss: 0.4226\n",
      "Epoch: 1988/2000... Training loss: 0.3385\n",
      "Epoch: 1988/2000... Training loss: 0.2507\n",
      "Epoch: 1988/2000... Training loss: 0.3671\n",
      "Epoch: 1988/2000... Training loss: 0.4891\n",
      "Epoch: 1988/2000... Training loss: 0.4244\n",
      "Epoch: 1988/2000... Training loss: 0.3788\n",
      "Epoch: 1988/2000... Training loss: 0.3833\n",
      "Epoch: 1988/2000... Training loss: 0.3898\n",
      "Epoch: 1988/2000... Training loss: 0.4044\n",
      "Epoch: 1988/2000... Training loss: 0.3258\n",
      "Epoch: 1988/2000... Training loss: 0.4327\n",
      "Epoch: 1988/2000... Training loss: 0.4334\n",
      "Epoch: 1988/2000... Training loss: 0.4279\n",
      "Epoch: 1989/2000... Training loss: 0.4113\n",
      "Epoch: 1989/2000... Training loss: 0.3672\n",
      "Epoch: 1989/2000... Training loss: 0.3946\n",
      "Epoch: 1989/2000... Training loss: 0.3058\n",
      "Epoch: 1989/2000... Training loss: 0.4316\n",
      "Epoch: 1989/2000... Training loss: 0.4964\n",
      "Epoch: 1989/2000... Training loss: 0.5496\n",
      "Epoch: 1989/2000... Training loss: 0.4410\n",
      "Epoch: 1989/2000... Training loss: 0.2205\n",
      "Epoch: 1989/2000... Training loss: 0.2665\n",
      "Epoch: 1989/2000... Training loss: 0.2853\n",
      "Epoch: 1989/2000... Training loss: 0.2879\n",
      "Epoch: 1989/2000... Training loss: 0.4518\n",
      "Epoch: 1989/2000... Training loss: 0.4200\n",
      "Epoch: 1989/2000... Training loss: 0.3661\n",
      "Epoch: 1989/2000... Training loss: 0.3421\n",
      "Epoch: 1989/2000... Training loss: 0.2813\n",
      "Epoch: 1989/2000... Training loss: 0.5055\n",
      "Epoch: 1989/2000... Training loss: 0.4381\n",
      "Epoch: 1989/2000... Training loss: 0.4001\n",
      "Epoch: 1989/2000... Training loss: 0.2785\n",
      "Epoch: 1989/2000... Training loss: 0.3450\n",
      "Epoch: 1989/2000... Training loss: 0.3794\n",
      "Epoch: 1989/2000... Training loss: 0.4221\n",
      "Epoch: 1989/2000... Training loss: 0.3931\n",
      "Epoch: 1989/2000... Training loss: 0.3620\n",
      "Epoch: 1989/2000... Training loss: 0.3045\n",
      "Epoch: 1989/2000... Training loss: 0.2949\n",
      "Epoch: 1989/2000... Training loss: 0.3853\n",
      "Epoch: 1989/2000... Training loss: 0.3668\n",
      "Epoch: 1989/2000... Training loss: 0.4981\n",
      "Epoch: 1990/2000... Training loss: 0.2801\n",
      "Epoch: 1990/2000... Training loss: 0.3522\n",
      "Epoch: 1990/2000... Training loss: 0.5308\n",
      "Epoch: 1990/2000... Training loss: 0.2658\n",
      "Epoch: 1990/2000... Training loss: 0.3688\n",
      "Epoch: 1990/2000... Training loss: 0.3335\n",
      "Epoch: 1990/2000... Training loss: 0.4627\n",
      "Epoch: 1990/2000... Training loss: 0.3564\n",
      "Epoch: 1990/2000... Training loss: 0.3600\n",
      "Epoch: 1990/2000... Training loss: 0.2711\n",
      "Epoch: 1990/2000... Training loss: 0.5081\n",
      "Epoch: 1990/2000... Training loss: 0.2926\n",
      "Epoch: 1990/2000... Training loss: 0.4455\n",
      "Epoch: 1990/2000... Training loss: 0.4458\n",
      "Epoch: 1990/2000... Training loss: 0.5334\n",
      "Epoch: 1990/2000... Training loss: 0.4229\n",
      "Epoch: 1990/2000... Training loss: 0.3775\n",
      "Epoch: 1990/2000... Training loss: 0.3367\n",
      "Epoch: 1990/2000... Training loss: 0.3159\n",
      "Epoch: 1990/2000... Training loss: 0.2947\n",
      "Epoch: 1990/2000... Training loss: 0.3294\n",
      "Epoch: 1990/2000... Training loss: 0.2854\n",
      "Epoch: 1990/2000... Training loss: 0.2581\n",
      "Epoch: 1990/2000... Training loss: 0.4214\n",
      "Epoch: 1990/2000... Training loss: 0.2473\n",
      "Epoch: 1990/2000... Training loss: 0.3606\n",
      "Epoch: 1990/2000... Training loss: 0.4838\n",
      "Epoch: 1990/2000... Training loss: 0.3771\n",
      "Epoch: 1990/2000... Training loss: 0.4008\n",
      "Epoch: 1990/2000... Training loss: 0.2643\n",
      "Epoch: 1990/2000... Training loss: 0.1814\n",
      "Epoch: 1991/2000... Training loss: 0.2887\n",
      "Epoch: 1991/2000... Training loss: 0.2642\n",
      "Epoch: 1991/2000... Training loss: 0.4848\n",
      "Epoch: 1991/2000... Training loss: 0.3597\n",
      "Epoch: 1991/2000... Training loss: 0.2648\n",
      "Epoch: 1991/2000... Training loss: 0.2912\n",
      "Epoch: 1991/2000... Training loss: 0.2502\n",
      "Epoch: 1991/2000... Training loss: 0.4423\n",
      "Epoch: 1991/2000... Training loss: 0.2412\n",
      "Epoch: 1991/2000... Training loss: 0.2460\n",
      "Epoch: 1991/2000... Training loss: 0.3312\n",
      "Epoch: 1991/2000... Training loss: 0.4980\n",
      "Epoch: 1991/2000... Training loss: 0.3138\n",
      "Epoch: 1991/2000... Training loss: 0.3213\n",
      "Epoch: 1991/2000... Training loss: 0.2524\n",
      "Epoch: 1991/2000... Training loss: 0.4441\n",
      "Epoch: 1991/2000... Training loss: 0.3728\n",
      "Epoch: 1991/2000... Training loss: 0.3536\n",
      "Epoch: 1991/2000... Training loss: 0.3455\n",
      "Epoch: 1991/2000... Training loss: 0.3359\n",
      "Epoch: 1991/2000... Training loss: 0.2787\n",
      "Epoch: 1991/2000... Training loss: 0.5252\n",
      "Epoch: 1991/2000... Training loss: 0.3422\n",
      "Epoch: 1991/2000... Training loss: 0.2749\n",
      "Epoch: 1991/2000... Training loss: 0.5476\n",
      "Epoch: 1991/2000... Training loss: 0.2932\n",
      "Epoch: 1991/2000... Training loss: 0.3473\n",
      "Epoch: 1991/2000... Training loss: 0.3902\n",
      "Epoch: 1991/2000... Training loss: 0.3373\n",
      "Epoch: 1991/2000... Training loss: 0.2730\n",
      "Epoch: 1991/2000... Training loss: 0.2612\n",
      "Epoch: 1992/2000... Training loss: 0.5251\n",
      "Epoch: 1992/2000... Training loss: 0.4240\n",
      "Epoch: 1992/2000... Training loss: 0.3317\n",
      "Epoch: 1992/2000... Training loss: 0.3491\n",
      "Epoch: 1992/2000... Training loss: 0.4765\n",
      "Epoch: 1992/2000... Training loss: 0.3795\n",
      "Epoch: 1992/2000... Training loss: 0.4122\n",
      "Epoch: 1992/2000... Training loss: 0.3499\n",
      "Epoch: 1992/2000... Training loss: 0.2538\n",
      "Epoch: 1992/2000... Training loss: 0.4807\n",
      "Epoch: 1992/2000... Training loss: 0.5394\n",
      "Epoch: 1992/2000... Training loss: 0.3455\n",
      "Epoch: 1992/2000... Training loss: 0.3879\n",
      "Epoch: 1992/2000... Training loss: 0.2619\n",
      "Epoch: 1992/2000... Training loss: 0.2897\n",
      "Epoch: 1992/2000... Training loss: 0.4417\n",
      "Epoch: 1992/2000... Training loss: 0.4231\n",
      "Epoch: 1992/2000... Training loss: 0.4139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1992/2000... Training loss: 0.3459\n",
      "Epoch: 1992/2000... Training loss: 0.3202\n",
      "Epoch: 1992/2000... Training loss: 0.3431\n",
      "Epoch: 1992/2000... Training loss: 0.2807\n",
      "Epoch: 1992/2000... Training loss: 0.2528\n",
      "Epoch: 1992/2000... Training loss: 0.3776\n",
      "Epoch: 1992/2000... Training loss: 0.4161\n",
      "Epoch: 1992/2000... Training loss: 0.3798\n",
      "Epoch: 1992/2000... Training loss: 0.3654\n",
      "Epoch: 1992/2000... Training loss: 0.2563\n",
      "Epoch: 1992/2000... Training loss: 0.4219\n",
      "Epoch: 1992/2000... Training loss: 0.4583\n",
      "Epoch: 1992/2000... Training loss: 0.3306\n",
      "Epoch: 1993/2000... Training loss: 0.2002\n",
      "Epoch: 1993/2000... Training loss: 0.2764\n",
      "Epoch: 1993/2000... Training loss: 0.3296\n",
      "Epoch: 1993/2000... Training loss: 0.3601\n",
      "Epoch: 1993/2000... Training loss: 0.3972\n",
      "Epoch: 1993/2000... Training loss: 0.3904\n",
      "Epoch: 1993/2000... Training loss: 0.5179\n",
      "Epoch: 1993/2000... Training loss: 0.3138\n",
      "Epoch: 1993/2000... Training loss: 0.3304\n",
      "Epoch: 1993/2000... Training loss: 0.5233\n",
      "Epoch: 1993/2000... Training loss: 0.4649\n",
      "Epoch: 1993/2000... Training loss: 0.4180\n",
      "Epoch: 1993/2000... Training loss: 0.2528\n",
      "Epoch: 1993/2000... Training loss: 0.2983\n",
      "Epoch: 1993/2000... Training loss: 0.3831\n",
      "Epoch: 1993/2000... Training loss: 0.4193\n",
      "Epoch: 1993/2000... Training loss: 0.2967\n",
      "Epoch: 1993/2000... Training loss: 0.3174\n",
      "Epoch: 1993/2000... Training loss: 0.3728\n",
      "Epoch: 1993/2000... Training loss: 0.3500\n",
      "Epoch: 1993/2000... Training loss: 0.3089\n",
      "Epoch: 1993/2000... Training loss: 0.4332\n",
      "Epoch: 1993/2000... Training loss: 0.4084\n",
      "Epoch: 1993/2000... Training loss: 0.3060\n",
      "Epoch: 1993/2000... Training loss: 0.4971\n",
      "Epoch: 1993/2000... Training loss: 0.3752\n",
      "Epoch: 1993/2000... Training loss: 0.3502\n",
      "Epoch: 1993/2000... Training loss: 0.4321\n",
      "Epoch: 1993/2000... Training loss: 0.2828\n",
      "Epoch: 1993/2000... Training loss: 0.3849\n",
      "Epoch: 1993/2000... Training loss: 0.3677\n",
      "Epoch: 1994/2000... Training loss: 0.4433\n",
      "Epoch: 1994/2000... Training loss: 0.3405\n",
      "Epoch: 1994/2000... Training loss: 0.1934\n",
      "Epoch: 1994/2000... Training loss: 0.2484\n",
      "Epoch: 1994/2000... Training loss: 0.2348\n",
      "Epoch: 1994/2000... Training loss: 0.4843\n",
      "Epoch: 1994/2000... Training loss: 0.4152\n",
      "Epoch: 1994/2000... Training loss: 0.3187\n",
      "Epoch: 1994/2000... Training loss: 0.2234\n",
      "Epoch: 1994/2000... Training loss: 0.3923\n",
      "Epoch: 1994/2000... Training loss: 0.5212\n",
      "Epoch: 1994/2000... Training loss: 0.4134\n",
      "Epoch: 1994/2000... Training loss: 0.2121\n",
      "Epoch: 1994/2000... Training loss: 0.4457\n",
      "Epoch: 1994/2000... Training loss: 0.3297\n",
      "Epoch: 1994/2000... Training loss: 0.3549\n",
      "Epoch: 1994/2000... Training loss: 0.4731\n",
      "Epoch: 1994/2000... Training loss: 0.2653\n",
      "Epoch: 1994/2000... Training loss: 0.3446\n",
      "Epoch: 1994/2000... Training loss: 0.4106\n",
      "Epoch: 1994/2000... Training loss: 0.2572\n",
      "Epoch: 1994/2000... Training loss: 0.4283\n",
      "Epoch: 1994/2000... Training loss: 0.5385\n",
      "Epoch: 1994/2000... Training loss: 0.4086\n",
      "Epoch: 1994/2000... Training loss: 0.3581\n",
      "Epoch: 1994/2000... Training loss: 0.3457\n",
      "Epoch: 1994/2000... Training loss: 0.5541\n",
      "Epoch: 1994/2000... Training loss: 0.4016\n",
      "Epoch: 1994/2000... Training loss: 0.3420\n",
      "Epoch: 1994/2000... Training loss: 0.2824\n",
      "Epoch: 1994/2000... Training loss: 0.2771\n",
      "Epoch: 1995/2000... Training loss: 0.2094\n",
      "Epoch: 1995/2000... Training loss: 0.3398\n",
      "Epoch: 1995/2000... Training loss: 0.5754\n",
      "Epoch: 1995/2000... Training loss: 0.4213\n",
      "Epoch: 1995/2000... Training loss: 0.2738\n",
      "Epoch: 1995/2000... Training loss: 0.2907\n",
      "Epoch: 1995/2000... Training loss: 0.3969\n",
      "Epoch: 1995/2000... Training loss: 0.4937\n",
      "Epoch: 1995/2000... Training loss: 0.2311\n",
      "Epoch: 1995/2000... Training loss: 0.2677\n",
      "Epoch: 1995/2000... Training loss: 0.6054\n",
      "Epoch: 1995/2000... Training loss: 0.2901\n",
      "Epoch: 1995/2000... Training loss: 0.4098\n",
      "Epoch: 1995/2000... Training loss: 0.4918\n",
      "Epoch: 1995/2000... Training loss: 0.4644\n",
      "Epoch: 1995/2000... Training loss: 0.5469\n",
      "Epoch: 1995/2000... Training loss: 0.3915\n",
      "Epoch: 1995/2000... Training loss: 0.3015\n",
      "Epoch: 1995/2000... Training loss: 0.4081\n",
      "Epoch: 1995/2000... Training loss: 0.2776\n",
      "Epoch: 1995/2000... Training loss: 0.3403\n",
      "Epoch: 1995/2000... Training loss: 0.3579\n",
      "Epoch: 1995/2000... Training loss: 0.2778\n",
      "Epoch: 1995/2000... Training loss: 0.3319\n",
      "Epoch: 1995/2000... Training loss: 0.3296\n",
      "Epoch: 1995/2000... Training loss: 0.3684\n",
      "Epoch: 1995/2000... Training loss: 0.3269\n",
      "Epoch: 1995/2000... Training loss: 0.5679\n",
      "Epoch: 1995/2000... Training loss: 0.4053\n",
      "Epoch: 1995/2000... Training loss: 0.2666\n",
      "Epoch: 1995/2000... Training loss: 0.2086\n",
      "Epoch: 1996/2000... Training loss: 0.2961\n",
      "Epoch: 1996/2000... Training loss: 0.4856\n",
      "Epoch: 1996/2000... Training loss: 0.4624\n",
      "Epoch: 1996/2000... Training loss: 0.2508\n",
      "Epoch: 1996/2000... Training loss: 0.3012\n",
      "Epoch: 1996/2000... Training loss: 0.4156\n",
      "Epoch: 1996/2000... Training loss: 0.2806\n",
      "Epoch: 1996/2000... Training loss: 0.3727\n",
      "Epoch: 1996/2000... Training loss: 0.4738\n",
      "Epoch: 1996/2000... Training loss: 0.2459\n",
      "Epoch: 1996/2000... Training loss: 0.3420\n",
      "Epoch: 1996/2000... Training loss: 0.4797\n",
      "Epoch: 1996/2000... Training loss: 0.4763\n",
      "Epoch: 1996/2000... Training loss: 0.2727\n",
      "Epoch: 1996/2000... Training loss: 0.1806\n",
      "Epoch: 1996/2000... Training loss: 0.3088\n",
      "Epoch: 1996/2000... Training loss: 0.5208\n",
      "Epoch: 1996/2000... Training loss: 0.4267\n",
      "Epoch: 1996/2000... Training loss: 0.5534\n",
      "Epoch: 1996/2000... Training loss: 0.4439\n",
      "Epoch: 1996/2000... Training loss: 0.2777\n",
      "Epoch: 1996/2000... Training loss: 0.2474\n",
      "Epoch: 1996/2000... Training loss: 0.3616\n",
      "Epoch: 1996/2000... Training loss: 0.2779\n",
      "Epoch: 1996/2000... Training loss: 0.3854\n",
      "Epoch: 1996/2000... Training loss: 0.3806\n",
      "Epoch: 1996/2000... Training loss: 0.4079\n",
      "Epoch: 1996/2000... Training loss: 0.2682\n",
      "Epoch: 1996/2000... Training loss: 0.3686\n",
      "Epoch: 1996/2000... Training loss: 0.2831\n",
      "Epoch: 1996/2000... Training loss: 0.5050\n",
      "Epoch: 1997/2000... Training loss: 0.6134\n",
      "Epoch: 1997/2000... Training loss: 0.4048\n",
      "Epoch: 1997/2000... Training loss: 0.5617\n",
      "Epoch: 1997/2000... Training loss: 0.3695\n",
      "Epoch: 1997/2000... Training loss: 0.2603\n",
      "Epoch: 1997/2000... Training loss: 0.3015\n",
      "Epoch: 1997/2000... Training loss: 0.4518\n",
      "Epoch: 1997/2000... Training loss: 0.3667\n",
      "Epoch: 1997/2000... Training loss: 0.3700\n",
      "Epoch: 1997/2000... Training loss: 0.4369\n",
      "Epoch: 1997/2000... Training loss: 0.4129\n",
      "Epoch: 1997/2000... Training loss: 0.5087\n",
      "Epoch: 1997/2000... Training loss: 0.4936\n",
      "Epoch: 1997/2000... Training loss: 0.3336\n",
      "Epoch: 1997/2000... Training loss: 0.3719\n",
      "Epoch: 1997/2000... Training loss: 0.4754\n",
      "Epoch: 1997/2000... Training loss: 0.1814\n",
      "Epoch: 1997/2000... Training loss: 0.4741\n",
      "Epoch: 1997/2000... Training loss: 0.2581\n",
      "Epoch: 1997/2000... Training loss: 0.2791\n",
      "Epoch: 1997/2000... Training loss: 0.3259\n",
      "Epoch: 1997/2000... Training loss: 0.4795\n",
      "Epoch: 1997/2000... Training loss: 0.3421\n",
      "Epoch: 1997/2000... Training loss: 0.3564\n",
      "Epoch: 1997/2000... Training loss: 0.2751\n",
      "Epoch: 1997/2000... Training loss: 0.2116\n",
      "Epoch: 1997/2000... Training loss: 0.5036\n",
      "Epoch: 1997/2000... Training loss: 0.2816\n",
      "Epoch: 1997/2000... Training loss: 0.2934\n",
      "Epoch: 1997/2000... Training loss: 0.2466\n",
      "Epoch: 1997/2000... Training loss: 0.3974\n",
      "Epoch: 1998/2000... Training loss: 0.2029\n",
      "Epoch: 1998/2000... Training loss: 0.3177\n",
      "Epoch: 1998/2000... Training loss: 0.3191\n",
      "Epoch: 1998/2000... Training loss: 0.3890\n",
      "Epoch: 1998/2000... Training loss: 0.3491\n",
      "Epoch: 1998/2000... Training loss: 0.4161\n",
      "Epoch: 1998/2000... Training loss: 0.3885\n",
      "Epoch: 1998/2000... Training loss: 0.2566\n",
      "Epoch: 1998/2000... Training loss: 0.2414\n",
      "Epoch: 1998/2000... Training loss: 0.4349\n",
      "Epoch: 1998/2000... Training loss: 0.2862\n",
      "Epoch: 1998/2000... Training loss: 0.2098\n",
      "Epoch: 1998/2000... Training loss: 0.2349\n",
      "Epoch: 1998/2000... Training loss: 0.2227\n",
      "Epoch: 1998/2000... Training loss: 0.4074\n",
      "Epoch: 1998/2000... Training loss: 0.3975\n",
      "Epoch: 1998/2000... Training loss: 0.3469\n",
      "Epoch: 1998/2000... Training loss: 0.2312\n",
      "Epoch: 1998/2000... Training loss: 0.4233\n",
      "Epoch: 1998/2000... Training loss: 0.4016\n",
      "Epoch: 1998/2000... Training loss: 0.3701\n",
      "Epoch: 1998/2000... Training loss: 0.3248\n",
      "Epoch: 1998/2000... Training loss: 0.6474\n",
      "Epoch: 1998/2000... Training loss: 0.2899\n",
      "Epoch: 1998/2000... Training loss: 0.3373\n",
      "Epoch: 1998/2000... Training loss: 0.5985\n",
      "Epoch: 1998/2000... Training loss: 0.3648\n",
      "Epoch: 1998/2000... Training loss: 0.4154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1998/2000... Training loss: 0.3138\n",
      "Epoch: 1998/2000... Training loss: 0.2799\n",
      "Epoch: 1998/2000... Training loss: 0.2966\n",
      "Epoch: 1999/2000... Training loss: 0.3532\n",
      "Epoch: 1999/2000... Training loss: 0.4194\n",
      "Epoch: 1999/2000... Training loss: 0.2748\n",
      "Epoch: 1999/2000... Training loss: 0.4170\n",
      "Epoch: 1999/2000... Training loss: 0.3003\n",
      "Epoch: 1999/2000... Training loss: 0.4280\n",
      "Epoch: 1999/2000... Training loss: 0.4209\n",
      "Epoch: 1999/2000... Training loss: 0.3241\n",
      "Epoch: 1999/2000... Training loss: 0.2076\n",
      "Epoch: 1999/2000... Training loss: 0.4180\n",
      "Epoch: 1999/2000... Training loss: 0.2817\n",
      "Epoch: 1999/2000... Training loss: 0.2936\n",
      "Epoch: 1999/2000... Training loss: 0.2890\n",
      "Epoch: 1999/2000... Training loss: 0.3955\n",
      "Epoch: 1999/2000... Training loss: 0.3456\n",
      "Epoch: 1999/2000... Training loss: 0.6288\n",
      "Epoch: 1999/2000... Training loss: 0.2777\n",
      "Epoch: 1999/2000... Training loss: 0.3146\n",
      "Epoch: 1999/2000... Training loss: 0.3833\n",
      "Epoch: 1999/2000... Training loss: 0.3945\n",
      "Epoch: 1999/2000... Training loss: 0.3334\n",
      "Epoch: 1999/2000... Training loss: 0.4153\n",
      "Epoch: 1999/2000... Training loss: 0.2046\n",
      "Epoch: 1999/2000... Training loss: 0.3852\n",
      "Epoch: 1999/2000... Training loss: 0.3135\n",
      "Epoch: 1999/2000... Training loss: 0.3258\n",
      "Epoch: 1999/2000... Training loss: 0.3795\n",
      "Epoch: 1999/2000... Training loss: 0.3385\n",
      "Epoch: 1999/2000... Training loss: 0.3528\n",
      "Epoch: 1999/2000... Training loss: 0.5235\n",
      "Epoch: 1999/2000... Training loss: 0.3775\n",
      "Epoch: 2000/2000... Training loss: 0.3107\n",
      "Epoch: 2000/2000... Training loss: 0.2836\n",
      "Epoch: 2000/2000... Training loss: 0.4007\n",
      "Epoch: 2000/2000... Training loss: 0.3818\n",
      "Epoch: 2000/2000... Training loss: 0.2625\n",
      "Epoch: 2000/2000... Training loss: 0.4108\n",
      "Epoch: 2000/2000... Training loss: 0.3808\n",
      "Epoch: 2000/2000... Training loss: 0.3884\n",
      "Epoch: 2000/2000... Training loss: 0.2743\n",
      "Epoch: 2000/2000... Training loss: 0.3179\n",
      "Epoch: 2000/2000... Training loss: 0.3699\n",
      "Epoch: 2000/2000... Training loss: 0.4331\n",
      "Epoch: 2000/2000... Training loss: 0.4538\n",
      "Epoch: 2000/2000... Training loss: 0.3150\n",
      "Epoch: 2000/2000... Training loss: 0.4644\n",
      "Epoch: 2000/2000... Training loss: 0.4050\n",
      "Epoch: 2000/2000... Training loss: 0.4803\n",
      "Epoch: 2000/2000... Training loss: 0.3451\n",
      "Epoch: 2000/2000... Training loss: 0.2961\n",
      "Epoch: 2000/2000... Training loss: 0.2646\n",
      "Epoch: 2000/2000... Training loss: 0.4089\n",
      "Epoch: 2000/2000... Training loss: 0.2643\n",
      "Epoch: 2000/2000... Training loss: 0.3454\n",
      "Epoch: 2000/2000... Training loss: 0.2226\n",
      "Epoch: 2000/2000... Training loss: 0.4020\n",
      "Epoch: 2000/2000... Training loss: 0.1802\n",
      "Epoch: 2000/2000... Training loss: 0.4567\n",
      "Epoch: 2000/2000... Training loss: 0.3457\n",
      "Epoch: 2000/2000... Training loss: 0.3443\n",
      "Epoch: 2000/2000... Training loss: 0.3397\n",
      "Epoch: 2000/2000... Training loss: 0.4922\n"
     ]
    }
   ],
   "source": [
    "# Adam optimizer\n",
    "opt = tf.train.AdamOptimizer(0.0005).minimize(cost)\n",
    "\n",
    "# Create the session\n",
    "sess = tf.Session()\n",
    "\n",
    "epochs = 2000\n",
    "batch_size = 128\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for e in range(epochs):\n",
    "    for ii in range((len(features)//batch_size) - 1):\n",
    "        batch = features[ii*batch_size:(ii + 1)*batch_size]\n",
    "        \n",
    "        # add random noise\n",
    "        for jj in batch:\n",
    "            jj[np.random.randint(vocab_size)] += 0.05\n",
    "        \n",
    "        targ = np.array(labels[ii*batch_size:(ii + 1)*batch_size])\n",
    "\n",
    "        feed = {inputs_: batch, targets_: targ}\n",
    "        batch_cost, _ = sess.run([cost, opt], feed_dict=feed)\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lawrence', 'aulola'), ('walker valley', 'wale'), ('valencia boro', 'orchard park'), ('rivervale', 'holland'), ('toboyne', 'boston'), ('enfield', 'colden'), ('laporte boro', 'concord'), ('huntingdon valley', 'collins'), ('new berlin', 'north collins'), ('upper tulpehocken', 'sardinia'), ('bayonne', 'brant'), ('great barrington', 'evans'), ('alden', 'eden'), ('upperstrasburg', 'hamburg'), ('coaldale boro', 'lackawanna'), ('beaver falls', 'west seneca'), ('sunapee', 'elma'), ('hamlin', 'marilla'), ('beaver falls', 'cheektowaga'), ('glen gardner', 'lancapter'), ('friendship', 'aldest'), ('osceola', 'grant islands'), ('rosendale', 'amherst'), ('hoosick', 'clarence'), ('beaver falls', 'newstead'), ('andes', 'tonawanda'), ('woodcock twp', 'bufalo'), ('landenburg', 'goshen ny'), ('london', 'monroe ny')]\n"
     ]
    }
   ],
   "source": [
    "test_words = ['aulola', 'wale', 'orchard park', 'holland', 'boston', 'colden', \n",
    "              'concord', 'collins', 'north collins', 'sardinia', 'brant', 'evans', 'eden',\n",
    "              'hamburg', 'lackawanna', 'west seneca', 'elma', 'marilla', 'cheektowaga', \n",
    "              'lancapter', 'aldest', 'grant islands', 'amherst', 'clarence', 'newstead', \n",
    "              'tonawanda', 'bufalo', 'goshen ny', 'monroe ny']\n",
    "\n",
    "in_words = []\n",
    "for word in test_words:\n",
    "    X = np.zeros(vocab_size)\n",
    "    for j in range(len(word)):\n",
    "        if word[j] in vocab_to_id:\n",
    "            X[vocab_to_id[word[j]]] += 1\n",
    "            \n",
    "    # normalize input sizes \n",
    "    X = ((X - X.min()) / (X.max() - X.min())) + 0.001\n",
    "    in_words.append(X)\n",
    "    \n",
    "in_words = np.array(in_words)\n",
    "\n",
    "reconstructed, compressed = sess.run([logits, encoded], feed_dict={inputs_: in_words})\n",
    "\n",
    "out_words = []\n",
    "for out, inp in zip(reconstructed, test_words):\n",
    "    best_match = np.argmax(out)\n",
    "    \n",
    "    out_words.append((id_to_address[best_match], inp))\n",
    "    \n",
    "        \n",
    "print(out_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
